<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Today in Privacy &amp; AI Tooling - introducing a nifty new tool to examine where data goes in open-source apps on  ü§ó</title><link>https://huggingface.co/posts/yjernite/333157611385452</link><description>Today in Privacy &amp; AI Tooling - introducing a nifty new tool to examine where data goes in open-source apps on ü§ó HF Spaces have tons (100Ks!) of cool demos leveraging or examining AI systems - and because most of them are OSS we can see exactly how they handle user data üìöüîç That requires actually reading the code though, which isn't always easy or quick! Good news: code LMs have gotten pretty good at automatic review, so we can offload some of the work - here I'm using Qwen/Qwen2.5-Coder-32B-Instruct to generate reports and it works pretty OK üôå The app works in three stages: 1. Download all code files 2. Use the Code LM to generate a detailed report pointing to code where data is transferred/(AI-)processed (screen 1) 3. Summarize the app's main functionality and data journeys (screen 2) 4. Build a Privacy TLDR with those inputs It comes with a bunch of pre-reviewed apps/Spaces, great to see how many process data locally or through (private) HF endpoints ü§ó Note that this is a POC,...</description><pubDate>Fri, 18 Apr 2025 17:22:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yjernite/333157611385452</guid></item><item><title>FramePack Full Tutorial: 1-Click to Install on Windows - Up to 120 Second Image-to-Videos with 6GB &gt;</title><link>https://huggingface.co/posts/MonsterMMORPG/401294184812659</link><description>FramePack Full Tutorial: 1-Click to Install on Windows - Up to 120 Second Image-to-Videos with 6GB &gt; https://youtu.be/HwMngohRmHg Tutorial video : https://youtu.be/HwMngohRmHg FramePack from legendary lllyasviel full Windows local tutorial with a very advanced Gradio app to generate consistent videos from images with as long as 120 seconds and as low as 6 GB GPUs. This tutorial will show you step by step how to install and use FramePack locall with a very advanced Graido app. Moreover, I have published installers for cloud services such as RunPod and Massed Compute for those GPU poor and who wants to scale. üîó Full Instructions, Installers and Links Shared Post (the one used in the tutorial) ‚§µÔ∏è ‚ñ∂Ô∏è https://www.patreon.com/posts/click-to-open-post-used-in-tutorial-126855226 üîó SECourses Official Discord 10500+ Members ‚§µÔ∏è ‚ñ∂Ô∏è https://discord.com/servers/software-engineering-courses-secourses-772774097734074388 üîó Stable Diffusion, FLUX, Generative AI Tutorials and Resources GitHub ‚§µÔ∏è ‚ñ∂Ô∏è...</description><pubDate>Fri, 18 Apr 2025 17:22:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/401294184812659</guid></item><item><title>We are excited to announce the release of our paper, "Cobra: Efficient Line Art COlorization with BRoAder References," along with the official code! Cobra is a novel efficient long-context fine-grained ID preservation framework for line art colorization, achieving high precision, efficiency, and flexible usability for comic colorization. By effectively integrating extensive contextual references, it transforms black-and-white line art into vibrant illustrations.</title><link>https://huggingface.co/posts/JunhaoZhuang/767916607687281</link><description>We are excited to announce the release of our paper, "Cobra: Efficient Line Art COlorization with BRoAder References," along with the official code! Cobra is a novel efficient long-context fine-grained ID preservation framework for line art colorization, achieving high precision, efficiency, and flexible usability for comic colorization. By effectively integrating extensive contextual references, it transforms black-and-white line art into vibrant illustrations. We invite you to explore Cobra and share your feedback! You can access the paper and code via the following links: [PDF]( https://arxiv.org/abs/2504.12240 ) and [Project page]( https://zhuang2002.github.io/Cobra/ ). We eagerly anticipate your engagement and support! Thank you for your interest! See translation</description><pubDate>Fri, 18 Apr 2025 17:22:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/JunhaoZhuang/767916607687281</guid></item><item><title>Want to see machine learning algorithms training?</title><link>https://huggingface.co/posts/gavinkhung/300399121852584</link><description>Want to see machine learning algorithms training? I made a website: https://gavinkhung.github.io/machine-learning-visualized/ The website implements, visualizes, and mathematically derives machine learning algorithms from first-principles. Feel free to contribute to this open-source resource: https://github.com/gavinkhung/machine-learning-visualized See translation</description><pubDate>Fri, 18 Apr 2025 17:22:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/gavinkhung/300399121852584</guid></item><item><title>Try out the demo for Multimodal OCR featuring the implementation of models including</title><link>https://huggingface.co/posts/prithivMLmods/125773384088431</link><description>Try out the demo for Multimodal OCR featuring the implementation of models including RolmOCR and Qwen2VL OCR . The use case showcases image-text-to-text conversion and video understanding support for the RolmOCR model ! üöÄ ü§óMultimodal OCR Space : prithivMLmods/Multimodal-OCR üì¶The models implemented in this Space are: + Qwen2VL OCR : prithivMLmods/Qwen2-VL-OCR-2B-Instruct [ or ] + Qwen2VL OCR2 : prithivMLmods/Qwen2-VL-OCR2-2B-Instruct + RolmOCR : reducto/RolmOCR Qwen2VL OCR supports only image-text-to-text in the space. See translation</description><pubDate>Fri, 18 Apr 2025 17:22:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/125773384088431</guid></item><item><title>OpenAI just released a 34-page practical guide to building agents,</title><link>https://huggingface.co/posts/hesamation/750913380201236</link><description>OpenAI just released a 34-page practical guide to building agents, Here's 10 things it teaches us: 1‚ûú agents are different from workflows: they are complete autonomous systems that perform tasks on your behalf. many applications use LLMs for workflows, but this is not an agent. 2‚ûú use them for tricky stuff: complex decision making, dynamic rules, unstructured data 3‚ûú core recipe: each agent has three main components: Model (the brain), Tools, Instructions on how to behave 4‚ûú choose the right brain: set up evals to get a baseline performance, use a smart model to see what's possible, gradually downgrade the model for cost and speed 5‚ûú tools are key: choose well-defined and tested tools. an agent needs tools to retrieve data and context, and take actions. 6‚ûú instruction matters A LOT: be super clear telling the agent its goals, steps, and rules. Vague instructions = unpredictable agent. Be explicit. 7‚ûú start simple, then scale: often a single agent with several tools is ok. don't jump...</description><pubDate>Fri, 18 Apr 2025 17:22:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/750913380201236</guid></item><item><title>Introducing BioClinicalBERT-Triage: A Medical Triage Classification Model</title><link>https://huggingface.co/posts/VolodymyrPugachov/428721082134198</link><description>Introducing BioClinicalBERT-Triage: A Medical Triage Classification Model I'm excited to share my latest project: a fine-tuned model for medical triage classification! What is BioClinicalBERT-Triage? BioClinicalBERT-Triage is a specialized model that classifies patient-reported symptoms into appropriate triage categories. Built on the foundation of emilyalsentzer/Bio_ClinicalBERT, this model helps healthcare providers prioritize patient care by analyzing symptom descriptions and medical history. Why I Built This As healthcare systems face increasing demands, efficient triage becomes crucial. This model aims to support healthcare professionals in quickly assessing the urgency of medical situations, particularly in telehealth and high-volume settings. Model Performance The model was trained on 42,513 medical symptom descriptions, using an 80:20 train/test split. After 3 epochs of training, the model achieved: Final training loss: 0.3246 Processing speed: 13.99 samples/second The loss...</description><pubDate>Fri, 18 Apr 2025 17:22:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/VolodymyrPugachov/428721082134198</guid></item><item><title>Reasoning models like o3 and o4-mini are advancing faster than ever, but imagine what will be possible when they can run locally in your browser! ü§Ø</title><link>https://huggingface.co/posts/Xenova/811708183292240</link><description>Reasoning models like o3 and o4-mini are advancing faster than ever, but imagine what will be possible when they can run locally in your browser! ü§Ø Well, with ü§ó Transformers.js, you can do just that! Here's Zyphra's new ZR1 model running at over 100 tokens/second on WebGPU! ‚ö°Ô∏è Giving models access to browser APIs (like File System, Screen Capture, and more) could unlock an entirely new class of web experiences that are personalized, interactive, and run locally in a secure, sandboxed environment. For now, try out the demo! üëá webml-community/Zyphra-ZR1-WebGPU See translation</description><pubDate>Fri, 18 Apr 2025 17:22:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Xenova/811708183292240</guid></item><item><title>Just tested something this morning that feels kind of game-changing for how we publish, discover, and consume news with AI: connecting Claude directly to the New York Times through MCP.</title><link>https://huggingface.co/posts/fdaudens/110266162418000</link><description>Just tested something this morning that feels kind of game-changing for how we publish, discover, and consume news with AI: connecting Claude directly to the New York Times through MCP. Picture this: You ask Claude about a topic, and it instantly pulls verified and trusted NYT content ‚Äî no more guessing if the info is accurate. The cool part? Publishers stay in control of what they share via API, and users get fast, reliable access through the AI tools they already use. Instead of scraping random stuff off the web, we get a future where publishers actively shape how their journalism shows up in AI. It‚Äôs still a bit technical to set up right now, but this could get super simple soon ‚Äî like installing apps on your phone, but for your chatbot. And you keep the brand connection, too. Not saying it solves everything, but it‚Äôs definitely a new way to distribute content ‚Äî and maybe even find some fresh value in the middle of this whole news + AI shakeup. Early movers will have a head...</description><pubDate>Fri, 18 Apr 2025 17:22:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/110266162418000</guid></item><item><title>Access requests enabled for latest GLM models</title><link>https://huggingface.co/posts/bartowski/160920719239523</link><description>Access requests enabled for latest GLM models While a fix is being implemented ( https://github.com/ggml-org/llama.cpp/pull/12957 ) I want to leave the models up for visibility and continued discussion, but want to prevent accidental downloads of known broken models (even though there are settings that could fix it at runtime for now) With this goal, I've enabled access requests. I don't really want your data, so I'm sorry that I don't think there's a way around that? But that's what I'm gonna do for now, and I'll remove the gate when a fix is up and verified and I have a chance to re-convert and quantize! Hope you don't mind in the mean time :D See translation</description><pubDate>Fri, 18 Apr 2025 17:22:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/bartowski/160920719239523</guid></item></channel></rss>