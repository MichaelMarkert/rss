<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>You can now run Kimi K2.5 locally! ğŸ”¥</title><link>https://huggingface.co/posts/danielhanchen/602916263790271</link><description>You can now run Kimi K2.5 locally! ğŸ”¥ We shrank the 1T model to 240GB (-60%) via Dynamic 1-bit. Get &gt;40 tok/s on 242GB or 622GB VRAM/RAM for near full precision. GGUF: unsloth/Kimi-K2.5-GGUF Guide: https://unsloth.ai/docs/models/kimi-k2.5 See translation</description><pubDate>Fri, 30 Jan 2026 13:57:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/602916263790271</guid></item><item><title>Just built my entire AI Engineer portfolio by pasting 2 links (GitHub and LinkedIn)  into</title><link>https://huggingface.co/posts/RakshitAralimatti/297622038982343</link><description>Just built my entire AI Engineer portfolio by pasting 2 links (GitHub and LinkedIn) into moonshotai Kimi 2.5. That's it. That's the workflow. Zero coding. Zero iteration. Zero "make the button bigger." See for yourself: https://rakshit2020.github.io/rakshitaralimatti.github.io/ The model: âœ… Scraped my GitHub repos automatically âœ… Pulled my experience from LinkedIn âœ… Designed an Aurora Glass theme âœ… Mapped every skill to projects âœ… Added animations I'd never code myself See translation</description><pubDate>Fri, 30 Jan 2026 13:57:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/297622038982343</guid></item><item><title>KittenTTS Nano â€” Tiny, Expressive, Practical</title><link>https://huggingface.co/posts/Javedalam/795050655622219</link><description>KittenTTS Nano â€” Tiny, Expressive, Practical KittenTTS Nano is a lightweight, CPU-only text-to-speech model designed to prove that natural, expressive voices donâ€™t require massive cloud stacks or GPUs. At roughly ~15M parameters, it runs fast on modest hardware, supports multiple expressive voices, and exposes simple controls for pacing and tone. This makes it ideal for edge devices, demos, and anyone who wants full control over TTS without latency, lock-in, or infrastructure overhead. Try it here Javedalam/KittenTTS The model page KittenML/kitten-tts-nano-0.2 See translation</description><pubDate>Fri, 30 Jan 2026 13:57:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Javedalam/795050655622219</guid></item><item><title>Daggr UI version of the Qwen3-TTS demo.ğŸ”¥</title><link>https://huggingface.co/posts/prithivMLmods/617850685706562</link><description>Daggr UI version of the Qwen3-TTS demo.ğŸ”¥ (custom voice, voice design, qwen3-asr and voice cloning) nodes. No remote spaces used for API inference; all functions run in-app fn . Powered by t4-m and built with daggr@0.5.2 and gradio@6. ğŸ‘‰Demo: prithivMLmods/Qwen3-TTS-Daggr-UI â­Github: https://github.com/PRITHIVSAKTHIUR/Qwen3-TTS-Daggr-UI See translation</description><pubDate>Fri, 30 Jan 2026 13:57:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/617850685706562</guid></item><item><title>ğŸ’¥</title><link>https://huggingface.co/posts/alvarobartt/259854920222577</link><description>ğŸ’¥ hf-mem v0.4.1 now also estimates KV cache memory requirements for any context length and batch size with the --experimental flag! uvx hf-mem --model-id ... --experimental will automatically pull the required information from the Hugging Face Hub to include the KV cache estimation, when applicable. ğŸ’¡ Alternatively, you can also set the --max-model-len , --batch-size and --kv-cache-dtype arguments (Ã  la vLLM) manually if preferred. See translation</description><pubDate>Fri, 30 Jan 2026 13:57:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/alvarobartt/259854920222577</guid></item><item><title>Big day in open source AI!!</title><link>https://huggingface.co/posts/AdinaY/365964215058126</link><description>Big day in open source AI!! âœ¨ DeepSeek released OCR2 ğŸ’¥ deepseek-ai/DeepSeek-OCR-2 âœ¨ Kimi K2.5 just landed ğŸ”¥ moonshotai/Kimi-K2.5 With the Chinese Spring Festival 3 weeks away, whatâ€™s coming next?ğŸ‘€ See translation</description><pubDate>Fri, 30 Jan 2026 13:57:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/365964215058126</guid></item><item><title>Transformers v5 just landed! ğŸš€</title><link>https://huggingface.co/posts/IlyasMoutawwakil/848772925772411</link><description>Transformers v5 just landed! ğŸš€ It significantly unifies and reduces modeling code across architectures, while opening the door to a whole new class of performance optimizations. My favorite new feature? ğŸ¤” The new dynamic weight loader + converter. Hereâ€™s why ğŸ‘‡ Over the last few months, the core Transformers maintainers built an incredibly fast weight loader, capable of converting tensors on the fly while loading them in parallel threads. This means weâ€™re no longer constrained by how parameters are laid out inside the safetensors weight files. In practice, this unlocks two big things: - Much more modular modeling code. You can now clearly see how architectures build on top of each other (DeepSeek v2 â†’ v3, Qwen v2 â†’ v3 â†’ MoE, etc.). This makes shared bottlenecks obvious and lets us optimize the right building blocks once, for all model families. - Performance optimizations beyond what torch.compile can do alone. torch.compile operates on the computation graph, but it canâ€™t change...</description><pubDate>Fri, 30 Jan 2026 13:57:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/IlyasMoutawwakil/848772925772411</guid></item><item><title>I created list of models based on permissive license (apache2, mit, openrail) and raw fp16 weights.</title><link>https://huggingface.co/posts/kostakoff/684419257851296</link><description>I created list of models based on permissive license (apache2, mit, openrail) and raw fp16 weights. LLM: - Mistral 7b v1 - Falcon 7b - GLM4 9b - Olmo3 7b - Yi 9b - Qwen3 8b - Internlm3 8B - PHI4 Multimodal LLM: - Pixtral 12b - Qwen3-VL-8B-Instruct Picture generation: - Stable Diffusion 1.5 - Stable Diffusion 2.0 - Stable Diffusion XL Video generation: - WAN 2.1 VACE Diffusers TTS: - SUNO Bark This can be very useful for those who are just starting their AI LLM journey in PyTorch, like me. Suggestions in the comments are welcome. See translation</description><pubDate>Fri, 30 Jan 2026 13:57:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kostakoff/684419257851296</guid></item><item><title>New TRL + OpenEnv example! ğŸ’¥</title><link>https://huggingface.co/posts/sergiopaniego/799346316619936</link><description>New TRL + OpenEnv example! ğŸ’¥ Fine tune an LLM for playing Sudoku using an RL env via OpenEnv Includes a script that runs on 1 or multiple GPUs with vLLM, plus a Colab-ready notebook. Enjoy! Notebook: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/openenv_sudoku_grpo.ipynb Script: https://github.com/huggingface/trl/blob/main/examples/scripts/openenv/sudoku.py See translation</description><pubDate>Fri, 30 Jan 2026 13:57:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/799346316619936</guid></item><item><title>ğŸš€ Geilim-1B-Instruct â€” Implicit Deep Reasoning, Zero Verbosity</title><link>https://huggingface.co/posts/OzTianlu/906408758517388</link><description>ğŸš€ Geilim-1B-Instruct â€” Implicit Deep Reasoning, Zero Verbosity NoesisLab/Geilim-1B-Instruct https://huggingface.co/collections/NoesisLab/geilim-large-language-models No &lt;think&gt; tags. No long CoT. Reasoning happens inside the hidden states, not in the output. Whatâ€™s different ğŸ§  Implicit reasoning: deep causal reasoning without exposing chains ğŸ•¸ï¸ ASPP (Adjacency-Structured Parallel Propagation): parent-only causal graph, O(n) message passing ğŸŒŠ Ï€-flow: internal probability-space refinement instead of token-level deliberation âš–ï¸ Hybrid gating: learns when to use structure vs attention Why it matters Lower latency &amp; token cost Cleaner, production-ready outputs CoT-level reasoning depth without verbosity tax Built on Llama-3.2-1B-Instruct, trained for math, logic, and commonsense. Designed for small-model reasoning at the edge. #ImplicitReasoning #SmallLLM #EfficientAI #ReasoningModels #ASPP #PiFlow See translation</description><pubDate>Fri, 30 Jan 2026 13:57:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/OzTianlu/906408758517388</guid></item></channel></rss>