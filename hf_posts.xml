<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>MiniCPM-V 4.5 ğŸš€ New MLLM for image, multi-image &amp; video understanding, running even on your phone, released by OpenBMB</title><link>https://huggingface.co/posts/AdinaY/473496396970919</link><description>MiniCPM-V 4.5 ğŸš€ New MLLM for image, multi-image &amp; video understanding, running even on your phone, released by OpenBMB openbmb/MiniCPM-V-4_5 âœ¨ SOTA vision language capability âœ¨ 96Ã— video token compression &gt; high-FPS &amp; long video reasoning âœ¨ Switchable fast vs deep thinking modes âœ¨ Strong OCR, document parsing, supports 30+ languages See translation</description><pubDate>Thu, 28 Aug 2025 17:19:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/473496396970919</guid></item><item><title>first vision language model built off</title><link>https://huggingface.co/posts/merve/292180054306518</link><description>first vision language model built off openai/gpt-oss-20b just dropped! ğŸ”¥ InternVL3.5 comes with 32 models ğŸ¤¯ pre-trained, fine-tuned, aligned in various sizes OpenGVLab/internvl35-68ac87bd52ebe953485927fb comes with gpt-oss or Qwen3 for LLM part â¤µï¸ See translation</description><pubDate>Thu, 28 Aug 2025 17:19:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/292180054306518</guid></item><item><title>I recently added a recipe in ellora to improve reasoning capabilities to Gemma-3-1B using self-supervised learning. Model now shows step-by-step thinking in &lt;think&gt; tags before answering.</title><link>https://huggingface.co/posts/codelion/968650774475150</link><description>I recently added a recipe in ellora to improve reasoning capabilities to Gemma-3-1B using self-supervised learning. Model now shows step-by-step thinking in &lt;think&gt; tags before answering. Logic puzzle accuracy: 61% â†’ 84%. 3 hours training on single GPU. ğŸ§  Used GRPO where model generates multiple responses and learns to prefer better reasoning. Works surprisingly well for making smaller models more transparent. ğŸ”— Colab: https://colab.research.google.com/github/codelion/ellora/blob/main/Ellora_Recipe_2_Reasoning_LoRA_with_Self-Rewarding_GRPO.ipynb ğŸ¤— Model: codelion/gemma-3-1b-it-reasoning-grpo-lora ğŸ’» Code: https://github.com/codelion/ellora See translation</description><pubDate>Thu, 28 Aug 2025 17:19:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/968650774475150</guid></item><item><title>I wanted to share a technique that's been working really well for recovering performance after INT4 quantization.</title><link>https://huggingface.co/posts/codelion/741062673202173</link><description>I wanted to share a technique that's been working really well for recovering performance after INT4 quantization. Typically, quantizing the LLM to INT4 (unlike say INT8) for inference can incur some accuracy loss. Instead of accepting the quality loss, we used the FP16 model as a teacher to train a tiny LoRA adapter (rank=16) for the quantized model. The cool part: the model generates its own training data using the Magpie technique so no external datasets needed. This is critical because we want to remain as much as possible in the distribution of the model's natural responses. Last year Apple's foundational models paper ( https://arxiv.org/pdf/2407.21075 ) had proposed a similar technique and found "By using accuracy-recovery LoRA adapters with only rank 16, Alpaca win rate can be improved by 7-18%, GMS8K accuracy is boosted by 5-10%." (page 47). We saw similar results on Qwen3-0.6B: Perplexity: 2.40 â†’ 2.09 (only 5.7% degradation from FP16 baseline) Memory: Only 0.28GB vs 1.0GB...</description><pubDate>Thu, 28 Aug 2025 17:19:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/741062673202173</guid></item><item><title>ğŸ‰ Fashion Fit 360: The New Standard in AI Virtual Try-On!</title><link>https://huggingface.co/posts/ginipick/809439997973106</link><description>ğŸ‰ Fashion Fit 360: The New Standard in AI Virtual Try-On! ğŸš€ Now Live and Free to Use!Say goodbye to online shopping uncertainty - "Will this look good on me?" - with our revolutionary solution!Fashion Fit 360 is a cutting-edge AI-powered virtual fitting service that transforms your fashion shopping experience. LINK: ginigen/Fashion-Fit360 âœ¨ Core Features ğŸ”„ 360-Degree Multi-Pose Generation Transform a single front-facing photo into 6 different viewing angles! Front, side, and back views for complete visualization Experience a real fitting room mirror effect Check fit and style from every perspective ğŸ‘— 15 Fashion Item Categories Apparel: Tops, bottoms, dresses Jewelry: Necklaces, earrings, rings, bracelets Accessories: Sunglasses, eyewear, hats, ties, bow ties, belts Essentials: Bags, shoes ğŸ¯ Perfect For: ğŸ›ï¸ Online Shopping Enthusiasts: Preview before purchase - zero return hassles! ğŸ’ Jewelry Lovers: Virtually try expensive pieces before investing ğŸ Thoughtful Gift-Givers: Test items...</description><pubDate>Thu, 28 Aug 2025 17:19:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/809439997973106</guid></item><item><title>ğŸŒ Nano Banana + Video: AI Image Style Transfer &amp; Video Generation Tool</title><link>https://huggingface.co/posts/ginipick/948503650396274</link><description>ğŸŒ Nano Banana + Video: AI Image Style Transfer &amp; Video Generation Tool ğŸ¨ Key Features 1ï¸âƒ£ Image Style Transfer ginigen/Nano-Banana-Video ğŸ“¸ Upload up to 2 images for style fusion âœ¨ High-quality image generation with Google Nano Banana model ğŸ­ Apply desired styles with text prompts 2ï¸âƒ£ Video Generation ğŸ¬ Convert generated images to videos ğŸ“ Maintain original aspect ratio option â±ï¸ Adjustable duration (1-4 seconds) ğŸš€ How to Use Step-by-Step Guide Step 1: Image Generation ğŸ–¼ï¸ Enter style description Upload 1-2 images (optional) Click "Generate Magic âœ¨" Step 2: Video Creation ğŸ“¹ Send generated image to video tab Set animation style Generate video! ğŸ’¡ Use Cases ğŸï¸ Transform landscape photos into artistic masterpieces ğŸ¤– Bring static images to life ğŸ¨ Mix styles from two different images ğŸ“± Create short videos for social media âš¡ Tech Stack Google Nano Banana Stable Video Diffusion Gradio Replicate API #AIVideoGenerator #ImageToVideoConverter #StyleTransferAI #GoogleNanoBanana...</description><pubDate>Thu, 28 Aug 2025 17:19:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/948503650396274</guid></item><item><title>ğŸŒ Nano Banana: Google AI Completely Free!</title><link>https://huggingface.co/posts/openfree/636576339128278</link><description>ğŸŒ Nano Banana: Google AI Completely Free! ğŸ‰ Finally, Google's Nano Banana AI is available for everyone - absolutely FREE! ğŸ¯ Choose Your Perfect Version! ğŸŒŸ Free Nano Banana - For Everyone Transform images with AI - It's that simple! ğŸš€ Start in 3 Seconds 1ï¸âƒ£ Click Here 2ï¸âƒ£ Upload Image 3ï¸âƒ£ Enter Style â†’ Done! âœ¨ No Sign-up âŒ | No Payment âŒ | No Ads âŒ | Just Free â­• ğŸ“¸ Simple drag &amp; drop upload âœï¸ Describe styles in any language âš¡ Results in under 30 seconds ğŸ¨ Perfect for SNS, blogs, presentations ğŸ‘‰ Start Now: openfree/Free-Nano-Banana ğŸ” Nano Banana Upscale - For Designers Professional high-resolution output when you need it! ğŸ–¼ï¸ 4x resolution upscaling (Real-ESRGAN) ğŸ¯ Optimized for print &amp; large displays ğŸ’ Premium quality with preserved details ğŸ“ Professional quality without Photoshop ğŸ‘‰ Create in HD: openfree/Nano-Banana-Upscale ğŸ’» Nano Banana API - For Developers Power your app with AI! ğŸ”§ Instant RESTful API integration ğŸ“¦ Python, JS, Java code examples included âš™ï¸ Batch processing &amp;...</description><pubDate>Thu, 28 Aug 2025 17:19:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/636576339128278</guid></item><item><title>OpenGVLab's InternVL3_5-2B-MPO [Mixed Preference Optimization (MPO)] is a compact vision-language model in the InternVL3.5 series. You can now experience it in the Tiny VLMs Lab, an app featuring 15+ multimodal VLMs ranging from 250M to 4B parameters. These models support tasks such as OCR, reasoning, single-shot answering with small models, and captioning (including ablated variants), across a broad range of visual categories. They are also capable of handling images with complex, sensitive, or nuanced content, while adapting to varying aspect ratios and resolutions.</title><link>https://huggingface.co/posts/prithivMLmods/835910169748717</link><description>OpenGVLab's InternVL3_5-2B-MPO [Mixed Preference Optimization (MPO)] is a compact vision-language model in the InternVL3.5 series. You can now experience it in the Tiny VLMs Lab, an app featuring 15+ multimodal VLMs ranging from 250M to 4B parameters. These models support tasks such as OCR, reasoning, single-shot answering with small models, and captioning (including ablated variants), across a broad range of visual categories. They are also capable of handling images with complex, sensitive, or nuanced content, while adapting to varying aspect ratios and resolutions. âœ¨ Space/App : prithivMLmods/Tiny-VLMs-Lab ğŸ«™ Model : OpenGVLab/InternVL3_5-2B-MPO â†—ï¸ Collection: OpenGVLab/internvl35-68ac87bd52ebe953485927fb ğŸ—ï¸ Paper : https://arxiv.org/pdf/2508.18265 â†—ï¸ Multimodal Space Collection : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 To learn more, visit the relevant spaces, collections, and model cards. See translation</description><pubDate>Thu, 28 Aug 2025 17:19:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/835910169748717</guid></item><item><title>ğŸ”’ Ansim Blur: Privacy-First Face Blurring for the AI Era</title><link>https://huggingface.co/posts/openfree/464899901167090</link><description>ğŸ”’ Ansim Blur: Privacy-First Face Blurring for the AI Era ğŸš¨ The Privacy Crisis is Now Smart CCTVs ğŸ“¹, delivery robots ğŸ¤–, and autonomous vehicles ğŸš— are everywhere. Your face is being captured, transmitted, and stored without your knowledge or consent. openfree/Face-blurring The privacy threat is real: 24/7 surveillance cameras recording your every move Companies harvesting facial biometric data at scale Your face becoming a commodity without your permission ğŸ’¡ The Solution: Ansim Blur Real-time face anonymization powered by YOLOv8 ğŸ¯ âœ… Process images, videos, and live streams âœ… Automatic GPU/CPU detection for universal deployment âœ… Choose between Gaussian blur or mosaic pixelation âœ… Fine-tune detection sensitivity for your needs âœ… Preserve audio tracks in video processing ğŸ›¡ï¸ Real-World Applications Enterprise Use Cases Privacy compliance for robotics and drone footage CCTV feed anonymization for regulatory requirements Customer data protection in retail analytics Personal Protection...</description><pubDate>Thu, 28 Aug 2025 17:19:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/464899901167090</guid></item><item><title>Are you coming to SF this Fall?</title><link>https://huggingface.co/posts/salma-remyx/113451614574436</link><description>Are you coming to SF this Fall? Next week, we'll be at the AI Agent Builders Summit. And in late October, GitHub Universe, ODSC West, and Experiment 2025. We're sharing what we've learned while building agents to help you test new research ideas out of the arXiv into PRs for your repo. This Summer, we've analyzed thousands of papers, ranking each for relevance to our work before building hundreds of Docker images and opening hundreds of PRs for our repos. Read more about PapersWithPRs: https://www.reddit.com/r/LocalLLaMA/comments/1mq7715/paperswithprs_dont_just_read_the_paper_replicate/ ğ—”ğ—œ ğ—”ğ—´ğ—²ğ—»ğ˜ ğ—•ğ˜‚ğ—¶ğ—¹ğ—±ğ—²ğ—¿ğ˜€ ğ—¦ğ˜‚ğ—ºğ—ºğ—¶ğ˜: https://luma.com/agents-world-tour-sf ğ—šğ—¶ğ˜ğ—›ğ˜‚ğ—¯ ğ—¨ğ—»ğ—¶ğ˜ƒğ—²ğ—¿ğ˜€ğ—²: https://githubuniverse.com/ DISCOUNT CODE: TAKEMETOUNIVERSE ğ—˜ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—ºğ—²ğ—»ğ˜ ğŸ®ğŸ¬ğŸ®ğŸ±: https://luma.com/145xyuyw See translation</description><pubDate>Thu, 28 Aug 2025 17:19:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/113451614574436</guid></item></channel></rss>