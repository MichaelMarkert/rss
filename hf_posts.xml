<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Say hello to my little friends! I just unboxed this trio of HP Z2 G1a!</title><link>https://huggingface.co/posts/mitkox/390180686171042</link><description>Say hello to my little friends! I just unboxed this trio of HP Z2 G1a! Three is always better than one! 3x AMD Ryzen AI Max+ Pro 395 384GB RAM 24TB of RAID storage Ubuntu 24.04 ROCm 7.0.2 llama cpp, vLLM and Aibrix Small, cheap GPUs are about to become the Raspberry Pi of edge AI inference. Sprinkle some kubectl fairy dust on top, and suddenly it's a high-availability, self-healing, cloud-native, enterprise-grade AI cluster camping in a closet. Make sure you own your AI. AI in the cloud is not aligned with you; itâ€™s aligned with the company that owns it. See translation</description><pubDate>Sun, 26 Oct 2025 13:26:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/390180686171042</guid></item><item><title>Is it hot in here, or is it just me?</title><link>https://huggingface.co/posts/nroggendorff/194600363127405</link><description>Is it hot in here, or is it just me? See translation</description><pubDate>Sun, 26 Oct 2025 13:26:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/194600363127405</guid></item><item><title>Iâ€™m just reading that Ryzen AI 395 has to be 30% slower than DGX Spark in LLM inferencingâ€¦ and only 96GB GPU RAMâ€¦ good I havenâ€™t RTFM upfront, so I made the AMD faster with 128GB unified RAM ğŸ«¡</title><link>https://huggingface.co/posts/mitkox/949505421454376</link><description>Iâ€™m just reading that Ryzen AI 395 has to be 30% slower than DGX Spark in LLM inferencingâ€¦ and only 96GB GPU RAMâ€¦ good I havenâ€™t RTFM upfront, so I made the AMD faster with 128GB unified RAM ğŸ«¡ Z2 mini G1a can run Qwen3 Coder 30B BF16 at 26.8 tok/sec in ~60GB GPU RAM See translation</description><pubDate>Sun, 26 Oct 2025 13:26:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/949505421454376</guid></item><item><title>ğŸš€ğŸ‘ï¸ğŸŒŸ New Research Alert - ICCV 2025! ğŸŒŸğŸ‘ï¸ğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/275169930033420</link><description>ğŸš€ğŸ‘ï¸ğŸŒŸ New Research Alert - ICCV 2025! ğŸŒŸğŸ‘ï¸ğŸš€ ğŸ“„ Title: Token Activation Map to Visually Explain Multimodal LLMs ğŸ” ğŸ“ Description: The Token Activation Map (TAM) is an advanced explainability method for multimodal LLMs. Using causal inference and a Rank Gaussian Filter, TAM reveals token-level interactions and eliminates redundant activations. The result is clearer, high-quality visualizations that enhance understanding of object localization, reasoning and multimodal alignment across models. ğŸ‘¥ Authors: Yi Li, Hualiang Wang, Xinpeng Ding, Haonan Wang, and Xiaomeng Li ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: Token Activation Map to Visually Explain Multimodal LLMs (2506.23270) ğŸ“ Repository: https://github.com/xmed-lab/TAM ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the Multi-Modal Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/multi-modal-learning.md ğŸ“š More...</description><pubDate>Sun, 26 Oct 2025 13:26:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/275169930033420</guid></item><item><title>FlashPack: Lightning-Fast Model Loading for PyTorch</title><link>https://huggingface.co/posts/gokaygokay/758462412009896</link><description>FlashPack: Lightning-Fast Model Loading for PyTorch https://github.com/fal-ai/flashpack FlashPack â€” a new, high-throughput file format and loading mechanism for PyTorch that makes model checkpoint I/O blazingly fast, even on systems without access to GPU Direct Storage (GDS). With FlashPack, loading any model can be 3â€“6Ã— faster than with the current state-of-the-art methods like accelerate or the standard load_state_dict() and to() flow â€” all wrapped in a lightweight, pure-Python package that works anywhere. See translation</description><pubDate>Sun, 26 Oct 2025 13:26:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/gokaygokay/758462412009896</guid></item><item><title>How Financial News Can Be Used to Train Good Financial Models ğŸ“°</title><link>https://huggingface.co/posts/SelmaNajih001/973945902491280</link><description>How Financial News Can Be Used to Train Good Financial Models ğŸ“° Numbers tell you what happened, but news tells you why. Iâ€™ve written an article explaining how news can be used to train AI models for sentiment analysis and better forecasting. Hope you find it interesting! Read it here: https://huggingface.co/blog/SelmaNajih001/llms-applied-to-finance I would love to read your opinions! Iâ€™m open to suggestions on how to improve the methodology and the training See translation</description><pubDate>Sun, 26 Oct 2025 13:26:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/SelmaNajih001/973945902491280</guid></item><item><title>Meet OpenEnv ğŸ‘‹, an open ecosystem of environments for intelligent agents. Build, share, and test agents safely and consistently.</title><link>https://huggingface.co/posts/sergiopaniego/527159223589050</link><description>Meet OpenEnv ğŸ‘‹, an open ecosystem of environments for intelligent agents. Build, share, and test agents safely and consistently. Ideal for training with TRL (we include examplesğŸ¤“), deployment, and community collaboration via the HF Hub Blog: https://huggingface.co/blog/openenv Hub for Environments: openenv OpenEnv repo: https://github.com/meta-pytorch/OpenEnv Try it out using TRL: https://huggingface.co/docs/trl/main/en/openenv See translation</description><pubDate>Sun, 26 Oct 2025 13:26:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/527159223589050</guid></item><item><title>Which is the best model to use as a signal for investment?</title><link>https://huggingface.co/posts/SelmaNajih001/829546518885653</link><description>Which is the best model to use as a signal for investment? Here who is gaining the most: SelmaNajih001/InvestmentStrategyBasedOnSentiment The Space uses titles from this dataset: ğŸ“Š SelmaNajih001/Cnbc_MultiCompany Given a news title, it calculates a sentiment score : if the score crosses a certain threshold, the strategy decides to buy or sell. Each trade lasts one day, and the strategy then computes the daily return. For Tesla the best model seems to be the regression ğŸ‘€ Just a quick note: the model uses the closing price as the buy price, meaning it already reflects the impact of the news. See translation</description><pubDate>Sun, 26 Oct 2025 13:26:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/SelmaNajih001/829546518885653</guid></item><item><title>Weâ€™re preparing an in-depth review of the Feetech STS-3250 servo motor â€” exploring its performance, features. One of the top serial bus servo for hobby robotics compatible with SO-ARM 100 manipulator.</title><link>https://huggingface.co/posts/branikita/215658672762252</link><description>Weâ€™re preparing an in-depth review of the Feetech STS-3250 servo motor â€” exploring its performance, features. One of the top serial bus servo for hobby robotics compatible with SO-ARM 100 manipulator. See translation</description><pubDate>Sun, 26 Oct 2025 13:26:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/215658672762252</guid></item><item><title>I am on the model layer and focus on atomic tasks, so I don't get involved in product discussions. But this provocative article provoked the community quite a bit. The case in point is Claude Code, which happens to be my biggest productivity revolution since ChatGPT.</title><link>https://huggingface.co/posts/onekq/981925262392459</link><description>I am on the model layer and focus on atomic tasks, so I don't get involved in product discussions. But this provocative article provoked the community quite a bit. The case in point is Claude Code, which happens to be my biggest productivity revolution since ChatGPT. RAG predated TUI and agents. So to be fair it's quite an achievement to survive the AI evolution. But I feel it is overshadowed by context engineering in the agent era. How does everyone feel about this? https://www.nicolasbustamante.com/p/the-rag-obituary-killed-by-agents See translation</description><pubDate>Sun, 26 Oct 2025 13:26:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/981925262392459</guid></item></channel></rss>