<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Finally finished my extensive **Qwen 3 evaluations** across a range of formats and quantisations, focusing on **MMLU-Pro** (Computer Science).</title><link>https://huggingface.co/posts/wolfram/819510719695955</link><description>Finally finished my extensive **Qwen 3 evaluations** across a range of formats and quantisations, focusing on **MMLU-Pro** (Computer Science). A few take-aways stood out - especially for those interested in local deployment and performance trade-offs: 1ï¸âƒ£ **Qwen3-235B-A22B** (via Fireworks API) tops the table at **83.66%** with ~55 tok/s. 2ï¸âƒ£ But the **30B-A3B Unsloth** quant delivered **82.20%** while running locally at ~45 tok/s and with zero API spend. 3ï¸âƒ£ The same Unsloth build is ~5x faster than Qwen's **Qwen3-32B**, which scores **82.20%** as well yet crawls at &lt;10 tok/s. 4ï¸âƒ£ On Apple silicon, the **30B MLX** port hits **79.51%** while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups. 5ï¸âƒ£ The **0.6B** micro-model races above 180 tok/s but tops out at **37.56%** - that's why it's not even on the graph (50 % performance cut-off). All local runs were done with LM Studio on an M4 MacBook Pro, using Qwen's official recommended settings....</description><pubDate>Sat, 10 May 2025 13:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wolfram/819510719695955</guid></item><item><title>VisionScout â€” Now with Video Analysis! ğŸš€</title><link>https://huggingface.co/posts/DawnC/287598166225995</link><description>VisionScout â€” Now with Video Analysis! ğŸš€ Iâ€™m excited to announce a major update to VisionScout, my interactive vision tool that now supports VIDEO PROCESSING, in addition to powerful object detection and scene understanding! â­ï¸ NEW: Video Analysis Is Here! ğŸ¬ Upload any video file to detect and track objects using YOLOv8. â±ï¸ Customize processing intervals to balance speed and thoroughness. ğŸ“Š Get comprehensive statistics and summaries showing object appearances across the entire video. What else can VisionScout do? ğŸ–¼ï¸ Analyze any image and detect 80 object types with YOLOv8. ğŸ”„ Switch between Nano, Medium, and XLarge models for speed or accuracy. ğŸ¯ Filter by object classes (people, vehicles, animals, etc.) to focus on what matters. ğŸ“Š View detailed stats on detections, confidence levels, and distributions. ğŸ§  Understand scenes â€” interpreting environments and potential activities. âš ï¸ Automatically identify possible safety concerns based on detected objects. Whatâ€™s coming next? ğŸ” Expanding...</description><pubDate>Sat, 10 May 2025 13:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/287598166225995</guid></item><item><title>PawMatchAI ğŸ¾: The Complete Dog Breed Platform</title><link>https://huggingface.co/posts/DawnC/256239584865203</link><description>PawMatchAI ğŸ¾: The Complete Dog Breed Platform PawMatchAI offers a comprehensive suite of features designed for dog enthusiasts and prospective owners alike. This all-in-one platform delivers five essential tools to enhance your canine experience: 1. ğŸ”Breed Detection: Upload any dog photo and the AI accurately identifies breeds from an extensive database of 124+ different dog breeds. The system detects dogs in the image and provides confident breed identification results. 2.ğŸ“ŠBreed Information: Access detailed profiles for each breed covering exercise requirements, typical lifespan, grooming needs, health considerations, and noise behavior - giving you complete understanding of any breed's characteristics. 3.ğŸ“‹ Breed Comparison : Compare any two breeds side-by-side with intuitive visualizations highlighting differences in care requirements, personality traits, health factors, and more - perfect for making informed decisions. 4.ğŸ’¡ Breed Recommendation: Receive personalized breed...</description><pubDate>Sat, 10 May 2025 13:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/256239584865203</guid></item><item><title>nvidia</title><link>https://huggingface.co/posts/clem/655781573301725</link><description>nvidia dominating the top trending open datasets these days! http://hf.co/datasets See translation</description><pubDate>Sat, 10 May 2025 13:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/655781573301725</guid></item><item><title>I've made an open version of Google's NotebookLM, and it shows the superiority of the open source tech task! ğŸ’ª</title><link>https://huggingface.co/posts/m-ric/347153743407715</link><description>I've made an open version of Google's NotebookLM, and it shows the superiority of the open source tech task! ğŸ’ª The app's workflow is simple. Given a source PDF or URL, it extracts the content from it, then tasks Meta's Llama 3.3-70B with writing the podcast script, with a good prompt crafted by @ gabrielchua ("two hosts, with lively discussion, fun notes, insightful question etc.") Then it hands off the text-to-speech conversion to Kokoro-82M, and there you go, you have two hosts discussion any article. The generation is nearly instant, because: &gt; Llama 3.3 70B is running at 1,000 tokens/seconds with Cerebras inference &gt; The audio is generated in streaming mode by the tiny (yet powerful) Kokoro, generating voices faster than real-time. And the audio generation runs for free on Zero GPUs, hosted by HF on H200s. Overall, open source solutions rival the quality of closed-source solutions at close to no cost! Try it here ğŸ‘‰ğŸ‘‰ m-ric/open-notebooklm See translation</description><pubDate>Sat, 10 May 2025 13:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/347153743407715</guid></item><item><title>This time Gemini is very quick with API support on its 2.5 pro May release. The performance is impressive too, now it is among top contenders like o4, R1, and Claude.</title><link>https://huggingface.co/posts/onekq/737528561026190</link><description>This time Gemini is very quick with API support on its 2.5 pro May release. The performance is impressive too, now it is among top contenders like o4, R1, and Claude. onekq-ai/WebApp1K-models-leaderboard See translation</description><pubDate>Sat, 10 May 2025 13:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/737528561026190</guid></item><item><title>We (</title><link>https://huggingface.co/posts/juhoinkinen/699503520254777</link><description>We ( @ osma , @ MonaLehtinen &amp; me, i.e. the Annif team at the National Library of Finland) recently took part in the LLMs4Subjects challenge at the SemEval-2025 workshop. The task was to use large language models (LLMs) to generate good quality subject indexing for bibliographic records, i.e. titles and abstracts. We are glad to report that our system performed well; it was ranked ğŸ¥‡ 1st in the category where the full vocabulary was used ğŸ¥ˆ 2nd in the smaller vocabulary category ğŸ… 4th in the qualitative evaluations. 14 participating teams developed their own solutions for generating subject headings and the output of each system was assessed using both quantitative and qualitative evaluations. Research papers about most of the systems are going to be published around the time of the workshop in late July, and many pre-prints are already available. We applied Annif together with several LLMs that we used to preprocess the data sets: translated the GND vocabulary terms to English,...</description><pubDate>Sat, 10 May 2025 13:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/juhoinkinen/699503520254777</guid></item><item><title>Ever notice how some AI assistants feel like tools while others feel like companions? Turns out, it's not always about fancy tech upgrades, because sometimes it's just clever design.</title><link>https://huggingface.co/posts/giadap/315154856088110</link><description>Ever notice how some AI assistants feel like tools while others feel like companions? Turns out, it's not always about fancy tech upgrades, because sometimes it's just clever design. Our latest blog post at Hugging Face dives into how minimal design choices can completely transform how users experience AI. We've seen our community turn the same base models into everything from swimming coaches to interview prep specialists with surprisingly small tweaks. The most fascinating part? When we tested identical models with different "personalities" in our Inference Playground, the results were mind-blowing. Want to experiment yourself? Our Inference Playground lets anyone (yes, even non-coders!) test these differences in real-time. You can: - Compare multiple models side-by-side - Customize system prompts - Adjust parameters like temperature - Test multi-turn conversations It's fascinating how a few lines of instruction text can transform the same AI from strictly professional to...</description><pubDate>Sat, 10 May 2025 13:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/315154856088110</guid></item><item><title>I Did a Thing!</title><link>https://huggingface.co/posts/nomadicsynth/333343195611996</link><description>I Did a Thing! I made an embedding model to find answers in research papers. It goes deeper than plain "semantic search" by identifying deeply reasoned connections and interdisciplinary insights that might have been overlooked. The goal is to find the solutions that might have been missed and to uncover answers that are already out there. Iâ€™ve set up a demo Space - nomadicsynth/inkling . Itâ€™s early days, and Iâ€™d love some feedback on the modelâ€™s results. Try it out and let me know what you think! Oh, and if it finds your Nobel-winning answer, I want a cut! ğŸ˜‰ See translation</description><pubDate>Sat, 10 May 2025 13:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nomadicsynth/333343195611996</guid></item><item><title>The new Mistral medium model is very impressive for its size. Will it be open sourced given the history of Mistral? Does anyone have insights?</title><link>https://huggingface.co/posts/onekq/350712908160959</link><description>The new Mistral medium model is very impressive for its size. Will it be open sourced given the history of Mistral? Does anyone have insights? onekq-ai/WebApp1K-models-leaderboard See translation</description><pubDate>Sat, 10 May 2025 13:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/350712908160959</guid></item></channel></rss>