<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>âœ¨ DreamO Video: From Customized Images to Videos âœ¨</title><link>https://huggingface.co/posts/openfree/538970335354687</link><description>âœ¨ DreamO Video: From Customized Images to Videos âœ¨ Hello, AI creators! Today I'm introducing a truly special project. DreamO Video is an integrated framework that generates customized images based on reference images and transforms them into videos with natural movement. ğŸ¬âœ¨ openfree/DreamO-video ğŸ” Key Features Image Reference (IP): Maintain object appearance while applying to new backgrounds and situations ID Preservation: Retain facial features across various environments Style Transfer: Apply unique styles from reference images to other content ğŸï¸ Video Generation: Create natural 2-second videos from generated images ğŸ’¡ How to Use Upload Reference Images: One or two images (people, objects, landscapes, etc.) Select Task Type: Choose between IP (Image Preservation), ID (Face Feature Retention), or Style Enter Prompt: Describe your desired result (e.g., "a woman playing guitar on a cloud") Click Generate Image: âœ¨ Create customized AI images! Generate Video: Click the ğŸ¬ button on the...</description><pubDate>Thu, 15 May 2025 05:23:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/538970335354687</guid></item><item><title>Iâ€™ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things.</title><link>https://huggingface.co/posts/ArturoNereu/644085701737970</link><description>Iâ€™ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things. I turned this into a GitHub repo: https://github.com/ArturoNereu/AI-Study-Group If youâ€™re just getting started, I recommend: ğŸ“˜ Deep Learning â€“ A Visual Approach: https://www.glassner.com/portfolio/deep-learning-a-visual-approach ğŸ¥ Dive into LLMs with Andrej Karpathy: https://youtu.be/7xTGNNLPyMI?si=aUTq_qUzyUx36BsT ğŸ§  The ğŸ¤— Agents course]( https://huggingface.co/learn/agents-course/ The repo has grown with help from the community (Reddit, Discord, etc.) and Iâ€™ll keep updating it. If you have any favorite resources, Iâ€™d love to include them. See translation</description><pubDate>Thu, 15 May 2025 05:23:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ArturoNereu/644085701737970</guid></item><item><title>ğŸŒŠ CycleNavigator: Visualizing Economic and Political Cycles Through AI at a Glance! ğŸ§ ğŸ’¹</title><link>https://huggingface.co/posts/openfree/905523908666849</link><description>ğŸŒŠ CycleNavigator: Visualizing Economic and Political Cycles Through AI at a Glance! ğŸ§ ğŸ’¹ ğŸ’« Strategic Intelligence Tool for Navigating Historical Waves and Forecasting the Future Hello there! ğŸ™Œ CycleNavigator brings you an innovative fusion of economic history, data visualization, and generative AI. This open-source project revolutionizes decision-making by displaying four major economic and political cycles through interactive visualizations! ğŸ“Š Experience Four Major Cycles in One View: Business Cycle (â‰ˆ9 years) â±ï¸ - The 'heartbeat' of investment and inventory Kondratiev Wave (â‰ˆ50 years) ğŸŒ - Long technological innovation waves Finance Cycle (â‰ˆ80 years) ğŸ’° - Rhythm of debt and financial crises Hegemony Cycle (â‰ˆ250 years) ğŸ›ï¸ - Transitions in global order âœ¨ Cutting-Edge Features: Interactive Wave Visualization ğŸ¯ - Intuitive graphs powered by Plotly AI-Powered Historical Similarity Mapping ğŸ§© - Connecting past events via SBERT embeddings Real-time News Integration ğŸ“° - Linking current issues...</description><pubDate>Thu, 15 May 2025 05:23:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/905523908666849</guid></item><item><title>Transfer Any Clothing Into A New Person &amp; Turn Any Person Into A 3D Figure - ComfyUI Tutorial</title><link>https://huggingface.co/posts/MonsterMMORPG/869555651580897</link><description>Transfer Any Clothing Into A New Person &amp; Turn Any Person Into A 3D Figure - ComfyUI Tutorial ComfyUI is super hard to use but I have literally prepared 1-click way to install and use 2 amazing workflows. First workflow is generating a person wearing any clothing. The second workflow is turning any person image into a 3D toy like figure image. Tutorial Link : https://youtu.be/ZzYnhKeaJBs Video Chapters 0:00:00 Intro: Two One-Click ComfyUI Workflows (Clothing Gen &amp; 3D Figure) 0:00:34 Effort &amp; Goal: Easy Installation &amp; Use of Complex Workflows 0:00:49 Setup Part 1: ComfyUI Prerequisite &amp; Downloading Project Zip File 0:01:06 Setup Part 2: Extracting Zip into ComfyUI Folder (WinRAR 'Extract Here' Tip) 0:01:18 Setup Part 3: Running update_comfyui.bat for Latest ComfyUI Version 0:01:37 Setup Part 4: Running install_clothing_and_3D.bat (Installs Nodes &amp; Requirements) 0:02:03 Model Downloads: Intro to Swarm UI Auto-Installer &amp; Automatic Updater 0:02:28 Using Swarm UI: Launching...</description><pubDate>Thu, 15 May 2025 05:23:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/869555651580897</guid></item><item><title>Very cool to see</title><link>https://huggingface.co/posts/clem/170733821735878</link><description>Very cool to see pytorch contributing on Hugging Face. Time to follow them to see what they're cooking! See translation</description><pubDate>Thu, 15 May 2025 05:23:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/170733821735878</guid></item><item><title>ğŸš€ VisionScout Now Speaks More Like Me â€” Thanks to LLMs!</title><link>https://huggingface.co/posts/DawnC/683112818630492</link><description>ğŸš€ VisionScout Now Speaks More Like Me â€” Thanks to LLMs! I'm thrilled to share a major update to VisionScout, my end-to-end vision system. Beyond robust object detection (YOLOv8) and semantic context (CLIP), VisionScout now features a powerful LLM-based scene narrator (Llama 3.2), improving the clarity, accuracy, and fluidity of scene understanding. This isnâ€™t about replacing the pipeline , itâ€™s about giving it a better voice. âœ¨ â­ï¸ What the LLM Brings Fluent, Natural Descriptions: The LLM transforms structured outputs into human-readable narratives. Smarter Contextual Flow: It weaves lighting, objects, zones, and insights into a unified story. Grounded Expression: Carefully prompt-engineered to stay factual â€” it enhances, not hallucinates. Helpful Discrepancy Handling: When YOLO and CLIP diverge, the LLM adds clarity through reasoning. VisionScout Still Includes: ğŸ–¼ï¸ YOLOv8-based detection (Nano / Medium / XLarge) ğŸ“Š Real-time stats &amp; confidence insights ğŸ§  Scene understanding via...</description><pubDate>Thu, 15 May 2025 05:23:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/683112818630492</guid></item><item><title>The era of local Computer Use AI Agents is here.</title><link>https://huggingface.co/posts/dhruv3006/692657412350660</link><description>The era of local Computer Use AI Agents is here. Meet UI-TARS-1.5-7B-6bit, now running natively on Apple Silicon via MLX. The video is of UI-TARS-1.5-7B-6bit completing the prompt "draw a line from the red circle to the green circle, then open reddit in a new tab" running entirely on MacBook. The video is just a replay, during actual usage it took between 15s to 50s per turn with 720p screenshots (on avg its ~30s per turn), this was also with many apps open so it had to fight for memory at times. Built using c/ua : https://github.com/trycua/cua Join us making them here: https://discord.gg/4fuebBsAUj Kudos to the MLX community here on huggingface : mlx-community See translation</description><pubDate>Thu, 15 May 2025 05:23:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/692657412350660</guid></item><item><title>Introducing bitnet-r1-llama-8b and bitnet-r1-qwen-32b preview! These models are the first successful sub 1-billion-token finetune to BitNet architecture. We discovered that by adding an aditional input RMSNorm to each linear, you can finetune directly to BitNet with fast convergence to original model performance!</title><link>https://huggingface.co/posts/codys12/705081891087680</link><description>Introducing bitnet-r1-llama-8b and bitnet-r1-qwen-32b preview! These models are the first successful sub 1-billion-token finetune to BitNet architecture. We discovered that by adding an aditional input RMSNorm to each linear, you can finetune directly to BitNet with fast convergence to original model performance! We are working on a pull request to use this extra RMS for any model. To test these models now, install this fork of transformers: pip install git+https://github.com /Codys12/ transformers.git Then load the models and test: from transformers import ( AutoModelForCausalLM , AutoTokenizer ) model_id = "codys12/bitnet-r1-qwen-32b" model = AutoModelForCausalLM .from_pretrained( model_id, device_map= "cuda" , ) tokenizer = AutoTokenizer .from_pretrained(model_id, padding_side= "left" ) bitnet-r1-llama-8b and bitnet-r1-llama-32b were trained on ~ 300M and 200M tokens of the open-thoughts/OpenThoughts-114k dataset respectively, and were still significantly improving at the end of...</description><pubDate>Thu, 15 May 2025 05:23:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codys12/705081891087680</guid></item><item><title># ğŸŒŸ 3D Model to Video: Easy GLB Conversion Tool ğŸŒŸ</title><link>https://huggingface.co/posts/ginipick/766230066345476</link><description># ğŸŒŸ 3D Model to Video: Easy GLB Conversion Tool ğŸŒŸ demo link: ginigen/3D-VIDEO Hello there! Would you like to transform your 3D models into stunning animations? This space can help you! âœ¨ ## ğŸ” What Can It Do? This tool converts your uploaded GLB model into: 1. ğŸ® A transformed GLB file 2. ğŸ¬ An animated GIF preview 3. ğŸ“‹ A metadata JSON file ## âœ… Key Features * ğŸ–¥ï¸ Works in headless server environments (EGL + pyglet-headless â†’ pyrender fallback) * ğŸ” Objects in GIFs appear 3x larger (global scale Ã—3) * ğŸ¨ Clean interface with pastel background ## ğŸ® Animation Types * ğŸ”„ Rotate - Object rotates around the Y-axis * â¬†ï¸ Float - Object moves smoothly up and down * ğŸ’¥ Explode - Object moves sideways * ğŸ§© Assemble - Object returns to its original position * ğŸ’“ Pulse - Object changes in size * ğŸ”„ Swing - Object swings around the Z-axis ## ğŸ› ï¸ How to Use 1. Upload your GLB model ğŸ“¤ 2. Select your desired animation type ğŸ¬ 3. Adjust the duration and FPS â±ï¸ 4. Click the "Generate Animation" button â–¶ï¸ 5....</description><pubDate>Thu, 15 May 2025 05:23:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/766230066345476</guid></item><item><title>Tried something new: an AI-generated podcast that breaks down the top research paper each day. Fully automated, now live on Spotify.</title><link>https://huggingface.co/posts/fdaudens/617387724043904</link><description>Tried something new: an AI-generated podcast that breaks down the top research paper each day. Fully automated, now live on Spotify. I built this prototype to help keep up with the rapid pace of AI developments and, hopefully, make cutting-edge research more accessible. I donâ€™t know about you, but just listening to a conversation about a paper really helps the content sink in for me. This build taught me a lot about full automation. If youâ€™re into the technical weeds: Qwen3 runs on Inference to handle the script, Kokoro does the voice, and the whole thing gets published automatically thanks to the Hugging Face Jobs API and Gradio deployment. Itâ€™s not perfect yet â€” Iâ€™ll be monitoring for hallucinations and incoherence. The voice model still needs polish, but itâ€™s a promising start. Would love to build this with the community â€” submit a PR or send feedback. Itâ€™s just a beta of an experimental idea! Big kudos to @ m-ric , whose Open NotebookLM this is based on, and to @ nielsr for his...</description><pubDate>Thu, 15 May 2025 05:23:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/617387724043904</guid></item></channel></rss>