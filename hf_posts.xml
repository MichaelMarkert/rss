<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>‚ú® DeepSeek V3.1 just dropped on the hub.</title><link>https://huggingface.co/posts/AdinaY/251517572573964</link><description>‚ú® DeepSeek V3.1 just dropped on the hub. deepseek-ai/DeepSeek-V3.1-Base See translation</description><pubDate>Fri, 22 Aug 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/251517572573964</guid></item><item><title>üöÄ Introducing MGM-Omni, an omni-chatbot capable of processing text, image, video, and speech inputs, and can generate both text and speech responses.</title><link>https://huggingface.co/posts/wcy1122/435759509322871</link><description>üöÄ Introducing MGM-Omni, an omni-chatbot capable of processing text, image, video, and speech inputs, and can generate both text and speech responses. üëÇ MGM-Omni support hour-level audio understanding. üó£Ô∏è MGM-Omni support 10-minute speech generation and voice cloning. For more details, please check: üìù Blog: https://mgm-omni.notion.site/MGM-Omni-An-Open-source-Omni-Chatbot-2395728e0b0180149ac9f24683fc9907 üåü Code: https://github.com/dvlab-research/MGM-Omni ü§ñ Model: wcy1122/mgm-omni-6896075e97317a88825032e1 üéÆ Demo: wcy1122/MGM-Omni See translation</description><pubDate>Fri, 22 Aug 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wcy1122/435759509322871</guid></item><item><title>why did 36 people unfollow me üò≠</title><link>https://huggingface.co/posts/ProCreations/419010322512677</link><description>why did 36 people unfollow me üò≠ we are back in the hundreds. if you become my 500th follower and have proof I'll give you 5 dollars worth of openrouter credits as an API key See translation</description><pubDate>Fri, 22 Aug 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ProCreations/419010322512677</guid></item><item><title>fascinating read!</title><link>https://huggingface.co/posts/Jaward/861738398724390</link><description>fascinating read! staying bullish on search with rl might just help us get rid of hallucination entirely. I really like their approach: 1) &lt;think&gt;on prompt/context &amp;&amp; what u know &lt;/think&gt; 2) self &lt;search&gt;when u don‚Äôt know&lt;/search&gt; (iteratively) with no external tool 3) &lt;information&gt;cite sources to support claim(s)&lt;/information&gt; 4) &lt;answer&gt;final answer&lt;/answer&gt; their rl training was done cost efficiently too, see code: https://github.com/TsinghuaC3I/SSRL See translation</description><pubDate>Fri, 22 Aug 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/861738398724390</guid></item><item><title>üé® Open Nano-Banana: Revolution in Ultra-Fast AI Image Editing!</title><link>https://huggingface.co/posts/seawolf2357/250380945868372</link><description>üé® Open Nano-Banana: Revolution in Ultra-Fast AI Image Editing! üöÄ Introduction **Open Nano-Banana** is an innovative image editing tool based on the Qwen-Image-Edit model. Experience amazing quality image editing in just 8 steps! Heartsync/Nano-Banana ‚ú® Core Features ‚ö° Lightning-Fast Editing * **8-Step Generation**: Ultra-fast processing with Qwen-Image-Lightning LoRA * **Real-time Editing**: 10x faster than conventional methods * **GPU Optimization**: Maximized memory efficiency with xformers ü§ñ AI Prompt Enhancement * **Automatic Prompt Improvement**: Intelligent rewriting with Cerebras' Qwen3-235B model * **Multilingual Support**: Auto-detection for Korean/Chinese/English * **Context Understanding**: Sophisticated command generation aligned with image context üéØ Versatile Editing Functions ‚úÖ Add/Delete/Replace objects ‚úÖ Text editing and style transformation ‚úÖ Person editing (expressions, hairstyles) ‚úÖ Vintage restoration and style conversion ‚úÖ Background replacement and enhancement...</description><pubDate>Fri, 22 Aug 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/seawolf2357/250380945868372</guid></item><item><title>üó∫Ô∏è New blog post üó∫Ô∏è</title><link>https://huggingface.co/posts/frimelle/763316628765853</link><description>üó∫Ô∏è New blog post üó∫Ô∏è Old Maps, New Terrain: Updating Labour Taxonomies for the AI Era For decades, we‚Äôve relied on labour taxonomies like O*NET to understand how technology changes work. These taxonomies break down jobs into tasks and skills, but they were built in a world before most work became digital-first, and long before generative AI could create marketing campaigns, voiceovers, or even whole professions in one step. That leaves us with a mismatch: we‚Äôre trying to measure the future of work with tools from the past. With @ yjernite we describe why these frameworks are falling increasingly short in the age of generative AI. We argue that instead of discarding taxonomies, we need to adapt them. Imagine taxonomies that: ‚ú® Capture new AI-native tasks and hybrid human-AI workflows ‚ú® Evolve dynamically as technology shifts ‚ú® Give workers a voice in deciding what gets automated and what stays human If we don‚Äôt act, we‚Äôll keep measuring the wrong things. If we do, we can design...</description><pubDate>Fri, 22 Aug 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/frimelle/763316628765853</guid></item><item><title>Seed-OSS üî• The latest open LLM from Bytedance Seed team</title><link>https://huggingface.co/posts/AdinaY/149610366794720</link><description>Seed-OSS üî• The latest open LLM from Bytedance Seed team ByteDance-Seed/seed-oss-68a609f4201e788db05b5dcd ‚ú® 36B - Base &amp; Instruct ‚ú® Apache 2.0 ‚ú® Native 512K long context ‚ú® Strong reasoning &amp; agentic intelligence ‚ú® 2 Base versions: with &amp; without synthetic data See translation</description><pubDate>Fri, 22 Aug 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/149610366794720</guid></item><item><title>We've improved the Deploy button on Hugging Face model pages for Microsoft Azure</title><link>https://huggingface.co/posts/pagezyhf/949187233847606</link><description>We've improved the Deploy button on Hugging Face model pages for Microsoft Azure 1/ no more long waits before seeing model support status 2/ ready-to-use CLI and Python snippets 3/ redirection to Azure AI Foundry rather than Azure ML ‚úã if you see any bugs or have feedback, open an issue on our repo: https://github.com/huggingface/Microsoft-Azure See translation</description><pubDate>Fri, 22 Aug 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/pagezyhf/949187233847606</guid></item><item><title>12 hours ago:</title><link>https://huggingface.co/posts/ccocks-deca/499605656909204</link><description>12 hours ago: Something big* coming * big = biggest in the world Annnnnd... here it is! deca-ai/3-alpha-ultra ‚Äîthe largest AI model in the world by deca-ai , clocking in at a whopping 4.6T parameters. Apologies for the delay, but we‚Äôre stoked to finally drop this, even in its alpha stage. Before you dive in, here are a few things to keep in mind: 1. **No commercial use yet**: We're still working on Deca 2.5 (Proprietary), and releasing Deca 3 for commercial use right now would impact that. Once Deca 3.5 hits in early '26, we‚Äôll be opening it up with a more permissive license. 2. **Built on existing models**: Deca 3 isn‚Äôt a ground-up creation‚Äîit‚Äôs a huge step forward, building on what‚Äôs already out there. 3. **It‚Äôs experimental**: As much as we‚Äôre hyped about its scale, it‚Äôs still in testing. 4. **DynaMoE architecture**: Run a (very) small part of the model with 64GB of RAM/VRAM (when quantized - quants coming soon), or the whole thing with 1TB. It‚Äôs that scalable. 5. **Not widely...</description><pubDate>Fri, 22 Aug 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ccocks-deca/499605656909204</guid></item><item><title>Wan 2.2, FLUX, FLUX Krea &amp; Qwen Image Just got Upgraded: Ultimate Tutorial for Open Source SOTA Image &amp; Video Gen Models - With easy to use SwarmUI with ComfyUI Backend :</title><link>https://huggingface.co/posts/MonsterMMORPG/826512832075444</link><description>Wan 2.2, FLUX, FLUX Krea &amp; Qwen Image Just got Upgraded: Ultimate Tutorial for Open Source SOTA Image &amp; Video Gen Models - With easy to use SwarmUI with ComfyUI Backend : https://youtu.be/3BFDcO2Ysu4 Tutorial Video : https://youtu.be/3BFDcO2Ysu4 Wan 2.2, Qwen Image, FLUX, FLUX Krea, all these models are the SOTA open-source models and in this master tutorial I will show you how to use these models in the easiest, most performant, and most accurate way. After doing almost one week of research, I have determined the very best presets and prepared this tutorial. With literally one click you will be able to install, download models, set presets, and use these amazing models. Wan 2.2 is currently the king of video generation models and now it is super fast with lightx2v Wan2.2-Lightning LoRAs. Moreover, Qwen Image is now ultra-fast with the recently released 8-step LoRA with almost no quality loss. Furthermore, I have updated FLUX and FLUX Krea presets to improve image generation...</description><pubDate>Fri, 22 Aug 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/826512832075444</guid></item></channel></rss>