<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>12 Types of JEPA</title><link>https://huggingface.co/posts/Kseniase/762937246285628</link><description>12 Types of JEPA Since Yann LeCun together with Randall Balestriero released a new paper on JEPA (Joint-Embedding Predictive Architecture), laying out its theory and introducing an efficient practical version called LeJEPA, we figured you might need even more JEPA. Here are 7 recent JEPA variants plus 5 iconic ones: 1. LeJEPA ‚Üí LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics (2511.08544) Explains a full theory for JEPAs, defining the ‚Äúideal‚Äù JEPA embedding as an isotropic Gaussian, and proposes the SIGReg objective to push JEPA toward this ideal, resulting in practical LeJEPA 2. JEPA-T ‚Üí JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation (2510.00974) A text-to-image model that tokenizes images and captions with a joint predictive Transformer, enhances fusion with cross-attention and text embeddings before training loss, and generates images by iteratively denoising visual tokens conditioned on text 3. Text-JEPA ‚Üí...</description><pubDate>Tue, 18 Nov 2025 17:23:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/762937246285628</guid></item><item><title>Next level Realism with Qwen Image is now possible after new realism LoRA workflow - Top images are new realism workflow - Bottom ones are older default - Full tutorial published - 4+4 Steps only</title><link>https://huggingface.co/posts/MonsterMMORPG/695072229497754</link><description>Next level Realism with Qwen Image is now possible after new realism LoRA workflow - Top images are new realism workflow - Bottom ones are older default - Full tutorial published - 4+4 Steps only Tutorial of realism : https://youtu.be/XWzZ2wnzNuQ Tutorial of training : https://youtu.be/DPX3eBTuO_Y This is a full comprehensive step-by-step tutorial for how to train Qwen Image models. This tutorial covers how to do LoRA training and full Fine-Tuning / DreamBooth training on Qwen Image models. It covers both the Qwen Image base model and the Qwen Image Edit Plus 2509 model. This tutorial is the product of 21 days of full R&amp;D, costing over $800 in cloud services to find the best configurations for training. Furthermore, we have developed an amazing, ultra-easy-to-use Gradio app to use the legendary Kohya Musubi Tuner trainer with ease. You will be able to train locally on your Windows computer with GPUs with as little as 6 GB of VRAM for both LoRA and Fine-Tuning. See translation</description><pubDate>Tue, 18 Nov 2025 17:23:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/695072229497754</guid></item><item><title>Hindi Speech to Text just crossed 20 million downloads. Grateful for everyone using it.</title><link>https://huggingface.co/posts/theainerd/926652286906905</link><description>Hindi Speech to Text just crossed 20 million downloads. Grateful for everyone using it. theainerd/Wav2Vec2-large-xlsr-hindi See translation</description><pubDate>Tue, 18 Nov 2025 17:23:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/theainerd/926652286906905</guid></item><item><title>Announcing RefCOCO-M, a refreshed RefCOCO with pixel-accurate masks and the problematic prompts removed.</title><link>https://huggingface.co/posts/vikhyatk/769322438249892</link><description>Announcing RefCOCO-M, a refreshed RefCOCO with pixel-accurate masks and the problematic prompts removed. moondream/refcoco-m See translation</description><pubDate>Tue, 18 Nov 2025 17:23:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/vikhyatk/769322438249892</guid></item><item><title>Made a demo for multimodal understanding of Qwen3-VL space for tasks including point annotation, detection, captioning, guided text inferences, and more. Find the demo link below. ü§ó‚ÜóÔ∏è</title><link>https://huggingface.co/posts/prithivMLmods/902988810263838</link><description>Made a demo for multimodal understanding of Qwen3-VL space for tasks including point annotation, detection, captioning, guided text inferences, and more. Find the demo link below. ü§ó‚ÜóÔ∏è ‚Æû Space[Demo]: prithivMLmods/Qwen3-VL-HF-Demo ‚Æû Model Used: Qwen/Qwen3-VL-4B-Instruct ‚Æû Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations ‚Æû GitHub: https://github.com/PRITHIVSAKTHIUR/Qwen-3VL-Multimodal-Understanding To know more about it, visit the app page or the respective model page! See translation</description><pubDate>Tue, 18 Nov 2025 17:23:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/902988810263838</guid></item><item><title>üéâ Wow. Congratulations</title><link>https://huggingface.co/posts/ZennyKenny/159598235519685</link><description>üéâ Wow. Congratulations @ bfirsh and the Replicate team on the CloudFlare acquisition! ‚úåÔ∏è You've really built an incredible ecosystem and product offering and should be super proud. See translation</description><pubDate>Tue, 18 Nov 2025 17:23:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/159598235519685</guid></item><item><title>&gt;&gt;&gt; We're writing a new book,  &lt;Planetary Causal Inference&gt;, on how to model counterfactuals at planetary scale by combining satellite imagery + other global data with local studies and RCTs. Forthcoming in 2026+.</title><link>https://huggingface.co/posts/cjerzak/918588861809536</link><description>&gt;&gt;&gt; We're writing a new book, &lt;Planetary Causal Inference&gt;, on how to model counterfactuals at planetary scale by combining satellite imagery + other global data with local studies and RCTs. Forthcoming in 2026+. &gt;&gt;&gt; Book info: https://planetarycausalinference.org/book-launch &gt;&gt;&gt; All datasets used in the book will be openly available on our lab‚Äôs Hugging Face hub: theaidevlab See translation</description><pubDate>Tue, 18 Nov 2025 17:23:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/cjerzak/918588861809536</guid></item><item><title>Running large language models efficiently is more than just raw GPU power. The latest guide breaks down the essential math to determine if your LLM workload is compute-bound or memory-bound.</title><link>https://huggingface.co/posts/flozi00/635605102777732</link><description>Running large language models efficiently is more than just raw GPU power. The latest guide breaks down the essential math to determine if your LLM workload is compute-bound or memory-bound. We apply these principles to a real-world example: Qwen's 32B parameter model on the new NVIDIA RTX PRO 6000 Blackwell Edition. In this guide, you will learn how to: Calculate your GPU's operational intensity (Ops:Byte Ratio) Determine your model's arithmetic intensity Identify whether your workload is memory-bound or compute-bound Read the full guide here: https://flozi.net/en/guides/ai/llm-inference-math See translation</description><pubDate>Tue, 18 Nov 2025 17:23:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/flozi00/635605102777732</guid></item><item><title>Implemented a proof of concept sampler in pure PyTorch and transformers.</title><link>https://huggingface.co/posts/grimjim/343972883837810</link><description>Implemented a proof of concept sampler in pure PyTorch and transformers. Max P consists of a dynamic token filter which applies Winsorization to cap the probabilties of top tokens. Specifically, a base probability in the range of [0,1] is used to cap individual token probability; the sampler then redistributes excess proportionally. https://github.com/jim-plus/maxp-sampler-poc Combined with Temperature and Min P, this could represent a more intuitive way of reducing repetition in text generation. See translation</description><pubDate>Tue, 18 Nov 2025 17:23:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/grimjim/343972883837810</guid></item><item><title>Made a small write up and experimental finetuning guide for MetaCLIP2 for Image Classification on Downstream Tasks. The blog titled</title><link>https://huggingface.co/posts/prithivMLmods/462277719397337</link><description>Made a small write up and experimental finetuning guide for MetaCLIP2 for Image Classification on Downstream Tasks. The blog titled Fine Tuning MetaCLIP 2 for Image Classification on Downstream Tasks demonstrates the step by step finetuning using CIFAR10 and is also flexible for adapting to other datasets. For more details, check out the linked blog below. ü§ó‚ÜóÔ∏è ‚Æû Blog Article: https://huggingface.co/blog/prithivMLmods/metaclip2-downstream-finetune ‚Æû Demo Space[Zero-Shot Classification]: prithivMLmods/metaclip-2-demo Some other models ‚ï∞‚Ä∫ MetaCLIP-2-Cifar10: prithivMLmods/MetaCLIP-2-Cifar10 ‚ï∞‚Ä∫ MetaCLIP-2-Age-Range-Estimator: prithivMLmods/MetaCLIP-2-Age-Range-Estimator ‚ï∞‚Ä∫ MetaCLIP-2-Gender-Identifier: prithivMLmods/MetaCLIP-2-Gender-Identifier ‚ï∞‚Ä∫ MetaCLIP-2-Open-Scene: prithivMLmods/MetaCLIP-2-Open-Scene ‚Æû Collection: https://huggingface.co/collections/prithivMLmods/metaclip2-image-classification-experiments To know more about it, visit the app page or the respective model page! See...</description><pubDate>Tue, 18 Nov 2025 17:23:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/462277719397337</guid></item></channel></rss>