<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>We got a visitor to the office today!</title><link>https://huggingface.co/posts/clem/645090569186989</link><description>We got a visitor to the office today! pollen-robotics , lerobot , unitreerobotics meetings! See translation</description><pubDate>Sat, 21 Jun 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/645090569186989</guid></item><item><title>y'all have been asking my opinion on how OCR models compare to each other üëÄ</title><link>https://huggingface.co/posts/merve/300457326273979</link><description>y'all have been asking my opinion on how OCR models compare to each other üëÄ I will leave three apps to compare newest models by @ prithivMLmods instead ‚§µÔ∏è &gt; compare Nanonets-OCR-s, Qwen2-VL-OCR-2B-Instruct, RolmOCR, Aya-Vision prithivMLmods/Multimodal-OCR &gt; SmolDocling, Nanonets-OCR-s, MonkeyOCR, Typhoon-OCR-7B prithivMLmods/Multimodal-OCR2 &gt; docscopeOCR, MonkeyOCR, coreOCR prithivMLmods/core-OCR See translation</description><pubDate>Sat, 21 Jun 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/300457326273979</guid></item><item><title>WAN 2.1 FusionX + Self Forcing LoRA are the New Best of Local Video Generation with Only 8 Steps + FLUX Upscaling Guide :</title><link>https://huggingface.co/posts/MonsterMMORPG/682969091190201</link><description>WAN 2.1 FusionX + Self Forcing LoRA are the New Best of Local Video Generation with Only 8 Steps + FLUX Upscaling Guide : https://www.youtube.com/watch?v=Xbn93GRQKsQ Tutorial : https://www.youtube.com/watch?v=Xbn93GRQKsQ Video Chapters 0:00 Introduction to the New FusionX Video Model &amp; FLUX Upscaling 0:30 One-Click Presets &amp; The SwarmUI Model Downloader Explained 1:07 Achieving Hyper-Realism with the FLUX 2x Latent Upscale Preset 1:58 How to Download &amp; Install the SwarmUI Model Downloader 2:49 Downloading Full Models vs. Downloading Just The LoRAs 3:48 Final Setup: Updating SwarmUI &amp; Importing The New Presets 4:32 Generating a Video: Applying the FusionX Image-to-Video Preset 5:03 Critical Step: Correcting The Model's Native Resolution Metadata 5:55 Finalizing Image-to-Video Settings (Frame Count &amp; RIFE Interpolation) 6:49 Troubleshooting Performance: Identifying Low GPU Usage &amp; Shared VRAM Bug 8:35 The Solution: Disabling Sage Attention for Image-to-Video Models 10:02 Final Result:...</description><pubDate>Sat, 21 Jun 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/682969091190201</guid></item><item><title>üó£Ô∏è Whose voice do we hear when AI speaks?</title><link>https://huggingface.co/posts/giadap/887797954746272</link><description>üó£Ô∏è Whose voice do we hear when AI speaks? Every language carries its own cultural values and worldviews. So, when we build AI systems, we're not just deciding how they speak but also whose perspectives they represent. Even choosing which dialect to train on in Norway becomes a question of inclusion and power. In Kenya, will AI speak Swahili from Nairobi or coastal regions? What about indigenous languages with rich oral traditions but limited written text, like Quechua in Peru or Cherokee in North America? The path forward? Building WITH communities, not just FOR them. Working with local partners (libraries, universities, civil society), testing for cultural alignment, and asking hard questions about representation. Just published some thoughts on this after my keynote in Norway a few weeks ago: https://huggingface.co/blog/giadap/when-ai-speaks See translation</description><pubDate>Sat, 21 Jun 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/887797954746272</guid></item><item><title>Self-Forcing - a real-time video distilled model from Wan 2.1 by</title><link>https://huggingface.co/posts/multimodalart/420236527922092</link><description>Self-Forcing - a real-time video distilled model from Wan 2.1 by @ adobe is out, and they open sourced it üêê I've built a live real time demo on Spaces üìπüí® multimodalart/self-forcing See translation</description><pubDate>Sat, 21 Jun 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/multimodalart/420236527922092</guid></item><item><title>Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.</title><link>https://huggingface.co/posts/dhruv3006/605499380772806</link><description>Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs. Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development. Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing. What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments. Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and...</description><pubDate>Sat, 21 Jun 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/605499380772806</guid></item><item><title>ü§î Ready to build better AI models with synthetic data, but don't know where to start? Why go at it alone?üí°</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/352866416610611</link><description>ü§î Ready to build better AI models with synthetic data, but don't know where to start? Why go at it alone?üí° üëã Join Duality AI‚Äôs Falcon community! It is one of the best resources for support, creativity, and growth as you move along your synthetic data journey. ‚û°Ô∏èOur current Kaggle competition is the easiest way to get started: https://www.kaggle.com/competitions/multi-instance-object-detection-challenge When you join, you'll meet some of the rising stars in our community, such as: üåü Sergio Sanz, @ sergio-sanz-rodriguez , who took 1st and 2nd place in recent computer vision Kaggle competitions and shared his process of using R-CNN and Falcon-generated images in this article: https://www.duality.ai/blog/leveraging-synthetic-data-for-real-world-object-detection üåüMohana pavan Bezawada, @ mohanapavan , who has risen in the ranks from the top 25 in the first competition all the way to top scorer in our current competition! His journey illustrates how dedication + Falcon can take you far in...</description><pubDate>Sat, 21 Jun 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/352866416610611</guid></item><item><title>üöÄ  Try deep-research without Network Connection and Data Leak, Jan-Nano production-ready on-device AI in 6 hours!</title><link>https://huggingface.co/posts/yeonseok-zeticai/509797148011121</link><description>üöÄ Try deep-research without Network Connection and Data Leak, Jan-Nano production-ready on-device AI in 6 hours! Jan-Nano has been making waves as one of HuggingFace's most trending 4B parameter models, outperforming even 671B models on SimpleQA benchmarks. But here's what changes everything: ZETIC.MLange just transformed Jan-Nano into a blazing-fast on-device AI solution. ‚ú® 6-hour deployment from huggingface to production-ready library! Zero cloud dependency - complete privacy and offline capability While others struggle with complex on-device deployments taking weeks or months, ZETIC.MLange's automated pipeline makes it effortless. No manual optimization, no vendor-specific coding, no compromise on performance. üì± Ready to transform your AI models? Try ZETIC.MLange, it is totally free now! The future of AI is on-device. Make it happen in hours, not months. #OnDeviceAI #EdgeAI #MLOptimization #NPU #PrivacyFirst See translation</description><pubDate>Sat, 21 Jun 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/509797148011121</guid></item><item><title>The demo for smoldocling / nanonets ocr / typhoon ocr /  monkey ocr  explores the document OCR capabilities of various newly released multimodal VLMs in a single space. And if you're experiencing or demoing long document image OCR, kindly use the Smoldocling 256M preview [ Smoldocling is back in demo here. ] ü§ó.</title><link>https://huggingface.co/posts/prithivMLmods/142374005596149</link><description>The demo for smoldocling / nanonets ocr / typhoon ocr / monkey ocr explores the document OCR capabilities of various newly released multimodal VLMs in a single space. And if you're experiencing or demoing long document image OCR, kindly use the Smoldocling 256M preview [ Smoldocling is back in demo here. ] ü§ó. ‚ú¶ Try the demo here : prithivMLmods/Multimodal-OCR2 ‚§∑ MonkeyOCR Recognition : echo840/MonkeyOCR ‚§∑ Nanonets-OCR-s : nanonets/Nanonets-OCR-s ‚§∑ SmolDocling-256M-preview : ds4sd/SmolDocling-256M-preview ‚§∑ typhoon-ocr-7b : scb10x/typhoon-ocr-7b ‚§∑ Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 ‚§∑ Github : https://github.com/PRITHIVSAKTHIUR/Multimodal-OCR2 The community GPU grant was given by Hugging Face ‚Äî special thanks to them. ü§óüöÄ To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Sat, 21 Jun 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/142374005596149</guid></item><item><title>Now you can make Flux.1 your own within just 10GBs of VRAM. In our new blog post we walk you through the process step by step.</title><link>https://huggingface.co/posts/derekl35/148956797193810</link><description>Now you can make Flux.1 your own within just 10GBs of VRAM. In our new blog post we walk you through the process step by step. Check it out here: https://huggingface.co/blog/flux-qlora See translation</description><pubDate>Sat, 21 Jun 2025 17:19:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/derekl35/148956797193810</guid></item></channel></rss>