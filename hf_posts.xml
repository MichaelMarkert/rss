<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Qwen releases Qwen3-Coder-Next! üíú Run the locally on 46GB RAM or less.</title><link>https://huggingface.co/posts/danielhanchen/824171868881117</link><description>Qwen releases Qwen3-Coder-Next! üíú Run the locally on 46GB RAM or less. Thhe model excels at agentic coding &amp; local use. With 256K context, it delivers similar performance to models with 10-20√ó more active parameters. GGUF: unsloth/Qwen3-Coder-Next-GGUF Guide: https://unsloth.ai/docs/models/qwen3-coder-next See translation</description><pubDate>Wed, 04 Feb 2026 17:50:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/824171868881117</guid></item><item><title>A single lock on a door isn't enough. Real security is about layers.</title><link>https://huggingface.co/posts/MikeDoes/512575404125311</link><description>A single lock on a door isn't enough. Real security is about layers. The same is true for AI privacy. A new paper, "Whispered Tuning", offers a fantastic layered solution that aims to fortify LLMs against privacy infringements. We're proud that the first, essential layer, a high-precision PII redaction model was built on the foundation of the Ai4Privacy/pii-65k dataset. Our dataset provided the necessary training material for their initial anonymization step, which then enabled them to develop further innovations like differential privacy fine-tuning and output filtering. This is a win-win: our data helps create a solid base, and researchers build powerful, multi-stage privacy architectures on top of it. Together, we're making AI safer. üîó Read the full paper to see how a strong foundation enables a complete privacy solution: https://www.scirp.org/journal/paperinformation?paperid=130659 üöÄ Stay updated on the latest in privacy-preserving AI‚Äîfollow us on LinkedIn:...</description><pubDate>Wed, 04 Feb 2026 17:50:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/512575404125311</guid></item><item><title>The 2025 Chinese LLM Showdown: Western Models Still Dominate Top 4, but China Leads the Open-Source Arena.</title><link>https://huggingface.co/posts/imnotkitty/790273915312125</link><description>The 2025 Chinese LLM Showdown: Western Models Still Dominate Top 4, but China Leads the Open-Source Arena. üèÜ The Champions: Claude-Opus-4.5, Gemini-3-Pro, GPT-5.2, and Gemini-3-Flash sweep the top four spots. üöÄ The Pursuers: Doubao and DeepSeek-V3.2 tie for first place among Chinese models; GLM-4.7, ERNIE-5.0, and Kimi secure their positions in the domestic top five. üî• The Biggest Highlight: The top three spots on the open-source leaderboard are entirely held by Team China (DeepSeek, GLM, Kimi), outperforming the best western open-source models. See translation</description><pubDate>Wed, 04 Feb 2026 17:50:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/imnotkitty/790273915312125</guid></item><item><title>I‚Äôm excited to share PlaiTO, a reasoning-focused language model built on LLaMA 3.1 (8B) and optimized for humanities and social sciences.</title><link>https://huggingface.co/posts/alibidaran/992533889532684</link><description>I‚Äôm excited to share PlaiTO, a reasoning-focused language model built on LLaMA 3.1 (8B) and optimized for humanities and social sciences. PlaiTO is designed to go beyond surface-level text generation, emphasizing structured reasoning, conceptual clarity, and analytical depth‚Äîespecially in domains centered on human behavior and social systems. üéØ Focus Areas Psychology Management &amp; Organizational Studies Sociology üìä MMLU Benchmark Results (100 samples per domain) Professional Psychology: 76% Management: 74% Sociology: 75% These results highlight PlaiTO‚Äôs strong performance in abstract, theory-heavy, and reasoning-driven tasks. üí° Why PlaiTO? Strong analytical and reasoning capabilities Better handling of complex human-centered problems Suitable for academic, educational, and research use cases Balanced performance across multiple humanities disciplines PlaiTO is ideal for conceptual analysis, case reasoning, academic discussion, and decision-support scenarios‚Äîwhile still requiring...</description><pubDate>Wed, 04 Feb 2026 17:50:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/alibidaran/992533889532684</guid></item><item><title>I submitted a "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning" Paper by Tanyu Chen, Tairan Chen, Kai shen , Zhenghua Bao, Zhihui Zhang, Man Yuan, Yi Shi From</title><link>https://huggingface.co/posts/rajkumarrawal/904260944141642</link><description>I submitted a "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning" Paper by Tanyu Chen, Tairan Chen, Kai shen , Zhenghua Bao, Zhihui Zhang, Man Yuan, Yi Shi From FlashLabs to Daily Papers on huggingface . Chroma 1.0 enables real time spoken dialogue with personalized voice cloning through discrete speech representations and interleaved text audio token scheduling. Chroma 1.0 , the world‚Äôs first open source, real time speech to speech model with voice cloning. FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning (2601.11141) See translation</description><pubDate>Wed, 04 Feb 2026 17:50:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/rajkumarrawal/904260944141642</guid></item><item><title>SECourses Musubi Trainer upgraded to V27 and FLUX 2, FLUX Klein, Z-Image training added with demo configs - amazing VRAM optimized - read the news</title><link>https://huggingface.co/posts/MonsterMMORPG/876855019351468</link><description>SECourses Musubi Trainer upgraded to V27 and FLUX 2, FLUX Klein, Z-Image training added with demo configs - amazing VRAM optimized - read the news App is here : https://www.patreon.com/posts/137551634 Full tutorial how to use and train : https://youtu.be/DPX3eBTuO_Y See translation</description><pubDate>Wed, 04 Feb 2026 17:50:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/876855019351468</guid></item><item><title>GLM-OCR: A Tiny 0.9B-Parameter Model That Punches Far Above Its Weight</title><link>https://huggingface.co/posts/Javedalam/705319100384927</link><description>GLM-OCR: A Tiny 0.9B-Parameter Model That Punches Far Above Its Weight Released today by Z.ai, GLM-OCR is a compact vision-language model designed specifically for document understanding. At just 0.9 billion parameters, it belongs to a new generation of lightweight AI systems proving that raw model size is no longer the only path to high performance. Despite its small footprint, GLM-OCR posts exceptionally strong results across major document benchmarks. It scores 94.6 on OmniDocBench, 94.0 on OCRBench, and an impressive 96.5 on UniMERNet for formula recognition‚Äînumbers that place it alongside, and in some cases ahead of, significantly larger specialized OCR models. The takeaway is clear: efficiency is rapidly becoming a defining feature of modern AI design. Developed by Z.ai, a research group focused on advancing multimodal foundation models, GLM-OCR reflects a broader shift toward highly optimized architectures that deliver serious capability without requiring massive compute...</description><pubDate>Wed, 04 Feb 2026 17:50:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Javedalam/705319100384927</guid></item><item><title>ü§î Do you have a Hugging Face Space that you wish you could programmatically restart to induce data refresh or some other behavior?</title><link>https://huggingface.co/posts/ZennyKenny/621232711062929</link><description>ü§î Do you have a Hugging Face Space that you wish you could programmatically restart to induce data refresh or some other behavior? üëâ Try Spaces Scheduler for this use case: https://github.com/kghamilton89/spaces-scheduler ‚û°Ô∏è Lightweight ‚û°Ô∏è Easy to setup ‚û°Ô∏è Just works üòé Happy to share some tooling with the Hugging Face community that's given me so much. See translation</description><pubDate>Wed, 04 Feb 2026 17:50:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/621232711062929</guid></item><item><title>PromptRL: Language Models as Co-Learners in Flow-Based Image Generation RL üöÄ</title><link>https://huggingface.co/posts/wangfuyun/891739381347198</link><description>PromptRL: Language Models as Co-Learners in Flow-Based Image Generation RL üöÄ We found two critical failure modes in flow-based RL: 1Ô∏è‚É£ Quality-Diversity Dilemma: High-quality models produce similar outputs, bottlenecking RL exploration 2Ô∏è‚É£ Prompt Linguistic Hacking: Models overfit to surface patterns‚Äîparaphrase the prompt and performance tanks Solution: **Jointly train LM + FM** ‚Äî the LM dynamically generates semantically-consistent but diverse prompt variants üìä Results: ‚Ä¢ GenEval: 0.97 ‚Ä¢ OCR accuracy: 0.98 ‚Ä¢ PickScore: 24.05 ‚Ä¢ 2√ó+ fewer rollouts than flow-only RL Paper: arxiv.org/abs/2602.01382 Code: github.com/G-U-N/UniRL #AI #TextToImage #ReinforcementLearning #Diffusion See translation</description><pubDate>Wed, 04 Feb 2026 17:50:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wangfuyun/891739381347198</guid></item><item><title>üëÄ While we're in between Kaggle competitions, users can work on earning this certificate from Duality AI! üôå</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/938156483514129</link><description>üëÄ While we're in between Kaggle competitions, users can work on earning this certificate from Duality AI! üôå Although these competitions are over, you can still submit results, achieve above a threshold, and earn the title of üí•OBJECT DETECTION EXPERTüí•. ‚è≥ No time limit üèÉ No competition üß† Just you against your own brain Start here: https://falcon.duality.ai/secure/documentation/certificate-challenge-object-detection?utm_source=linkedin&amp;utm_medium=post&amp;utm_campaign=HF People who already participated in one or more competitions last year already have a leg up! Your previous scores count towards this certificate, and now you just have to collect them all üòΩ As always, special thanks to our collaborators LunateAI and 3LC.AI ü´∞ See translation</description><pubDate>Wed, 04 Feb 2026 17:50:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/938156483514129</guid></item></channel></rss>