<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>üöÄ NEW DROP: run your own on-device LLM‚Äîin minutes, on any phone</title><link>https://huggingface.co/posts/yeonseok-zeticai/994151475169260</link><description>üöÄ NEW DROP: run your own on-device LLM‚Äîin minutes, on any phone Today we‚Äôre open-sourcing everything you need to put Qwen3-0.6B straight into a production-ready mobile app: üé• Watch Qwen3-0.6B chat in real time on any smartphones! üìä TPS benchmarks ‚Äì slides comparing token-per-second across heterogeneous mobile devices üíª Plug-and-play source ‚Äì Just Copy &amp; Run the source to your project for Android (Kotlin &amp; Java) and iOS (Swift). ü§û Cross-platform, one pipeline ‚Äì ZETIC.MLange auto-tunes kernels for every different devices, we‚Äôve tested. üë®‚Äçüíª Ready for production ‚Äì swap in your own model, re-benchmark with one command, publish. Get started Just Sign-up and check the playground project, QWEN-0.6B - https://mlange.zetic.ai/p/zetic-example/Qwen3-0.6B We built this to show that cloud-free LLMs are ready today. Dive in, fork it, and tag ZETIC.ai when you launch your own on-device assistant, game NPC, or offline content generator‚Äîwe‚Äôll spotlight the best projects. See translation</description><pubDate>Thu, 05 Jun 2025 13:35:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/994151475169260</guid></item><item><title>We just dropped SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics!</title><link>https://huggingface.co/posts/danaaubakirova/558502564618988</link><description>We just dropped SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics! check out the blog: https://huggingface.co/blog/smolvla read the technical report: SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics (2506.01844) access the model weights: lerobot/smolvla_base See translation</description><pubDate>Thu, 05 Jun 2025 13:35:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danaaubakirova/558502564618988</guid></item><item><title>I built an AI Website: ai-garden.netlify.app</title><link>https://huggingface.co/posts/VirtualOasis/960727410952690</link><description>I built an AI Website: ai-garden.netlify.app It is a curated garden of AI resources. It's my database for writing and research, organized by category and designed for quick access. Whether you're looking for learning materials, development tools, research papers, or industry news, everything's laid out in a clean, searchable format. Feel free to suggest new resources or improvements - this garden grows better with community input. üçª See translation</description><pubDate>Thu, 05 Jun 2025 13:35:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/VirtualOasis/960727410952690</guid></item><item><title>NEW: Real-time conversational AI models can now run 100% locally in your browser! ü§Ø</title><link>https://huggingface.co/posts/Xenova/927328273503233</link><description>NEW: Real-time conversational AI models can now run 100% locally in your browser! ü§Ø üîê Privacy by design (no data leaves your device) üí∞ Completely free... forever üì¶ Zero installation required, just visit a website ‚ö°Ô∏è Blazingly-fast WebGPU-accelerated inference Try it out: webml-community/conversational-webgpu For those interested, here's how it works: - Silero VAD for voice activity detection - Whisper for speech recognition - SmolLM2-1.7B for text generation - Kokoro for text to speech Powered by Transformers.js and ONNX Runtime Web! ü§ó I hope you like it! See translation</description><pubDate>Thu, 05 Jun 2025 13:35:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Xenova/927328273503233</guid></item><item><title>New DeepSeek-R1-0528 1.65-bit Dynamic GGUF!</title><link>https://huggingface.co/posts/danielhanchen/109493636647945</link><description>New DeepSeek-R1-0528 1.65-bit Dynamic GGUF! Run the model locally even easier! Will fit on a 192GB Macbook and run at 7 tokens/s. DeepSeek-R1-0528 GGUFs: unsloth/DeepSeek-R1-0528-GGUF Qwen3-8B DeepSeek-R1-0528 GGUFs: unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF And read our Guide: https://docs.unsloth.ai/basics/deepseek-r1-0528 See translation</description><pubDate>Thu, 05 Jun 2025 13:35:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/109493636647945</guid></item><item><title>We have been working on a project called</title><link>https://huggingface.co/posts/danieldk/385505075920135</link><description>We have been working on a project called kernels . kernels makes it possible to load compute kernels directly from the Hub! üöÄ We plan to give kernels a more proper introduction soon. But for those who have been following along, we are happy to announce a new release: - New layer API with torch.compile support. - Experimental support for loading Apple Silicon Metal ü§ò Kernels. - Generate wheels from Hub kernels for legacy deployments. Full release notes here: https://github.com/huggingface/kernels/releases/tag/v0.6.0 See translation</description><pubDate>Thu, 05 Jun 2025 13:35:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danieldk/385505075920135</guid></item><item><title>Agents &amp; MCP Hackathon Day 2</title><link>https://huggingface.co/posts/azettl/543079037838951</link><description>Agents &amp; MCP Hackathon Day 2 Again, a short night, but here are some updates from my Hackathon projects before starting night #3. I managed to get the first version of both submissions (custom Gradio component and MCP server) online! You can check the roundtable MCP where multiple AIs discuss your question and try to reach consensus: azettl/consilium_mcp . The Gradio component is here: azettl/gradio_consilium_roundtable . I placed my API keys in the env variables, so you can test without needing your own keys, but I will remove them soon as I did not find a limit setting in Sambanova. Still, you can check them by adding your own keys in the config tab. Looking forward to your feedback, there are still many days I can and will improve this. See translation</description><pubDate>Thu, 05 Jun 2025 13:35:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/azettl/543079037838951</guid></item><item><title>Past week was insanely packed for open AI! üò±</title><link>https://huggingface.co/posts/merve/599865137438975</link><description>Past week was insanely packed for open AI! üò± Luckily we picked some highlights for you ‚ù§Ô∏è lfg! üí¨ LLMs/VLMs &gt; Deepseek üê≥ released deepseek-ai/DeepSeek-R1-0528 , 38B model, only 0.2 and 1.4 points behind o3 in AIME 24/25 ü§Ø they also released an 8B distilled version based on Qwen3 (OS) deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d &gt; Xiaomi released MiMo-7B-RL (LLM for code and math) and MiMo-VL-7B-RL (VLM for visual reasoning, GUI agentic task and general use) (OS) üòç XiaomiMiMo/mimo-vl-68382ccacc7c2875500cd212 &gt; NVIDIA released , new reasoning model nvidia/Nemotron-Research-Reasoning-Qwen-1.5B &gt; DS: MiniMax released https://huggingface.co/MiniMaxAI/SynLogic , new 49k logical reasoning examples across 35 tasks including solving cipher, sudoku and more! üñºÔ∏è Image/Video Generation &gt; tencent released tencent/HunyuanPortrait , a new model for consistent portrait generation with SVD Research license. They also released tencent/HunyuanVideo-Avatar , audio driven avatar generation (OS) &gt;...</description><pubDate>Thu, 05 Jun 2025 13:35:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/599865137438975</guid></item><item><title>Yesterday was the day of vision language action models (VLAs)!</title><link>https://huggingface.co/posts/merve/820895577634325</link><description>Yesterday was the day of vision language action models (VLAs)! &gt; SmolVLA: open-source small VLA for robotics by Hugging Face LeRobot team ü§ñ Blog: https://huggingface.co/blog/smolvla Model: lerobot/smolvla_base &gt; Holo-1: 3B &amp; 7B web/computer use agentic VLAs by H Company üíª Model family: Hcompany/holo1-683dd1eece7eb077b96d0cbd Demo: https://huggingface.co/spaces/multimodalart/Holo1 Blog: https://huggingface.co/blog/Hcompany/holo1 super exciting times!! See translation</description><pubDate>Thu, 05 Jun 2025 13:35:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/820895577634325</guid></item><item><title>bumped into one of the OG reads today!! handwriting generation &amp; synthesis is still my favorite application of RNNs - supper amazed at how such a small model (3.6M params), trained overnight on cpu could reach such peak performance. Huge credit to the data (IAM-OnDBüî•) which was meticulously curated using an infra-red device to track pen position.</title><link>https://huggingface.co/posts/Jaward/820923946821201</link><description>bumped into one of the OG reads today!! handwriting generation &amp; synthesis is still my favorite application of RNNs - supper amazed at how such a small model (3.6M params), trained overnight on cpu could reach such peak performance. Huge credit to the data (IAM-OnDBüî•) which was meticulously curated using an infra-red device to track pen position. Try demo here: https://www.calligrapher.ai/ Code: https://github.com/sjvasquez/handwriting-synthesis See translation</description><pubDate>Thu, 05 Jun 2025 13:35:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/820923946821201</guid></item></channel></rss>