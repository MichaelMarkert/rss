<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Starts erasing! 🎉 🎉 🎉</title><link>https://huggingface.co/posts/piercus/167394123498038</link><description>Starts erasing! 🎉 🎉 🎉 This is made with a one-step SD1.5 LBM [1] eraser ! Data is open. Data pipeline is open. Training code is open. On our LBM fork : https://github.com/finegrain-ai/LBM [1] LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/piercus/167394123498038</guid></item><item><title>After training 𝐒𝐦𝐨𝐥𝐋𝐌𝟑 on 𝟑𝟖𝟒 𝐇𝟏𝟎𝟎𝐬 for nearly a month, I've come to realize something most people overlook: 𝐢𝐧𝐟𝐫𝐚𝐬𝐭𝐫𝐮𝐜𝐭𝐮𝐫𝐞 𝐢𝐬 𝐭𝐡𝐞 𝐦𝐚𝐤𝐞-𝐨𝐫-𝐛𝐫𝐞𝐚𝐤 𝐟𝐚𝐜𝐭𝐨𝐫 𝐢𝐧 𝐋𝐋𝐌 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠. 🔥</title><link>https://huggingface.co/posts/nouamanetazi/972464132222376</link><description>After training 𝐒𝐦𝐨𝐥𝐋𝐌𝟑 on 𝟑𝟖𝟒 𝐇𝟏𝟎𝟎𝐬 for nearly a month, I've come to realize something most people overlook: 𝐢𝐧𝐟𝐫𝐚𝐬𝐭𝐫𝐮𝐜𝐭𝐮𝐫𝐞 𝐢𝐬 𝐭𝐡𝐞 𝐦𝐚𝐤𝐞-𝐨𝐫-𝐛𝐫𝐞𝐚𝐤 𝐟𝐚𝐜𝐭𝐨𝐫 𝐢𝐧 𝐋𝐋𝐌 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠. 🔥 Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious 𝐍𝐂𝐂𝐋 𝐞𝐫𝐫𝐨𝐫𝐬, or when your expensive GPU cluster is running at 𝟔𝟎% 𝐞𝐟𝐟𝐢𝐜𝐢𝐞𝐧𝐜𝐲, the problem isn't your model. It's most probably a 𝐦𝐢𝐬𝐮𝐬𝐞 𝐨𝐟 𝐭𝐡𝐞 𝐡𝐚𝐫𝐝𝐰𝐚𝐫𝐞. 🛠️ Questions that seemed simple but had no clear answers: Why is 𝐌𝐨𝐄 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠 𝐬𝐥𝐨𝐰𝐞𝐫 𝐭𝐡𝐚𝐧 𝐝𝐞𝐧𝐬𝐞 𝐦𝐨𝐝𝐞𝐥𝐬? Which 𝐍𝐂𝐂𝐋 𝐟𝐥𝐚𝐠𝐬 should we actually set? How often should we checkpoint without killing throughput? That's why we built 𝐓𝐡𝐞 𝐒𝐦𝐨𝐥 𝐓𝐫𝐚𝐢𝐧𝐢𝐧𝐠 𝐏𝐥𝐚𝐲𝐛𝐨𝐨𝐤 📖: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the 𝐢𝐧𝐟𝐫𝐚𝐬𝐭𝐫𝐮𝐜𝐭𝐮𝐫𝐞 𝐥𝐚𝐲𝐞𝐫 that most teams get wrong. We validated real vs...</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nouamanetazi/972464132222376</guid></item><item><title>🤖 Did you know your voice might be cloned without your consent from just *one sentence* of audio?</title><link>https://huggingface.co/posts/meg/795374277994612</link><description>🤖 Did you know your voice might be cloned without your consent from just *one sentence* of audio? That's not great. So with @ frimelle , we brainstormed a new idea for developers who want to curb malicious use: ✨The Voice Consent Gate.✨ Details, code, here: https://huggingface.co/blog/voice-consent-gate See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/meg/795374277994612</guid></item><item><title>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the</title><link>https://huggingface.co/posts/sergiopaniego/207791817757812</link><description>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the @ TogetherAgent x @ huggingface workshop we hosted in our Paris office 🎃! Link: https://github.com/sergiopaniego/talks/blob/main/fine_tuning_with_trl/Fine%20tuning%20with%20TRL%20(Oct%2025).pdf See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/207791817757812</guid></item><item><title>AI Speedpainting of a Tranquil Mountain Temple!</title><link>https://huggingface.co/posts/wang12390/946400201713761</link><description>AI Speedpainting of a Tranquil Mountain Temple! Just upload one image then it will generate hand-drawn video. Please watch till the end, if you like the result, please upvote. See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wang12390/946400201713761</guid></item><item><title>🚀💡🌟 New Research Alert - ICCV 2025 (Oral)! 🌟🪄🚀</title><link>https://huggingface.co/posts/DmitryRyumin/213442382070723</link><description>🚀💡🌟 New Research Alert - ICCV 2025 (Oral)! 🌟🪄🚀 📄 Title: LoftUp: Learning a Coordinate-based Feature Upsampler for Vision Foundation Models 🔝 📝 Description: LoftUp is a coordinate-based transformer that upscales the low-resolution features of VFMs (e.g. DINOv2 and CLIP) using cross-attention and self-distilled pseudo-ground truth (pseudo-GT) from SAM. 👥 Authors: Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang 📅 Conference: ICCV, 19 – 23 Oct, 2025 | Honolulu, Hawai'i, USA 🇺🇸 📄 Paper: LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models (2504.14032) 🌐 Github Page: https://andrehuang.github.io/loftup-site 📁 Repository: https://github.com/andrehuang/loftup 🚀 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers 🚀 Added to the Foundation Models and Representation Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/foundation-models-and-representation-learning.md 📚...</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/213442382070723</guid></item><item><title>🚀👌🌟 New Research Alert - ICCV 2025 (Oral)! 🌟🤌🚀</title><link>https://huggingface.co/posts/DmitryRyumin/744756733617336</link><description>🚀👌🌟 New Research Alert - ICCV 2025 (Oral)! 🌟🤌🚀 📄 Title: Understanding Co-speech Gestures in-the-wild 🔝 📝 Description: JEGAL is a tri-modal model that learns from gestures, speech and text simultaneously, enabling devices to interpret co-speech gestures in the wild. 👥 Authors: @ sindhuhegde , K R Prajwal, Taein Kwon, and Andrew Zisserman 📅 Conference: ICCV, 19 – 23 Oct, 2025 | Honolulu, Hawai'i, USA 🇺🇸 📄 Paper: Understanding Co-speech Gestures in-the-wild (2503.22668) 🌐 Web Page: https://www.robots.ox.ac.uk/~vgg/research/jegal 📁 Repository: https://github.com/Sindhu-Hegde/jegal 📺 Video: https://www.youtube.com/watch?v=TYFOLKfM-rM 🚀 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers 🚀 Added to the Human Modeling Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/human-modeling.md 📚 More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers curated by @ DmitryRyumin 🔍 Keywords:...</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/744756733617336</guid></item><item><title>🚀 Big news for AI builders!</title><link>https://huggingface.co/posts/pagezyhf/128586778684407</link><description>🚀 Big news for AI builders! We’re thrilled to announce that the Qwen3-VL family of vision-language models is now available on Azure AI Foundry, thanks to our collaboration with Microsoft. We bring open-source innovation to enterprise-grade AI infrastructure, making it easier than ever for enterprise to deploy and scale the latest and greatest from models from hugging Face securely within Azure. 🔍 Highlights: - Deploy Qwen3-VL instantly via managed endpoints - Built-in governance, telemetry, and lifecycle management - True multimodal reasoning — vision, language, and code understanding - State-of-the-art performance, outperforming closed-source models like Gemini 2.5 Pro and GPT-5 - Available in both *Instruct* and *Thinking* modes, across 24 model sizes 👉 Get started today: search for Qwen3-VL in the Hugging Face Collection on Azure AI Foundry. See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/pagezyhf/128586778684407</guid></item><item><title>Kimi K2 is a bit disappointing by my expectations. It is on a par with Codex mini.</title><link>https://huggingface.co/posts/onekq/456763679689481</link><description>Kimi K2 is a bit disappointing by my expectations. It is on a par with Codex mini. onekq-ai/WebApp1K-models-leaderboard See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/456763679689481</guid></item><item><title>A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on ↗️</title><link>https://huggingface.co/posts/prithivMLmods/710644146568512</link><description>A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on ↗️ https://huggingface.co/blog/prithivMLmods/multimodal-ocr-vlms on behalf of strangervisionhf It discusses the latest trends in OCR models, the multilingual support offered by modern OCR systems, their unique capabilities, OCR benchmark model comparisons, transformer-based implementations, and strategies for streamlining transformers compatibility. See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/710644146568512</guid></item></channel></rss>