<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>We’ve reached a point where on device AI coding that is free, offline, and capable isn’t just a theoretical possibility; it’s sitting on my lap, barely warming my thighs.</title><link>https://huggingface.co/posts/mitkox/598805408500117</link><description>We’ve reached a point where on device AI coding that is free, offline, and capable isn’t just a theoretical possibility; it’s sitting on my lap, barely warming my thighs. My local MacBook Air setup includes a Qwen3 Coder Flash with a 1M context, Cline in a VSCode IDE. No internet, no cloud, no ID verification- this is the forbidden tech. Current stats: All agentic tools work great local, sandboxed, and MCP OK model output precision 17 tokens/sec. Not great, not terrible 65K tokens context, the model can do 1M, but let’s be real, my MacBook Air would probably achieve fusion before hitting that smoothly Standard backend and cache off for the test All inference and function calling happen locally, offline, untethered. The cloud didn’t even get a memo. See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/598805408500117</guid></item><item><title>Latest work on SWE-Bench 🐛</title><link>https://huggingface.co/posts/YerbaPage/437092763804097</link><description>Latest work on SWE-Bench 🐛 Our two new papers from the SJTU &amp; Huawei: Powered by DeepSeek-V3, we've achieved a new SOTA on the SWE-Bench benchmark! We introduce two innovative approaches: ⚔️ SWE-Debate: AI agents compete and "debate" to generate the best code fix. 🧠 SWE-Exp: An AI agent learns from past repair "experience" to solve new issues more efficiently. 👇 Explore the future of software development: SWE-Debate 📄 Paper: https://arxiv.org/abs/2507.23348 💻 Code: https://github.com/YerbaPage/SWE-Debate SWE-Exp 📄 Paper: https://arxiv.org/abs/2507.23361 💻 Code: https://github.com/YerbaPage/SWE-Exp See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/437092763804097</guid></item><item><title>🚀 Dhanishtha-2.0-preview-0825 Is Here</title><link>https://huggingface.co/posts/Abhaykoul/625756342268823</link><description>🚀 Dhanishtha-2.0-preview-0825 Is Here The Intermediate Thinking Model just leveled up again. With sharper reasoning, better tool use, and expanded capabilities, Dhanishtha-2.0-preview-0825 is now live and ready to impress. 🧠 What Makes Dhanishtha Special? Unlike typical CoT models that only thinks one time, Dhanishtha thinks iteratively: &gt; Think → Answer → Rethink → Improve → Rethink again if needed. 🔗 Try it now: HelpingAI/Dhanishtha-2.0-preview-0825 🔞 Dhanishtha NSFW Preview For those exploring more expressive and immersive roleplay scenarios, we’re also releasing: HelpingAI/Dhanishtha-nsfw A specialized version tuned for adult-themed interactions and character-driven roleplay. 🔗 Explore it here: HelpingAI/Dhanishtha-nsfw 💬 You can also try all of these live at chat.helpingai.co See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/625756342268823</guid></item><item><title>We’re excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100.</title><link>https://huggingface.co/posts/chintankp/680096865746882</link><description>We’re excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100. Super v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. Besides leading accuracy, it also delivers high throughput. Key features: - Leading accuracy on multi-step reasoning, math, coding, and function-calling - Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples - Fully transparent training data on HF (Nemotron-Post-Training-Dataset-v1) Try Super v1.5 on build.nvidia.com or download from Hugging Face See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/chintankp/680096865746882</guid></item><item><title>Cohere just dropped</title><link>https://huggingface.co/posts/merve/770620321222949</link><description>Cohere just dropped CohereLabs/command-a-vision-07-2025 , a 112B (dense!) vision LM &gt; based on SigLIP2 &amp; Command-A &gt; built for enterprise use cases 🔥 &gt; use with Inference Providers or transformers 🤗 read their blog https://huggingface.co/blog/CohereLabs/introducing-command-a-vision-07-2025 See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/770620321222949</guid></item><item><title>Qwen team did it again!!</title><link>https://huggingface.co/posts/AdinaY/564352975503737</link><description>Qwen team did it again!! They just released Qwen3-Coder-30B-A3B-Instruct on the hub🔥 Qwen/Qwen3-Coder-30B-A3B-Instruct ✨ Apache 2.0 ✨30B total / 3.3B active (128 experts, 8 top-k) ✨ Native 256K context, extendable to 1M via Yarn ✨ Built for Agentic Coding See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/564352975503737</guid></item><item><title>Introducing Camel-Doc-OCR-080125(v2), a document content-structure retrieval VLM designed for content extraction and summarization. This is the second model in the Camel Doc OCR VLM series, following Camel-Doc-OCR-062825(v1). The new version fixes formal table reconstruction issues in both en and zh language, achieving optimal performance for long-context inferences.🤗🐪</title><link>https://huggingface.co/posts/prithivMLmods/210600524016945</link><description>Introducing Camel-Doc-OCR-080125(v2), a document content-structure retrieval VLM designed for content extraction and summarization. This is the second model in the Camel Doc OCR VLM series, following Camel-Doc-OCR-062825(v1). The new version fixes formal table reconstruction issues in both en and zh language, achieving optimal performance for long-context inferences.🤗🐪 ⤷ Camel-Doc-OCR(v2) : prithivMLmods/Camel-Doc-OCR-080125 ⤷ Camel-Doc-OCR(v1) : prithivMLmods/Camel-Doc-OCR-062825 ⤷ Demo : prithivMLmods/core-OCR Multimodal Model Collections and Spaces: ➝ Camel-Doc-OCR : prithivMLmods/camel-doc-ocr-080125-688c0c61c5dba648756f31f8 ➝ Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a ➝ Multimodal Spaces : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 ➝ Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 . . . To know more about it, visit the model card of the respective model. !! See...</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/210600524016945</guid></item><item><title>🚀 Launch Alert: Dev-Stack-Agents</title><link>https://huggingface.co/posts/Parveshiiii/913365249348370</link><description>🚀 Launch Alert: Dev-Stack-Agents Meet your 50-agent senior AI team — principal-level experts in engineering, AI, DevOps, security, product, and more — all bundled into one modular repo. + Code. Optimize. Scale. Secure. - Full-stack execution, Claude-powered. No human bottlenecks. 🔧 Built for Claude Code Seamlessly plug into Claude’s dev environment: * 🧠 Each .md file = a fully defined expert persona * ⚙️ Claude indexes them as agents with roles, skills &amp; strategy * 🤖 You chat → Claude auto-routes to the right agent(s) * ✍️ Want precision? Just call @agent-name directly * 👥 Complex task? Mention multiple agents for team execution Examples: "@security-auditor please review auth flow for risks" "@cloud-architect + @devops-troubleshooter → design a resilient multi-region setup" "@ai-engineer + @legal-advisor → build a privacy-safe RAG pipeline" 🔗 https://github.com/Parveshiiii/Dev-Stack-Agents MIT License | Claude-Ready | PRs Welcome See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Parveshiiii/913365249348370</guid></item><item><title>Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, we’ve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources.</title><link>https://huggingface.co/posts/prithivMLmods/923940739727688</link><description>Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, we’ve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources. ✨ Multimodal-VLM-Thinking : prithivMLmods/Multimodal-VLM-Thinking ✨ Multimodal-VLM-OCR : prithivMLmods/Multimodal-VLM-OCR ✦ Models used in these spaces: ✨ Lumian-VLR-7B-Thinking : prithivMLmods/Lumian-VLR-7B-Thinking ✨ Enesidaon-VLR-7B-no-Thinking : prithivMLmods/Enesidaon-VLR-7B-no-Thinking ✨ GLM-4.1V-9B-Thinking : zai-org/GLM-4.1V-9B-Thinking ✨ DREX-062225-exp : prithivMLmods/DREX-062225-exp &amp; more ... ✦ Multimodal Model Collections and Spaces: ✨ Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a ✨ Multimodal Spaces : prithivMLmods/multimodal-...</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/923940739727688</guid></item><item><title>We've crossed 1 million repositories backed by Xet storage on Hugging Face! 🚀🚀🚀</title><link>https://huggingface.co/posts/jsulz/651298897017923</link><description>We've crossed 1 million repositories backed by Xet storage on Hugging Face! 🚀🚀🚀 You can follow along our progress converting the Hub from Git LFS to Xet at jsulz/ready-xet-go We have a lot of repos left to migrate, which means I have plenty of time to add more animations 🤪 See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/651298897017923</guid></item></channel></rss>