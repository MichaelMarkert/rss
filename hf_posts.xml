<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Qwen 3 can launch very soon. ðŸ‘€</title><link>https://huggingface.co/posts/merterbak/235850739835485</link><description>Qwen 3 can launch very soon. ðŸ‘€ https://github.com/ggml-org/llama.cpp/pull/12828 See translation</description><pubDate>Thu, 10 Apr 2025 17:19:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/235850739835485</guid></item><item><title>Google published a 69-page whitepaper on Prompt Engineering and its best practices, a must-read if you are using LLMs in production:</title><link>https://huggingface.co/posts/hesamation/789492772324435</link><description>Google published a 69-page whitepaper on Prompt Engineering and its best practices, a must-read if you are using LLMs in production: &gt; zero-shot, one-shot, few-shot &gt; system prompting &gt; chain-of-thought (CoT) &gt; ReAct LINK: https://www.kaggle.com/whitepaper-prompt-engineering &gt; code prompting &gt; best practices See translation</description><pubDate>Thu, 10 Apr 2025 17:19:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/789492772324435</guid></item><item><title>ðŸŽ¨ Designers, meet OmniSVG! This new model helps you create professional vector graphics from text/images, generate editable SVGs from icons to detailed characters, convert rasters to vectors, maintain style consistency with references, and integrate into your workflow.</title><link>https://huggingface.co/posts/fdaudens/513864434208106</link><description>ðŸŽ¨ Designers, meet OmniSVG! This new model helps you create professional vector graphics from text/images, generate editable SVGs from icons to detailed characters, convert rasters to vectors, maintain style consistency with references, and integrate into your workflow. @ OmniSVG See translation</description><pubDate>Thu, 10 Apr 2025 17:19:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/513864434208106</guid></item><item><title>ðŸ”¥ Yesterday was a fire day!</title><link>https://huggingface.co/posts/jasoncorkill/726469711226418</link><description>ðŸ”¥ Yesterday was a fire day! We dropped two brand-new datasets capturing Human Preferences for text-to-video and text-to-image generations powered by our own crowdsourcing tool! Whether you're working on model evaluation, alignment, or fine-tuning, this is for you. 1. Text-to-Video Dataset (Pika 2.2 model): Rapidata/text-2-video-human-preferences-pika2.2 2. Text-to-Image Dataset (Reve-AI Halfmoon): Rapidata/Reve-AI-Halfmoon_t2i_human_preference Letâ€™s train AI on AI-generated content with humans in the loop. Letâ€™s make generative models that actually get us. See translation</description><pubDate>Thu, 10 Apr 2025 17:19:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/726469711226418</guid></item><item><title>You can now run Llama 4 on your own local device! ðŸ¦™</title><link>https://huggingface.co/posts/danielhanchen/859959880164586</link><description>You can now run Llama 4 on your own local device! ðŸ¦™ Run our Dynamic 1.78-bit and 2.71-bit Llama 4 GGUFs: unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF You can run them on llama.cpp and other inference engines. See our guide here: https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4 See translation</description><pubDate>Thu, 10 Apr 2025 17:19:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/859959880164586</guid></item><item><title>Hi All,  I recently released two Audio datasets  which are generated using my earlier released dataset:</title><link>https://huggingface.co/posts/ajibawa-2023/282296415348325</link><description>Hi All, I recently released two Audio datasets which are generated using my earlier released dataset: ajibawa-2023/Children-Stories-Collection First Audio Dataset:https://huggingface.co/datasets/ajibawa-2023/Audio-Children-Stories-Collection-Large has 5600++ stories in .mp3 format. Second Audio Dataset:https://huggingface.co/datasets/ajibawa-2023/Audio-Children-Stories-Collection has 600 stories in .mp3 format. See translation</description><pubDate>Thu, 10 Apr 2025 17:19:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ajibawa-2023/282296415348325</guid></item><item><title>ðŸŽ‰ GitHub selected the ultralytics computer vision project, known for its YOLOv8/YOLO11 real-time SOTA computer vision models, as one of the top 5 open-source projects for first-time contributors in 2024!</title><link>https://huggingface.co/posts/fcakyon/248454580146320</link><description>ðŸŽ‰ GitHub selected the ultralytics computer vision project, known for its YOLOv8/YOLO11 real-time SOTA computer vision models, as one of the top 5 open-source projects for first-time contributors in 2024! Link to the project: https://github.com/ultralytics/ultralytics Link to the full GitHub 2024 recap report: https://github.blog/news-insights/octoverse/octoverse-2024/ See translation</description><pubDate>Thu, 10 Apr 2025 17:19:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fcakyon/248454580146320</guid></item><item><title>I got rejected from llama4.</title><link>https://huggingface.co/posts/Steven10429/887336506731659</link><description>I got rejected from llama4. So that means I can use quantinized model without following their TOS. Interesting. See translation</description><pubDate>Thu, 10 Apr 2025 17:19:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Steven10429/887336506731659</guid></item><item><title>What does it mean when models share the same bytes?</title><link>https://huggingface.co/posts/jsulz/855747629260036</link><description>What does it mean when models share the same bytes? We've investigated some quants and have seen that a considerable portion of quantizations of the same model share the same bytes and can be deduplicated to save considerable upload time for quantizers on the Hub. This space where we crack open a repo from @ bartowski shows we can get significant dedupe xet-team/quantization-dedup You can get a sense of why by reading this write-up: https://github.com/bartowski1182/llm-knowledge/blob/main/quantization/quantization.md But what about finetuned models? Since going into production the xet-team has migrated hundreds of repositories on the Hub to our storage layer, including classic "pre-Hub" open-source models like FacebookAI/xlm-roberta-large (XLM-R) from FacebookAI XLM-R, introduced in 2019, set new benchmarks for multilingual NLP by learning shared representations across 100 languages. It was then fine-tuned on English, Spanish, Dutch, and German, generating language-specific...</description><pubDate>Thu, 10 Apr 2025 17:19:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/855747629260036</guid></item><item><title>Why the  'how many r's in strawberry' prompt "breaks" llama4? :D</title><link>https://huggingface.co/posts/csabakecskemeti/971611835182279</link><description>Why the 'how many r's in strawberry' prompt "breaks" llama4? :D Quants DevQuasar/meta-llama.Llama-4-Scout-17B-16E-Instruct-GGUF See translation</description><pubDate>Thu, 10 Apr 2025 17:19:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/csabakecskemeti/971611835182279</guid></item></channel></rss>