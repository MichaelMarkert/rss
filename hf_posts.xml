<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸ¬ VEO3 Directors - All-in-One AI Video Creation Suite</title><link>https://huggingface.co/posts/ginipick/718905723783644</link><description>ğŸ¬ VEO3 Directors - All-in-One AI Video Creation Suite ğŸš€ What is VEO3 Directors? VEO3 Directors is a revolutionary end-to-end AI video creation platform that transforms your ideas into cinematic reality. From story conception to final video with synchronized audio - all in one seamless workflow! ğŸ”— Try It Now ginigen/VEO3-Directors ginigen/VEO3-Free ginigen/VEO3-Free-mirror âœ¨ Key Features ğŸ“ Story Seed Generator ğŸ² Instantly generate creative story ideas across multiple genres ğŸŒ Bilingual support (English/Korean) ğŸ­ Rich categories: Genre, Setting, Characters, and more ğŸ¥ AI Script &amp; Prompt Crafting ğŸ’¬ Powered by Friendli API for Hollywood-quality prompts ğŸ¤– AI Director writes detailed cinematography instructions ğŸ¬ Professional elements: camera movements, lighting, VFX ğŸ¬ Video + Audio Generation ğŸ¨ Wan2.1-T2V-14B for stunning visual quality âš¡ NAG 4-step inference - 10x faster generation ğŸµ MMAudio auto-generates matching soundscapes ğŸ›ï¸ Full control over resolution, duration, and style...</description><pubDate>Mon, 16 Jun 2025 09:27:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/718905723783644</guid></item><item><title>Beginnerâ€™s Guide â€” Generate Videos With SwarmUI</title><link>https://huggingface.co/posts/MonsterMMORPG/453570470195139</link><description>Beginnerâ€™s Guide â€” Generate Videos With SwarmUI Full article here please check out : https://huggingface.co/blog/MonsterMMORPG/beginners-guide-generate-videos-with-swarmui Proper ComfyUI backend then SwarmUI installation tutorial : https://youtu.be/fTzlQ0tjxj0 Proper ComfyUI backend then SwarmUI installation tutorial on RunPod : https://youtu.be/R02kPf9Y3_w See translation</description><pubDate>Mon, 16 Jun 2025 09:27:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/453570470195139</guid></item><item><title>âš¡ FusionX Enhanced Wan 2.1 I2V (14B) ğŸ¬</title><link>https://huggingface.co/posts/seawolf2357/480409853177984</link><description>âš¡ FusionX Enhanced Wan 2.1 I2V (14B) ğŸ¬ ğŸš€ Revolutionary Image-to-Video Generation Model Generate cinematic-quality videos in just 8 steps! Heartsync/WAN2-1-fast-T2V-FusioniX âœ¨ Key Features ğŸ¯ Ultra-Fast Generation: Premium quality in just 8-10 steps ğŸ¬ Cinematic Quality: Smooth motion with detailed textures ğŸ”¥ FusionX Technology: Enhanced with CausVid + MPS Rewards LoRA ğŸ“ Optimized Resolution: 576Ã—1024 default settings âš¡ 50% Speed Boost: Faster rendering compared to base models ğŸ› ï¸ Technical Stack Base Model: Wan2.1 I2V 14B Enhancement Technologies: ğŸ”— CausVid LoRA (1.0 strength) - Motion modeling ğŸ”— MPS Rewards LoRA (0.7 strength) - Detail optimization Scheduler: UniPC Multistep (flow_shift=8.0) Auto Prompt Enhancement: Automatic cinematic keyword injection ğŸ¨ How to Use Upload Image - Select your starting image Enter Prompt - Describe desired motion and style Adjust Settings - 8 steps, 2-5 seconds recommended Generate - Complete in just minutes! ğŸ’¡ Optimization Tips âœ… Recommended Settings:...</description><pubDate>Mon, 16 Jun 2025 09:27:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/seawolf2357/480409853177984</guid></item><item><title>I realised a small documentation on how to make your own LM architecture called [LM-From-Scratch](</title><link>https://huggingface.co/posts/FlameF0X/196232708808828</link><description>I realised a small documentation on how to make your own LM architecture called [LM-From-Scratch]( https://github.com/FlameF0X/LM-From-Scratch ) See translation</description><pubDate>Mon, 16 Jun 2025 09:27:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/FlameF0X/196232708808828</guid></item><item><title>New diffusion model for text-to-image and video-to-world generation: Cosmos Predict-2 ğŸ‘½</title><link>https://huggingface.co/posts/a-r-r-o-w/709852031491261</link><description>New diffusion model for text-to-image and video-to-world generation: Cosmos Predict-2 ğŸ‘½ Model collection: nvidia/cosmos-predict2-68028efc052239369a0f2959 Diffusers support: https://github.com/huggingface/diffusers/pull/11695 Documentation: https://huggingface.co/docs/diffusers/main/en/api/pipelines/cosmos These are results with the 2B param model. Imagine what you could do with the 14B version! Go check it out now! See translation</description><pubDate>Mon, 16 Jun 2025 09:27:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/a-r-r-o-w/709852031491261</guid></item><item><title>You can now edit operations with a discrete flow model, supercoolğŸ‘! It's amazing to see the progress on DFM within one year since its introduction - literally my litmus test for how fast the field is progressing:</title><link>https://huggingface.co/posts/Jaward/740369227920658</link><description>You can now edit operations with a discrete flow model, supercoolğŸ‘! It's amazing to see the progress on DFM within one year since its introduction - literally my litmus test for how fast the field is progressing: 1st Introduced (2024): https://arxiv.org/abs/2402.04997 Discrete Flow Matching (2024): https://arxiv.org/abs/2407.15595 Edit Discrete Flow (2025): https://arxiv.org/pdf/2506.09018 Looking forward to a SaaS level reach like that of dLLMs e.g Mercury by inception labs ğŸš€ See translation</description><pubDate>Mon, 16 Jun 2025 09:27:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/740369227920658</guid></item><item><title>Can AI models trained solely on 100% synthetic data achieve top-tier accuracy in real-world object detection?</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/399941870774449</link><description>Can AI models trained solely on 100% synthetic data achieve top-tier accuracy in real-world object detection? ğŸ‘‰ Sergio Sanz, PhD just proved it while winning Duality AIâ€™s Synthetic-to-Real Object Detection Challenge using Falcon-generated imagery. His model achieved perfect real-world detection accuracy without a single real image in the training loop. In this blog, Dr. Sanz walks us through his method, which includes the design and training of an advanced pipeline to achieve 100% detection accuracy. His full technical breakdown covers: ğŸ“ Synthetic-only training ğŸ“ Data augmentation with an ensemble learning approach for better generalization ğŸ“ Custom occlusion generation ğŸ“ A Faster R-CNN model fine-tuned with Falcon generated data ğŸ“ And much more! The results speak for themselves! ğŸ“– Read the blog here: https://www.duality.ai/blog/leveraging-synthetic-data-for-real-world-object-detection Congratulations Sergio! We can't wait to see what you do next. ğŸ”” Ready to take on the next...</description><pubDate>Mon, 16 Jun 2025 09:27:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/399941870774449</guid></item><item><title>New Research: Theoretical Foundations for In-Context Learning in Transformers</title><link>https://huggingface.co/posts/codelion/378799954783125</link><description>New Research: Theoretical Foundations for In-Context Learning in Transformers I'm excited to share our latest theoretical work that formally proves an interesting property of large language models: base transformer models can approximate fine-tuned capabilities using only inference-time techniques like in-context learning. The core question we investigated: Can specialized behaviors typically acquired through expensive supervised fine-tuning be elicited from base models without any parameter updates? Our theoretical contribution: We provide a formal proof, grounded in the Turing completeness of transformers, showing that this is indeed possible under certain assumptions. The work establishes mathematical bounds on the minimal dataset sizes needed for approximation. Key theoretical results: - For text generation tasks: O(mV/ÎµÂ²) examples suffice (where m = number of contexts, V = vocabulary size, Îµ = error tolerance) - For linear classification: O(d/Îµ) examples (where d = input...</description><pubDate>Mon, 16 Jun 2025 09:27:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/378799954783125</guid></item><item><title>Started</title><link>https://huggingface.co/posts/MikeDoes/954423414183854</link><description>Started aistatuscodes as a new project to create codes to understand AI performance better. Going to be posting daily here and on instagram until we get to 100m downloads :) https://www.instagram.com/MikeDoesDo/ Follow along the journey! See translation</description><pubDate>Mon, 16 Jun 2025 09:27:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/954423414183854</guid></item><item><title>11 Types of JEPA</title><link>https://huggingface.co/posts/Kseniase/659118746872319</link><description>11 Types of JEPA Since Meta released the newest V-JEPA 2 this week, we thought it's a good time to revisit a few other interesting JEPA variants. JEPA, or Joint Embedding Predictive Architecture, a self-supervised learning framework that predicts the latent representation of a missing part of the input. Here are 11 JEPA types that you should know about: 1. V-JEPA 2 -&gt; V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning (2506.09985) Trained on 1M+ hours of internet videos and a little bit of robot interaction data, V-JEPA 2 can watch, understand, answer questions, and help robots plan and act in physical world 2. Time-Series-JEPA (TS-JEPA) -&gt; Time-Series JEPA for Predictive Remote Control under Capacity-Limited Networks (2406.04853) It's a time-series predictive model that learns compact, meaningful representations. A self-supervised semantic actor then uses them to generate control commands without raw data 3. Denoising JEPA (D-JEPA) -&gt; Denoising...</description><pubDate>Mon, 16 Jun 2025 09:27:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/659118746872319</guid></item></channel></rss>