<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Transformers v5 just landed! ğŸš€</title><link>https://huggingface.co/posts/IlyasMoutawwakil/848772925772411</link><description>Transformers v5 just landed! ğŸš€ It significantly unifies and reduces modeling code across architectures, while opening the door to a whole new class of performance optimizations. My favorite new feature? ğŸ¤” The new dynamic weight loader + converter. Hereâ€™s why ğŸ‘‡ Over the last few months, the core Transformers maintainers built an incredibly fast weight loader, capable of converting tensors on the fly while loading them in parallel threads. This means weâ€™re no longer constrained by how parameters are laid out inside the safetensors weight files. In practice, this unlocks two big things: - Much more modular modeling code. You can now clearly see how architectures build on top of each other (DeepSeek v2 â†’ v3, Qwen v2 â†’ v3 â†’ MoE, etc.). This makes shared bottlenecks obvious and lets us optimize the right building blocks once, for all model families. - Performance optimizations beyond what torch.compile can do alone. torch.compile operates on the computation graph, but it canâ€™t change...</description><pubDate>Thu, 29 Jan 2026 09:50:43 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/IlyasMoutawwakil/848772925772411</guid></item><item><title>You can now run Kimi K2.5 locally! ğŸ”¥</title><link>https://huggingface.co/posts/danielhanchen/602916263790271</link><description>You can now run Kimi K2.5 locally! ğŸ”¥ We shrank the 1T model to 240GB (-60%) via Dynamic 1-bit. Get &gt;40 tok/s on 242GB or 622GB VRAM/RAM for near full precision. GGUF: unsloth/Kimi-K2.5-GGUF Guide: https://unsloth.ai/docs/models/kimi-k2.5 See translation</description><pubDate>Thu, 29 Jan 2026 09:50:43 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/602916263790271</guid></item><item><title>ovi054/LTX-2-19b-Squish-LoRA</title><link>https://huggingface.co/posts/ovi054/176731931119322</link><description>ovi054/LTX-2-19b-Squish-LoRA âš¡ I trained a Squish LoRA for LTX-2. Upload an image and give prompt "squish it" to get the squish video. Demo output videos are attached. ğŸ‘‰Try it now: ovi054/LTX-2-19b-Squish-LoRA ovi054/ltx-2-Audio-to-Video See translation</description><pubDate>Thu, 29 Jan 2026 09:50:43 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/176731931119322</guid></item><item><title>Just built my entire AI Engineer portfolio by pasting 2 links (GitHub and LinkedIn)  into</title><link>https://huggingface.co/posts/RakshitAralimatti/297622038982343</link><description>Just built my entire AI Engineer portfolio by pasting 2 links (GitHub and LinkedIn) into moonshotai Kimi 2.5. That's it. That's the workflow. Zero coding. Zero iteration. Zero "make the button bigger." See for yourself: https://rakshit2020.github.io/rakshitaralimatti.github.io/ The model: âœ… Scraped my GitHub repos automatically âœ… Pulled my experience from LinkedIn âœ… Designed an Aurora Glass theme âœ… Mapped every skill to projects âœ… Added animations I'd never code myself See translation</description><pubDate>Thu, 29 Jan 2026 09:50:43 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/297622038982343</guid></item><item><title>ğŸ’¥</title><link>https://huggingface.co/posts/alvarobartt/259854920222577</link><description>ğŸ’¥ hf-mem v0.4.1 now also estimates KV cache memory requirements for any context length and batch size with the --experimental flag! uvx hf-mem --model-id ... --experimental will automatically pull the required information from the Hugging Face Hub to include the KV cache estimation, when applicable. ğŸ’¡ Alternatively, you can also set the --max-model-len , --batch-size and --kv-cache-dtype arguments (Ã  la vLLM) manually if preferred. See translation</description><pubDate>Thu, 29 Jan 2026 09:50:43 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/alvarobartt/259854920222577</guid></item><item><title>New TRL + OpenEnv example! ğŸ’¥</title><link>https://huggingface.co/posts/sergiopaniego/799346316619936</link><description>New TRL + OpenEnv example! ğŸ’¥ Fine tune an LLM for playing Sudoku using an RL env via OpenEnv Includes a script that runs on 1 or multiple GPUs with vLLM, plus a Colab-ready notebook. Enjoy! Notebook: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/openenv_sudoku_grpo.ipynb Script: https://github.com/huggingface/trl/blob/main/examples/scripts/openenv/sudoku.py See translation</description><pubDate>Thu, 29 Jan 2026 09:50:43 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/799346316619936</guid></item><item><title>kernels 0.12 is out! ğŸ‰</title><link>https://huggingface.co/posts/danieldk/874011196597638</link><description>kernels 0.12 is out! ğŸ‰ Changes: * Support for kernel version branches to gracefully roll out kernel API changes. * Support for PyTorch 2.10. * kernel-builder is now merged into the kernels repo. * Initial support for standardized kernel benchmarks. https://github.com/huggingface/kernels/releases/tag/v0.12.0 See translation</description><pubDate>Thu, 29 Jan 2026 09:50:43 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danieldk/874011196597638</guid></item><item><title>ğŸ“ŒSame day, Two Releases.</title><link>https://huggingface.co/posts/imnotkitty/264000680649196</link><description>ğŸ“ŒSame day, Two Releases. Jan 27th just got interesting on Open-source AI modles. âœ…Kimi K2.5: How to make models "think" across text and vision natively? moonshotai/Kimi-K2.5 âœ…DeepSeek-OCR 2: How to make models "see" more like humans, not scanners? deepseek-ai/DeepSeek-OCR-2 One focuses on depth of reasoning, the other on precision of vision. What's the key differentiator for a multimodal model in your view: raw power or computational elegance? See translation</description><pubDate>Thu, 29 Jan 2026 09:50:43 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/imnotkitty/264000680649196</guid></item><item><title>Big day in open source AI!!</title><link>https://huggingface.co/posts/AdinaY/365964215058126</link><description>Big day in open source AI!! âœ¨ DeepSeek released OCR2 ğŸ’¥ deepseek-ai/DeepSeek-OCR-2 âœ¨ Kimi K2.5 just landed ğŸ”¥ moonshotai/Kimi-K2.5 With the Chinese Spring Festival 3 weeks away, whatâ€™s coming next?ğŸ‘€ See translation</description><pubDate>Thu, 29 Jan 2026 09:50:43 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/365964215058126</guid></item><item><title>âœ… New Article: *Post-Transformer Decision Cores* (v0.1)</title><link>https://huggingface.co/posts/kanaria007/711602614783831</link><description>âœ… New Article: *Post-Transformer Decision Cores* (v0.1) Title: ğŸš€ Post-Transformer Decision Cores: Goal-Native Engines Beyond LLMs ğŸ”— https://huggingface.co/blog/kanaria007/post-tranformer-decision-cores --- Summary: Transformers are powerfulâ€”but in SI-Core theyâ€™re *not the essence of intelligence*. A *Decision Core* is anything that satisfies the *Jump contracts* (OBS/ETH/MEM/ID/EVAL + RML), and those contracts donâ€™t require next-token prediction. This article sketches what â€œpost-Transformerâ€ looks like in practice: *goal-native, structure-aware controllers* that may use LLMs as toolsâ€”but donâ€™t depend on them as the runtime brain. &gt; Donâ€™t relax the contracts. &gt; Replace the engine behind them. --- Why It Matters: â€¢ Makes LLMs *optional*: shift them to â€œgenesis / exploration / explanation,â€ while routine high-stakes Jumps run on structured cores â€¢ Improves boring-but-critical properties: *determinism (CAS), fewer inconsistencies (SCI), fewer ETH violations (EAI), better rollback...</description><pubDate>Thu, 29 Jan 2026 09:50:43 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/711602614783831</guid></item></channel></rss>