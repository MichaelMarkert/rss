<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board ‚Äî including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. ü§óüî•</title><link>https://huggingface.co/posts/prithivMLmods/844227545389355</link><description>The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board ‚Äî including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. ü§óüî• ‚ö° Space / App: prithivMLmods/Qwen3-VL-HF-Demo The model‚Äôs demo supports a wide range of tasks, including; Image Inference, Video Inference, PDF Inference, Image Captioning (VLA), GIF Inference. ‚ö° Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Thanks for granting the blazing-fast Zero GPU access, @ merve üôè ‚ö° Other Pages &gt; Github: https://github.com/prithivsakthiur/qwen3-vl-hf-demo &gt; Multimodal VLMs July'25 : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 &gt; VL caption ‚Äî &lt; Sep 15 ‚Äô25 : prithivMLmods/vl-caption-sep-15-25-68c7f6d737985c63c13e2391 &gt; Multimodal...</description><pubDate>Tue, 14 Oct 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/844227545389355</guid></item><item><title>9 Powerful AI Video Generation Tools</title><link>https://huggingface.co/posts/Kseniase/543365627154110</link><description>9 Powerful AI Video Generation Tools Since Sora 2 is on fire these weeks, reminding us what high-quality video generation should look like, we decided you really need this list of video generation tools ‚Äì great alternatives or complements to it. 1. Sora 2 ‚Üí https://openai.com/sora/ It needs no introduction, but this OpenAI‚Äôs text-to-video model produces short, ultra-realistic clips across styles (cinematic, photorealistic, animated, etc.) with synced audio 2. Google Veo 3 (Gemini Video Generation) ‚Üí https://aistudio.google.com/models/veo-3 Part of Gemini AI. Generates 8-second high-fidelity videos from text or images with native sound: background soundtracks and realistic voices with near-perfect lip sync 3. Runway (Gen-4 by Runway ML) ‚Üí https://runwayml.com/ Text, image, or video-to-video generation with advanced editing like changing lighting, weather, camera angles or replacing objects. Popular in AI filmmaking 4. Pika Labs ‚Üí https://pollo.ai/m/pika-ai Provides creative, often...</description><pubDate>Tue, 14 Oct 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/543365627154110</guid></item><item><title>How to compress long code context? üìö</title><link>https://huggingface.co/posts/YerbaPage/639392890035292</link><description>How to compress long code context? üìö Check out our LongCodeZip! Paper just got accepted to ASE 2025. üî• Code &amp; Demo: https://github.com/YerbaPage/LongCodeZip Paper: LongCodeZip: Compress Long Context for Code Language Models (2510.00446) See translation</description><pubDate>Tue, 14 Oct 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/639392890035292</guid></item><item><title>Ovi is Local Version of VEO 3 &amp; SORA 2 - The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer on Windows even with a 6GB GPUs - Full Tutorial for Windows, RunPod and Massed Compute - Gradio App &gt;</title><link>https://huggingface.co/posts/MonsterMMORPG/350754844731753</link><description>Ovi is Local Version of VEO 3 &amp; SORA 2 - The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer on Windows even with a 6GB GPUs - Full Tutorial for Windows, RunPod and Massed Compute - Gradio App &gt; https://youtu.be/T00VmkMQRPQ Tutorial : https://youtu.be/T00VmkMQRPQ Forget waiting lists and expensive APIs. The era of closed-off, corporate-controlled AI video generation is soon over. This is Ovi : The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer‚Äîeven with a 6GB GPU! This isn't just a demo; it's a full, step-by-step revolution. Tutorial Info In this ultimate A-Z guide, I'll show you EVERYTHING you need to know to install and master this Sora 2 and VEO3 like AI. We'll go from zero to generating incredible talking videos from text or a single image. üî• In This Tutorial, You Will Learn To: üéì Master the Ultimate SORA 2 and VEO 3...</description><pubDate>Tue, 14 Oct 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/350754844731753</guid></item><item><title>4 must-try AI video models in 2026 ‚Äî all in one place on iMini! üé¨‚ú®</title><link>https://huggingface.co/posts/404Zen/731905286538257</link><description>4 must-try AI video models in 2026 ‚Äî all in one place on iMini! üé¨‚ú® Featuring Sora 2, Veo 3, Wan 2.5, and Seedance 3.0 ‚Äî no invite code, no watermark! Try it now üëâ https://imini.com/ See translation</description><pubDate>Tue, 14 Oct 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/404Zen/731905286538257</guid></item><item><title>You think those playful puppies are real? üê∂‚ú®</title><link>https://huggingface.co/posts/Monica997/870861781065582</link><description>You think those playful puppies are real? üê∂‚ú® Nope! It‚Äôs a video I created using iMini‚Äôs newly integrated Sora 2 model ‚Äî no invite code, no watermark, just one simple text prompt to generate dynamic videos in seconds! üé¨ Limited-time offer: members can create without using credits! üëâ Try it now: https://imini.com/ See translation</description><pubDate>Tue, 14 Oct 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Monica997/870861781065582</guid></item><item><title>vanta-research/apollo-astralis-8b</title><link>https://huggingface.co/posts/unmodeled-tyler/162043774477631</link><description>vanta-research/apollo-astralis-8b I ran the same prompt sequence on my model Apollo Astralis 8B and Hermes4 14B from Nous Research.. The raw chat logs were then given to 3 different architectures (DeepSeek 3.1, LLaMA 405B, GPT-OSS 120B). All 3 models were given the same, simple instructions to analyze the logs and determine which model performed better. All 3 independently chose Astralis 8B for stronger reasoning, alignment, transparency, and collaborative language. Astralis 8B is designed to keep you motivated by applying warm collaborative language mixed with rigorous logical reasoning and problem solving capabilities. Give Astralis a try! See translation</description><pubDate>Tue, 14 Oct 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/162043774477631</guid></item><item><title>‚úÖ New Article: *Procrastination as a Structural Loop*</title><link>https://huggingface.co/posts/kanaria007/687620663929284</link><description>‚úÖ New Article: *Procrastination as a Structural Loop* Title: ‚è≥ Procrastination as a Structural Loop: Why ‚ÄúI Know, But I Don‚Äôt Act‚Äù Persists ‚Äî and How Protocols Contain It üîó https://huggingface.co/blog/kanaria007/procrastination-as-structural-loop --- Summary: Procrastination isn‚Äôt laziness ‚Äî it‚Äôs a *miswired loop*. When task threat and reward prediction skew the thresholds, the *jump-generator* routes to *delay*, *reflexia* amplifies avoidance, and the *memory-loop* reinforces ‚Äúnot now.‚Äù Once we treat it as structure, we can *reindex costs, lower entry friction, and relink intention to execution*. &gt; Motivation wavers. &gt; *Loops can be rewired.* --- Why It Matters: ‚Ä¢ Reframes procrastination as *auditable mechanics*, not a moral flaw ‚Ä¢ Turns ‚Äústuck‚Äù into a stepwise *rollback ‚Üí small jump ‚Üí stable loop* ‚Ä¢ Applies from individuals to teams (deadlines, sprints, reviews) --- What‚Äôs Inside: ‚Ä¢ The Procrastination Loop: trigger ‚Üí aversion tag ‚Üí avoidance jump ‚Üí guilt overlay ‚Üí recurrence ‚Ä¢...</description><pubDate>Tue, 14 Oct 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/687620663929284</guid></item><item><title>David + Imagenet = high% val.</title><link>https://huggingface.co/posts/AbstractPhil/715642693504510</link><description>David + Imagenet = high% val. AbstractPhil/gated-david https://github.com/AbstractEyes/lattice_vocabulary/blob/master/src/geovocab2/train/model/core/david.py David's code has been released. I am currently setting up a trainer and will release the process on how to condition David to behave. This isn't the easiest process, but it's necessary to run David on a curriculum rather than simply feeding the model with cross-entropy and hoping for the best. David's internals involve a clock mechanism that allows direct control of David's freeze/unfreeze mechanisms at runtime - allowing for many opinions to be generated simultaneously. David is multiple models in one, not just one - and yet David is single-shot oriented. The prototype to the route of thought that led me to find the Cantor's Stairs positional encodings solution and the prototype to ViT-Zana, ViT-Beatrix, ViT-Beatrix-Dual-Block, and today the direct porting of David's complex architecture and the process to train David has...</description><pubDate>Tue, 14 Oct 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AbstractPhil/715642693504510</guid></item><item><title>Arm will be @ PyTorch Conference, Join Us!</title><link>https://huggingface.co/posts/sondhiArm/422203128655094</link><description>Arm will be @ PyTorch Conference, Join Us! Join us on site October 22-23 to see how Arm empowers developers to build and deploy AI applications with ease using PyTorch and ExecuTorch. Learn about the latest AI technologies from Arm and our ecosystem while expanding your professional network alongside like-minded AI engineers. Learn more here: https://huggingface.co/blog/Arm/arm-at-pytorch-conference See translation</description><pubDate>Tue, 14 Oct 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sondhiArm/422203128655094</guid></item></channel></rss>