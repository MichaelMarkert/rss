<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>2025.1  - DeepSeek entered the scene, backed by High Flyer Quant</title><link>https://huggingface.co/posts/AdinaY/447609737740306</link><description>2025.1 - DeepSeek entered the scene, backed by High Flyer Quant 2026.1 - IQuest enters the game, backed by Uniquant Quant üìà and launching IQuest-Coder on huggingface https://huggingface.co/collections/IQuestLab/iquest-coder ‚ú® 40B models: Instruct / Thinking / Loop ‚ú® Loop = MoE-level performance with only ~5% extra training cost ‚ú® Native 128K context See translation</description><pubDate>Tue, 06 Jan 2026 13:40:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/447609737740306</guid></item><item><title>The Architecture of 2026: Beyond the Token Trap üöÄ</title><link>https://huggingface.co/posts/mindchain/406965796956131</link><description>The Architecture of 2026: Beyond the Token Trap üöÄ We are witnessing a tectonic shift in Transformer architecture. It‚Äôs no longer just about "predicting the next token"‚Äîit‚Äôs about executing latent plans on a high-speed data highway. What happens when we combine DeepSeek‚Äôs stability with Google‚Äôs strategic intelligence? 1Ô∏è‚É£ The Infrastructure: DeepSeek‚Äôs mHC Moving from a single-lane residual stream to a multi-lane highway. Using the Birkhoff Polytope, mHC ensures mathematical stability (Identity Mapping) while routing specialized data through dedicated lanes. 2Ô∏è‚É£ The Intelligence: Google‚Äôs Meta-Controller An internal AI unit that lives inside the Transformer. It escapes the "Token Trap" by extracting data to create a latent plan, steering the model via Temporal Abstraction. The Synergy: In a Topological Transformer, the Meta-Controller finally has the "dedicated lanes" it needs to steer complex reasoning without causing gradient explosions. We aren't just making models bigger; we are...</description><pubDate>Tue, 06 Jan 2026 13:40:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mindchain/406965796956131</guid></item><item><title>üöÄ Excited to share: The vLLM container for NVIDIA DGX Spark!</title><link>https://huggingface.co/posts/Hellohal2064/482165788562535</link><description>üöÄ Excited to share: The vLLM container for NVIDIA DGX Spark! I've been working on getting vLLM to run natively on the new DGX Spark with its GB10 Blackwell GPU (SM121 architecture). The results? 2.5x faster inference compared to llama.cpp! üìä Performance Highlights: ‚Ä¢ Qwen3-Coder-30B: 44 tok/s (vs 21 tok/s with llama.cpp) ‚Ä¢ Qwen3-Next-80B: 45 tok/s (vs 18 tok/s with llama.cpp) üîß Technical Challenges Solved: ‚Ä¢ Built PyTorch nightly with CUDA 13.1 + SM121 support ‚Ä¢ Patched vLLM for Blackwell architecture ‚Ä¢ Created custom MoE expert configs for GB10 ‚Ä¢ Implemented TRITON_ATTN backend workaround üì¶ Available now: ‚Ä¢ Docker Hub: docker pull hellohal2064/vllm-dgx-spark-gb10:latest ‚Ä¢ HuggingFace: huggingface.co/Hellohal2064/vllm-dgx-spark-gb10 The DGX Spark's 119GB unified memory opens up possibilities for running massive models locally. Happy to connect with others working on the DGX Spark Blackwell! See translation</description><pubDate>Tue, 06 Jan 2026 13:40:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Hellohal2064/482165788562535</guid></item><item><title>üëâ What happened in AI in 2025? üëà</title><link>https://huggingface.co/posts/pcuenq/421927498996428</link><description>üëâ What happened in AI in 2025? üëà We prepared the 2025 version of the HF AI Timeline Grid, highlighting open vs API-based model releases, and allowing you to browse and filter by access, modality, and release type! Play with it here: 2025-ai-timeline/2025-ai-timeline Here's my personal quarterly TL;DR: 1Ô∏è‚É£ Q1 ‚Äî Learning to Reason Deepseek not only releases a top-notch reasoning model, but shows how to train them and compete with closed frontier models. OpenAI debuts Deep Research. Significant milestones: DeepSeek R1 &amp; R1-Zero, Qwen 2.5 VL, OpenAI Deep Research, Gemini 2.5 Pro (experimental) 2Ô∏è‚É£ Q2 ‚Äî Multimodality and Coding More LLMs embrace multimodality by default, and there's a surge in coding agents. Strong vision, audio, and generative models emerge. Significant milestones: Llama 4, Qwen 3, Imagen 4, OpenAI Codex, Google Jules, Claude 4 3Ô∏è‚É£ Q3 ‚Äî "Gold" rush, OpenAI opens up, the community goes bananas Flagship models get gold in Math olympiads and hard benchmarks. OpenAI...</description><pubDate>Tue, 06 Jan 2026 13:40:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/pcuenq/421927498996428</guid></item><item><title>Big news from CES ‚Äî Cosmos Reason 2 is here ‚Äî our most advanced reasoning vision-language model for physical AI, now topping the Physical AI Bench leaderboardüèÜ</title><link>https://huggingface.co/posts/tsungyi/839679910683551</link><description>Big news from CES ‚Äî Cosmos Reason 2 is here ‚Äî our most advanced reasoning vision-language model for physical AI, now topping the Physical AI Bench leaderboardüèÜ shi-labs/physical-ai-bench-leaderboard What‚Äôs new: - Enhanced physical reasoning &amp; spatio-temporal understanding - Flexible deployment with 2B &amp; 8B model sizes - Long-context understanding (up to 256K tokens) - Object detection with 2D/3D point localizations and trajectory data - New Cosmos Cookbook Recipes for faster onboarding Read the full blog üìñ https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning Download Cosmos Reason 2 üëâ nvidia/Cosmos-Reason2-8B On top of Cosmos Reason 2, we also rolled out other new updates, including: - Cosmos Predict 2.5 ‚Äì Unified Text2World/Image2World/Video2World model for higher-quality synthetic video worlds - Cosmos Transfer 2.5-2B ‚Äì Lightweight, high-fidelity world-to-world translation with stronger physics alignment - NVIDIA GR00T N1.6 ‚Äì Open robot foundation...</description><pubDate>Tue, 06 Jan 2026 13:40:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tsungyi/839679910683551</guid></item><item><title>Qwen Image 2512 &amp; 2511 Training Results Are Next Level + 33 ComfyUI Presets for Image &amp; Video Gen</title><link>https://huggingface.co/posts/MonsterMMORPG/816784228667629</link><description>Qwen Image 2512 &amp; 2511 Training Results Are Next Level + 33 ComfyUI Presets for Image &amp; Video Gen Full tutorial: https://www.youtube.com/watch?v=RcoXd9v1t_c Qwen Image 2512 Text to Image model is a massive upgrade in quality just as Qwen Image Edit 2511 was for image editing tasks based on commands. I have trained both Qwen Image older Base, Qwen Image 2512 new base, Qwen Image 2509 older edit model and Qwen Image 2511 newer edit model and compared them in this video. The results are astonishing. Moreover, we have converted all of our premium SwarmUI image and video generation presets into ComfyUI workflows. Just drag and drop the workflow and start using immediately. All models are downloaded with our premium 1-click to use model downloader app. ComfyUI and SwarmUI are also installed with 1-click installers and our ComfyUI fully supports Sage Attention, Flash Attention, xFormers, Triton and more with RTX 5000 series and all GPUs like 3000, 2000, 4000 series, etc for both Linux and...</description><pubDate>Tue, 06 Jan 2026 13:40:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/816784228667629</guid></item><item><title>Chinese open source AI in December 2025 was about the stack coming together: open, end to end, and ready to ship üî•</title><link>https://huggingface.co/posts/AdinaY/239073779898851</link><description>Chinese open source AI in December 2025 was about the stack coming together: open, end to end, and ready to ship üî• https://huggingface.co/collections/zh-ai-community/december-2025-china-open-source-highlights ‚ú® Big wave of foundation models: still scaling, but efficiency, reasoning, and deployment now matter more than size - DeepSeek-V3.2 - Z.ai GLM-4.7 - MiniMax-M2.1 - Xiaomi: MiMo-V2-Flash ‚ú® Multimodal reasoning is now default - Z.ai GLM-4.6V - Z.ai AutoGLM-Phone 9B - Bytedance: Dolphin-v2 ‚ú® Image &amp; video: editable assets and real workflows - Qwen-Image-Layered / Image-2512 - Meituan: LongCat-Image &amp; Image Edit - AIDC: Ovis-Image-7B - Live-Avatar / LongCat-Video-Avatar - HY-WorldPlay / RealVideo ‚ú® Audio goes edge ready - GLM-ASR-Nano / Fun-ASR-Nano - GLM-TTS / VoxCPM1.5 - CosyVoice 0.5B ‚ú® The quiet backbone: data &amp; infrastructure - Finch (FinWorkBench) - Tencent ARC: TimeLens-100K - BIGAI: TongSIM-Asset - MiniMax: VTP-Base ‚ú® Also congrats on Minimax and Z.ai announced their IPOs...</description><pubDate>Tue, 06 Jan 2026 13:40:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/239073779898851</guid></item><item><title>One of the most practical and genuinely useful use cases of agentic systems is a research assistant.</title><link>https://huggingface.co/posts/RakshitAralimatti/360718325875789</link><description>One of the most practical and genuinely useful use cases of agentic systems is a research assistant. I built a Deep Research multi-agent system using NVIDIA‚Äôs Nemotron-3-Nano-30B-A3B model and CrewAI. Try it out yourself üëá üîó GitHub: https://github.com/rakshit2020/Deep-Research-Agent-using-CrewAI What truly made this system feel next-level was powering it with NVIDIA Nemotron-3-Nano-30B-A3B, its built for real-world agentic applications. The agentic system I built: 1. First talks to you and clarifies what you actually want, removing ambiguity 2. Then creates a proper research plan based on that clarity 3. Performs deep research using web search and content extraction tools 4. Finally produces a well-structured research report grounded in sources See translation</description><pubDate>Tue, 06 Jan 2026 13:40:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/360718325875789</guid></item><item><title>‚úÖ New Article: *Operating an SI-Core (v0.1)*</title><link>https://huggingface.co/posts/kanaria007/128065561044409</link><description>‚úÖ New Article: *Operating an SI-Core (v0.1)* Title: üõ†Ô∏è Operating SI-Core: Dashboards, Playbooks, and Human Loops üîó https://huggingface.co/blog/kanaria007/operating-si-core --- Summary: Designing an SI-Core is only half the job ‚Äî the other half is *running it safely at 03:00*. This guide is a *non-normative ops runbook* for SRE/Ops teams and governance owners: what to put on the *one-page dashboard*, how to wire *alerts ‚Üí actions*, when to use *safe-mode*, and how to answer the question that always arrives after an incident: &gt; ‚ÄúWhy did the system do *that*?‚Äù --- Why It Matters: ‚Ä¢ Turns ‚Äúauditable AI‚Äù into *operational reality* (not a slide deck) ‚Ä¢ Makes *ethics + rollback* measurable, actionable, and drillable ‚Ä¢ Clarifies how humans stay in the loop without becoming the bottleneck ‚Ä¢ Provides templates for *postmortems, escalation, and regulator-grade explanations* --- What‚Äôs Inside: *Core Ops Dashboard (1 page):* ‚Ä¢ Determinism/consistency, ethics/oversight, rollback/recovery,...</description><pubDate>Tue, 06 Jan 2026 13:40:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/128065561044409</guid></item><item><title>Neural Traffic Control: Orchestrating Multi-Path Reasoning üö•</title><link>https://huggingface.co/posts/mindchain/795148891017992</link><description>Neural Traffic Control: Orchestrating Multi-Path Reasoning üö• The future of AI isn't just about "better" models‚Äîit‚Äôs about high-precision orchestration. We are moving from linear processing to Parallel MTP-Reasoning, where we manage neural traffic across stabilized, transparent, and recursive highways. 1Ô∏è‚É£ The Backbone: Stabilized High-Dimensional Routing (arXiv:2512.24880) Using DeepSeek‚Äôs mHC (Manifold-Constrained Hyper-Connections), we solve the instability of deep MoE architectures. By projecting weight updates onto the Birkhoff Polytope, we ensure that our "Simpsons-style" expert lanes maintain mathematical identity. This is the hardware-level stability needed to run multiple reasoning paths without collapse. 2Ô∏è‚É£ The Vision: Gemma Scope 2 &amp; Feature Steering You can't steer what you can't see. Gemma Scope 2 provides the "X-ray" for our highways. By using Sparse Autoencoders (SAEs), our Meta-Controller identifies the active features in each expert lane. We don't just route data;...</description><pubDate>Tue, 06 Jan 2026 13:40:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mindchain/795148891017992</guid></item></channel></rss>