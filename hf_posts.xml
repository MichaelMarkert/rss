<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AGENTS + FINETUNING! This week Hugging Face learn has a whole pathway on finetuning for agentic applications. You can follow these two courses to get knowledge on levelling up your agent game beyond prompts:</title><link>https://huggingface.co/posts/burtenshaw/189514834246661</link><description>AGENTS + FINETUNING! This week Hugging Face learn has a whole pathway on finetuning for agentic applications. You can follow these two courses to get knowledge on levelling up your agent game beyond prompts: 1ï¸âƒ£ New Supervised Fine-tuning unit in the NLP Course https://huggingface.co/learn/nlp-course/en/chapter11/1 2ï¸âƒ£New Finetuning for agents bonus module in the Agents Course https://huggingface.co/learn/agents-course/bonus-unit1/introduction Fine-tuning will squeeze everything out of your model for how youâ€™re using it, more than any prompt. See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/189514834246661</guid></item><item><title>Google just released PaliGemma 2 Mix: new versatile instruction vision language models ğŸ”¥</title><link>https://huggingface.co/posts/merve/467807900895850</link><description>Google just released PaliGemma 2 Mix: new versatile instruction vision language models ğŸ”¥ &gt; Three new models: 3B, 10B, 28B with res 224, 448 ğŸ’™ &gt; Can do vision language tasks with open-ended prompts, understand documents, and segment or detect anything ğŸ¤¯ Read more https://huggingface.co/blog/paligemma2mix Try the demo google/paligemma2-10b-mix All models are here google/paligemma-2-mix-67ac6a251aaf3ee73679dcc4 See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/467807900895850</guid></item><item><title>UIGEN for Tailwind v4 is coming soon!</title><link>https://huggingface.co/posts/smirki/311150694603392</link><description>UIGEN for Tailwind v4 is coming soon! See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/smirki/311150694603392</guid></item><item><title>ğŸš€ğŸ­ğŸŒŸ New Research Alert - WACV 2025 (Avatars Collection)! ğŸŒŸğŸ­ğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/189065722993769</link><description>ğŸš€ğŸ­ğŸŒŸ New Research Alert - WACV 2025 (Avatars Collection)! ğŸŒŸğŸ­ğŸš€ ğŸ“„ Title: EmoVOCA: Speech-Driven Emotional 3D Talking Heads ğŸ” ğŸ“ Description: EmoVOCA is a data-driven method for generating emotional 3D talking heads by combining speech-driven lip movements with expressive facial dynamics. This method has been developed to overcome the limitations of corpora and to achieve state-of-the-art animation quality. ğŸ‘¥ Authors: @ FedeNoce , Claudio Ferrari, and Stefano Berretti ğŸ“… Conference: WACV, 28 Feb â€“ 4 Mar, 2025 | Arizona, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: https://arxiv.org/abs/2403.12886 ğŸŒ Github Page: https://fedenoce.github.io/emovoca/ ğŸ“ Repository: https://github.com/miccunifi/EmoVOCA ğŸš€ CVPR-2023-24-Papers: https://github.com/DmitryRyumin/CVPR-2023-24-Papers ğŸš€ WACV-2024-Papers: https://github.com/DmitryRyumin/WACV-2024-Papers ğŸš€ ICCV-2023-Papers: https://github.com/DmitryRyumin/ICCV-2023-Papers ğŸ“š More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers...</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/189065722993769</guid></item><item><title>ğŸ”¥ Meet Muse: that can generate a game environment based on visuals or playersâ€™ controller actions. It was developed by Microsoft Research in collaboration with Ninja Theory (Hellblade developer). Itâ€™s built on something called the World and Human Action Model (WHAM-1.6B model). They trained on 7 years of Bleeding Edge gameplay and it can generate 2 minute long 3D game sequences with consistent physics and character behaviors all from just a second of input. Theyâ€™ve gone and open-sourced it too. Open weights, the WHAM Demonstrator, and sample data on Azure AI Foundry for anyone to play with. Hope so soon on Hugging Face ğŸ¤—.</title><link>https://huggingface.co/posts/merterbak/134010141714846</link><description>ğŸ”¥ Meet Muse: that can generate a game environment based on visuals or playersâ€™ controller actions. It was developed by Microsoft Research in collaboration with Ninja Theory (Hellblade developer). Itâ€™s built on something called the World and Human Action Model (WHAM-1.6B model). They trained on 7 years of Bleeding Edge gameplay and it can generate 2 minute long 3D game sequences with consistent physics and character behaviors all from just a second of input. Theyâ€™ve gone and open-sourced it too. Open weights, the WHAM Demonstrator, and sample data on Azure AI Foundry for anyone to play with. Hope so soon on Hugging Face ğŸ¤—. ğŸ“„ Paper: https://www.nature.com/articles/s41586-025-08600-3 Blog Post: https://www.microsoft.com/en-us/research/blog/introducing-muse-our-first-generative-ai-model-designed-for-gameplay-ideation/ See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/134010141714846</guid></item><item><title>ğŸ¯ Perplexity drops their FIRST open-weight model on Hugging Face: A decensored DeepSeek-R1 with full reasoning capabilities. Tested on 1000+ examples for unbiased responses.</title><link>https://huggingface.co/posts/fdaudens/121352437859372</link><description>ğŸ¯ Perplexity drops their FIRST open-weight model on Hugging Face: A decensored DeepSeek-R1 with full reasoning capabilities. Tested on 1000+ examples for unbiased responses. Check it out: perplexity-ai/r1-1776 Blog post: https://perplexity.ai/hub/blog/open-sourcing-r1-1776 See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/121352437859372</guid></item><item><title>Integrating human feedback is vital for evolving AI models. Boost quality, scalability, and cost-effectiveness with our crowdsourcing tool!</title><link>https://huggingface.co/posts/jasoncorkill/138106605710984</link><description>Integrating human feedback is vital for evolving AI models. Boost quality, scalability, and cost-effectiveness with our crowdsourcing tool! ..Or run A/B tests and gather thousands of responses in minutes. Upload two images, ask a question, and watch the insights roll in! Check it out here and let us know your feedback: https://app.rapidata.ai/compare See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/138106605710984</guid></item><item><title>ğŸš€ Excited to share our technical report on the Southeast Asian multilingual model Sailor2 and its latest updates!</title><link>https://huggingface.co/posts/dreamerdeo/426313569382827</link><description>ğŸš€ Excited to share our technical report on the Southeast Asian multilingual model Sailor2 and its latest updates! Our 49-page report details Sailor2's development journey, including multilingual data cleaning, small model data mixture simulations, multi-stage continual pre-training, multi-stage post-training, and multi-cultural multi-lingual evaluations. Sailor2 aims to streamline the multilingual model pre-training process efficiently for the community. ğŸ§­ We highlight Sailor2's impressive performance in low-resource language translation scenarios and its cultural understanding advantages in Southeast Asia, promoting practical applications for regional languages. Model updates include: ğŸ’¡ More precise outputs: Reduced redundancy in model outputs through refined post-training data and optimization techniques. ğŸŒˆ Handling longer texts: Expanded to handle up to 128K context length in Southeast Asian languages through long-text training. âš¡ï¸ Faster inference: Achieved 2.5x faster inference...</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dreamerdeo/426313569382827</guid></item><item><title>Me and my team have performed an in-depth investigation comparing o1 to R1 (and other reasoning models)</title><link>https://huggingface.co/posts/cogwheelhead/360341725112136</link><description>Me and my team have performed an in-depth investigation comparing o1 to R1 (and other reasoning models) Link: https://toloka.ai/blog/r1-is-not-on-par-with-o1-and-the-difference-is-qualitative-not-quantitative It started with us evaluating them on our own university-math benchmarks: U-MATH for problem-solving and Î¼-MATH for judging solution correctness (see the HF leaderboard: toloka/u-math-leaderboard ) tl;dr: R1 sure is amazing, but what we find is that it lags behind in novelty adaptation and reliability: * performance drops when updating benchmarks with fresh unseen tasks (e.g. AIME 2024 -&gt; 2025) * R1-o1 gap widens when evaluating niche subdomains (e.g. university-specific math instead of the more common Olympiad-style contests) * same with going into altogether unconventional domains (e.g. chess) or skills (e.g. judgment instead of problem-solving) * R1 also runs into failure modes way more often (e.g. making illegal chess moves or falling into endless generation loops) Our...</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/cogwheelhead/360341725112136</guid></item><item><title>Whatâ€™s in a name? More than you might think, especially for AI.</title><link>https://huggingface.co/posts/frimelle/972648838018664</link><description>Whatâ€™s in a name? More than you might think, especially for AI. Whenever I introduce myself, people often start speaking French to me, even though my French is trÃ¨s basic. It turns out that AI systems do something similar: Large language models infer cultural identity from names, shaping their responses based on presumed backgrounds. But is this helpful personalization or a reinforcement of stereotypes? In our latest paper, we explored this question by testing DeepSeek, Llama, Aya, Mistral-Nemo, and GPT-4o-mini on how they associate names with cultural identities. We analysed 900 names from 30 cultures and found strong assumptions baked into AI responses: some cultures were overrepresented, while others barely registered. For example, a name like "Jun" often triggered Japan-related responses, while "Carlos" was linked primarily to Mexico, even though these names exist in multiple countries. Meanwhile, names from places like Ireland led to more generic answers, suggesting weaker...</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/frimelle/972648838018664</guid></item></channel></rss>