<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Inspired by the heroes of day zero quants (</title><link>https://huggingface.co/posts/marksverdhei/460500590246249</link><description>Inspired by the heroes of day zero quants ( @ TheBloke @ danielhanchen @ shimmyshimmer @ bartowski ), I decided to join the race by releasing the first FP8 quant of glm-4.7-flash! Not as easy as i expected, but I'm happy i was still able to have it working within a few hours after the original model was released! Interested in feedback if anyone wants to try it out! marksverdhei/GLM-4.7-Flash-FP8 Note: If my PR to vLLM isn't merged yet you might have to use my fork. Cheers! ğŸ¤— See translation</description><pubDate>Wed, 21 Jan 2026 09:37:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/marksverdhei/460500590246249</guid></item><item><title>So, Koreans are also doing great progress behind Chinese,</title><link>https://huggingface.co/posts/Ujjwal-Tyagi/869541950904709</link><description>So, Koreans are also doing great progress behind Chinese, Their two open source ai models that are actually good in coding. upstage/Solar-Open-100B skt/A.X-K1 See translation</description><pubDate>Wed, 21 Jan 2026 09:37:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ujjwal-Tyagi/869541950904709</guid></item><item><title>VividFlow: Complete AI Image Transformation Platform ğŸ¬ğŸ¨âœ¨</title><link>https://huggingface.co/posts/DawnC/976121143006478</link><description>VividFlow: Complete AI Image Transformation Platform ğŸ¬ğŸ¨âœ¨ Three powerful creative tools in one streamlined workspace. VividFlow combines professional video generation, intelligent background replacement, and artistic style transfer to transform your images with precision and creativity. ğŸ­ Triple Creative Powers - Cinematic Video Generation transforms static images into smooth motion sequences from 0.5 to 5 seconds. Eight curated motion categories cover portraits, products, landscapes, and artistic content with precision-tuned templates. - Intelligent Background Replacement generates photorealistic scenes from 24 professionally crafted presets spanning studios, natural environments, urban settings, and seasonal atmospheres. Advanced edge refinement handles complex subjects, while the built-in Touch Up tool eliminates artifacts through AI-powered inpainting for flawless results. - Artistic Style Transfer converts photographs into stunning interpretations across six distinct styles...</description><pubDate>Wed, 21 Jan 2026 09:37:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/976121143006478</guid></item><item><title>Run GLM-4.7-Flash locally on your device with 24GB RAM!ğŸ”¥</title><link>https://huggingface.co/posts/danielhanchen/143027024579647</link><description>Run GLM-4.7-Flash locally on your device with 24GB RAM!ğŸ”¥ It's the best performing 30B model on SWE-Bench and GPQA. With 200K context, it excels at coding, agents, chat &amp; reasoning. GGUF: unsloth/GLM-4.7-Flash-GGUF Guide: https://unsloth.ai/docs/models/glm-4.7-flash See translation</description><pubDate>Wed, 21 Jan 2026 09:37:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/143027024579647</guid></item><item><title>ğŸ˜ My new personal website is live! Check out</title><link>https://huggingface.co/posts/ZennyKenny/848353801795401</link><description>ğŸ˜ My new personal website is live! Check out https://kennethhamilton.me to chat with an LLM about my professional skills and personal projects. ğŸ™ˆ Think of it like a really, really vain version of ChatGPT. See translation</description><pubDate>Wed, 21 Jan 2026 09:37:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/848353801795401</guid></item><item><title>Check out Orpheus Karaoke! Turn any MIDI into a unique Karaoke MIDI!</title><link>https://huggingface.co/posts/projectlosangeles/732365874551092</link><description>Check out Orpheus Karaoke! Turn any MIDI into a unique Karaoke MIDI! projectlosangeles/Orpheus-Karaoke See translation</description><pubDate>Wed, 21 Jan 2026 09:37:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/projectlosangeles/732365874551092</guid></item><item><title>Interesting paper: PhysRVG</title><link>https://huggingface.co/posts/efecelik/213200184330880</link><description>Interesting paper: PhysRVG The core idea: instead of treating physics as a soft condition the model can work around during optimization, enforce it strictly via reinforcement learning. The paper focuses on rigid body dynamics - collisions, pendulums, free fall, rolling. PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models (2601.11087) See translation</description><pubDate>Wed, 21 Jan 2026 09:37:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/efecelik/213200184330880</guid></item><item><title>ğŸš€ I built a Multimodal Vision-Language Model from using Gemma-270M + CLIP!</title><link>https://huggingface.co/posts/sagar007/387080486731099</link><description>ğŸš€ I built a Multimodal Vision-Language Model from using Gemma-270M + CLIP! Just finished training my multimodal model on the full LLaVA-Instruct-150K dataset (157K samples) and wanted to share the results! ğŸ”§ What I Built: A vision-language model that can understand images and answer questions about them, combining: - Google Gemma-3-270M (language) - OpenAI CLIP ViT-Large/14 (vision) - LoRA fine-tuning for efficiency ğŸ“Š Training Stats: - 157,712 training samples (full LLaVA dataset) - 3 epochs on A100 40GB - ~9 hours training time - Final loss: 1.333 training / 1.430 validation - Only 18.6M trainable params (3.4% of 539M total) ğŸ“ˆ sagar007/multigemma Benchmark Results: - VQA Accuracy: 53.8% - Works great for: animal detection, room identification, scene understanding ğŸ”— **Try it yourself:** - ğŸ¤— Model: sagar007/multigemma - ğŸ® Demo: https://huggingface.co/spaces/sagar007/Multimodal-Gemma - ğŸ’» GitHub: https://github.com/sagar431/multimodal-gemma-270m Built with PyTorch Lightning + MLflow...</description><pubDate>Wed, 21 Jan 2026 09:37:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sagar007/387080486731099</guid></item><item><title>ğŸ›ï¸ Google Code Archive Dataset -</title><link>https://huggingface.co/posts/nyuuzyou/331224318760046</link><description>ğŸ›ï¸ Google Code Archive Dataset - nyuuzyou/google-code-archive Expanding beyond the modern code series, this release presents a massive historical snapshot from the Google Code Archive. This dataset captures the open-source landscape from 2006 to 2016, offering a unique time capsule of software development patterns during the era before GitHub's dominance. Key Stats: - 65,825,565 files from 488,618 repositories - 47 GB compressed Parquet storage - 454 programming languages (Heavily featuring Java, PHP, and C++) - Extensive quality filtering (excluding vendor code and build artifacts) - Rich historical metadata: original repo names, file paths, and era-specific licenses This is one of those releases that I'm most interested in getting feedback on. Would you like to see more old code datasets? See translation</description><pubDate>Wed, 21 Jan 2026 09:37:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nyuuzyou/331224318760046</guid></item><item><title>ğŸ“½ï¸ New NVIDIA paper: Motion Attribution for Video Generation ğŸ“½ï¸</title><link>https://huggingface.co/posts/lorraine2/285073238542136</link><description>ğŸ“½ï¸ New NVIDIA paper: Motion Attribution for Video Generation ğŸ“½ï¸ We propose MOTIVE, a method for taking query video clips and identifying which training data will improve or degrade performance after finetuning, enabling sophisticated data curation and beyond! ğŸ” Project Page: https://research.nvidia.com/labs/sil/projects/MOTIVE/ ğŸ“– Full Paper: https://arxiv.org/abs/2601.08828 Check out more work from the NVIDIA Spatial Intelligence Lab here: https://research.nvidia.com/labs/sil/ This project was led by the great work of Xindi(Cindy) Wu, along with Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-TaixÃ©, Olga Russakovsky, and Sanja Fidler. See translation</description><pubDate>Wed, 21 Jan 2026 09:37:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lorraine2/285073238542136</guid></item></channel></rss>