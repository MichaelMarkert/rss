<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Huggingface Space Leaderboard üöÄ</title><link>https://huggingface.co/posts/openfree/729932908902684</link><description>Huggingface Space Leaderboard üöÄ Hello Huggingface Community! VIDraft/Space-Leaderboard We are excited to introduce the Huggingface Space Leaderboard, a service that lets you view the latest trending Spaces on the Huggingface platform at a glance. This service helps you quickly explore a wide range of creative projects and will spark new inspiration for your own ideas. üéâ Detailed Feature Overview 1. Real-time Trend Reflection Automated Aggregation: Analyzes and ranks over 500 popular Spaces on Huggingface in real time. Accurate Ranking: Combines various metrics such as likes, engagement, and creation time to accurately reflect the latest trends. Instant Updates: Data is continuously updated, so you always see the most current popular Spaces. 2. Intuitive Preview 70% Scaled Preview: Each Space is displayed at 70% scale, providing a neat and clear preview at a glance. Easy Visual Comparison: View multiple Spaces side by side to easily compare their designs and functionalities. Error...</description><pubDate>Fri, 14 Mar 2025 09:23:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/729932908902684</guid></item><item><title>üåê GraphMind: Phi-3 Instruct Graph Explorer</title><link>https://huggingface.co/posts/ginipick/554089753491641</link><description>üåê GraphMind: Phi-3 Instruct Graph Explorer ‚ú® Extract and visualize knowledge graphs from any text in multiple languages! GraphMind is a powerful tool that leverages the capabilities of Phi-3 to transform unstructured text into structured knowledge graphs, helping you understand complex relationships within any content. ginigen/Graph-Mind üöÄ Key Features Multi-language Support üåç: Process text in English, Korean, and many other languages Instant Visualization üß©: See extracted entities and relationships in an interactive graph Entity Recognition üè∑Ô∏è: Automatically identifies and categorizes named entities Optimized Performance ‚ö°: Uses caching to deliver faster results for common examples Intuitive Interface üëÜ: Simple design makes complex graph extraction accessible to everyone üí° Use Cases Content Analysis: Extract key entities and relationships from articles or documents Research Assistance: Quickly visualize connections between concepts in research papers Educational Tool: Help students...</description><pubDate>Fri, 14 Mar 2025 09:23:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/554089753491641</guid></item><item><title>We just crossed 1,500,000 public models on Hugging Face (and 500k spaces, 330k datasets, 50k papers). One new repository is created every 15 seconds. Congratulations all!</title><link>https://huggingface.co/posts/clem/238420842235482</link><description>We just crossed 1,500,000 public models on Hugging Face (and 500k spaces, 330k datasets, 50k papers). One new repository is created every 15 seconds. Congratulations all! See translation</description><pubDate>Fri, 14 Mar 2025 09:23:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/238420842235482</guid></item><item><title>Gemma-3-4B : Image and Video Inference üñºÔ∏èüé•</title><link>https://huggingface.co/posts/prithivMLmods/635315600525918</link><description>Gemma-3-4B : Image and Video Inference üñºÔ∏èüé• üß§Space: prithivMLmods/Gemma-3-Multimodal @ gemma3-4b : {Tag + Space_+ 'prompt'} @ video-infer : {Tag + Space_+ 'prompt'} + Gemma3-4B : google/gemma-3-4b-it + By default, it runs : prithivMLmods/Qwen2-VL-OCR-2B-Instruct Gemma 3 Technical Report : https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf Additionally, I have also tested Aya-Vision 8B vs Custom Qwen2-VL-OCR for OCR with test case samples on messy handwriting for experimental purposes to optimize edge device VLMs for Optical Character Recognition. üìúRead the blog here: https://huggingface.co/blog/prithivMLmods/aya-vision-vs-qwen2vl-ocr-2b See translation</description><pubDate>Fri, 14 Mar 2025 09:23:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/635315600525918</guid></item><item><title>Benchmarking Google's Veo2: How Does It Compare?</title><link>https://huggingface.co/posts/jasoncorkill/287919306604058</link><description>Benchmarking Google's Veo2: How Does It Compare? The results did not meet expectations. Veo2 struggled with style consistency and temporal coherence, falling behind competitors like Runway, Pika, Tencent, and even Alibaba. While the model shows promise, its alignment and quality are not yet there. Google recently launched Veo2, its latest text-to-video model, through select partners like fal.ai. As part of our ongoing evaluation of state-of-the-art generative video models, we rigorously benchmarked Veo2 against industry leaders. We generated a large set of Veo2 videos spending hundreds of dollars in the process and systematically evaluated them using our Python-based API for human and automated labeling. Check out the ranking here: https://www.rapidata.ai/leaderboard/video-models Rapidata/text-2-video-human-preferences-veo2 See translation</description><pubDate>Fri, 14 Mar 2025 09:23:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/287919306604058</guid></item><item><title>We've kept pushing our Open-R1 project, an open initiative to replicate and extend the techniques behind DeepSeek-R1.</title><link>https://huggingface.co/posts/thomwolf/597591144299421</link><description>We've kept pushing our Open-R1 project, an open initiative to replicate and extend the techniques behind DeepSeek-R1. And even we were mind-blown by the results we got with this latest model we're releasing: ‚ö°Ô∏èOlympicCoder ( open-r1/OlympicCoder-7B and open-r1/OlympicCoder-32B ) It's beating Claude 3.7 on (competitive) programming ‚Äìa domain Anthropic has been historically really strong at‚Äì and it's getting close to o1-mini/R1 on olympiad level coding with just 7B parameters! And the best part is that we're open-sourcing all about its training dataset, the new IOI benchmark, and more in our Open-R1 progress report #3: https://huggingface.co/blog/open-r1/update-3 Datasets are are releasing: - open-r1/codeforces - open-r1/codeforces-cots - open-r1/ioi - open-r1/ioi-test-cases - open-r1/ioi-sample-solutions - open-r1/ioi-cots - open-r1/ioi-2024-model-solutions See translation</description><pubDate>Fri, 14 Mar 2025 09:23:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/thomwolf/597591144299421</guid></item><item><title>Here‚Äôs a notebook to make Gemma reason with GRPO &amp; TRL. I made this whilst prepping the next unit of the reasoning course:</title><link>https://huggingface.co/posts/burtenshaw/548205593122339</link><description>Here‚Äôs a notebook to make Gemma reason with GRPO &amp; TRL. I made this whilst prepping the next unit of the reasoning course: In this notebooks I combine together google‚Äôs model with some community tooling - First, I load the model from the Hugging Face hub with transformers‚Äôs latest release for Gemma 3 - I use PEFT and bitsandbytes to get it running on Colab - Then, I took Will Browns processing and reward functions to make reasoning chains from GSM8k - Finally, I used TRL‚Äôs GRPOTrainer to train the model Next step is to bring Unsloth AI in, then ship it in the reasoning course. Links to notebook below. https://colab.research.google.com/drive/1Vkl69ytCS3bvOtV9_stRETMthlQXR4wX?usp=sharing See translation</description><pubDate>Fri, 14 Mar 2025 09:23:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/548205593122339</guid></item><item><title>Gemma3 family is out! Reading the tech report, and this section was really interesting to me from a methods/scientific fairness pov.</title><link>https://huggingface.co/posts/clefourrier/544255220051399</link><description>Gemma3 family is out! Reading the tech report, and this section was really interesting to me from a methods/scientific fairness pov. Instead of doing over-hyped comparisons, they clearly state that **results are reported in a setup which is advantageous to their models**. (Which everybody does, but people usually don't say) For a tech report, it makes a lot of sense to report model performance when used optimally! On leaderboards on the other hand, comparison will be apples to apples, but in a potentially unoptimal way for a given model family (like some user interact sub-optimally with models) Also contains a cool section (6) on training data memorization rate too! Important to see if your model will output the training data it has seen as such: always an issue for privacy/copyright/... but also very much for evaluation! Because if your model knows its evals by heart, you're not testing for generalization. See translation</description><pubDate>Fri, 14 Mar 2025 09:23:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clefourrier/544255220051399</guid></item><item><title>Open Sora 2.0 is out üî•</title><link>https://huggingface.co/posts/AdinaY/501114368839718</link><description>Open Sora 2.0 is out üî• hpcai-tech/open-sora-20-67cfb7efa80a73999ccfc2d5 ‚ú® 11B with Apache2.0 ‚ú® Low training cost - $200k ‚ú® open weights, code and training workflow See translation</description><pubDate>Fri, 14 Mar 2025 09:23:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/501114368839718</guid></item><item><title>testing post</title><link>https://huggingface.co/posts/pidou/518300311697108</link><description>testing post</description><pubDate>Fri, 14 Mar 2025 09:23:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/pidou/518300311697108</guid></item></channel></rss>