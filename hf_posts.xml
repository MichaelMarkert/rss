<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>8 New Types of RAG</title><link>https://huggingface.co/posts/Kseniase/113319295427497</link><description>8 New Types of RAG RAG techniques continuously evolve to enhance LLM response accuracy by retrieving relevant external data during generation. To keep up with current AI trends, new RAG types incorporate deep step-by-step reasoning, tree search, citations, multimodality and other effective techniques. Here's a list of 8 latest RAG advancements: 1. DeepRAG -&gt; DeepRAG: Thinking to Retrieval Step by Step for Large Language Models (2502.01142) Models retrieval-augmented reasoning as a Markov Decision Process, enabling strategic retrieval. It dynamically decides when to retrieve external knowledge and when rely on parametric reasoning. 2. RealRAG -&gt; RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning (2502.00848) Enhances novel object generation by retrieving real-world images and using self-reflective contrastive learning to fill knowledge gap, improve realism and reduce distortions. 3. Chain-of-Retrieval Augmented Generation (CoRAG) -&gt;...</description><pubDate>Tue, 11 Feb 2025 13:26:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/113319295427497</guid></item><item><title>üì¢ New Research Alert: Making Language Models Smaller &amp; Smarter!</title><link>https://huggingface.co/posts/schuler/395413718646507</link><description>üì¢ New Research Alert: Making Language Models Smaller &amp; Smarter! Thrilled to share the latest technical report demonstrating how to reduce language model parameters by 77% while maintaining performance. The secret? Grouped pointwise convolutions. Yes. We brought a method from computer vision to the transformers arena. üîë Key Findings: ‚Ä¢ 77% parameter reduction. ‚Ä¢ Maintained model capabilities. ‚Ä¢ Improved generalization. Paper: https://www.researchgate.net/publication/388835829_SAVING_77_OF_THE_PARAMETERS_IN_LARGE_LANGUAGE_MODELS_TECHNICAL_REPORT Code: https://github.com/joaopauloschuler/less-parameters-llm See translation</description><pubDate>Tue, 11 Feb 2025 13:26:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/schuler/395413718646507</guid></item><item><title>Time Stream ‚è≥üöÄ</title><link>https://huggingface.co/posts/ginipick/776720011919298</link><description>Time Stream ‚è≥üöÄ Time Stream is a groundbreaking AI tool that transforms your text into a mesmerizing video journey from the past to the future. With this innovative technology, your ideas evolve over time, visualized through a dynamic image strip and a fluid video narrative. Imagine typing a simple prompt and watching as your words transform into vivid scenes that capture every moment of change‚Äîlike a time machine for creativity! üé•‚ú® Key Features: ‚Ä¢ Text-to-Video Transformation: Enter any text, and Time Stream converts it into a compelling video that travels through time, turning your ideas into a visual story. üìΩÔ∏è ‚Ä¢ Dynamic Image Strip: Alongside the video, a vibrant image strip is created, showcasing each stage of the transformation so you can see every detail of the evolution. üì∏ ‚Ä¢ Customizable Settings: Adjust parameters such as strength, guidance scale, and more to fine-tune your video‚Äôs appearance and ensure it perfectly matches your creative vision. ‚öôÔ∏è ‚Ä¢ User-Friendly Interface:...</description><pubDate>Tue, 11 Feb 2025 13:26:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/776720011919298</guid></item><item><title>Tutorial üí• Training a non-English reasoning model with GRPO and Unsloth</title><link>https://huggingface.co/posts/s-emanuilov/736266652835078</link><description>Tutorial üí• Training a non-English reasoning model with GRPO and Unsloth I wanted to share my experiment with training reasoning models in languages other than English/Chinese. Using Llama 3.1 8B as base, GRPO trainer from trl, and Unsloth optimizations, I got a working prototype in Bulgarian after ~5 hours on an L40S GPU. The approach should work for any language where the base model has some pre-training coverage. Full code and tutorial here: https://unfoldai.com/reasoning-in-a-non-english-language/ The model itself: s-emanuilov/LLMBG-Llama-3.1-8B-BG-Reasoning-v0.1 I hope this helps anyone looking to build reasoning models in their language. See translation</description><pubDate>Tue, 11 Feb 2025 13:26:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/s-emanuilov/736266652835078</guid></item><item><title>Researchers developed Sonic AI enabling precise facial animation from speech cues üéß Decouples head/expression control via audio tone analysis + time-aware fusion for natural long-form synthesis</title><link>https://huggingface.co/posts/kadirnar/407959263704733</link><description>Researchers developed Sonic AI enabling precise facial animation from speech cues üéß Decouples head/expression control via audio tone analysis + time-aware fusion for natural long-form synthesis See translation</description><pubDate>Tue, 11 Feb 2025 13:26:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kadirnar/407959263704733</guid></item><item><title>Fascinating deep dive into Swiggy's Hermes - their in-house Text-to-SQL solution that's revolutionizing data accessibility!</title><link>https://huggingface.co/posts/singhsidhukuldeep/821835295778849</link><description>Fascinating deep dive into Swiggy's Hermes - their in-house Text-to-SQL solution that's revolutionizing data accessibility! Hermes enables natural language querying within Slack, generating and executing SQL queries with an impressive &lt;2 minute turnaround time. The system architecture is particularly intriguing: Technical Implementation: - Built on GPT-4 with a Knowledge Base + RAG approach for Swiggy-specific context - AWS Lambda middleware handles communication between Slack UI and the Gen AI model - Databricks jobs orchestrate query generation and execution Under the Hood: The pipeline employs a sophisticated multi-stage approach: 1. Metrics retrieval using embedding-based vector lookup 2. Table/column identification through metadata descriptions 3. Few-shot SQL retrieval with vector-based search 4. Structured prompt creation with data snapshots 5. Query validation with automated error correction Architecture Highlights: - Compartmentalized by business units (charters) for better...</description><pubDate>Tue, 11 Feb 2025 13:26:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/singhsidhukuldeep/821835295778849</guid></item><item><title>Introducing OpenR1-Math-220k!</title><link>https://huggingface.co/posts/lewtun/162100455547462</link><description>Introducing OpenR1-Math-220k! open-r1/OpenR1-Math-220k The community has been busy distilling DeepSeek-R1 from inference providers, but we decided to have a go at doing it ourselves from scratch üí™ What‚Äôs new compared to existing reasoning datasets? ‚ôæ Based on AI-MO/NuminaMath-1.5 : we focus on math reasoning traces and generate answers for problems in NuminaMath 1.5, an improved version of the popular NuminaMath-CoT dataset. üê≥ 800k R1 reasoning traces: We generate two answers for 400k problems using DeepSeek R1. The filtered dataset contains 220k problems with correct reasoning traces. üìÄ 512 H100s running locally: Instead of relying on an API, we leverage vLLM and SGLang to run generations locally on our science cluster, generating 180k reasoning traces per day. ‚è≥ Automated filtering: We apply Math Verify to only retain problems with at least one correct answer. We also leverage Llama3.3-70B-Instruct as a judge to retrieve more correct examples (e.g for cases with malformed answers...</description><pubDate>Tue, 11 Feb 2025 13:26:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lewtun/162100455547462</guid></item><item><title>The Hugging Face agents course is finally out!</title><link>https://huggingface.co/posts/burtenshaw/457613029588941</link><description>The Hugging Face agents course is finally out! üëâ https://huggingface.co/agents-course This first unit of the course sets you up with all the fundamentals to become a pro in agents. - What's an AI Agent? - What are LLMs? - Messages and Special Tokens - Understanding AI Agents through the Thought-Action-Observation Cycle - Thought, Internal Reasoning and the Re-Act Approach - Actions, Enabling the Agent to Engage with Its Environment - Observe, Integrating Feedback to Reflect and Adapt See translation</description><pubDate>Tue, 11 Feb 2025 13:26:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/457613029588941</guid></item><item><title>QwQ Edge Gets a Small Update..! üí¨</title><link>https://huggingface.co/posts/prithivMLmods/964278651693422</link><description>QwQ Edge Gets a Small Update..! üí¨ try now: prithivMLmods/QwQ-Edge üöÄNow, you can use the following commands for different tasks: üñºÔ∏è @ image 'prompt...' ‚Üí Generates an image üîâ@tts1 'prompt...' ‚Üí Generates speech in a female voice üîâ @ tts2 'prompt...' ‚Üí Generates speech in a male voice üÖ∞Ô∏è@text 'prompt...' ‚Üí Enables textual conversation (If not specified, text-to-text generation is the default mode) üí¨Multimodality Support : prithivMLmods/Qwen2-VL-OCR-2B-Instruct üí¨For text generation, the FastThink-0.5B model ensures quick and efficient responses, prithivMLmods/FastThink-0.5B-Tiny üí¨Image Generation: sdxl lightning model, SG161222/RealVisXL_V4.0_Lightning Github: https://github.com/PRITHIVSAKTHIUR/QwQ-Edge graph TD A[User Interface] --&gt; B[Chat Logic] B --&gt; C{Command Type } C --&gt;| Text | D [FastThink -0.5 B] C --&gt;| Image | E [Qwen2-VL-OCR -2 B] C --&gt;| @image | F [Stable Diffusion XL] C --&gt;| @tts | G [Edge TTS] D --&gt; H[Response] E --&gt; H F --&gt; H G --&gt; H See translation</description><pubDate>Tue, 11 Feb 2025 13:26:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/964278651693422</guid></item><item><title>We‚Äôre launching a FREE and CERTIFIED course on Agents!</title><link>https://huggingface.co/posts/burtenshaw/334573649974058</link><description>We‚Äôre launching a FREE and CERTIFIED course on Agents! We're thrilled to announce the launch of the Hugging Face Agents course on Learn! This interactive, certified course will guide you through building and deploying your own AI agents. Here's what you'll learn: - Understanding Agents: We'll break down the fundamentals of AI agents, showing you how they use LLMs to perceive their environment (observations), reason about it (thoughts), and take actions. Think of a smart assistant that can book appointments, answer emails, or even write code based on your instructions. - Building with Frameworks: You'll dive into popular agent frameworks like LangChain, LlamaIndex and smolagents. These tools provide the building blocks for creating complex agent behaviors. - Real-World Applications: See how agents are used in practice, from automating SQL queries to generating code and summarizing complex documents. - Certification: Earn a certification by completing the course modules, implementing...</description><pubDate>Tue, 11 Feb 2025 13:26:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/334573649974058</guid></item></channel></rss>