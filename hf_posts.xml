<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>9 Recent advances in Multi-Agent Systems (all open-source)</title><link>https://huggingface.co/posts/Kseniase/298526462161147</link><description>9 Recent advances in Multi-Agent Systems (all open-source) The idea to split tasks across multiple agents instead of relying on one universal agent is now seen as one of the most effective ways to build an AI stack. Concepts like ‚Äúagent swarms‚Äù were highlighted at the AI Engineer Code Summit in NYC (Nov 20‚Äì21) as the winning architecture. And this trend is not only about coding and software. It applies across all AI domains. So here is some recent research that helps keep multi-agent systems (MAS) better and up-to-date: 1. LatentMAS ‚Üí Latent Collaboration in Multi-Agent Systems (2511.20639) AI agents share their hidden "thoughts" directly in latent space instead of talking through text. This makes collaboration and reasoning way faster and accurate (no extra training needed) 2. Puppeteer ‚Üí Multi-Agent Collaboration via Evolving Orchestration (2505.19591) Uses a ‚Äúpuppeteer‚Äù LLM that dynamically decides which agents (‚Äúpuppets‚Äù) to call and in what order. By learning this orchestration...</description><pubDate>Tue, 02 Dec 2025 09:31:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/298526462161147</guid></item><item><title>Introducing Anim Lab AI‚ö°</title><link>https://huggingface.co/posts/ovi054/498416611324104</link><description>Introducing Anim Lab AI‚ö° My submission for the MCP 1st Birthday Hackathon Turn any math concept or logic into a clear video explanation instantly using AI. üëâ Try it now: MCP-1st-Birthday/anim-lab-ai Demo outputs are attached üëá See translation</description><pubDate>Tue, 02 Dec 2025 09:31:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/498416611324104</guid></item><item><title>Qwen3-Next can now be Run locally! (30GB RAM)</title><link>https://huggingface.co/posts/danielhanchen/212249714773740</link><description>Qwen3-Next can now be Run locally! (30GB RAM) Instruct GGUF: unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF The models come in Thinking and Instruct versions and utilize a new architecture, allowing it to have ~10x faster inference than Qwen32B. üíú Step-by-step Guide: https://docs.unsloth.ai/models/qwen3-next Thinking GGUF: unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF See translation</description><pubDate>Tue, 02 Dec 2025 09:31:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/212249714773740</guid></item><item><title>hello, who can help me setup a local LLM and RAG for my job i can pay</title><link>https://huggingface.co/posts/aiconta/768879073281998</link><description>hello, who can help me setup a local LLM and RAG for my job i can pay See translation</description><pubDate>Tue, 02 Dec 2025 09:31:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aiconta/768879073281998</guid></item><item><title>Exciting updates to the Wikipedia Monthly dataset for November! üöÄ</title><link>https://huggingface.co/posts/omarkamali/466219030497250</link><description>Exciting updates to the Wikipedia Monthly dataset for November! üöÄ „Éª Fixed a bug to remove infobox leftovers and other wiki markers such as __TOC__ „Éª New python package https://pypi.org/project/wikisets : a dataset builder with efficient sampling so you can combine the languages you want seamlessly for any date (ideal for pretraining data but works for any purpose) „Éª Moved the pipeline to a large server. Much higher costs but with better reliability and predictability (let me know if you'd like to sponsor this!). „Éª Dataset sizes are unfortunately missing for this month due to shenanigans with the migration, but should be back in December's update. Check out the dataset: omarkamali/wikipedia-monthly See translation</description><pubDate>Tue, 02 Dec 2025 09:31:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/omarkamali/466219030497250</guid></item><item><title>Hello everyone,</title><link>https://huggingface.co/posts/prithivMLmods/649242345044742</link><description>Hello everyone, The strangerzonehf [HF] Community / Organization Page, which is maintained by me, has reached the Top 10 Developer Pages ranking at 6th place, contributing 3.4% in the calendar cycle from August 2024 to August 2025. It is also the only South Asia / Indian page in the list. I could not be more proud to be doing things for the community. ‚ù§Ô∏èü§ó Source: https://www.dataprovenance.org/economies-of-open-intelligence.pdf It is a pleasure to be a part of it. Thank you! @ prithivMLmods See translation</description><pubDate>Tue, 02 Dec 2025 09:31:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/649242345044742</guid></item><item><title>want to use open models easily through an API?</title><link>https://huggingface.co/posts/sergiopaniego/422459569314108</link><description>want to use open models easily through an API? Inference Providers might be exactly what you‚Äôre looking for sooo here‚Äôs a complete beginner-friendly walkthrough üßê https://www.youtube.com/watch?v=oxwsizy1Spw See translation</description><pubDate>Tue, 02 Dec 2025 09:31:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/422459569314108</guid></item><item><title>We recently discussed how Tensor Parallelism slices matrices to reduce latency within a single node. But what happens when you need to scale beyond that, where the bandwidth drops?</title><link>https://huggingface.co/posts/flozi00/166942879954778</link><description>We recently discussed how Tensor Parallelism slices matrices to reduce latency within a single node. But what happens when you need to scale beyond that, where the bandwidth drops? That is where Pipeline Parallelism (PP) takes over. Instead of slicing the operation, PP slices the model depth. It turns your GPU cluster into an assembly line: GPU 0 handles layers 1-12, GPU 1 handles 13-24, and so on. The hardware challenge here isn't the interconnect speed‚Äîit is the "Pipeline Bubble." In a naive setup, expensive H100s sit idle for most of the cycle waiting for data to flow through the chain. My latest guide breaks down the scheduling strategies used to minimize this idle silicon time. In this deep dive, we cover: The Hardware Mechanics: Vertical Slicing Unlike TP which requires "chatty" All-Reduce operations, PP relies on lightweight Point-to-Point (Send/Recv) communication. This makes it the only viable strategy for crossing node boundaries over Ethernet or InfiniBand. Fighting the...</description><pubDate>Tue, 02 Dec 2025 09:31:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/flozi00/166942879954778</guid></item><item><title>September(2025) LLM Safety &amp; Reliability Benchmarks Report By AI Parivartan Research Lab (AIPRL-LIR)</title><link>https://huggingface.co/posts/rajkumarrawal/715420178191599</link><description>September(2025) LLM Safety &amp; Reliability Benchmarks Report By AI Parivartan Research Lab (AIPRL-LIR) Monthly LLM's Intelligence Reports for AI Decision Makers : Our "aiprl-llm-intelligence-report" repo to establishes (AIPRL-LIR) framework for Large Language Model overall evaluation and analysis through systematic monthly intelligence reports. Unlike typical AI research papers or commercial reports. It provides structured insights into AI model performance, benchmarking methodologies, Multi-hosting provider analysis, industry trends ... ( all in one monthly report ) Leading Models &amp; Companies, 23 Benchmarks in 6 Categories, Global Hosting Providers, &amp; Research Highlights Here‚Äôs what you‚Äôll find inside this month‚Äôs intelligence report:- Leading Models &amp; Companies : 23 Benchmarks in 6 Categories : With a special focus on Safety &amp; Reliability performance across diverse tasks. Global Hosting Providers : Research Highlights : Comparative insights, evaluation methodologies, and industry...</description><pubDate>Tue, 02 Dec 2025 09:31:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/rajkumarrawal/715420178191599</guid></item><item><title>Claude Opus 4.5 didn't make SOTA either. So many models are stuck at 0.75 now</title><link>https://huggingface.co/posts/onekq/477606916839102</link><description>Claude Opus 4.5 didn't make SOTA either. So many models are stuck at 0.75 now onekq-ai/WebApp1K-models-leaderboard See translation</description><pubDate>Tue, 02 Dec 2025 09:31:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/477606916839102</guid></item></channel></rss>