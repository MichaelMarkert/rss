<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Eyyyy 50 followers ğŸ¤¯</title><link>https://huggingface.co/posts/ProCreations/321100188234240</link><description>Eyyyy 50 followers ğŸ¤¯ See translation</description><pubDate>Thu, 29 May 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ProCreations/321100188234240</guid></item><item><title>Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity!</title><link>https://huggingface.co/posts/fdaudens/719212082746895</link><description>Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity! The final project uses the GAIA dataset - your agent has to solve tasks like analyzing Excel files, processing audio recordings, answering questions about YouTube videos, and diving into research papers. This isn't toy examples, it's the messy, multimodal stuff agents need to handle in practice. Whether youâ€™re just getting started with agents or want to go deeper with tools like LangChain, LlamaIndex, and SmolAgents, this course has tons of useful stuff. A few key insights: - Code agents are incredibly versatile once you get the architecture right - The sweet spot is finding the right balance of guidance vs autonomy for each use case - Once the logic clicks, the possibilities really are endless - it's like letting LLMs break free from the chatbox The course is free and the certification deadline is July 1st, 2025. The Hugging Face...</description><pubDate>Thu, 29 May 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/719212082746895</guid></item><item><title>VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration</title><link>https://huggingface.co/posts/DawnC/538322807718464</link><description>VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration I'm excited to share significant improvements to VisionScout that substantially enhance accuracy and analytical capabilities. â­ï¸ Key Enhancements - CLIP Zero-Shot Landmark Detection: The system now identifies famous landmarks and architectural features without requiring specific training data, expanding scene understanding beyond generic object detection. - Places365 Environmental Classification: Integration of MIT's Places365 model provides robust scene baseline classification across 365 categories, significantly improving lighting analysis accuracy and overall scene identification precision. - Enhanced Multi-Modal Fusion: Advanced algorithms now dynamically combine insights from YOLOv8, CLIP, and Places365 to optimize accuracy across diverse scenarios. - Refined LLM Narratives: Llama 3.2 integration continues to transform analytical data into fluent, contextually rich descriptions while maintaining...</description><pubDate>Thu, 29 May 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/538322807718464</guid></item><item><title>Cua : Docker for computer-use agents</title><link>https://huggingface.co/posts/dhruv3006/675063918098240</link><description>Cua : Docker for computer-use agents Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers. Github : https://github.com/trycua See translation</description><pubDate>Thu, 29 May 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/675063918098240</guid></item><item><title>Introducing our new work: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generationâ€‹â€‹ ğŸš€</title><link>https://huggingface.co/posts/BestWishYsh/693532821570217</link><description>Introducing our new work: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generationâ€‹â€‹ ğŸš€ We tackle the core challenges of â€‹â€‹Subject-to-Video Generation (S2V)â€‹â€‹ by systematically building the first complete infrastructureâ€”featuring an evaluation benchmark and a million-scale dataset! âœ¨ ğŸ§  Introducing â€‹â€‹OpenS2V-Evalâ€‹â€‹â€”the first â€‹â€‹fine-grained S2V benchmarkâ€‹â€‹, with â€‹â€‹180 multi-domain prompts + real/synthetic test pairsâ€‹â€‹. We propose â€‹â€‹NexusScoreâ€‹â€‹, â€‹â€‹NaturalScoreâ€‹â€‹, and â€‹â€‹GmeScoreâ€‹â€‹ to precisely quantify model performance across â€‹â€‹subject consistency, naturalness, and text alignmentâ€‹â€‹ âœ” ğŸ“Š Using this framework, we conduct a â€‹â€‹comprehensive evaluation of 16 leading S2V modelsâ€‹â€‹, revealing their strengths/weaknesses in complex scenarios! ğŸ”¥ â€‹â€‹OpenS2V-5M datasetâ€‹â€‹ now available! A â€‹â€‹5.4M 720P HDâ€‹â€‹ collection of â€‹â€‹subject-text-video tripletsâ€‹â€‹, enabled by â€‹â€‹cross-video association segmentation + multi-view synthesisâ€‹â€‹ for â€‹â€‹diverse subjects &amp; high-quality...</description><pubDate>Thu, 29 May 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/BestWishYsh/693532821570217</guid></item><item><title>I am so happy  to share to all that Iâ€™ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but Iâ€™m doing my best to keep up. Excited for whatâ€™s ahead!</title><link>https://huggingface.co/posts/lukmanaj/495766537273785</link><description>I am so happy to share to all that Iâ€™ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but Iâ€™m doing my best to keep up. Excited for whatâ€™s ahead! See translation</description><pubDate>Thu, 29 May 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lukmanaj/495766537273785</guid></item><item><title>I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book.</title><link>https://huggingface.co/posts/hesamation/260011784391977</link><description>I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book. It gives an overview, then goes into detail for each stage, even providing best practices. Itâ€™s 115 pages on arxiv, definitely worth a read. Check it out: https://arxiv.org/abs/2408.13296 See translation</description><pubDate>Thu, 29 May 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/260011784391977</guid></item><item><title>HunyuanPortrait ğŸ”¥ video model by Tencent Hunyuan team.</title><link>https://huggingface.co/posts/AdinaY/668826641388828</link><description>HunyuanPortrait ğŸ”¥ video model by Tencent Hunyuan team. HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation (2503.18860) tencent/HunyuanPortrait âœ¨Portrait animation from just one image + a video prompt âœ¨Diffusion-based, implicit motion control âœ¨Superior temporal consistency &amp; detail See translation</description><pubDate>Thu, 29 May 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/668826641388828</guid></item><item><title>It's just become easier to share your apps on the biggest AI app store (aka HF spaces) for unlimited storage, more visibility and community interactions.</title><link>https://huggingface.co/posts/clem/541152505631903</link><description>It's just become easier to share your apps on the biggest AI app store (aka HF spaces) for unlimited storage, more visibility and community interactions. Just pick a React, Svelte, or Vue template when you create your space or add app_build_command: npm run build in your README's YAML and app_file: build/index.html in your README's YAML block. Or follow this link: https://huggingface.co/new-space?sdk=static Let's build! See translation</description><pubDate>Thu, 29 May 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/541152505631903</guid></item><item><title>ğŸµ Dream come true for content creators! TIGER AI can extract voice, effects &amp; music from ANY audio file ğŸ¤¯</title><link>https://huggingface.co/posts/fdaudens/323840314242853</link><description>ğŸµ Dream come true for content creators! TIGER AI can extract voice, effects &amp; music from ANY audio file ğŸ¤¯ This lightweight model uses frequency band-split technology to separate speech like magic. Kudos to @ fffiloni for the amazing demo! fffiloni/TIGER-audio-extraction See translation</description><pubDate>Thu, 29 May 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/323840314242853</guid></item></channel></rss>