<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>DeepSeek, Alibaba, Skywork,  Xiaomi, Bytedance.....</title><link>https://huggingface.co/posts/AdinaY/438609555040169</link><description>DeepSeek, Alibaba, Skywork, Xiaomi, Bytedance..... And thatâ€™s just part of the companies from the Chinese community that released open models in April ğŸ¤¯ zh-ai-community/april-2025-open-releases-from-the-chinese-community-67ea699965f6e4c135cab10f ğŸ¬ Video &gt; MAGI-1 by SandAI &gt; SkyReels-A2 &amp; SkyReels-V2 by Skywork &gt; Wan2.1-FLF2V by Alibaba-Wan ğŸ¨ Image &gt; HiDream-I1 by Vivago AI &gt; Kimi-VL by Moonshot AI &gt; InstantCharacter by InstantX &amp; Tencent-Hunyuan &gt; Step1X-Edit by StepFun &gt; EasyControl by Shanghai Jiaotong University ğŸ§  Reasoning &gt; MiMo by Xiaomi &gt; Skywork-R1V 2.0 by Skywork &gt; ChatTS by ByteDance &gt; Kimina by Moonshot AI &amp; Numina &gt; GLM-Z1 by Zhipu AI &gt; Skywork OR1 by Skywork &gt; Kimi-VL-Thinking by Moonshot AI ğŸ”Š Audio &gt; Kimi-Audio by Moonshot AI &gt; IndexTTS by BiliBili &gt; MegaTTS3 by ByteDance &gt; Dolphin by DataOceanAI ğŸ”¢ Math &gt; DeepSeek Prover V2 by Deepseek ğŸŒ LLM &gt; Qwen by Alibaba-Qwen &gt; InternVL3 by Shanghai AI lab &gt; Ernie4.5 (demo) by Baidu ğŸ“Š Dataset &gt; PHYBench by Eureka-Lab &gt;...</description><pubDate>Thu, 01 May 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/438609555040169</guid></item><item><title>Meta released Llama Guard 4 and new Prompt Guard 2 models ğŸ”¥</title><link>https://huggingface.co/posts/merve/662080130760638</link><description>Meta released Llama Guard 4 and new Prompt Guard 2 models ğŸ”¥ Llama Guard 4 is a new model to filter model inputs/outputs both text-only and image ğŸ›¡ï¸ use it before and after LLMs/VLMs! meta-llama/Llama-Guard-4-12B Prompt Guard 2 22M &amp; 86M are smol models to prevent model jailbreaks and prompt injections âš” meta-llama/Llama-Prompt-Guard-2-22M meta-llama/Llama-Guard-4-12B Both come with new release of transformers ğŸ¤— Try the model right away ğŸ‘‰ğŸ»https://github.com/huggingface/huggingface-llama-recipes/blob/main/llama_guard_4.ipynb Read our blog to learn more and easily get started ğŸ‘‰ğŸ» https://huggingface.co/blog/llama-guard-4 ğŸ¦™ See translation</description><pubDate>Thu, 01 May 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/662080130760638</guid></item><item><title>At</title><link>https://huggingface.co/posts/jsulz/177281245715492</link><description>At xet-team we've been hard at work bringing a new generation of storage to the Hugging Face community, and weâ€™ve crossed some major milestones: ğŸ‘· Over 2,000 builders and nearing 100 organizations with access to Xet ğŸš€ Over 70,000 model and dataset repositories are Xet-backed ğŸ¤¯ 1.4 petabytes managed by Xet As we move repos from LFS to Xet for everyone we onboard, weâ€™re pushing our content-addressed store (CAS). Check out the chart below ğŸ‘‡ of CAS hitting up to 150 Gb/s throughput this past week. All of this growth is helping us build richer insights. We expanded our repo graph, which maps how Xet-backed repositories on the Hub share bytes with each other. Check out the current network in the image below (nodes are repositories, edges are where repos share bytes) and visit the space to see how different versions of Qwen, Llama, and Phi models are grouped together xet-team/repo-graph Join the waitlist to get access! https://huggingface.co/join/xet See translation</description><pubDate>Thu, 01 May 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/177281245715492</guid></item><item><title>Want to know which AI models are least likely to hallucinate â€” and how to keep yours from spiking hallucinations by 20%?</title><link>https://huggingface.co/posts/fdaudens/447589021070314</link><description>Want to know which AI models are least likely to hallucinate â€” and how to keep yours from spiking hallucinations by 20%? A new benchmark called Phare, by Giskard, tested leading models across multiple languages, revealing three key findings: 1ï¸âƒ£ Popular models aren't necessarily factual. Some models ranking highest in user satisfaction benchmarks like LMArena are actually more prone to hallucination. 2ï¸âƒ£ The way you ask matters - a lot. When users present claims confidently ("My teacher said..."), models are 15% less likely to correct misinformation vs. neutral framing ("I heard..."). 3ï¸âƒ£ Telling models to "be concise" can increase hallucination by up to 20%. What's also cool is that the full dataset is public - use them to test your own models or dive deeper into the results! H/t @ davidberenstein1957 for the link. - Study: https://www.giskard.ai/knowledge/good-answers-are-not-necessarily-factual-answers-an-analysis-of-hallucination-in-leading-llms - Leaderboard:...</description><pubDate>Thu, 01 May 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/447589021070314</guid></item><item><title>ğŸš€ Excited to Share Our Latest Work: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformerï½</title><link>https://huggingface.co/posts/sanaka87/703703147958180</link><description>ğŸš€ Excited to Share Our Latest Work: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformerï½ ğŸ¨ Daily Paper: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer (2504.20690) ğŸ”“ Code is now open source! ğŸ”¥ Huggingface DEMO: RiverZ/ICEdit ğŸŒ Project Website: https://river-zhang.github.io/ICEdit-gh-pages/ ğŸ  GitHub Repository: https://github.com/River-Zhang/ICEdit/blob/main/scripts/gradio_demo.py ğŸ¤— Huggingface: sanaka87/ICEdit-MoE-LoRA ğŸ“„ arxiv Paper: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer (2504.20690) ğŸ”¥ Why itâ€™s cool: - Achieves high-quality, multi-task image editing. - Uses only 1% of the training parameters and 0.1% of the training data compared to existing methods â€” extremely efficient - Beats several commercial models on background preservation, ID control, and consistency - Open-...</description><pubDate>Thu, 01 May 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sanaka87/703703147958180</guid></item><item><title>Hi folks! Excited to share a new feature from the Gradio team along with a tutorial.</title><link>https://huggingface.co/posts/abidlabs/767083410530735</link><description>Hi folks! Excited to share a new feature from the Gradio team along with a tutorial. If you don't already know, Gradio is an open-source Python library used to build interfaces for machine learning models. Beyond just creating UIs, Gradio also exposes API capabilities and now, Gradio apps can be launched Model Context Protocol (MCP) servers for LLMs. If you already know how to use Gradio, there are only two additional things you need to do: * Add standard docstrings to your function (these will be used to generate the descriptions for your tools for the LLM) * Set mcp_server=True in launch() Here's a complete example (make sure you already have the latest version of Gradio installed): import gradio as gr def letter_counter ( word, letter ): """Count the occurrences of a specific letter in a word. Args: word: The word or phrase to analyze letter: The letter to count occurrences of Returns: The number of times the letter appears in the word """ return...</description><pubDate>Thu, 01 May 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/abidlabs/767083410530735</guid></item><item><title>ğ—œ ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± ğ—® ğ—Ÿğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ˜ğ—¼ ğ˜€ğ—°ğ—µğ—²ğ—±ğ˜‚ğ—¹ğ—² ğ—²ğ˜ƒğ—²ğ—»ğ˜ğ˜€ ğ˜„ğ—¶ğ˜ğ—µ ğ—šğ—¥ğ—£ğ—¢! ğŸ‘‘ ğŸ—“ï¸</title><link>https://huggingface.co/posts/anakin87/692858936883406</link><description>ğ—œ ğ˜ğ—¿ğ—®ğ—¶ğ—»ğ—²ğ—± ğ—® ğ—Ÿğ—®ğ—»ğ—´ğ˜‚ğ—®ğ—´ğ—² ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ˜ğ—¼ ğ˜€ğ—°ğ—µğ—²ğ—±ğ˜‚ğ—¹ğ—² ğ—²ğ˜ƒğ—²ğ—»ğ˜ğ˜€ ğ˜„ğ—¶ğ˜ğ—µ ğ—šğ—¥ğ—£ğ—¢! ğŸ‘‘ ğŸ—“ï¸ âœï¸ Blog post: https://huggingface.co/blog/anakin87/qwen-scheduler-grpo I experimented with GRPO lately. I am fascinated by models learning from prompts and rewards - no example answers needed like in Supervised Fine-Tuning. After the DeepSeek boom, everyone is trying GRPO with GSM8K or the Countdown Game... I wanted a different challenge, like ğ˜ğ—²ğ—®ğ—°ğ—µğ—¶ğ—»ğ—´ ğ—® ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ˜ğ—¼ ğ—°ğ—¿ğ—²ğ—®ğ˜ğ—² ğ—® ğ˜€ğ—°ğ—µğ—²ğ—±ğ˜‚ğ—¹ğ—² ğ—³ğ—¿ğ—¼ğ—º ğ—® ğ—¹ğ—¶ğ˜€ğ˜ ğ—¼ğ—³ ğ—²ğ˜ƒğ—²ğ—»ğ˜ğ˜€ ğ—®ğ—»ğ—± ğ—½ğ—¿ğ—¶ğ—¼ğ—¿ğ—¶ğ˜ğ—¶ğ—²ğ˜€. Choosing an original problem forced me to: ğŸ¤” Think about the problem setting ğŸ§¬ Generate data ğŸ¤ Choose the right base model ğŸ† Design reward functions (and experiencing reward hacking) ğŸ”„ Run multiple rounds of training, hoping that my model would learn something. A fun and rewarding ğŸ˜„ experience. I learned a lot of things, that I want to share with you. ğŸ‘‡ âœï¸ Blog post: https://huggingface.co/blog/anakin87/qwen-scheduler-grpo ğŸ’» Code: https://github.com/anakin87/qwen-scheduler-grpo ğŸ¤— Hugging Face collection...</description><pubDate>Thu, 01 May 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/anakin87/692858936883406</guid></item><item><title>Qwen 3 models releasedğŸ”¥</title><link>https://huggingface.co/posts/merterbak/337137510653930</link><description>Qwen 3 models releasedğŸ”¥ It offers 2 MoE and 6 dense models with following parameter sizes: 0.6B, 1.7B, 4B, 8B, 14B, 30B(MoE), 32B, and 235B(MoE). Models: Qwen/qwen3-67dd247413f0e2e4f653967f Blog: https://qwenlm.github.io/blog/qwen3/ Demo: Qwen/Qwen3-Demo GitHub: https://github.com/QwenLM/Qwen3 âœ… Pre-trained 119 languages(36 trillion tokens) and dialects with strong translation and instruction following abilities. (Qwen2.5 was pre-trained on 18 trillion tokens.) âœ…Qwen3 dense models match the performance of larger Qwen2.5 models. For example, Qwen3-1.7B/4B/8B/14B/32B perform like Qwen2.5-3B/7B/14B/32B/72B. âœ… Three stage done while pretraining: â€¢ Stage 1: General language learning and knowledge building. â€¢ Stage 2: Reasoning boost with STEM, coding, and logic skills. â€¢ Stage 3: Long context training âœ… It supports MCP in the model âœ… Strong agent skills âœ… Supports seamless between thinking mode (for hard tasks like math and coding) and non-thinking mode (for fast chatting) inside chat...</description><pubDate>Thu, 01 May 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/337137510653930</guid></item><item><title>Introducing the ONNX model explorer: Browse, search, and visualize neural networks directly in your browser. ğŸ¤¯ A great tool for anyone studying Machine Learning! We're also releasing the entire dataset of graphs so you can use them in your own projects! ğŸ¤—</title><link>https://huggingface.co/posts/Xenova/886388075601859</link><description>Introducing the ONNX model explorer: Browse, search, and visualize neural networks directly in your browser. ğŸ¤¯ A great tool for anyone studying Machine Learning! We're also releasing the entire dataset of graphs so you can use them in your own projects! ğŸ¤— Check it out! ğŸ‘‡ Demo: onnx-community/model-explorer Dataset: onnx-community/model-explorer Source code: https://github.com/xenova/model-explorer See translation</description><pubDate>Thu, 01 May 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Xenova/886388075601859</guid></item><item><title>I tested Qwen3 235b and 32b and they are both worse than Qwen2.5 32b.</title><link>https://huggingface.co/posts/onekq/480034917828267</link><description>I tested Qwen3 235b and 32b and they are both worse than Qwen2.5 32b. onekq-ai/WebApp1K-models-leaderboard I used non-thinking mode because the thinking mode is too slow ğŸ¢ğŸ¢ğŸ¢ to be usable in any way. Sigh ... See translation</description><pubDate>Thu, 01 May 2025 17:20:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/480034917828267</guid></item></channel></rss>