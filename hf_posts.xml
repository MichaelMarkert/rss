<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸ§  ThinkFlow: The Revolutionary Platform That Gives LLMs the Power to Think ğŸš€</title><link>https://huggingface.co/posts/openfree/953705052271600</link><description>ğŸ§  ThinkFlow: The Revolutionary Platform That Gives LLMs the Power to Think ğŸš€ Hello AI community! We're excited to introduce you to ThinkFlow, an innovative service that transforms how language models solve problems. ğŸ‰ VIDraft/ThinkFlow-llama âœ¨ What is ThinkFlow? ThinkFlow is a groundbreaking platform that automatically applies step-by-step reasoning capabilities to existing LLM models without any modifications. It makes complex problem-solving transparent, allowing you to witness the model's thought process in real-time. ğŸ” Key Features Reasoning Without Model Modifications: Add step-by-step reasoning while utilizing existing LLMs as they are âš™ï¸ Visualized Thinking Process: See exactly how the model analyzes and solves problems ğŸ‘ï¸ Before &amp; After Comparison: Compare standard responses with reasoning-enhanced outputs in real-time ğŸ“Š Improved Accuracy: Deliver more accurate solutions for complex math and logic problems ğŸ“ˆ Educational Value: Teach students systematic approaches to problem-...</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/953705052271600</guid></item><item><title>Dropping an entire collection of Style Intermixing Adapters on StrangerZone HF â€” including Realism, Anime, Sketch, Texture-Rich 3D Experimentals, Automotive Concept Images, and LoRA models based on Flux.1, SD 3.5 Turbo/Large, Stable Diffusion XL ğŸ¨</title><link>https://huggingface.co/posts/prithivMLmods/567717355691306</link><description>Dropping an entire collection of Style Intermixing Adapters on StrangerZone HF â€” including Realism, Anime, Sketch, Texture-Rich 3D Experimentals, Automotive Concept Images, and LoRA models based on Flux.1, SD 3.5 Turbo/Large, Stable Diffusion XL ğŸ¨ â•°â”ˆâ¤Collection : âœ sketch : strangerzonehf/sketch-fav-675ba869c7ceaec7e652ee1c âœ sketch2 : strangerzonehf/q-series-sketch-678e3503bf3a661758429717 âœ automotive : strangerzonehf/automotive-3d-675bb31a491d8c264d45d843 âœ texture 3d : strangerzonehf/flux-3dxl-engine-674833c14a001d5b1fdb5139 âœ super 3d : strangerzonehf/super-3d-engine-6743231d69f496df97addd2b âœ style mix : strangerzonehf/mixer-engine-673582c9c5939d8aa5bf9533 âœ realism : strangerzonehf/realism-engine-67343495b6daf0fbdb904cc1 â•°â”ˆâ¤The Entire Collection : âœ flux.1 : prithivMLmods/flux-lora-collections-66dd5908be2206cfaa8519be âœ flux-ultimate-lora-collection : strangerzonehf/Flux-Ultimate-LoRA-Collection âœ sd 3.5 large / turbo : prithivMLmods/sd-35-large-lora-671b39d7bc2e7f71a446b163...</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/567717355691306</guid></item><item><title>FramePack Full Tutorial: 1-Click to Install on Windows - Up to 120 Second Image-to-Videos with 6GB &gt;</title><link>https://huggingface.co/posts/MonsterMMORPG/401294184812659</link><description>FramePack Full Tutorial: 1-Click to Install on Windows - Up to 120 Second Image-to-Videos with 6GB &gt; https://youtu.be/HwMngohRmHg Tutorial video : https://youtu.be/HwMngohRmHg FramePack from legendary lllyasviel full Windows local tutorial with a very advanced Gradio app to generate consistent videos from images with as long as 120 seconds and as low as 6 GB GPUs. This tutorial will show you step by step how to install and use FramePack locall with a very advanced Graido app. Moreover, I have published installers for cloud services such as RunPod and Massed Compute for those GPU poor and who wants to scale. ğŸ”— Full Instructions, Installers and Links Shared Post (the one used in the tutorial) â¤µï¸ â–¶ï¸ https://www.patreon.com/posts/click-to-open-post-used-in-tutorial-126855226 ğŸ”— SECourses Official Discord 10500+ Members â¤µï¸ â–¶ï¸ https://discord.com/servers/software-engineering-courses-secourses-772774097734074388 ğŸ”— Stable Diffusion, FLUX, Generative AI Tutorials and Resources GitHub â¤µï¸ â–¶ï¸...</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/401294184812659</guid></item><item><title>OpenAI just released a 34-page practical guide to building agents,</title><link>https://huggingface.co/posts/hesamation/750913380201236</link><description>OpenAI just released a 34-page practical guide to building agents, Here's 10 things it teaches us: 1âœ agents are different from workflows: they are complete autonomous systems that perform tasks on your behalf. many applications use LLMs for workflows, but this is not an agent. 2âœ use them for tricky stuff: complex decision making, dynamic rules, unstructured data 3âœ core recipe: each agent has three main components: Model (the brain), Tools, Instructions on how to behave 4âœ choose the right brain: set up evals to get a baseline performance, use a smart model to see what's possible, gradually downgrade the model for cost and speed 5âœ tools are key: choose well-defined and tested tools. an agent needs tools to retrieve data and context, and take actions. 6âœ instruction matters A LOT: be super clear telling the agent its goals, steps, and rules. Vague instructions = unpredictable agent. Be explicit. 7âœ start simple, then scale: often a single agent with several tools is ok. don't jump...</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/750913380201236</guid></item><item><title>New king of open VLMs: InternVL3 takes Qwen 2.5's crown! ğŸ‘‘</title><link>https://huggingface.co/posts/m-ric/531366391123392</link><description>New king of open VLMs: InternVL3 takes Qwen 2.5's crown! ğŸ‘‘ InternVL have been a wildly successful series of model : and the latest iteration has just taken back their crown thanks to their superior, natively multimodal vision training pipeline. â¡ï¸ Most of the vision language models (VLMs) these days are built like Frankenstein : take a good text-only Large Language Model (LLM) backbone, stitch a specific vision transformer (ViT) on top of it. Then the training is sequential ğŸ”¢ : 1. Freeze the LLM weights while you train the ViT only to work with the LLM part, then 2. Unfreeze all weights to train all weights in order to work together. ğŸ’« The Shanghai Lab decided to challenge this paradigm and chose this approach that they call "native". For each of their model sizes, they still start from a good LLM (mostly Qwen-2.5 series, did I tell you I'm a huge fan of Qwen? â¤ï¸), and stitch the ViT, but they don't freeze anything : they train all weights together with interleaved text and image...</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/531366391123392</guid></item><item><title>Gemini 2.5 Flash is here! We excited launch our first hybrid reasoning Gemini model. In Flash 2.5 developer can turn thinking off.</title><link>https://huggingface.co/posts/philschmid/318540305385241</link><description>Gemini 2.5 Flash is here! We excited launch our first hybrid reasoning Gemini model. In Flash 2.5 developer can turn thinking off. **TL;DR:** - ğŸ§  Controllable "Thinking" with thinking budget with up to 24k token - ğŸŒŒ 1 Million multimodal input context for text, image, video, audio, and pdf - ğŸ› ï¸ Function calling, structured output, google search &amp; code execution. - ğŸ¦ $0.15 1M input tokens; $0.6 or $3.5 (thinking on) per million output tokens (thinking tokens are billed as output tokens) - ğŸ’¡ Knowledge cut of January 2025 - ğŸš€ Rate limits - Free 10 RPM 500 req/day - ğŸ…Outperforms 2.0 Flash on every benchmark Try it â¬‡ï¸ https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-preview-04-17 See translation</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/philschmid/318540305385241</guid></item><item><title>Wan2.1-FLF2VğŸ¥  a 14B start-end frame video generation model just released by Alibaba_WanğŸ”¥</title><link>https://huggingface.co/posts/AdinaY/926684469376880</link><description>Wan2.1-FLF2VğŸ¥ a 14B start-end frame video generation model just released by Alibaba_WanğŸ”¥ Wan-AI/Wan2.1-FLF2V-14B-720P âœ¨ Give it two images (start &amp; end), it generates a smooth, high-quality video in between. âœ¨ Apache 2.0 licensed âœ¨ Built on DiT + Flow Matching See translation</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/926684469376880</guid></item><item><title>anyone have all their spaces stuck in building now?</title><link>https://huggingface.co/posts/educrpg/528156241277720</link><description>anyone have all their spaces stuck in building now? See translation</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/educrpg/528156241277720</guid></item><item><title>ğŸ§‘â€ğŸ« I wrote a brief blogpost to give An Introduction to AI Model Optimization Techniques!</title><link>https://huggingface.co/posts/davidberenstein1957/707396604835513</link><description>ğŸ§‘â€ğŸ« I wrote a brief blogpost to give An Introduction to AI Model Optimization Techniques! URL: https://huggingface.co/blog/PrunaAI/introduction-to-ai-model-optimization-techniques See translation</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/davidberenstein1957/707396604835513</guid></item><item><title>GPT-4.1 dropped this week - and it puts OpenAI back in the race for coding &amp; agentic leadership.</title><link>https://huggingface.co/posts/eugenesiow/891114817044097</link><description>GPT-4.1 dropped this week - and it puts OpenAI back in the race for coding &amp; agentic leadership. âš™ï¸ API only - no ChatGPT toggle for this. ğŸ’» Coding performance is back on par with Claude 3.7 Sonnet &amp; Gemini 2.5 Pro (though Gemini still leads). ğŸ’¸ Pricing: â€¢ Full: $3.50 / 1M tokens â€¢ Mini: $0.70 / 1M â€¢ Nano: $0.17 / 1M ğŸ‘‰ Gemini 2.5 Pro = best price/perf ($3.44 / 1M) ğŸ˜µ Claude 3.5 Sonnet = $6 / 1M (!) ğŸ§  Not a "thinking" model. ğŸ“Š Mini shines on general reasoning tasks (e.g. GPQA), but only the full model holds up in SWE-bench-verified (GitHub issue solving). See translation</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/eugenesiow/891114817044097</guid></item></channel></rss>