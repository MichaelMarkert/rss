<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>There is no anxiety quite like powering up 2KW of basement compute after rewiring it all. Small bit of trouble with the horizontal 3090 because I misread my motherboard manual, but otherwise so far so good.. Next we see if I've built up enough cooling to hit my target TDP on those 3-slot nvlinked cards especially.  The 4-slot bridges are much easier to work with but their prices went bananas and I couldn't acquire a second, so gotta get a little creative with intakes.</title><link>https://huggingface.co/posts/mike-ravkine/305096078349013</link><description>There is no anxiety quite like powering up 2KW of basement compute after rewiring it all. Small bit of trouble with the horizontal 3090 because I misread my motherboard manual, but otherwise so far so good.. Next we see if I've built up enough cooling to hit my target TDP on those 3-slot nvlinked cards especially. The 4-slot bridges are much easier to work with but their prices went bananas and I couldn't acquire a second, so gotta get a little creative with intakes. See translation</description><pubDate>Sun, 09 Nov 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mike-ravkine/305096078349013</guid></item><item><title>Hugging Face MCP Server v0.2.40</title><link>https://huggingface.co/posts/evalstate/266178055669254</link><description>Hugging Face MCP Server v0.2.40 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Improved progressive disclosure and descriptions for Jobs tool. See translation</description><pubDate>Sun, 09 Nov 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/evalstate/266178055669254</guid></item><item><title>Kimi K2 Thinking is now live on the hub ðŸ”¥</title><link>https://huggingface.co/posts/AdinaY/911192969719025</link><description>Kimi K2 Thinking is now live on the hub ðŸ”¥ moonshotai/Kimi-K2-Thinking âœ¨ 1T MoE for deep reasoning &amp; tool use âœ¨ Native INT4 quantization = 2Ã— faster inference âœ¨ 256K context window âœ¨ Modified MIT license See translation</description><pubDate>Sun, 09 Nov 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/911192969719025</guid></item><item><title>You can now run Kimi K2 Thinking locally with our Dynamic 1-bit GGUFs:</title><link>https://huggingface.co/posts/danielhanchen/603265034116639</link><description>You can now run Kimi K2 Thinking locally with our Dynamic 1-bit GGUFs: unsloth/Kimi-K2-Thinking-GGUF We shrank the 1T model to 245GB (-62%) &amp; retained ~85% of accuracy on Aider Polyglot. Run on &gt;247GB RAM for fast inference. We also collaborated with the Moonshot AI Kimi team on a system prompt fix! ðŸ¥° Guide + fix details: https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally See translation</description><pubDate>Sun, 09 Nov 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/603265034116639</guid></item><item><title>ðŸ’¸ðŸ¤‘You donâ€™t need 100 GPUs to train something amazing!</title><link>https://huggingface.co/posts/lunarflu/887027437678824</link><description>ðŸ’¸ðŸ¤‘You donâ€™t need 100 GPUs to train something amazing! Our Smol Training Playbook teaches you a better path to world-class LLMs, for free! Check out the #1 trending space on ðŸ¤— : HuggingFaceTB/smol-training-playbook See translation</description><pubDate>Sun, 09 Nov 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lunarflu/887027437678824</guid></item><item><title>Qwen Image Base Model Training vs FLUX SRPO Training 20 images comparison (top ones Qwen bottom ones FLUX) - Same Dataset (28 imgs) - I can't return back to FLUX such as massive difference - Qwen destroys the FLUX at complex prompts and emotions</title><link>https://huggingface.co/posts/MonsterMMORPG/857881021159484</link><description>Qwen Image Base Model Training vs FLUX SRPO Training 20 images comparison (top ones Qwen bottom ones FLUX) - Same Dataset (28 imgs) - I can't return back to FLUX such as massive difference - Qwen destroys the FLUX at complex prompts and emotions Full tutorial link &gt; https://www.youtube.com/watch?v=DPX3eBTuO_Y Info This is a full comprehensive step-by-step tutorial for how to train Qwen Image models. This tutorial covers how to do LoRA training and full Fine-Tuning / DreamBooth training on Qwen Image models. It covers both the Qwen Image base model and the Qwen Image Edit Plus 2509 model. This tutorial is the product of 21 days of full R&amp;D, costing over $800 in cloud services to find the best configurations for training. Furthermore, we have developed an amazing, ultra-easy-to-use Gradio app to use the legendary Kohya Musubi Tuner trainer with ease. You will be able to train locally on your Windows computer with GPUs with as little as 6 GB of VRAM for both LoRA and Fine-Tuning....</description><pubDate>Sun, 09 Nov 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/857881021159484</guid></item><item><title>FEETECH STS3250 Stall Torque and Repeatability Tests</title><link>https://huggingface.co/posts/branikita/935466322450846</link><description>FEETECH STS3250 Stall Torque and Repeatability Tests We recently tested the FEETECH STS3250 servo, comparing actual performance with the official specifications. While the datasheet lists a stall torque of 50 kgÂ·cm at 12 V, our real-world measurements showed: - 25 kgÂ·cm sustained torque after protection activation - Up to 48 kgÂ·cm peak torque for a split second Although the built-in protection limits continuous stall torque, the servo demonstrated excellent stability and control precision. Precision and Repeatability: - Repeatability tolerance: Â±0.02 mm at the end of a 95 mm arm - Smooth motion response with PID control and 12-bit (4096-step) magnetic encoder - Reliable performance for high-accuracy robotics and automation applications #Feetech #STS3250 #ServoTest #Robotics #Engineering #PrecisionControl #HardwareReview See translation</description><pubDate>Sun, 09 Nov 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/935466322450846</guid></item><item><title>Qwen Image Models Training - 0 to Hero Level Tutorial - LoRA &amp; Fine Tuning - Base &amp; Edit Model -</title><link>https://huggingface.co/posts/MonsterMMORPG/482948371636786</link><description>Qwen Image Models Training - 0 to Hero Level Tutorial - LoRA &amp; Fine Tuning - Base &amp; Edit Model - https://youtu.be/DPX3eBTuO_Y This is a full comprehensive step-by-step tutorial for how to train Qwen Image models. This tutorial covers how to do LoRA training and full Fine-Tuning / DreamBooth training on Qwen Image models. It covers both the Qwen Image base model and the Qwen Image Edit Plus 2509 model. This tutorial is the product of 21 days of full R&amp;D, costing over $800 in cloud services to find the best configurations for training. Furthermore, we have developed an amazing, ultra-easy-to-use Gradio app to use the legendary Kohya Musubi Tuner trainer with ease. You will be able to train locally on your Windows computer with GPUs with as little as 6 GB of VRAM for both LoRA and Fine-Tuning. Furthermore, I have shown how to train a character (person), a product (perfume) and a style (GTA5 artworks). Tutorial Link : https://youtu.be/DPX3eBTuO_Y See translation</description><pubDate>Sun, 09 Nov 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/482948371636786</guid></item><item><title>In November, with</title><link>https://huggingface.co/posts/branikita/750415743005221</link><description>In November, with Robonine are starting work on the next version of the SO ARM 102 manipulator. The version will be open source and agreed upon with @ therobotbuilder the creator of the original manipulator. We are planning to: - increase positioning accuracy by approximately 2x using Feetech STS 3250 motors - increase working payload from 200g to 300g - increase rigidity using parametric design optimization and stiffer plastic - increase length to 550 mm - increase folding angles - use ISO 9409-1-50-4-M6 mounting standard for the gripper - use a parallel gripper in the default version - update the mounting plate for different camera types, M3 grid with 12.5 mm pitch - add table mounting standard 80x80 M8 The number of degrees of freedom and basic kinematics will remain the same. Are there other things missing for working with SO ARM 100? - Any standard inputs/outputs, for example? - Status indicators? - Perhaps some types of mounting for third-party grippers are more preferable? -...</description><pubDate>Sun, 09 Nov 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/750415743005221</guid></item><item><title>NEW DATASET:</title><link>https://huggingface.co/posts/unmodeled-tyler/682622498255135</link><description>NEW DATASET: vanta-research/human-ai-collaboration-1 We are excited to share the first iteration of our dataset focused on human-AI collaborative tasks! This dataset contains 3,050 lines of warm, collaborative, and natural conversational examples designed to teach the model how to effectively and efficiently problem solve back and forth with a human. Additionally, the examples include &lt;think&gt; tags, showing the model proper internal reasoning. VANTA Research is committed to contributing back to the open source community in order to make AI development more accessible, transparent, and beneficial for all. See translation</description><pubDate>Sun, 09 Nov 2025 05:21:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/682622498255135</guid></item></channel></rss>