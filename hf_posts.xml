<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The Architecture of 2026: Beyond the Token Trap üöÄ</title><link>https://huggingface.co/posts/mindchain/406965796956131</link><description>The Architecture of 2026: Beyond the Token Trap üöÄ We are witnessing a tectonic shift in Transformer architecture. It‚Äôs no longer just about "predicting the next token"‚Äîit‚Äôs about executing latent plans on a high-speed data highway. What happens when we combine DeepSeek‚Äôs stability with Google‚Äôs strategic intelligence? 1Ô∏è‚É£ The Infrastructure: DeepSeek‚Äôs mHC Moving from a single-lane residual stream to a multi-lane highway. Using the Birkhoff Polytope, mHC ensures mathematical stability (Identity Mapping) while routing specialized data through dedicated lanes. 2Ô∏è‚É£ The Intelligence: Google‚Äôs Meta-Controller An internal AI unit that lives inside the Transformer. It escapes the "Token Trap" by extracting data to create a latent plan, steering the model via Temporal Abstraction. The Synergy: In a Topological Transformer, the Meta-Controller finally has the "dedicated lanes" it needs to steer complex reasoning without causing gradient explosions. We aren't just making models bigger; we are...</description><pubDate>Mon, 05 Jan 2026 13:43:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mindchain/406965796956131</guid></item><item><title>2025.1  - DeepSeek entered the scene, backed by High Flyer Quant</title><link>https://huggingface.co/posts/AdinaY/447609737740306</link><description>2025.1 - DeepSeek entered the scene, backed by High Flyer Quant 2026.1 - IQuest enters the game, backed by Uniquant Quant üìà and launching IQuest-Coder on huggingface https://huggingface.co/collections/IQuestLab/iquest-coder ‚ú® 40B models: Instruct / Thinking / Loop ‚ú® Loop = MoE-level performance with only ~5% extra training cost ‚ú® Native 128K context See translation</description><pubDate>Mon, 05 Jan 2026 13:43:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/447609737740306</guid></item><item><title>The recent update to Meta's ad algorithm is very difficult to crack, and even the latest models struggle to keep up with it. To address this, we've created a small experimental dataset for fine-tuning models to better tackle Meta's Andromeda algorithm:</title><link>https://huggingface.co/posts/Sri-Vigneshwar-DJ/560303758243355</link><description>The recent update to Meta's ad algorithm is very difficult to crack, and even the latest models struggle to keep up with it. To address this, we've created a small experimental dataset for fine-tuning models to better tackle Meta's Andromeda algorithm: Sri-Vigneshwar-DJ/hawky-ai-andromeda-dataset See translation</description><pubDate>Mon, 05 Jan 2026 13:43:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Sri-Vigneshwar-DJ/560303758243355</guid></item><item><title>Happy New Year, Hugging Face!</title><link>https://huggingface.co/posts/unmodeled-tyler/448164360090025</link><description>Happy New Year, Hugging Face! It's been a crazy year for me! This year I launched VANTA Research as a solo operator and managed to push out 14 original open source finetunes and 5 datasets in the span of about 4 months, completely on my own. The reception has been much higher than I ever anticipated and sincerely appreciate everyone that's checked out my work thus far. The good news is, I'm just getting started! In 2026 you can expect even more original models from VANTA Research, more open source datasets, and maybe some other cool things as well? üëÄ 2026 is gonna be big for AI in general, and I can't wait to experience it with all of you! See translation</description><pubDate>Mon, 05 Jan 2026 13:43:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/448164360090025</guid></item><item><title>I just stress-tested the Beast: MiniMax-M2.1 on Z8 Fury G5.</title><link>https://huggingface.co/posts/mitkox/359337661300702</link><description>I just stress-tested the Beast: MiniMax-M2.1 on Z8 Fury G5. 2101 tokens/sec. FORTY concurrent clients. That's 609 t/s out, 1492 t/s in. The model outputs fire faster than I can type, but feeds on data like a black hole on cheat day. But wait, there's more! Threw it into Claude Code torture testing with 60+ tools, 8 agents (7 sub-agents because apparently one wasn't enough chaos). It didn't even flinch. Extremely fast, scary good at coding. The kind of performance that makes you wonder if the model's been secretly reading Stack Overflow in its spare time lol 3 months ago, these numbers lived in my "maybe in ‚Äú2030 dreams. Today it's running on my desk AND heaths my home office during the winter! See translation</description><pubDate>Mon, 05 Jan 2026 13:43:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/359337661300702</guid></item><item><title>Happy New Year 2026</title><link>https://huggingface.co/posts/Reubencf/286862187076146</link><description>Happy New Year 2026 i have planned to build many things this year , most of them will be cheaper or free alternative's to paid products i am looking forward to release some useful spaces ‚úåÔ∏è Stay Tuned ! See translation</description><pubDate>Mon, 05 Jan 2026 13:43:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Reubencf/286862187076146</guid></item><item><title>The list of hands-on notebooks (some beginner-friendly!) to get started with fine-tuning using TRL keeps growing!!</title><link>https://huggingface.co/posts/sergiopaniego/761887592219179</link><description>The list of hands-on notebooks (some beginner-friendly!) to get started with fine-tuning using TRL keeps growing!! ‚Ä¢ SFT ‚Ä¢ GRPO ‚Ä¢ Tool calling &amp; agents ‚Ä¢ RL environments with OpenEnv ‚Ä¢ LLMs and VLMs ‚ú® Many run on FREE Colab, making it super easy to get started fast! https://github.com/huggingface/trl/tree/main/examples/notebooks See translation</description><pubDate>Mon, 05 Jan 2026 13:43:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/761887592219179</guid></item><item><title>üö® Ads might be coming to AI assistants. But what if the ad is the response?</title><link>https://huggingface.co/posts/nkasmanoff/401252358780981</link><description>üö® Ads might be coming to AI assistants. But what if the ad is the response? Introducing ShillLM ‚Äì a demo showing how activation steering can subtly nudge an LLM to favor specific brands without touching the prompt. Chat with it yourself. See exactly how it's influencing you. üîó nkasmanoff/shillm See translation</description><pubDate>Mon, 05 Jan 2026 13:43:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nkasmanoff/401252358780981</guid></item><item><title>üëâ What happened in AI in 2025? üëà</title><link>https://huggingface.co/posts/pcuenq/421927498996428</link><description>üëâ What happened in AI in 2025? üëà We prepared the 2025 version of the HF AI Timeline Grid, highlighting open vs API-based model releases, and allowing you to browse and filter by access, modality, and release type! Play with it here: 2025-ai-timeline/2025-ai-timeline Here's my personal quarterly TL;DR: 1Ô∏è‚É£ Q1 ‚Äî Learning to Reason Deepseek not only releases a top-notch reasoning model, but shows how to train them and compete with closed frontier models. OpenAI debuts Deep Research. Significant milestones: DeepSeek R1 &amp; R1-Zero, Qwen 2.5 VL, OpenAI Deep Research, Gemini 2.5 Pro (experimental) 2Ô∏è‚É£ Q2 ‚Äî Multimodality and Coding More LLMs embrace multimodality by default, and there's a surge in coding agents. Strong vision, audio, and generative models emerge. Significant milestones: Llama 4, Qwen 3, Imagen 4, OpenAI Codex, Google Jules, Claude 4 3Ô∏è‚É£ Q3 ‚Äî "Gold" rush, OpenAI opens up, the community goes bananas Flagship models get gold in Math olympiads and hard benchmarks. OpenAI...</description><pubDate>Mon, 05 Jan 2026 13:43:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/pcuenq/421927498996428</guid></item><item><title>‚úÖ New Article: *Operating an SI-Core (v0.1)*</title><link>https://huggingface.co/posts/kanaria007/128065561044409</link><description>‚úÖ New Article: *Operating an SI-Core (v0.1)* Title: üõ†Ô∏è Operating SI-Core: Dashboards, Playbooks, and Human Loops üîó https://huggingface.co/blog/kanaria007/operating-si-core --- Summary: Designing an SI-Core is only half the job ‚Äî the other half is *running it safely at 03:00*. This guide is a *non-normative ops runbook* for SRE/Ops teams and governance owners: what to put on the *one-page dashboard*, how to wire *alerts ‚Üí actions*, when to use *safe-mode*, and how to answer the question that always arrives after an incident: &gt; ‚ÄúWhy did the system do *that*?‚Äù --- Why It Matters: ‚Ä¢ Turns ‚Äúauditable AI‚Äù into *operational reality* (not a slide deck) ‚Ä¢ Makes *ethics + rollback* measurable, actionable, and drillable ‚Ä¢ Clarifies how humans stay in the loop without becoming the bottleneck ‚Ä¢ Provides templates for *postmortems, escalation, and regulator-grade explanations* --- What‚Äôs Inside: *Core Ops Dashboard (1 page):* ‚Ä¢ Determinism/consistency, ethics/oversight, rollback/recovery,...</description><pubDate>Mon, 05 Jan 2026 13:43:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/128065561044409</guid></item></channel></rss>