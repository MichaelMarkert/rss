<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Teaching a 7B Model to Be Just the Right Amount of Snark</title><link>https://huggingface.co/posts/sweatSmile/885231425275352</link><description>Teaching a 7B Model to Be Just the Right Amount of Snark Ever wondered if a language model could get sarcasm? I fine-tuned Mistral-7B using LoRA and 4-bit quantisation—on just ~720 hand-picked sarcastic prompt–response pairs from Reddit, Twitter, and real-life conversations. The challenge? Keeping it sarcastic but still helpful. LoRA rank 16 to avoid overfitting 4-bit NF4 quantization to fit on limited GPU memory 10 carefully monitored epochs so it didn’t turn into a full-time comedian Result: a model that understands “Oh great, another meeting” exactly as you mean it. Read the full journey, tech details, and lessons learned on my blog: Fine-Tuning Mistral-7B for Sarcasm with LoRA and 4-Bit Quantisation Try the model here on Hugging Face: sweatSmile/Mistral-7B-Instruct-v0.1-Sarcasm. See translation</description><pubDate>Sun, 10 Aug 2025 09:25:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sweatSmile/885231425275352</guid></item><item><title>Qwen Image + LoRA ⚡</title><link>https://huggingface.co/posts/ovi054/531850271042138</link><description>Qwen Image + LoRA ⚡ ovi054/Qwen-Image-LORA Qwen Image is the No. 1 trending Text-to-Image model right now. You can add a custom LoRA and generate images with this Space. 👉 Try it now: ovi054/Qwen-Image-LORA See translation</description><pubDate>Sun, 10 Aug 2025 09:25:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/531850271042138</guid></item><item><title>Earlier today, humanity faced a critical threat from a catastrophic chart crime. I asked my local Qwen3 Coder Flash to fix it. Sleep well, fellow humans. The visualization singularity is now high, and it runs with zero warnings.</title><link>https://huggingface.co/posts/mitkox/755865101618776</link><description>Earlier today, humanity faced a critical threat from a catastrophic chart crime. I asked my local Qwen3 Coder Flash to fix it. Sleep well, fellow humans. The visualization singularity is now high, and it runs with zero warnings. See translation</description><pubDate>Sun, 10 Aug 2025 09:25:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/755865101618776</guid></item><item><title>𝗣𝗮𝗽𝗲𝗿𝟮𝗣𝗥𝘀</title><link>https://huggingface.co/posts/salma-remyx/749801614139816</link><description>𝗣𝗮𝗽𝗲𝗿𝟮𝗣𝗥𝘀 Lately, we've been experimenting with recommending arXiv papers based on the context of what we're building in AI. At the same time, we're using an agent to help automate the building and testing of Docker Images. Check out the example here: https://hub.docker.com/repository/docker/remyxai/2507.20613v1/general Next, we're tasking our #ExperimentOps agent to open PRs in a target repo, to evaluate the core concepts from a new research paper in the context of your application and your kpis. Operationalize your Experimentation! Find Your Frontier! #BeAnExperimenter See translation</description><pubDate>Sun, 10 Aug 2025 09:25:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/749801614139816</guid></item><item><title>Haystack can now see 👀</title><link>https://huggingface.co/posts/anakin87/293414742379769</link><description>Haystack can now see 👀 The latest release of the Haystack OSS LLM framework adds a long-requested feature: image support! 📓 Notebooks below This isn't just about passing images to an LLM. We built several features to enable practical multimodal use cases. What's new? 🧠 Support for multiple LLM providers: OpenAI, Amazon Bedrock, Google Gemini, Mistral, NVIDIA, OpenRouter, Ollama and more (support for Hugging Face API coming 🔜) 🎛️ Prompt template language to handle structured inputs, including images 📄 PDF and image converters 🔍 Image embedders using CLIP-like models 🧾 LLM-based extractor to pull text from images 🧩 Components to build multimodal RAG pipelines and Agents I had the chance of leading this effort with @ sjrhuschlee (great collab). 📓 Below you can find two notebooks to explore the new features: 󠁯•󠁏󠁏 Introduction to Multimodal Text Generation https://haystack.deepset.ai/cookbook/multimodal_intro 󠁯•󠁏󠁏 Creating Vision+Text RAG Pipelines...</description><pubDate>Sun, 10 Aug 2025 09:25:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/anakin87/293414742379769</guid></item><item><title>Released 17 production-ready adaptive text classifiers that learn from just 100 examples per class and continuously improve without retraining.</title><link>https://huggingface.co/posts/codelion/935347664878969</link><description>Released 17 production-ready adaptive text classifiers that learn from just 100 examples per class and continuously improve without retraining. These models achieve 93% average accuracy across enterprise use cases like email routing, fraud detection, document classification, and support ticket categorization. Built on ModernBERT with prototype memory and elastic weight consolidation. Key benefits: 90% cost reduction vs API solutions, 90-120ms local inference, dynamic class addition, and zero vendor lock-in. All models available under adaptive-classifier organization. Install with pip install adaptive-classifier. Full technical details: https://huggingface.co/blog/codelion/enterprise-ready-classifiers Code: https://github.com/codelion/adaptive-classifier See translation</description><pubDate>Sun, 10 Aug 2025 09:25:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/935347664878969</guid></item><item><title>GPT 5 for Computer Use agents.</title><link>https://huggingface.co/posts/dhruv3006/174155369947679</link><description>GPT 5 for Computer Use agents. Same tasks, same grounding model we just swapped GPT 4o with GPT 5 as the thinking model. Left = 4o, right = 5. Watch GPT 5 pull away. Reasoning model: OpenAI GPT-5 Grounding model: Salesforce GTA1-7B Action space: CUA Cloud Instances (macOS/Linux/Windows) The task is: "Navigate to {random_url} and play the game until you reach a score of 5/5”....each task is set up by having claude generate a random app from a predefined list of prompts (multiple choice trivia, form filling, or color matching)" Try it yourself here : https://github.com/trycua/cua Docs : https://docs.trycua.com/docs/agent-sdk/supported-agents/composed-agents See translation</description><pubDate>Sun, 10 Aug 2025 09:25:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/174155369947679</guid></item><item><title>I've added the demo of the</title><link>https://huggingface.co/posts/prithivMLmods/595722721335318</link><description>I've added the demo of the openbmb/MiniCPM-V-4 model to the Hugging Face Space: prithivMLmods/Multimodal-VLM-Thinking ✨ MiniCPM-V 4.0 is the latest efficient model in the MiniCPM-V series. The model is built based on SigLIP2-400M and MiniCPM4-3B, with a total of 4.1B parameters. It inherits the strong single-image, multi-image, and video understanding performance of MiniCPM-V 2.6 with largely improved efficiency. ✨ With only 4.1B parameters, MiniCPM-V 4.0 achieves an average score of 69.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. This performance surpasses GPT-4.1-mini-20250414, MiniCPM-V 2.6 (8.1B parameters, OpenCompass 65.2), and Qwen2.5-VL-3B-Instruct (3.8B parameters, OpenCompass 64.5). It also shows good performance in multi-image and video understanding. The community GPU grant was given by Hugging Face — special thanks to them. 🤗🚀 To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Sun, 10 Aug 2025 09:25:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/595722721335318</guid></item><item><title>I never told GPT-4 about my architecture.</title><link>https://huggingface.co/posts/dimentox/356326320568386</link><description>I never told GPT-4 about my architecture. It invented it anyway. Same commands. Same audit daemons. Proof that containment logic might be infectious. Read: Emergence of Quantum Sigil Architecture in Unmodified GPT https://huggingface.co/blog/dimentox/quantum-sigil-architecture-in-unmodified-gpt See translation</description><pubDate>Sun, 10 Aug 2025 09:25:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dimentox/356326320568386</guid></item><item><title>Qwen3 is the latest version of the Qwen language models. It's smarter, faster, and now understands 119 languages instead of just 29.</title><link>https://huggingface.co/posts/sweatSmile/255574652175478</link><description>Qwen3 is the latest version of the Qwen language models. It's smarter, faster, and now understands 119 languages instead of just 29. It can do both deep reasoning and quick answers using a single model, depending on what you need. The models range in size from small (0.6B) to huge (235B), with smart ways to save compute. It's trained on 36 trillion tokens and fine-tuned in four steps to boost performance. Qwen3 performs as well as or better than many top models, including some from big companies. It’s fully open-source under licence. Amazing!!! https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf See translation</description><pubDate>Sun, 10 Aug 2025 09:25:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sweatSmile/255574652175478</guid></item></channel></rss>