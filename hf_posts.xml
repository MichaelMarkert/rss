<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸ‰ Dhanishtha 2.0 Preview is Now Open Source!</title><link>https://huggingface.co/posts/Abhaykoul/404767027882987</link><description>ğŸ‰ Dhanishtha 2.0 Preview is Now Open Source! The world's first Intermediate Thinking Model is now available to everyone! Dhanishtha 2.0 Preview brings revolutionary intermediate thinking capabilities to the open-source community. Unlike traditional reasoning models that think once, Dhanishtha can think, answer, rethink, answer again, and continue rethinking as needed using multiple blocks between responses. ğŸš€ Key Features - Intermediate thinking: Think â†’ Answer â†’ Rethink â†’ Answer â†’ Rethink if needed... - Token efficient: Uses up to 79% fewer tokens than DeepSeek R1 on similar queries - Transparent thinking: See the model's reasoning process in real-time - Open source: Freely available for research and development HelpingAI/Dhanishtha-2.0-preview https://helpingai.co/chat See translation</description><pubDate>Wed, 02 Jul 2025 09:28:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/404767027882987</guid></item><item><title>Inference for generative ai models looks like a mine field, but thereâ€™s a simple protocol for picking the best inference:</title><link>https://huggingface.co/posts/burtenshaw/697123415535373</link><description>Inference for generative ai models looks like a mine field, but thereâ€™s a simple protocol for picking the best inference: ğŸŒ 95% of users &gt;&gt; If youâ€™re using open (large) models and need fast online inference, then use Inference providers on auto mode, and let it choose the best provider for the model. https://huggingface.co/docs/inference-providers/index ğŸ‘· fine-tuners/ bespoke &gt;&gt; If youâ€™ve got custom setups, use Inference Endpoints to define a configuration from AWS, Azure, GCP. https://endpoints.huggingface.co/ ğŸ¦« Locals &gt;&gt; If youâ€™re trying to stretch everything you can out of a server or local machine, use Llama.cpp, Jan, LMStudio or vLLM. https://huggingface.co/settings/local-apps#local-apps ğŸªŸ Browsers &gt;&gt; If you need open models running right here in the browser, use transformers.js. https://github.com/huggingface/transformers.js Let me know what youâ€™re using, and if you think itâ€™s more complex than this. See translation</description><pubDate>Wed, 02 Jul 2025 09:28:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/697123415535373</guid></item><item><title>A few months ago, I shared that I was building with</title><link>https://huggingface.co/posts/blaise-tk/599826348587266</link><description>A few months ago, I shared that I was building with @ deeivihh something like "the Steam for open source apps"... ğŸš€ Today, Iâ€™m excited to announce that Dione is now open source and live in public beta! Our mission is simple: make it easier to discover, use, and contribute to open source applications. ğŸ”— GitHub: https://github.com/dioneapp/dioneapp ğŸ’¬ Join the community: https://discord.gg/JDFJp33vrM Want to give it a try? Iâ€™d love your feedback! ğŸ‘€ See translation</description><pubDate>Wed, 02 Jul 2025 09:28:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/blaise-tk/599826348587266</guid></item><item><title>so many multimodal releases these days ğŸ¤ </title><link>https://huggingface.co/posts/merve/587280854326828</link><description>so many multimodal releases these days ğŸ¤  &gt; ERNIE-4.5-VL: new vision language MoE models by Baidu https://huggingface.co/models?search=ernie-4.5-vl &gt; new visual document retrievers by NVIDIA (sota on ViDoRe!) nvidia/llama-nemoretriever-colembed-3b-v1 nvidia/llama-nemoretriever-colembed-1b-v1 &gt; Ovis-3b: new image-text in image-text out models by Alibaba â¤µï¸ https://huggingface.co/spaces/AIDC-AI/Ovis-U1- See translation</description><pubDate>Wed, 02 Jul 2025 09:28:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/587280854326828</guid></item><item><title>The full Celestia 3 science-reasoning dataset is here!</title><link>https://huggingface.co/posts/sequelbox/523631078445392</link><description>The full Celestia 3 science-reasoning dataset is here! - 91k high-quality synthetic science prompts answered by DeepSeek-R1-0528 - subjects include physics, biology, chemistry, computer science, Earth science, astronomy, and information theory - one of the reasoning datasets powering the upcoming Shining Valiant 3 :) coming soon! GET IT NOW, FOR EVERYONE: sequelbox/Celestia3-DeepSeek-R1-0528 SUPPORT OUR RELEASES: sequelbox/SupportOpenSource with love, allegra See translation</description><pubDate>Wed, 02 Jul 2025 09:28:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sequelbox/523631078445392</guid></item><item><title>Check out new symbolic music AI front end and CLI training app</title><link>https://huggingface.co/posts/asigalov61/301808424415801</link><description>Check out new symbolic music AI front end and CLI training app https://webchatappai.github.io/midi-gen/ https://github.com/WebChatAppAi/Orpheus-Midi-Model-Maker @ Timzoid @ Csplk @ not-lain @ victor @ bartowski @ John6666 See translation</description><pubDate>Wed, 02 Jul 2025 09:28:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/asigalov61/301808424415801</guid></item><item><title>I'm auto-generating Docker Images to smoke-test new research repos ğŸ”¥</title><link>https://huggingface.co/posts/salma-remyx/520178128759841</link><description>I'm auto-generating Docker Images to smoke-test new research repos ğŸ”¥ Shared to Docker Hub daily! ğŸ³ Today's featured paper+Image: LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs https://hub.docker.com/repository/docker/remyxai/2506.21862v1/general See translation</description><pubDate>Wed, 02 Jul 2025 09:28:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/520178128759841</guid></item><item><title>In case you missed it, Hugging Face expanded its collaboration with Azure a few weeks ago with a curated catalog of 10,000 models, accessible from Azure AI Foundry and Azure ML!</title><link>https://huggingface.co/posts/pagezyhf/189638803943526</link><description>In case you missed it, Hugging Face expanded its collaboration with Azure a few weeks ago with a curated catalog of 10,000 models, accessible from Azure AI Foundry and Azure ML! @ alvarobartt cooked during these last days to prepare the one and only documentation you need, if you wanted to deploy Hugging Face models on Azure. It comes with an FAQ, great guides and examples on how to deploy VLMs, LLMs, smolagents and more to come very soon. We need your feedback: come help us and let us know what else you want to see, which model we should add to the collection, which model task we should prioritize adding, what else we should build a tutorial for. Youâ€™re just an issue away on our GitHub repo! https://huggingface.co/docs/microsoft-azure/index See translation</description><pubDate>Wed, 02 Jul 2025 09:28:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/pagezyhf/189638803943526</guid></item><item><title>ğŸ§° Free up space on the Hub with</title><link>https://huggingface.co/posts/anakin87/460502915743038</link><description>ğŸ§° Free up space on the Hub with super_squash_history ğŸ§¹ As you may know, Hugging Face Hub has storage limits on private repos (100 GB for free users, 1 TB for PROs). This weekend I did some cleanup on my private repos I went 1.58 TB down to 1 GB. ğŸ˜… Besides deleting old, unused models, the main tool I used was a lesser-known command: super_squash_history . When you train a model, you often push multiple checkpoints to the Hub. Each checkpoint = a commit. A 2.6B model in BF16 is ~5 GB. So 10 checkpoints = 50 GB. That adds up fast. While full commit history can be useful for rollbacks, it's often unnecessary for older experiments where only the final model matters. In these cases, you can use super_squash_history : it reduces your entire repo history to a single commit. https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.super_squash_history âš ï¸ super_squash_history is a non-revertible operation. Once squashed, the commit history cannot be...</description><pubDate>Wed, 02 Jul 2025 09:28:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/anakin87/460502915743038</guid></item><item><title>ğŸ“£ CALL FOR CONTRIBUTORS! ğŸ“£</title><link>https://huggingface.co/posts/sergiopaniego/945882073606324</link><description>ğŸ“£ CALL FOR CONTRIBUTORS! ğŸ“£ Following last weekâ€™s full release of Gemma 3n, we launched a dedicated recipes repo to explore and share use cases. We already added some! ğŸ§‘â€ğŸ³ Now weâ€™re inviting the community to contribute and showcase how these models shine! âœ¨ Let them cook. Check it out: https://github.com/huggingface/huggingface-gemma-recipes/issues/4 See translation</description><pubDate>Wed, 02 Jul 2025 09:28:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/945882073606324</guid></item></channel></rss>