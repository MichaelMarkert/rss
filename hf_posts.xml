<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Transformers v5 just landed! üöÄ</title><link>https://huggingface.co/posts/IlyasMoutawwakil/848772925772411</link><description>Transformers v5 just landed! üöÄ It significantly unifies and reduces modeling code across architectures, while opening the door to a whole new class of performance optimizations. My favorite new feature? ü§î The new dynamic weight loader + converter. Here‚Äôs why üëá Over the last few months, the core Transformers maintainers built an incredibly fast weight loader, capable of converting tensors on the fly while loading them in parallel threads. This means we‚Äôre no longer constrained by how parameters are laid out inside the safetensors weight files. In practice, this unlocks two big things: - Much more modular modeling code. You can now clearly see how architectures build on top of each other (DeepSeek v2 ‚Üí v3, Qwen v2 ‚Üí v3 ‚Üí MoE, etc.). This makes shared bottlenecks obvious and lets us optimize the right building blocks once, for all model families. - Performance optimizations beyond what torch.compile can do alone. torch.compile operates on the computation graph, but it can‚Äôt change...</description><pubDate>Wed, 28 Jan 2026 17:36:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/IlyasMoutawwakil/848772925772411</guid></item><item><title>ovi054/LTX-2-19b-Squish-LoRA</title><link>https://huggingface.co/posts/ovi054/176731931119322</link><description>ovi054/LTX-2-19b-Squish-LoRA ‚ö° I trained a Squish LoRA for LTX-2. Upload an image and give prompt "squish it" to get the squish video. Demo output videos are attached. üëâTry it now: ovi054/LTX-2-19b-Squish-LoRA ovi054/ltx-2-Audio-to-Video See translation</description><pubDate>Wed, 28 Jan 2026 17:36:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/176731931119322</guid></item><item><title>You can now run Kimi K2.5 locally! üî•</title><link>https://huggingface.co/posts/danielhanchen/602916263790271</link><description>You can now run Kimi K2.5 locally! üî• We shrank the 1T model to 240GB (-60%) via Dynamic 1-bit. Get &gt;40 tok/s on 242GB or 622GB VRAM/RAM for near full precision. GGUF: unsloth/Kimi-K2.5-GGUF Guide: https://unsloth.ai/docs/models/kimi-k2.5 See translation</description><pubDate>Wed, 28 Jan 2026 17:36:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/602916263790271</guid></item><item><title>Date idea: read the entire Transformers v5.0.0 release notes</title><link>https://huggingface.co/posts/sergiopaniego/425017162094241</link><description>Date idea: read the entire Transformers v5.0.0 release notes Officially stable now: https://github.com/huggingface/transformers/releases/tag/v5.0.0 See translation</description><pubDate>Wed, 28 Jan 2026 17:36:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/425017162094241</guid></item><item><title>SecureCode: security-aware code models (3B‚Äì20B), trained for review + remediation</title><link>https://huggingface.co/posts/scthornton/483996346337767</link><description>SecureCode: security-aware code models (3B‚Äì20B), trained for review + remediation I‚Äôve been frustrated by how often code assistants recommend patterns that pass tests but fail security review (e.g., string-built SQL, brittle auth logic, unsafe parsing, insecure defaults, etc.). So I built **SecureCode**: a collection of **8 code models (3B ‚Üí 20B)** trained to behave more like a security reviewer. What you should expect from SecureCode: - identify likely vuln patterns and explain *why* they‚Äôre risky - outline plausible abuse paths (defensive framing) - propose a secure rewrite (drop-in where possible) - include defense-in-depth guidance + regression tests/checks Links: - **Models:** https://huggingface.co/collections/scthornton/securecode - **Dataset:** scthornton/securecode-v2 - **Paper:** https://arxiv.org/html/2512.18542v1 SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models (2512.18542) **How to test it (copy/paste prompt):** &gt; You are a...</description><pubDate>Wed, 28 Jan 2026 17:36:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/scthornton/483996346337767</guid></item><item><title>‚úÖ New Article: *Post-Transformer Decision Cores* (v0.1)</title><link>https://huggingface.co/posts/kanaria007/711602614783831</link><description>‚úÖ New Article: *Post-Transformer Decision Cores* (v0.1) Title: üöÄ Post-Transformer Decision Cores: Goal-Native Engines Beyond LLMs üîó https://huggingface.co/blog/kanaria007/post-tranformer-decision-cores --- Summary: Transformers are powerful‚Äîbut in SI-Core they‚Äôre *not the essence of intelligence*. A *Decision Core* is anything that satisfies the *Jump contracts* (OBS/ETH/MEM/ID/EVAL + RML), and those contracts don‚Äôt require next-token prediction. This article sketches what ‚Äúpost-Transformer‚Äù looks like in practice: *goal-native, structure-aware controllers* that may use LLMs as tools‚Äîbut don‚Äôt depend on them as the runtime brain. &gt; Don‚Äôt relax the contracts. &gt; Replace the engine behind them. --- Why It Matters: ‚Ä¢ Makes LLMs *optional*: shift them to ‚Äúgenesis / exploration / explanation,‚Äù while routine high-stakes Jumps run on structured cores ‚Ä¢ Improves boring-but-critical properties: *determinism (CAS), fewer inconsistencies (SCI), fewer ETH violations (EAI), better rollback...</description><pubDate>Wed, 28 Jan 2026 17:36:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/711602614783831</guid></item><item><title>Introducing OpenMALx</title><link>https://huggingface.co/posts/tegridydev/651013007690920</link><description>Introducing OpenMALx openmalx Repository for Infosec and Machine Learning Resources OpenMALx is an organization focused on the development of datasets and models for security analysis. The project objective is to provide structured data for training and evaluating large language models in a security context. --- Technical Focus **Dataset Formatting:** Processing raw security tool logs into instruction/response pairs for model training. **Local Execution:** Optimizing models for local hardware to ensure data remains on-premises. **Response Logic:** Developing structured formats for explaining security vulnerabilities and remediation steps. Active Projects **infosec-tool-output:** A dataset mapping static and dynamic analysis tool outputs to technical summaries. openmalx/infosec-tool-output **open-malsec:** A collection of text-based security threats, including phishing and social engineering samples, for classification tasks. openmalx/open-malsec See translation</description><pubDate>Wed, 28 Jan 2026 17:36:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tegridydev/651013007690920</guid></item><item><title>üöÄ Wanna train your own AI Model or Tokenizer from scratch?</title><link>https://huggingface.co/posts/Parveshiiii/254137909606959</link><description>üöÄ Wanna train your own AI Model or Tokenizer from scratch? Building models isn‚Äôt just for big labs anymore ‚Äî with the right data, compute, and workflow, you can create **custom AI models** and **tokenizers** tailored to any domain. Whether it‚Äôs NLP, domain‚Äëspecific datasets, or experimental architectures, training from scratch gives you full control over vocabulary, embeddings, and performance. ‚ú® Why train your own? - Full control over vocabulary &amp; tokenization - Domain‚Äëspecific optimization (medical, legal, technical, etc.) - Better performance on niche datasets - Freedom to experiment with architectures ‚ö° The best part? - Tokenizer training (TikToken / BPE) can be done in **just 3 lines of code**. - Model training runs smoothly on **Google Colab notebooks** ‚Äî no expensive hardware required. üìÇ Try out my work: - üîó https://github.com/OE-Void/Tokenizer-from_scratch - üîó https://github.com/OE-Void/GPT See translation</description><pubDate>Wed, 28 Jan 2026 17:36:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Parveshiiii/254137909606959</guid></item><item><title>HunyuanImage 3.0-Instruct just dropped</title><link>https://huggingface.co/posts/wangbuer999/572194851679348</link><description>HunyuanImage 3.0-Instruct just dropped fresh -sourceImage 3.0model! Spent 20 mins testing it on a Messi + retro scrambler fusion case Ran on diffusers v0.26.3 + CUDA 12.1 | 8B MoE params (1.3B activated) | zero VRAM issues strength=0.9 Messi #10 kit/tattoo sharp, moto‚Äôs rusted metal texture blurred (classic open-source pain) strength=0.7 Moto/cobblestone background crisp, Messi‚Äôs jersey details faded completely strength=0.75 + prompt "Blend seamlessly, keep all original details": both subject &amp; background sharp No ControlNet, no manual masking the model‚Äôs chain-of-thought reasoning parses image+prompt first Already outperforms Qwen-Image-Edit 2511 (GSB eval +25.7% on single-image edits) | 100% open-source üëâ Repo: https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&amp;modelId=Hunyuan-Image-3.0-Instruct technical reportÔºöhttps://arxiv.org/abs/2509.23951 Anyone else struggled with strength tweaks for fusion? This fixed it for my Messi+moto case did it work as well for yours?...</description><pubDate>Wed, 28 Jan 2026 17:36:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wangbuer999/572194851679348</guid></item><item><title>kernels 0.12 is out! üéâ</title><link>https://huggingface.co/posts/danieldk/874011196597638</link><description>kernels 0.12 is out! üéâ Changes: * Support for kernel version branches to gracefully roll out kernel API changes. * Support for PyTorch 2.10. * kernel-builder is now merged into the kernels repo. * Initial support for standardized kernel benchmarks. https://github.com/huggingface/kernels/releases/tag/v0.12.0 See translation</description><pubDate>Wed, 28 Jan 2026 17:36:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danieldk/874011196597638</guid></item></channel></rss>