<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>4 must-try AI video models in 2026 — all in one place on iMini! 🎬✨</title><link>https://huggingface.co/posts/404Zen/731905286538257</link><description>4 must-try AI video models in 2026 — all in one place on iMini! 🎬✨ Featuring Sora 2, Veo 3, Wan 2.5, and Seedance 3.0 — no invite code, no watermark! Try it now 👉 https://imini.com/ See translation</description><pubDate>Wed, 15 Oct 2025 05:22:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/404Zen/731905286538257</guid></item><item><title>How to compress long code context? 📚</title><link>https://huggingface.co/posts/YerbaPage/639392890035292</link><description>How to compress long code context? 📚 Check out our LongCodeZip! Paper just got accepted to ASE 2025. 🔥 Code &amp; Demo: https://github.com/YerbaPage/LongCodeZip Paper: LongCodeZip: Compress Long Context for Code Language Models (2510.00446) See translation</description><pubDate>Wed, 15 Oct 2025 05:22:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/639392890035292</guid></item><item><title>You think those playful puppies are real? 🐶✨</title><link>https://huggingface.co/posts/Monica997/870861781065582</link><description>You think those playful puppies are real? 🐶✨ Nope! It’s a video I created using iMini’s newly integrated Sora 2 model — no invite code, no watermark, just one simple text prompt to generate dynamic videos in seconds! 🎬 Limited-time offer: members can create without using credits! 👉 Try it now: https://imini.com/ See translation</description><pubDate>Wed, 15 Oct 2025 05:22:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Monica997/870861781065582</guid></item><item><title>9 Powerful AI Video Generation Tools</title><link>https://huggingface.co/posts/Kseniase/543365627154110</link><description>9 Powerful AI Video Generation Tools Since Sora 2 is on fire these weeks, reminding us what high-quality video generation should look like, we decided you really need this list of video generation tools – great alternatives or complements to it. 1. Sora 2 → https://openai.com/sora/ It needs no introduction, but this OpenAI’s text-to-video model produces short, ultra-realistic clips across styles (cinematic, photorealistic, animated, etc.) with synced audio 2. Google Veo 3 (Gemini Video Generation) → https://aistudio.google.com/models/veo-3 Part of Gemini AI. Generates 8-second high-fidelity videos from text or images with native sound: background soundtracks and realistic voices with near-perfect lip sync 3. Runway (Gen-4 by Runway ML) → https://runwayml.com/ Text, image, or video-to-video generation with advanced editing like changing lighting, weather, camera angles or replacing objects. Popular in AI filmmaking 4. Pika Labs → https://pollo.ai/m/pika-ai Provides creative, often...</description><pubDate>Wed, 15 Oct 2025 05:22:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/543365627154110</guid></item><item><title>Just tried to create an educational assistant for younger people who can struggle with visualsation of 'what is this sorcery all about'.</title><link>https://huggingface.co/posts/s3nh/172255383269757</link><description>Just tried to create an educational assistant for younger people who can struggle with visualsation of 'what is this sorcery all about'. Its first step of my spare time projects, sft on Qwen3-8B, EduHelper is a child-friendly tutoring assistant fine-tuned from the Qwen3-8B base model using parameter-efficient fine-tuning (PEFT) with LoRA on the ajibawa-2023/Education-Young-Children dataset. s3nh/EduHelp-8B Glad to share my work, have a wonderful day! See translation</description><pubDate>Wed, 15 Oct 2025 05:22:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/s3nh/172255383269757</guid></item><item><title>vanta-research/apollo-astralis-8b</title><link>https://huggingface.co/posts/unmodeled-tyler/162043774477631</link><description>vanta-research/apollo-astralis-8b I ran the same prompt sequence on my model Apollo Astralis 8B and Hermes4 14B from Nous Research.. The raw chat logs were then given to 3 different architectures (DeepSeek 3.1, LLaMA 405B, GPT-OSS 120B). All 3 models were given the same, simple instructions to analyze the logs and determine which model performed better. All 3 independently chose Astralis 8B for stronger reasoning, alignment, transparency, and collaborative language. Astralis 8B is designed to keep you motivated by applying warm collaborative language mixed with rigorous logical reasoning and problem solving capabilities. Give Astralis a try! See translation</description><pubDate>Wed, 15 Oct 2025 05:22:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/162043774477631</guid></item><item><title>The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board — including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. 🤗🔥</title><link>https://huggingface.co/posts/prithivMLmods/844227545389355</link><description>The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board — including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. 🤗🔥 ⚡ Space / App: prithivMLmods/Qwen3-VL-HF-Demo The model’s demo supports a wide range of tasks, including; Image Inference, Video Inference, PDF Inference, Image Captioning (VLA), GIF Inference. ⚡ Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Thanks for granting the blazing-fast Zero GPU access, @ merve 🙏 ⚡ Other Pages &gt; Github: https://github.com/prithivsakthiur/qwen3-vl-hf-demo &gt; Multimodal VLMs July'25 : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 &gt; VL caption — &lt; Sep 15 ’25 : prithivMLmods/vl-caption-sep-15-25-68c7f6d737985c63c13e2391 &gt; Multimodal...</description><pubDate>Wed, 15 Oct 2025 05:22:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/844227545389355</guid></item><item><title>📢 Product Update: SalesPilot 1.2 Released!</title><link>https://huggingface.co/posts/andywu-kby/521155221047550</link><description>📢 Product Update: SalesPilot 1.2 Released! 🔧 What’s New: - Sales Forecasting, Sales Analysis using Excel - No technical skills required - Dashboard and Delete Functionality - Chatbot Application https://miragic.ai/products/sales-pilot Looking forward to your feedback! See translation</description><pubDate>Wed, 15 Oct 2025 05:22:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/andywu-kby/521155221047550</guid></item><item><title>✅ New Article: *Procrastination as a Structural Loop*</title><link>https://huggingface.co/posts/kanaria007/687620663929284</link><description>✅ New Article: *Procrastination as a Structural Loop* Title: ⏳ Procrastination as a Structural Loop: Why “I Know, But I Don’t Act” Persists — and How Protocols Contain It 🔗 https://huggingface.co/blog/kanaria007/procrastination-as-structural-loop --- Summary: Procrastination isn’t laziness — it’s a *miswired loop*. When task threat and reward prediction skew the thresholds, the *jump-generator* routes to *delay*, *reflexia* amplifies avoidance, and the *memory-loop* reinforces “not now.” Once we treat it as structure, we can *reindex costs, lower entry friction, and relink intention to execution*. &gt; Motivation wavers. &gt; *Loops can be rewired.* --- Why It Matters: • Reframes procrastination as *auditable mechanics*, not a moral flaw • Turns “stuck” into a stepwise *rollback → small jump → stable loop* • Applies from individuals to teams (deadlines, sprints, reviews) --- What’s Inside: • The Procrastination Loop: trigger → aversion tag → avoidance jump → guilt overlay → recurrence •...</description><pubDate>Wed, 15 Oct 2025 05:22:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/687620663929284</guid></item><item><title>David + Imagenet = high% val.</title><link>https://huggingface.co/posts/AbstractPhil/715642693504510</link><description>David + Imagenet = high% val. AbstractPhil/gated-david https://github.com/AbstractEyes/lattice_vocabulary/blob/master/src/geovocab2/train/model/core/david.py David's code has been released. I am currently setting up a trainer and will release the process on how to condition David to behave. This isn't the easiest process, but it's necessary to run David on a curriculum rather than simply feeding the model with cross-entropy and hoping for the best. David's internals involve a clock mechanism that allows direct control of David's freeze/unfreeze mechanisms at runtime - allowing for many opinions to be generated simultaneously. David is multiple models in one, not just one - and yet David is single-shot oriented. The prototype to the route of thought that led me to find the Cantor's Stairs positional encodings solution and the prototype to ViT-Zana, ViT-Beatrix, ViT-Beatrix-Dual-Block, and today the direct porting of David's complex architecture and the process to train David has...</description><pubDate>Wed, 15 Oct 2025 05:22:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AbstractPhil/715642693504510</guid></item></channel></rss>