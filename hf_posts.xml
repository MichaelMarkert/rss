<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>gpt-oss-120B scored 28 (one of the lowest) on AHA leaderboard. not very human aligned model.</title><link>https://huggingface.co/posts/etemiz/710778843328598</link><description>gpt-oss-120B scored 28 (one of the lowest) on AHA leaderboard. not very human aligned model. these kind of models are not really "free": they are costing you your freedom if you know what i mean. See translation</description><pubDate>Sat, 16 Aug 2025 09:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/710778843328598</guid></item><item><title>Want to learn to build an AI Agent? I put together a cookbook for creating your own news research agent with OpenAI GPT-OSS:</title><link>https://huggingface.co/posts/fdaudens/770107969696647</link><description>Want to learn to build an AI Agent? I put together a cookbook for creating your own news research agent with OpenAI GPT-OSS: - Searches headlines &amp; specific sites - Pulls full articles when you need depth - Summarizes with clickable sources - Runs in a simple Gradio chat UI - No GPU, no local setup ‚Äî just open-weight GPT-OSS models via Hugging Face If you‚Äôve been wanting to try agents but weren‚Äôt sure where to start, this is an end-to-end example you can fork, run, and adapt. Full guide + code https://huggingface.co/blog/fdaudens/openai-gpt-oss-agent-inference-providers See translation</description><pubDate>Sat, 16 Aug 2025 09:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/770107969696647</guid></item><item><title>No, I did not create those bots that just got banned today.</title><link>https://huggingface.co/posts/nroggendorff/812423234168314</link><description>No, I did not create those bots that just got banned today. See translation</description><pubDate>Sat, 16 Aug 2025 09:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/812423234168314</guid></item><item><title>Update on</title><link>https://huggingface.co/posts/ovi054/459497213356295</link><description>Update on ovi054/Qwen-Image-LORA ‚ö° You can now load a Qwen LoRA in this space as follows: 1. Model ID: flymy-ai/qwen-image-realism-lora 2. Model link: https://huggingface.co /flymy-ai/ qwen-image-realism-lora 3. Specific file link: https://huggingface.co /flymy-ai/ qwen-image-realism-lora /blob/m ain/flymy_realism.safetensors 4. Direct download link: https://huggingface.co /flymy-ai/ qwen-image-realism-lora /resolve/m ain/flymy_realism.safetensors You can also use an external .safetensors download link (if Hugging Face doesn‚Äôt block it). It is useful if a model repository contains multiple weights and you want to load a specific one. üëâ Try it now: ovi054/Qwen-Image-LORA See translation</description><pubDate>Sat, 16 Aug 2025 09:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/459497213356295</guid></item><item><title>Liquid just released two 450M and 1.6B param VLMs!</title><link>https://huggingface.co/posts/mlabonne/575026837446793</link><description>Liquid just released two 450M and 1.6B param VLMs! They're super fast and leverage SigLIP2 NaFlex encoders to handle native resolutions without distortion. It's ideal for on-device deployment in constrained environments like phones. It's available today on Hugging Face, with an inference and a fine-tuning Colab notebooks. LiquidAI/LFM2-VL-450M LiquidAI/LFM2-VL-1.6B See translation</description><pubDate>Sat, 16 Aug 2025 09:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/575026837446793</guid></item><item><title>Image-to-Prompt‚ö°</title><link>https://huggingface.co/posts/ovi054/657358125503535</link><description>Image-to-Prompt‚ö° ovi054/image-to-prompt Extract text prompt from image. And you can reuse the prompt to generate similar images! Useful for prompt engineering, studying image-to-text alignment, making training datasets, or recreating similar outputs. Powered by: Gradio, Florence 2 üëâ Try it now: ovi054/image-to-prompt See translation</description><pubDate>Sat, 16 Aug 2025 09:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/657358125503535</guid></item><item><title>‚úÖ New Article: *Acting as Structured Cognition*</title><link>https://huggingface.co/posts/kanaria007/826383234496581</link><description>‚úÖ New Article: *Acting as Structured Cognition* Title: üé≠ Structural Acting: Cognitive Performance via Jump Protocols üîó https://huggingface.co/blog/kanaria007/structural-acting --- Summary: Acting is often framed as *losing yourself in a role*. Structured Intelligence reframes performance as *cognitive architecture*: * Jumping across *identity‚Äëadjacent frames* * Maintaining *self‚Äëcoherence* while simulating others * *Modeling judgment structures* instead of emotional fusion &gt; Acting isn‚Äôt about disappearing ‚Äî &gt; *it‚Äôs about showing the structure behind identity.* --- Why It Matters: ‚Ä¢ Reduces *psychological strain* by keeping self anchored ‚Ä¢ Enables *traceable and reversible performance states* ‚Ä¢ Turns acting into a *laboratory for identity, motivation, and simulation* --- What‚Äôs Inside: ‚Ä¢ Structured approach to *role construction and safe detachment* ‚Ä¢ *Recursive rehearsal loops* for adaptive performance ‚Ä¢ Insights for *theater, therapy, and human‚ÄëAI collaboration* ‚Ä¢ Comparison of...</description><pubDate>Sat, 16 Aug 2025 09:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/826383234496581</guid></item><item><title>After responding to this question on my blog about SLMs, I'm beginning to wonder if the term "Small Language Model" is already dated.</title><link>https://huggingface.co/posts/jjokah/756442516282804</link><description>After responding to this question on my blog about SLMs, I'm beginning to wonder if the term "Small Language Model" is already dated. Ref (article): https://huggingface.co/blog/jjokah/small-language-model See translation</description><pubDate>Sat, 16 Aug 2025 09:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jjokah/756442516282804</guid></item><item><title>Thanks to popular request, I've just added two subsets to the CommonCrawl-Creative Commons Corpus (C5;</title><link>https://huggingface.co/posts/BramVanroy/381312664935235</link><description>Thanks to popular request, I've just added two subsets to the CommonCrawl-Creative Commons Corpus (C5; BramVanroy/CommonCrawl-CreativeCommons ) so that you do not have to do filtering manually - C5f ( BramVanroy/CommonCrawl-CreativeCommons-fine ): only retains high-quality samples that are also present in FineWeb or FineWeb-2; - C5r ( BramVanroy/CommonCrawl-CreativeCommons-recommended ): additional strict filtering that removes samples with license disagreement, non-commercial licenses, and Wikipedia samples. The latter because you should probably get those from a more reliable source that provides better parsed content. It goes without saying that these filters lead to a massive reduction in quantity. Doc and token counts are given on the dataset pages. See translation</description><pubDate>Sat, 16 Aug 2025 09:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/BramVanroy/381312664935235</guid></item><item><title>‚òÄÔ∏è ApertureDB Summer of Workflows #6 is here!  Ingest from SQL</title><link>https://huggingface.co/posts/dmoxy/476103184884522</link><description>‚òÄÔ∏è ApertureDB Summer of Workflows #6 is here! Ingest from SQL Multimodal data (text, images, videos) stuck in PostgreSQL? Getting it out for AI applications is usually painful ‚Äî but now it‚Äôs just: connect ‚Üí click ‚Üí ready for AI. üé¨ See It In Action: https://youtu.be/Uh3fJc1lkFo ApertureDB Ingest from SQL Workflow: Works with PostgreSQL credentials you already have Ingests directly into ApertureDB for unified multimodal search Powers RAG, retrieval, and agentic AI without glue code üëâ Try It Now!: https://cloud.aperturedata.io/signup ? Read The Docs: https://shorturl.at/jop9b Explore The Code: https://shorturl.at/gwFyJ Get Additional Resources: https://shorturl.at/zNND4 We‚Äôre halfway through the series ‚Äî 6 workflows down, 6 to go! üëá Let us know what you think in the comments below. We are listening. Team ApertureData See translation</description><pubDate>Sat, 16 Aug 2025 09:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dmoxy/476103184884522</guid></item></channel></rss>