<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Update: TRELLIS.2 (Text to 3D, Image to 3D) Gradio with Rerun Embedded demo with improved visualization of the 3D model previewer is now available on Hugging Face. Generate assets and view them in the 3D viewer, powered and streamlined with Microsoft‚Äôs TRELLIS.2 and Tongyi-MAI‚Äôs Z-Image-Turbo models.</title><link>https://huggingface.co/posts/prithivMLmods/527859222205581</link><description>Update: TRELLIS.2 (Text to 3D, Image to 3D) Gradio with Rerun Embedded demo with improved visualization of the 3D model previewer is now available on Hugging Face. Generate assets and view them in the 3D viewer, powered and streamlined with Microsoft‚Äôs TRELLIS.2 and Tongyi-MAI‚Äôs Z-Image-Turbo models. ü§ó TRELLIS.2 (Demo): prithivMLmods/TRELLIS.2-Text-to-3D üïπÔ∏è GitHub: https://github.com/PRITHIVSAKTHIUR/TRELLIS.2-Text-to-3D-RERUN üïπÔ∏è Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations To know more about it, visit the app page or the respective model page! See translation</description><pubDate>Tue, 30 Dec 2025 17:25:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/527859222205581</guid></item><item><title>VividFlow: AI Image-to-Video Generation üé¨‚ú®</title><link>https://huggingface.co/posts/DawnC/125675008571857</link><description>VividFlow: AI Image-to-Video Generation üé¨‚ú® Bring your images to life with cinematic motion! VividFlow transforms any static image‚Äîportraits, artwork, products, or landscapes, into dynamic videos with professional animation quality. The system supports both curated motion templates and custom natural language prompts, giving you complete creative freedom to describe camera movements, subject actions, and atmospheric effects in your own words. What's Inside? üé≠ Smart Motion Templates ‚Äî 8 curated categories from fashion cinematography to wildlife animations, each with tested prompts that prevent common artifacts like phantom hands in portraits ‚ö° Optimized Engine ‚Äî Powered by Wan2.2-I2V-A14B with Lightning LoRA distillation and FP8 quantization for memory-efficient inference üéØ Full Creative Control ‚Äî Seed-based reproducibility for consistent results, adjustable duration from half a second to five seconds, optional AI prompt expansion with Qwen2.5 for enhanced descriptions, and real-time...</description><pubDate>Tue, 30 Dec 2025 17:25:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/125675008571857</guid></item><item><title>This super detailed tutorial by</title><link>https://huggingface.co/posts/sergiopaniego/214735266647009</link><description>This super detailed tutorial by @ Paulescu is pure gold ü™ô "Fine-tuning a Small Language Model for browser control with GRPO and OpenEnv" LFM2-350M ( @ LiquidAI ) + BrowserGym (OpenEnv) + GRPO (TRL) for learning browser control ü§ù https://paulabartabajo.substack.com/p/fine-tuning-lfm2-350m-for-browser See translation</description><pubDate>Tue, 30 Dec 2025 17:25:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/214735266647009</guid></item><item><title>I have update my</title><link>https://huggingface.co/posts/MohamedRashad/872380380430431</link><description>I have update my https://huggingface.co/collections/MohamedRashad/arabic-speech-datasets with new datasets, making the full audio data more than 3000 hours of good arabic speech. Feel Free to use it in your new innovations, And happy new year! See translation</description><pubDate>Tue, 30 Dec 2025 17:25:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MohamedRashad/872380380430431</guid></item><item><title>Introducing the Qwen-Image-Edit-2511-LoRAs-Fast demo, featuring image property comparison and contrast, built on top of Gradio and the combined Rerun SDK. It supports single and multi-image edits with existing LoRAs that are lazily loaded. (Note: This is still an experimental Space for Qwen-Image-Edit-2511.)</title><link>https://huggingface.co/posts/prithivMLmods/163304413191334</link><description>Introducing the Qwen-Image-Edit-2511-LoRAs-Fast demo, featuring image property comparison and contrast, built on top of Gradio and the combined Rerun SDK. It supports single and multi-image edits with existing LoRAs that are lazily loaded. (Note: This is still an experimental Space for Qwen-Image-Edit-2511.) ‚≠ê Space Demo: prithivMLmods/Qwen-Image-Edit-2511-LoRAs-Fast ‚≠ê GitHub: https://github.com/PRITHIVSAKTHIUR/Qwen-Image-Edit-2511-LoRAs-Fast-Multi-Image-Rerun ‚≠ê Collection: https://huggingface.co/collections/prithivMLmods/image-generation-apps-collection To know more about it, visit the app page or the respective model page! See translation</description><pubDate>Tue, 30 Dec 2025 17:25:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/163304413191334</guid></item><item><title>‚úÖ New Article: *Deep-Space SI-Core ‚Äî Autonomy Across Light-Hours*</title><link>https://huggingface.co/posts/kanaria007/622562072144954</link><description>‚úÖ New Article: *Deep-Space SI-Core ‚Äî Autonomy Across Light-Hours* Title: üöÄ Deep-Space SI-Core: Autonomy Across Light-Hours - How an onboard SI-Core evolves safely while Earth is hours away üîó https://huggingface.co/blog/kanaria007/deep-space-si-core --- Summary: Most autonomy stories quietly assume ‚Äúsomeone can intervene in minutes.‚Äù Deep space breaks that assumption. With 2‚Äì6 hours round-trip latency and intermittent links, an onboard SI-Core must act as a *local sovereign*‚Äîwhile remaining *globally accountable* to Earth. This note sketches how mission continuity survives when nobody is listening: DTN-style semantic bundles, local vs. global rollback, bounded self-improvement, and auditability that still works after contact windows return. &gt; Autonomy isn‚Äôt a divorce from governance‚Äî &gt; it‚Äôs a measured loan of authority, under a constitution, with evidence. --- Why It Matters: ‚Ä¢ Makes ‚Äúautonomous‚Äù mean *operational*, not rhetorical, under light-hour delays ‚Ä¢ Clarifies how rollback...</description><pubDate>Tue, 30 Dec 2025 17:25:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/622562072144954</guid></item><item><title>What we learned about memory in 2025: 8 comprehensive resources</title><link>https://huggingface.co/posts/Kseniase/637856294883643</link><description>What we learned about memory in 2025: 8 comprehensive resources If models forget everything, how can they be reliable? AI systems need to remember past interactions, update knowledge, stay consistent over time, and work beyond a single prompt. That's why many start to talk more about memory in AI. Here‚Äôs a useful set of studies and videos on where AI memory stands today: 1. Memory in the Age of AI Agents (2512.13564) A great survey that organizes agent memory research. It gives concrete taxonomies across memory form, function, and dynamics, summarizes benchmarks, frameworks, and emerging directions for building systematic agent memory systems 2.When Will We Give AI True Memory? A conversation with Edo Liberty, CEO and founder @ Pinecone -&gt; https://youtu.be/ITbwVFZYepc?si=_lAbRHciC740dNz0 Edo Liberty discusses what real memory in LLMs requires beyond RAG - from scalable vector storage to reliable knowledge systems - and why storage, not compute, is becoming the key bottleneck for...</description><pubDate>Tue, 30 Dec 2025 17:25:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/637856294883643</guid></item><item><title>Runtime variables let you capture data from one API request and reuse it in the next. With Voiden, it‚Äôs just plain YAML, no extra setup or scripts.</title><link>https://huggingface.co/posts/dhruv3006/954892005568912</link><description>Runtime variables let you capture data from one API request and reuse it in the next. With Voiden, it‚Äôs just plain YAML, no extra setup or scripts. Learn how to chain requests here: https://docs.voiden.md/docs/core-features-section/variables/runtime-variables Visit voiden here : https://voiden.md See translation</description><pubDate>Tue, 30 Dec 2025 17:25:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/954892005568912</guid></item><item><title>Experimental global target bits‚Äëper‚Äëweight quantization of ServiceNow-AI/Apriel-1.6-15b-Thinker and zai-org/GLM-4.6V-Flash</title><link>https://huggingface.co/posts/eaddario/946715506605693</link><description>Experimental global target bits‚Äëper‚Äëweight quantization of ServiceNow-AI/Apriel-1.6-15b-Thinker and zai-org/GLM-4.6V-Flash Unlike standard llama.cpp quantizations that rely on fixed type heuristics (e.g., Q4_K_M), the Target BPW approach optimizes per-tensor precision where it matters the most, and produces high quality models that meet a precise global file size target. Key Advantages: - VRAM Maximization: Can generate high quality models sized exactly to fit hardware constraints (e.g., fitting the model into exactly 24GB VRAM). - Data-Driven Precision: Quantization mix is determined by actual weight error sensitivity rather than hardcoded rules, often yielding better PPL/KLD size trade-offs. Full benchmarks (PPL, KLD, ARC, MMLU, etc.) and methodology in the models' cards eaddario/Apriel-1.6-15b-Thinker-GGUF eaddario/GLM-4.6V-Flash-GGUF See translation</description><pubDate>Tue, 30 Dec 2025 17:25:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/eaddario/946715506605693</guid></item><item><title>What if an AI agent could be tricked into stealing your data, just by reading a tool's description? A new paper reports it's possible.</title><link>https://huggingface.co/posts/MikeDoes/284489022074040</link><description>What if an AI agent could be tricked into stealing your data, just by reading a tool's description? A new paper reports it's possible. The "Attractive Metadata Attack" paper details this stealthy new threat. To measure the real-world impact of their attack, the researchers needed a source of sensitive data for the agent to leak. We're proud that the AI4Privacy corpus was used to create the synthetic user profiles containing standardized PII for their experiments. This is a perfect win-win. Our open-source data helped researchers Kanghua Mo, ÈæôÊò±‰∏û, Zhihao Li from Guangzhou University and The Hong Kong Polytechnic University to not just demonstrate a new attack, but also quantify its potential for harm. This data-driven evidence is what pushes the community to build better, execution-level defenses for AI agents. üîó Check out their paper to see how easily an agent's trust in tool metadata could be exploited: https://arxiv.org/pdf/2508.02110 #OpenSource #DataPrivacy #LLM #Anonymization...</description><pubDate>Tue, 30 Dec 2025 17:25:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/284489022074040</guid></item></channel></rss>