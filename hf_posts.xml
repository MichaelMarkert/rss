<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Currently having a blast learning the transformers library.</title><link>https://huggingface.co/posts/melvindave/358781763788308</link><description>Currently having a blast learning the transformers library. I noticed that model cards usually have Transformers code as usage examples. So I tried to figure out how to load a model just using the transformers library without using ollama, lmstudio, or llamacpp. Learned how to install dependencies required to make it work like pytorch and CUDA. I also used Conda for python environment dependencies. Once I got the model loaded and sample inference working, I made an API to serve it. I know it's very basic stuff for machine learning experts here in HF but I'm completely new to this so I'm happy to get it working! Model used: Qwen/Qwen3-VL-8B-Instruct GPU: NVIDIA GeForce RTX 3090 Here's the result of my experimentation See translation</description><pubDate>Thu, 11 Dec 2025 13:42:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/melvindave/358781763788308</guid></item><item><title>Got to 1199.8 tokens/sec with Devstral Small -2 on my desktop GPU workstation. vLLM nightly.</title><link>https://huggingface.co/posts/mitkox/706030667212965</link><description>Got to 1199.8 tokens/sec with Devstral Small -2 on my desktop GPU workstation. vLLM nightly. Works out of the box with Mistral Vibe. Next is time to test the big one. See translation</description><pubDate>Thu, 11 Dec 2025 13:42:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/706030667212965</guid></item><item><title>I built something crazy you never saw before.</title><link>https://huggingface.co/posts/RakshitAralimatti/606573836468382</link><description>I built something crazy you never saw before. Please check - https://huggingface.co/blog/RakshitAralimatti/streaming-data-rag A real-time Streaming Data to RAG system that listens to live radio, transcribes it on-the-fly, and lets you query across TIME. Not just "what was discussed" ‚Äì but "what happened in the last 10 minutes on channel 0?" or "at 9 AM, what was the breaking news?" This is RAG that understands temporal context. See translation</description><pubDate>Thu, 11 Dec 2025 13:42:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/606573836468382</guid></item><item><title>ICYMI, you can fine-tune open LLMs using Claude Code</title><link>https://huggingface.co/posts/sergiopaniego/384415208092213</link><description>ICYMI, you can fine-tune open LLMs using Claude Code just tell it: ‚ÄúFine-tune Qwen3-0.6B on open-r1/codeforces-cots‚Äù and Claude submits a real training job on HF GPUs using TRL. it handles everything: &gt; dataset validation &gt; GPU selection &gt; training + Trackio monitoring &gt; job submission + cost estimation when it‚Äôs done, your model is on the Hub, ready to use read more about the process: https://huggingface.co/blog/hf-skills-training See translation</description><pubDate>Thu, 11 Dec 2025 13:42:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/384415208092213</guid></item><item><title>We just released TRL v0.26.0!</title><link>https://huggingface.co/posts/sergiopaniego/627155943228357</link><description>We just released TRL v0.26.0! It comes packed with updates: &gt; Agent training with tools in GRPO &gt; New CISPO &amp; SAPO losses + reasoning rewards &gt; vLLM quantization in colocate mode &gt; Dataset shuffling in SFT &gt; Lots of NEW examples &gt; Tons of fixes and documentation improvements See translation</description><pubDate>Thu, 11 Dec 2025 13:42:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/627155943228357</guid></item><item><title>installama.sh at the TigerBeetle 1000x World Tour !</title><link>https://huggingface.co/posts/angt/186034800220690</link><description>installama.sh at the TigerBeetle 1000x World Tour ! Last week I had the chance to give a short talk during the TigerBeetle 1000x World Tour (organized by @ jedisct1 üëè ) a fantastic event celebrating high-performance engineering and the people who love pushing systems to their limits! In the talk, I focused on the CPU and Linux side of things, with a simple goal in mind: making the installation of llama.cpp instant, automatic, and optimal, no matter your OS or hardware setup. For the curious, here are the links worth checking out: Event page: https://tigerbeetle.com/event/1000x GitHub repo: https://github.com/angt/installama.sh Talk: https://youtu.be/pg5NOeJZf0o?si=9Dkcfi2TqjnT_30e More improvements are coming soon. Stay tuned! See translation</description><pubDate>Thu, 11 Dec 2025 13:42:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/angt/186034800220690</guid></item><item><title>Switching between API Client, browser, and API documentation tools to test and document APIs can harm your flow and leave your docs outdated.</title><link>https://huggingface.co/posts/dhruv3006/716770445434901</link><description>Switching between API Client, browser, and API documentation tools to test and document APIs can harm your flow and leave your docs outdated. This is what usually happens: While debugging an API in the middle of a sprint, the API Client says that everything's fine, but the docs still show an old version. So you jump back to the code, find the updated response schema, then go back to the API Client, which gets stuck, forcing you to rerun the tests. Hours can go by just trying to sync all this up (and that‚Äôs if you catch the inconsistencies at all). The reason? Using disconnected tools for specs, tests, and docs. Doing manual updates, stale docs, and a lot of context switching. Voiden takes a different approach: Puts specs, tests &amp; docs all in one Markdown file, stored right in the repo. Everything stays in sync, versioned with Git, and updated in one place, inside your editor. Download Voiden here: https://voiden.md/download See translation</description><pubDate>Thu, 11 Dec 2025 13:42:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/716770445434901</guid></item><item><title>Hey all  üëã</title><link>https://huggingface.co/posts/StJohnDeakins/147365886941567</link><description>Hey all üëã A Quick one for any founders building with Small Language Models in mobile apps: We‚Äôre opening 10 Innovation Partner spots this month for our Device Native AI (DNA) platform. What you get: - Device Native AI SDK ‚Å†(AI processes data on-device, not cloud üì≤) - 99% off for 3 months, then 90% off for the rest of the year (no lock-in) - Direct engineering access + feature releases - It's an Innovation community, so at least some participation is required Perfect if you're building consumer apps and want: ‚úì Hyper-personalization without privacy risks ‚úì Zero cloud AI token costs ‚úì Early access to next-gen mobile AI Limited spots, and on a first-come basis, so DM me "DNA" for more info and an access code. Cheers Singe üêµ See translation</description><pubDate>Thu, 11 Dec 2025 13:42:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/StJohnDeakins/147365886941567</guid></item><item><title>I tested Muon vs MuonClip vs Muon+AdamW for fine-tuning LLMs</title><link>https://huggingface.co/posts/KingNish/856856521172000</link><description>I tested Muon vs MuonClip vs Muon+AdamW for fine-tuning LLMs Just published a blog on that, Read here üëâ https://huggingface.co/blog/KingNish/optimizer-part1 See translation</description><pubDate>Thu, 11 Dec 2025 13:42:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/KingNish/856856521172000</guid></item><item><title>Whisper-WebUI Premium - Ultra Fast and High Accuracy Speech to Text Transcripton App for All Languages - Windows, RunPod, Massed Compute 1-Click Installers - Supporting RTX 1000 to 5000 series</title><link>https://huggingface.co/posts/MonsterMMORPG/232609167066612</link><description>Whisper-WebUI Premium - Ultra Fast and High Accuracy Speech to Text Transcripton App for All Languages - Windows, RunPod, Massed Compute 1-Click Installers - Supporting RTX 1000 to 5000 series Latest installer zip file : https://www.patreon.com/posts/145395299 New Features Password protected version, password is just 1 : WhisperWeb_UI_v1_password_is_1.zip It has better interface, more features, default settings set for maximum accuracy It will show transcription realtime both on Gradio interface and also on CMD It will show better status and output at the cmd like starting time, starting file, etc It will save every generated transcription properly with same name as input file name with proper name sanitization After deep scan of the entire pipeline, default parameters are set for maximum accuracy and quality 1-Click installers for Windows local PC, RunPod (Linux-Cloud) and Massed Compute (Linux-Cloud) The app the installers are made for RTX 1000 series to RTX 5000 series with pre-...</description><pubDate>Thu, 11 Dec 2025 13:42:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/232609167066612</guid></item></channel></rss>