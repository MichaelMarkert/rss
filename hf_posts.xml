<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸ¯ Open Ghibli Studio: Transform Your Photos into Ghibli-Style Artwork! âœ¨</title><link>https://huggingface.co/posts/ginipick/807578740801859</link><description>ğŸ¯ Open Ghibli Studio: Transform Your Photos into Ghibli-Style Artwork! âœ¨ Hello AI enthusiasts! ğŸ™‹â€â™€ï¸ Today I'm introducing a truly magical project: Open Ghibli Studio ğŸ¨ ginigen/FLUX-Open-Ghibli-Studio ğŸŒŸ What Can It Do? Upload any regular photo and watch it transform into a beautiful, fantastical image reminiscent of Hayao Miyazaki's Studio Ghibli animations! ğŸï¸âœ¨ ğŸ”§ How Does It Work? ğŸ“¸ Upload your photo ğŸ¤– Florence-2 AI analyzes the image and generates a description âœï¸ "Ghibli style" is added to the description ğŸ­ Magic transformation happens using the FLUX.1 model and Ghibli LoRA! âš™ï¸ Customization Options Want more control? Adjust these in the advanced settings: ğŸ² Set a seed (for reproducible results) ğŸ“ Adjust image dimensions ğŸ” Guidance scale (prompt adherence) ğŸ”„ Number of generation steps ğŸ’« Ghibli style intensity ğŸš€ Try It Now! Click the "Transform to Ghibli Style" button below to create your own Ghibli world! Ready to meet Totoro, Howl, Sophie, or Chihiro? ğŸŒˆ ğŸŒ¿ Note: For best results,...</description><pubDate>Thu, 03 Apr 2025 05:21:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/807578740801859</guid></item><item><title>ğŸ”¥ 'Open Meme Studio': Your Creative Meme Factory ğŸ­âœ¨</title><link>https://huggingface.co/posts/openfree/925352420925810</link><description>ğŸ”¥ 'Open Meme Studio': Your Creative Meme Factory ğŸ­âœ¨ Hello everyone! Today I'm introducing 'Open Meme Studio', an amazing space where you can easily create and transform fun and original meme images. ğŸš€ VIDraft/Open-Meme-Studio ğŸ¯ Taking Meme Creation to the Next Level! This application leverages the powerful Kolors model and IP-Adapter-Plus to upgrade your meme-making abilities. Go beyond simple image editing and experience a completely new meme world powered by AI! ğŸ› ï¸ Features You'll Love ğŸ“¸ Transform and reinterpret existing meme templates ğŸ­ Freely change expressions and poses ğŸ‘“ Add props (sunglasses, hats, etc.) ğŸï¸ Change backgrounds and composite characters ğŸ¨ Apply various artistic styles ğŸ’ª Why 'Open Meme Studio' is So Effective Fast Meme Generation: High-quality memes completed in seconds Unlimited Creativity: Completely different results just by changing prompts User-Friendly Interface: Simple prompt input and image upload is all you need Fine-tuned Control: Adjust how much of...</description><pubDate>Thu, 03 Apr 2025 05:21:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/925352420925810</guid></item><item><title>Before 2020, most of the AI field was open and collaborative. For me, that was the key factor that accelerated scientific progress and made the impossible possibleâ€”just look at the â€œTâ€ in ChatGPT, which comes from the Transformer architecture openly shared by Google.</title><link>https://huggingface.co/posts/clem/267300235555885</link><description>Before 2020, most of the AI field was open and collaborative. For me, that was the key factor that accelerated scientific progress and made the impossible possibleâ€”just look at the â€œTâ€ in ChatGPT, which comes from the Transformer architecture openly shared by Google. Then came the myth that AI was too dangerous to share, and companies started optimizing for short-term revenue. That led many major AI labs and researchers to stop sharing and collaborating. With OAI and sama now saying they're willing to share open weights again, we have a real chance to return to a golden age of AI progress and democratizationâ€”powered by openness and collaboration, in the US and around the world. This is incredibly exciting. Letâ€™s go, open science and open-source AI! See translation</description><pubDate>Thu, 03 Apr 2025 05:21:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/267300235555885</guid></item><item><title>Curious how Duality AI crafts synthetic data that can bridge the sim2real gap?</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/493599242742163</link><description>Curious how Duality AI crafts synthetic data that can bridge the sim2real gap? We just published an article here on HuggingFace outlining our process, with bonus dataset releases! Read it here: https://huggingface.co/blog/DualityAI-RebekahBogdanoff/training-yolov8-with-synthetic-data-from-falcon See translation</description><pubDate>Thu, 03 Apr 2025 05:21:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/493599242742163</guid></item><item><title>ğŸ‰ Thrilled to share our #CVPR2025 accepted work:</title><link>https://huggingface.co/posts/ZhiyuanthePony/617713430689574</link><description>ğŸ‰ Thrilled to share our #CVPR2025 accepted work: Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data (2503.21694) ğŸ”¥ â€‹Key Innovations: 1ï¸âƒ£ First to adapt SD for â€‹direct textured mesh generation (1-2s inference) 2ï¸âƒ£ Novel teacher-student framework leveraging multi-view diffusion models ([MVDream]( https://arxiv.org/abs/2308.16512 ) &amp; [RichDreamer]( https://arxiv.org/abs/2311.16918) ) 3ï¸âƒ£ â€‹Parameter-efficient tuning - â€‹only +2.6% params over base SD 4ï¸âƒ£ â€‹3D data-free training liberates model from dataset constraints ğŸ’¡ Why matters? â†’ A novel â€‹3D-Data-Free paradigm â†’ Outperforms data-driven methods on creative concept generation â†’ Unlocks web-scale text corpus for 3D content creation ğŸŒ Project: https://theericma.github.io/TriplaneTurbo/ ğŸ® Demo: ZhiyuanthePony/TriplaneTurbo ğŸ’» Code: https://github.com/theEricMa/TriplaneTurbo See translation</description><pubDate>Thu, 03 Apr 2025 05:21:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZhiyuanthePony/617713430689574</guid></item><item><title>ok, there must be a problem. HF charged me 0.12$ for 3 inference requests to text models</title><link>https://huggingface.co/posts/Reality123b/155118307932581</link><description>ok, there must be a problem. HF charged me 0.12$ for 3 inference requests to text models See translation</description><pubDate>Thu, 03 Apr 2025 05:21:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Reality123b/155118307932581</guid></item><item><title>DeepGit: Your GitHub Gold Digger! ğŸ’°ğŸš€</title><link>https://huggingface.co/posts/zamal/271014113300033</link><description>DeepGit: Your GitHub Gold Digger! ğŸ’°ğŸš€ Hey Hugging Face gang! Meet DeepGitâ€”my open-source sidekick that rips through GitHub to snag repos that fit you. Done with dead-end searches? Me too. Built it with LangGraph and some dope tricks: Embeddings grab the good stuff (HF magic, baby!) Re-ranking nails the best picks Snoops docs, code, and buzz in one slick flow Drops a clean list of hidden gems ğŸ’ Unearth that sneaky ML lib or Python gemâ€”run python app.py or langgraph dev and boom! Peek it at https://github.com/zamalali/DeepGit . Fork it, tweak it, love itâ€”Dockerâ€™s in, HF vibes are strong. Drop a ğŸŒŸ or a crazy ideaâ€”Iâ€™m pumped to jam with you all! ğŸª‚ See translation</description><pubDate>Thu, 03 Apr 2025 05:21:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/zamal/271014113300033</guid></item><item><title>What, How, Where, and How Well? This paper reviews test-time scaling methods and all you need to know about them:</title><link>https://huggingface.co/posts/hesamation/178289696524550</link><description>What, How, Where, and How Well? This paper reviews test-time scaling methods and all you need to know about them: &gt; parallel, sequential, hybrid, internal scaling &gt; how to scale (SFT, RL, search, verification) &gt; metrics and evals of test-time scaling ğŸ”—paper: What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models (2503.24235) If you want to learn what inference-time compute scaling is @ rasbt has a great blog post on that: https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling See translation</description><pubDate>Thu, 03 Apr 2025 05:21:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/178289696524550</guid></item><item><title>I have Compared Kohya vs OneTrainer for FLUX Dev Finetuning / DreamBooth Training</title><link>https://huggingface.co/posts/MonsterMMORPG/151523216975273</link><description>I have Compared Kohya vs OneTrainer for FLUX Dev Finetuning / DreamBooth Training OneTrainer can train FLUX Dev with Text-Encoders unlike Kohya so I wanted to try it. Unfortunately, the developer doesn't want to add feature to save trained Clip L or T5 XXL as safetensors or merge them into output so basically they are useless without so much extra effort. I still went ahead and wanted to test EMA training. EMA normally improves quality significantly in SD 1.5 training. With FLUX I have to use CPU for EMA and it was really slow but i wanted to test. I have tried to replicate Kohya config. The below you will see results. Sadly the quality is nothing sort of. More research has to be made and since we still don't get text-encoder training due to developer decision, I don't see any benefit of using OneTrainer for FLUX training instead of using Koha. 1st image : Kohya best config : https://www.patreon.com/posts/112099700 2nd image : One Trainer Kohya config with EMA update every 1 step...</description><pubDate>Thu, 03 Apr 2025 05:21:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/151523216975273</guid></item><item><title>You can now run DeepSeek-V3-0324 on your own local device!</title><link>https://huggingface.co/posts/danielhanchen/465464088880734</link><description>You can now run DeepSeek-V3-0324 on your own local device! Run our Dynamic 2.42 and 2.71-bit DeepSeek GGUFs: unsloth/DeepSeek-V3-0324-GGUF You can run them on llama.cpp and other inference engines. See our guide here: https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-v3-0324-locally See translation</description><pubDate>Thu, 03 Apr 2025 05:21:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/465464088880734</guid></item></channel></rss>