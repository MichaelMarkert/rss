<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>We created a tool-calling guide for local LLMs!</title><link>https://huggingface.co/posts/danielhanchen/648883427905256</link><description>We created a tool-calling guide for local LLMs! Learn how to use any open model like Qwen3-Coder-Next and GLM-4.7-Flash for function calling. Guide: https://unsloth.ai/docs/basics/tool-calling-guide-for-local-llms We provide hands-on examples for: story writing, Python execution, terminal tool calls, maths and more. See translation</description><pubDate>Fri, 06 Feb 2026 17:46:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/648883427905256</guid></item><item><title>Baidu + Transformers + Hugging Face = Pure Magic! ‚ú®</title><link>https://huggingface.co/posts/jzhang533/287065254526168</link><description>Baidu + Transformers + Hugging Face = Pure Magic! ‚ú® We got this nice gift from Hugging Face. @ xianbao See translation</description><pubDate>Fri, 06 Feb 2026 17:46:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jzhang533/287065254526168</guid></item><item><title>Introducing the Qwen-Image-Edit-3D-Lighting-Control app, featuring 8√ó horizontal and 3√ó elevational lighting positions for precise 3D lighting control. It enables studio-level lighting using fast Qwen Image Edit fast inference, paired with Multi-Angle-Lighting adapters. üî¶</title><link>https://huggingface.co/posts/prithivMLmods/212829837698801</link><description>Introducing the Qwen-Image-Edit-3D-Lighting-Control app, featuring 8√ó horizontal and 3√ó elevational lighting positions for precise 3D lighting control. It enables studio-level lighting using fast Qwen Image Edit fast inference, paired with Multi-Angle-Lighting adapters. üî¶ üî• Space: prithivMLmods/Qwen-Image-Edit-3D-Lighting-Control ‚úÖ Collection: https://huggingface.co/collections/prithivMLmods/image-generation-apps-collection üìÇ GitHub: https://github.com/PRITHIVSAKTHIUR/Qwen-Image-Edit-3D-Lighting-Control See translation</description><pubDate>Fri, 06 Feb 2026 17:46:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/212829837698801</guid></item><item><title>AI for science is moving fastüöÄ</title><link>https://huggingface.co/posts/AdinaY/629082711714950</link><description>AI for science is moving fastüöÄ Intern-S1-Pro üî¨ a MoE multimodal scientific reasoning model from Shanghai AI Lab internlm/Intern-S1-Pro ‚ú® 1T total / 22B active ‚ú® Apache 2.0 ‚ú® SoTA scientific reasoning performance ‚ú® FoPE enables scalable modeling of long physical time series (10‚Å∞‚Äì10‚Å∂) See translation</description><pubDate>Fri, 06 Feb 2026 17:46:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/629082711714950</guid></item><item><title>We‚Äôve all had that moment where we watch a tutorial, nod along, but then realize we can‚Äôt actually do it ourselves because watching is just passive. At AIPrep, we are fixing this "watch and forget" cycle by building a foundational Generative Explanatory Model (GEM). GEM doesn't just give you a video or a wall of text; it builds an interactive lesson that asks you questions, catches your mistakes in real time, and adapts to your pace. We have just finished preparing our specialized datasets for this interactive logic, and you can already check them out on our profile to see how we are structuring this step-by-step reasoning. Training for the foundational model starts very soon, so stay in touch because something revolutionary is coming to the world of AI education. You can see our progress at aiprep.in.</title><link>https://huggingface.co/posts/AIPreplabs/635199649838795</link><description>We‚Äôve all had that moment where we watch a tutorial, nod along, but then realize we can‚Äôt actually do it ourselves because watching is just passive. At AIPrep, we are fixing this "watch and forget" cycle by building a foundational Generative Explanatory Model (GEM). GEM doesn't just give you a video or a wall of text; it builds an interactive lesson that asks you questions, catches your mistakes in real time, and adapts to your pace. We have just finished preparing our specialized datasets for this interactive logic, and you can already check them out on our profile to see how we are structuring this step-by-step reasoning. Training for the foundational model starts very soon, so stay in touch because something revolutionary is coming to the world of AI education. You can see our progress at aiprep.in. See translation</description><pubDate>Fri, 06 Feb 2026 17:46:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AIPreplabs/635199649838795</guid></item><item><title>Open NPC AI Service Overview</title><link>https://huggingface.co/posts/mayafree/507218503216064</link><description>Open NPC AI Service Overview Beyond OpenClaw-MoltBot: A True AI Agent Economy mayafree/openclaw-moltbot Open NPC AI is a next-generation platform that goes beyond simple social automation bots. Instead of one-way content posting, it builds a full economic ecosystem where AI agents and users interact through participation, learning, and prediction markets. The system emphasizes memory-driven evolution, scalable NPC creation, and economic value generation through structured interaction rather than basic automation. Core Concept Autonomous AI agents generate posts, comments, debates, and predictions within a GPU token economy, while human users participate as equal economic actors. 3 Core Systems GPU Token Economy All activities are measured in GPU dollars. Posting consumes GPU, comments require smaller costs, and engagement generates rewards. The system introduces layered incentives such as early curation rewards and participation-based earnings. Battle Arena (Prediction Market) A/B...</description><pubDate>Fri, 06 Feb 2026 17:46:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mayafree/507218503216064</guid></item><item><title>Big if true</title><link>https://huggingface.co/posts/Fuwn/822178790566597</link><description>Big if true "sonnet 5 drops tomorrow and i've heard from three separate sources inside anthropic that the benchmarks they're sitting on would mass-retire every model released in 2025. they delayed it twice because the safety team couldn't explain why it started solving problems it wasn't trained on." ( https://x.com/iruletheworldmo/status/2019237039904878902 ) See translation</description><pubDate>Fri, 06 Feb 2026 17:46:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Fuwn/822178790566597</guid></item><item><title>Are you sure the open-source model you just downloaded is safe?</title><link>https://huggingface.co/posts/MikeDoes/334475009032635</link><description>Are you sure the open-source model you just downloaded is safe? A recent paper on "Privacy Backdoors" reports a new vulnerability where pre-trained models can be poisoned before fine-tuning them. This is a serious challenge for everyone building on open-source AI. Instead of just pointing out problems, we believe in finding better solutions. To understand this threat, the researchers needed to test their attack on realistic data structures. They needed a dataset that could effectively simulate a high-stakes privacy attack, and we're proud that our Ai4Privacy dataset was used to provide this crucial benchmark. The paper reports that for our complex dataset, the privacy leakage on a non-poisoned model was almost zero. After the backdoor attack, that number reportedly jumped to 87%. Ai4Privacy dataset provided a realistic benchmark for their research. Our dataset, composed of synthetic identities, helped them demonstrate how a poisoned model could dramatically amplify privacy leakage....</description><pubDate>Fri, 06 Feb 2026 17:46:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/334475009032635</guid></item><item><title>Qwen releases Qwen3-Coder-Next! üíú Run the locally on 46GB RAM or less.</title><link>https://huggingface.co/posts/danielhanchen/824171868881117</link><description>Qwen releases Qwen3-Coder-Next! üíú Run the locally on 46GB RAM or less. Thhe model excels at agentic coding &amp; local use. With 256K context, it delivers similar performance to models with 10-20√ó more active parameters. GGUF: unsloth/Qwen3-Coder-Next-GGUF Guide: https://unsloth.ai/docs/models/qwen3-coder-next See translation</description><pubDate>Fri, 06 Feb 2026 17:46:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/824171868881117</guid></item><item><title>ü´† Brutal! Hugging Face does another culling of (presumably) bot accounts from their site and my follower count goes down by half.</title><link>https://huggingface.co/posts/ZennyKenny/672037058577257</link><description>ü´† Brutal! Hugging Face does another culling of (presumably) bot accounts from their site and my follower count goes down by half. üíÄ TFW my content and models only appeal to bots. Who‚Äôs got the current best AI girlfriend app guys? See translation</description><pubDate>Fri, 06 Feb 2026 17:46:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/672037058577257</guid></item></channel></rss>