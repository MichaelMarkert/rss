<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>✨ DreamO Video: From Customized Images to Videos ✨</title><link>https://huggingface.co/posts/openfree/538970335354687</link><description>✨ DreamO Video: From Customized Images to Videos ✨ Hello, AI creators! Today I'm introducing a truly special project. DreamO Video is an integrated framework that generates customized images based on reference images and transforms them into videos with natural movement. 🎬✨ openfree/DreamO-video 🔍 Key Features Image Reference (IP): Maintain object appearance while applying to new backgrounds and situations ID Preservation: Retain facial features across various environments Style Transfer: Apply unique styles from reference images to other content 🎞️ Video Generation: Create natural 2-second videos from generated images 💡 How to Use Upload Reference Images: One or two images (people, objects, landscapes, etc.) Select Task Type: Choose between IP (Image Preservation), ID (Face Feature Retention), or Style Enter Prompt: Describe your desired result (e.g., "a woman playing guitar on a cloud") Click Generate Image: ✨ Create customized AI images! Generate Video: Click the 🎬 button on the...</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/538970335354687</guid></item><item><title>VLMS 2025 UPDATE 🔥</title><link>https://huggingface.co/posts/merve/544378273517703</link><description>VLMS 2025 UPDATE 🔥 We just shipped a blog on everything latest on vision language models, including 🤖 GUI agents, agentic VLMs, omni models 📑 multimodal RAG ⏯️ video LMs 🤏🏻 smol models ..and more! https://huggingface.co/blog/vlms-2025 See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/544378273517703</guid></item><item><title>I’ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things.</title><link>https://huggingface.co/posts/ArturoNereu/644085701737970</link><description>I’ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things. I turned this into a GitHub repo: https://github.com/ArturoNereu/AI-Study-Group If you’re just getting started, I recommend: 📘 Deep Learning – A Visual Approach: https://www.glassner.com/portfolio/deep-learning-a-visual-approach 🎥 Dive into LLMs with Andrej Karpathy: https://youtu.be/7xTGNNLPyMI?si=aUTq_qUzyUx36BsT 🧠 The 🤗 Agents course]( https://huggingface.co/learn/agents-course/ The repo has grown with help from the community (Reddit, Discord, etc.) and I’ll keep updating it. If you have any favorite resources, I’d love to include them. See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ArturoNereu/644085701737970</guid></item><item><title>Transfer Any Clothing Into A New Person &amp; Turn Any Person Into A 3D Figure - ComfyUI Tutorial</title><link>https://huggingface.co/posts/MonsterMMORPG/869555651580897</link><description>Transfer Any Clothing Into A New Person &amp; Turn Any Person Into A 3D Figure - ComfyUI Tutorial ComfyUI is super hard to use but I have literally prepared 1-click way to install and use 2 amazing workflows. First workflow is generating a person wearing any clothing. The second workflow is turning any person image into a 3D toy like figure image. Tutorial Link : https://youtu.be/ZzYnhKeaJBs Video Chapters 0:00:00 Intro: Two One-Click ComfyUI Workflows (Clothing Gen &amp; 3D Figure) 0:00:34 Effort &amp; Goal: Easy Installation &amp; Use of Complex Workflows 0:00:49 Setup Part 1: ComfyUI Prerequisite &amp; Downloading Project Zip File 0:01:06 Setup Part 2: Extracting Zip into ComfyUI Folder (WinRAR 'Extract Here' Tip) 0:01:18 Setup Part 3: Running update_comfyui.bat for Latest ComfyUI Version 0:01:37 Setup Part 4: Running install_clothing_and_3D.bat (Installs Nodes &amp; Requirements) 0:02:03 Model Downloads: Intro to Swarm UI Auto-Installer &amp; Automatic Updater 0:02:28 Using Swarm UI: Launching...</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/869555651580897</guid></item><item><title># 🌟 3D Model to Video: Easy GLB Conversion Tool 🌟</title><link>https://huggingface.co/posts/ginipick/766230066345476</link><description># 🌟 3D Model to Video: Easy GLB Conversion Tool 🌟 demo link: ginigen/3D-VIDEO Hello there! Would you like to transform your 3D models into stunning animations? This space can help you! ✨ ## 🔍 What Can It Do? This tool converts your uploaded GLB model into: 1. 🎮 A transformed GLB file 2. 🎬 An animated GIF preview 3. 📋 A metadata JSON file ## ✅ Key Features * 🖥️ Works in headless server environments (EGL + pyglet-headless → pyrender fallback) * 🔍 Objects in GIFs appear 3x larger (global scale ×3) * 🎨 Clean interface with pastel background ## 🎮 Animation Types * 🔄 Rotate - Object rotates around the Y-axis * ⬆️ Float - Object moves smoothly up and down * 💥 Explode - Object moves sideways * 🧩 Assemble - Object returns to its original position * 💓 Pulse - Object changes in size * 🔄 Swing - Object swings around the Z-axis ## 🛠️ How to Use 1. Upload your GLB model 📤 2. Select your desired animation type 🎬 3. Adjust the duration and FPS ⏱️ 4. Click the "Generate Animation" button ▶️ 5....</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/766230066345476</guid></item><item><title>Matrix Game 🎮 an interactive foundation model for controllable game world generation, released by Skywork AI.</title><link>https://huggingface.co/posts/AdinaY/746841015963520</link><description>Matrix Game 🎮 an interactive foundation model for controllable game world generation, released by Skywork AI. Skywork/Matrix-Game ✨ 17B with MIT licensed ✨ Diffusion-based image-to-world video generation via keyboard &amp; mouse input ✨ GameWorld Score benchmark for Minecraft world models ✨ Massive Matrix Game Dataset with fine-grained action labels See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/746841015963520</guid></item><item><title>🚀 VisionScout Now Speaks More Like Me — Thanks to LLMs!</title><link>https://huggingface.co/posts/DawnC/683112818630492</link><description>🚀 VisionScout Now Speaks More Like Me — Thanks to LLMs! I'm thrilled to share a major update to VisionScout, my end-to-end vision system. Beyond robust object detection (YOLOv8) and semantic context (CLIP), VisionScout now features a powerful LLM-based scene narrator (Llama 3.2), improving the clarity, accuracy, and fluidity of scene understanding. This isn’t about replacing the pipeline , it’s about giving it a better voice. ✨ ⭐️ What the LLM Brings Fluent, Natural Descriptions: The LLM transforms structured outputs into human-readable narratives. Smarter Contextual Flow: It weaves lighting, objects, zones, and insights into a unified story. Grounded Expression: Carefully prompt-engineered to stay factual — it enhances, not hallucinates. Helpful Discrepancy Handling: When YOLO and CLIP diverge, the LLM adds clarity through reasoning. VisionScout Still Includes: 🖼️ YOLOv8-based detection (Nano / Medium / XLarge) 📊 Real-time stats &amp; confidence insights 🧠 Scene understanding via...</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/683112818630492</guid></item><item><title>The era of local Computer Use AI Agents is here.</title><link>https://huggingface.co/posts/dhruv3006/692657412350660</link><description>The era of local Computer Use AI Agents is here. Meet UI-TARS-1.5-7B-6bit, now running natively on Apple Silicon via MLX. The video is of UI-TARS-1.5-7B-6bit completing the prompt "draw a line from the red circle to the green circle, then open reddit in a new tab" running entirely on MacBook. The video is just a replay, during actual usage it took between 15s to 50s per turn with 720p screenshots (on avg its ~30s per turn), this was also with many apps open so it had to fight for memory at times. Built using c/ua : https://github.com/trycua/cua Join us making them here: https://discord.gg/4fuebBsAUj Kudos to the MLX community here on huggingface : mlx-community See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/692657412350660</guid></item><item><title>Automatic Multi-Modal Research Agent</title><link>https://huggingface.co/posts/VirtualOasis/965866013655862</link><description>Automatic Multi-Modal Research Agent I am thinking of building an Automatic Research Agent that can boost creativity! Input: Topics or data sources Processing: Automated deep research Output: multimodal results (such as reports, videos, audio, diagrams) &amp; multi-platform publishing. There is a three-stage process In the initial Stage, output for text-based content in markdown format allows for user review before transformation into various other formats, such as PDF or HTML. The second stage transforms the output into other modalities, like audio, video, diagrams, and translations into different languages. The final stage focuses on publishing multi-modal content across multiple platforms like X, GitHub, Hugging Face, YouTube, and podcasts, etc. See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/VirtualOasis/965866013655862</guid></item><item><title>Very cool to see</title><link>https://huggingface.co/posts/clem/170733821735878</link><description>Very cool to see pytorch contributing on Hugging Face. Time to follow them to see what they're cooking! See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/170733821735878</guid></item></channel></rss>