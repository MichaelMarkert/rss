<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Tried something new: an AI-generated podcast that breaks down the top research paper each day. Fully automated, now live on Spotify.</title><link>https://huggingface.co/posts/fdaudens/617387724043904</link><description>Tried something new: an AI-generated podcast that breaks down the top research paper each day. Fully automated, now live on Spotify. I built this prototype to help keep up with the rapid pace of AI developments and, hopefully, make cutting-edge research more accessible. I don’t know about you, but just listening to a conversation about a paper really helps the content sink in for me. This build taught me a lot about full automation. If you’re into the technical weeds: Qwen3 runs on Inference to handle the script, Kokoro does the voice, and the whole thing gets published automatically thanks to the Hugging Face Jobs API and Gradio deployment. It’s not perfect yet — I’ll be monitoring for hallucinations and incoherence. The voice model still needs polish, but it’s a promising start. Would love to build this with the community — submit a PR or send feedback. It’s just a beta of an experimental idea! Big kudos to @ m-ric , whose Open NotebookLM this is based on, and to @ nielsr for his...</description><pubDate>Fri, 16 May 2025 05:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/617387724043904</guid></item><item><title>I’ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things.</title><link>https://huggingface.co/posts/ArturoNereu/644085701737970</link><description>I’ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things. I turned this into a GitHub repo: https://github.com/ArturoNereu/AI-Study-Group If you’re just getting started, I recommend: 📘 Deep Learning – A Visual Approach: https://www.glassner.com/portfolio/deep-learning-a-visual-approach 🎥 Dive into LLMs with Andrej Karpathy: https://youtu.be/7xTGNNLPyMI?si=aUTq_qUzyUx36BsT 🧠 The 🤗 Agents course]( https://huggingface.co/learn/agents-course/ The repo has grown with help from the community (Reddit, Discord, etc.) and I’ll keep updating it. If you have any favorite resources, I’d love to include them. See translation</description><pubDate>Fri, 16 May 2025 05:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ArturoNereu/644085701737970</guid></item><item><title>Lumier – Run macOS &amp; Linux VMs in a Docker</title><link>https://huggingface.co/posts/dhruv3006/465197265329383</link><description>Lumier – Run macOS &amp; Linux VMs in a Docker Lumier is an open-source tool for running macOS virtual machines in Docker containers on Apple Silicon Macs. When building virtualized environments for AI agents, we needed a reliable way to package and distribute macOS VMs. Inspired by projects like dockur/macos that made macOS running in Docker possible, we wanted to create something similar but optimized for Apple Silicon. The existing solutions either didn't support M-series chips or relied on KVM/Intel emulation, which was slow and cumbersome. We realized we could leverage Apple's Virtualization Framework to create a much better experience. Lumier takes a different approach: It uses Docker as a delivery mechanism (not for isolation) and connects to a lightweight virtualization service (lume) running on your Mac. Lumier is 100% open-source under MIT license and part of C/ua. Github : https://github.com/trycua/cua/tree/main/libs/lumier Join the discussion here :...</description><pubDate>Fri, 16 May 2025 05:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/465197265329383</guid></item><item><title>🔥 Hidream I1 is online! 🔥</title><link>https://huggingface.co/posts/jasoncorkill/313090798327696</link><description>🔥 Hidream I1 is online! 🔥 We just added Hidream I1 to our T2I leaderboard ( https://www.rapidata.ai/leaderboard/image-models ) benchmarked using 195k+ human responses from 38k+ annotators, all collected in under 24 hours. It landed #3 overall, right behind: - @ openai 4o - @ black-forest-labs Flux 1 Pro ...and just ahead of @ black-forest-labs Flux 1.1 Pro, @ xai-org Aurora and @ google Imagen3. Want to dig into the data? Check out our dataset here: Rapidata/Hidream_t2i_human_preference What model should we benchmark next? See translation</description><pubDate>Fri, 16 May 2025 05:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/313090798327696</guid></item><item><title>𝗔𝗯𝘀𝗼𝗹𝘂𝘁𝗲 𝗭𝗲𝗿𝗼: 𝗟𝗟𝗠𝘀 𝗰𝗮𝗻 𝘁𝗿𝗮𝗶𝗻 𝘄𝗶𝘁𝗵𝗼𝘂𝘁 𝗮𝗻𝘆 𝗲𝘅𝘁𝗲𝗿𝗻𝗮𝗹 𝗱𝗮𝘁𝗮 🤯</title><link>https://huggingface.co/posts/m-ric/402808163323191</link><description>𝗔𝗯𝘀𝗼𝗹𝘂𝘁𝗲 𝗭𝗲𝗿𝗼: 𝗟𝗟𝗠𝘀 𝗰𝗮𝗻 𝘁𝗿𝗮𝗶𝗻 𝘄𝗶𝘁𝗵𝗼𝘂𝘁 𝗮𝗻𝘆 𝗲𝘅𝘁𝗲𝗿𝗻𝗮𝗹 𝗱𝗮𝘁𝗮 🤯 Has the "data wall" just been breached? Recent RL paradigms often relied on a set of questions an answers that needs to be manually curated. Researchers from Tsinghua University went like "why though". 🤔 Indeed, why learn from question designed by a human teacher, when the model can start from their base knowledge and learn by experimenting in a code environment, proposing coding tasks themselves and trying to solve them? Thus they created “Absolute Zero Reasoning” (AZR), an approach that removes any need for human curated data. 🎭 𝗗𝘂𝗮𝗹 𝗿𝗼𝗹𝗲𝘀: ‣ Proposer: Generates challenging but solvable coding tasks ‣ Solver: Attempts to solve those self-proposed tasks 🧪 𝗧𝗵𝗿𝗲𝗲 𝘁𝗮𝘀𝗸 𝘁𝘆𝗽𝗲𝘀: all types are defined as triplets of program, input and output ‣ Deduction: Give model an input and program, it must deduce the output ‣ Abduction: Give model an program and output, it must find the input that gave said output ‣ Induction: Synthesize a...</description><pubDate>Fri, 16 May 2025 05:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/402808163323191</guid></item><item><title>60+ Generative AI projects for your resume. grind this GitHub repo if you want to level up:</title><link>https://huggingface.co/posts/hesamation/190820854172664</link><description>60+ Generative AI projects for your resume. grind this GitHub repo if you want to level up: &gt; LLM fine-tuning and applications &gt; advanced RAG apps &gt; Agentic AI projects &gt; MCP and A2A (new) GitHub: https://github.com/aishwaryanr/awesome-generative-ai-guide/blob/main/resources/60_ai_projects.md See translation</description><pubDate>Fri, 16 May 2025 05:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/190820854172664</guid></item><item><title>Transcribing 1 hour of audio for less than $0.01 🤯</title><link>https://huggingface.co/posts/jeffboudier/904543868043384</link><description>Transcribing 1 hour of audio for less than $0.01 🤯 @ mfuntowicz cooked with 8x faster Whisper speech recognition - whisper-large-v3-turbo transcribes at 100x real time on a $0.80/hr L4 GPU! How they did it: https://huggingface.co/blog/fast-whisper-endpoints 1-click deploy with HF Inference Endpoints: https://endpoints.huggingface.co/new?repository=openai%2Fwhisper-large-v3-turbo&amp;vendor=aws&amp;region=us-east&amp;accelerator=gpu&amp;instance_id=aws-us-east-1-nvidia-l4-x1&amp;task=automatic-speech-recognition&amp;no_suggested_compute=true See translation</description><pubDate>Fri, 16 May 2025 05:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jeffboudier/904543868043384</guid></item><item><title>Samsung Hacking Incident: Samsung Electronics' Official Hugging Face Account Compromised</title><link>https://huggingface.co/posts/seawolf2357/424129432408590</link><description>Samsung Hacking Incident: Samsung Electronics' Official Hugging Face Account Compromised Samsung Electronics' official Hugging Face account has been hacked. Approximately 17 hours ago, two new language models (LLMs) were registered under Samsung Electronics' official Hugging Face account. These models are: https://huggingface.co/Samsung/MuTokenZero2-32B https://huggingface.co/Samsung/MythoMax-L2-13B The model descriptions contain absurd and false claims, such as being trained on "1 million W200 GPUs," hardware that doesn't even exist. Moreover, community participants on Hugging Face who have noticed this issue are continuously posting that Samsung Electronics' account has been compromised. There is concern about potential secondary and tertiary damage if users download these LLMs released under the Samsung Electronics account, trusting Samsung's reputation without knowing about the hack. Samsung Electronics appears to be unaware of this situation, as they have not taken any visible...</description><pubDate>Fri, 16 May 2025 05:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/seawolf2357/424129432408590</guid></item><item><title>Skywork-VL Reward🔥A multimodal reward model for both understanding &amp; reasoning tasks, released by Skywork 昆仑万物-天工</title><link>https://huggingface.co/posts/AdinaY/684233720375951</link><description>Skywork-VL Reward🔥A multimodal reward model for both understanding &amp; reasoning tasks, released by Skywork 昆仑万物-天工 Paper: Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning (2505.07263) Model: Skywork/Skywork-VL-Reward-7B ✨ 7B ✨ Trained on large scale, high-quality preference data ✨ SOTA on VL-RewardBench + boosts reasoning via MPO See translation</description><pubDate>Fri, 16 May 2025 05:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/684233720375951</guid></item><item><title>Very cool to see</title><link>https://huggingface.co/posts/clem/170733821735878</link><description>Very cool to see pytorch contributing on Hugging Face. Time to follow them to see what they're cooking! See translation</description><pubDate>Fri, 16 May 2025 05:23:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/170733821735878</guid></item></channel></rss>