<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>IN: video fine-tuning support for</title><link>https://huggingface.co/posts/merve/227325205054335</link><description>IN: video fine-tuning support for facebook V-JEPA 2 in HF transformers üî• it comes with &gt; four models fine-tuned on Diving48 and SSv2 dataset facebook/v-jepa-2-6841bad8413014e185b497a6 &gt; FastRTC demo on V-JEPA2 SSv2 qubvel-hf/vjepa2-streaming-video-classification &gt; fine-tuning script on UCF-101 https://gist.github.com/ariG23498/28bccc737c11d1692f6d0ad2a0d7cddb &gt; fine-tuning notebook on UCF-101 https://colab.research.google.com/drive/16NWUReXTJBRhsN3umqznX4yoZt2I7VGc?usp=sharing we're looking forward to see what you will build! ü§ó See translation</description><pubDate>Thu, 19 Jun 2025 05:23:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/227325205054335</guid></item><item><title>The demo for the MonkeyOCR Recognition model, which adopts a Structure-Recognition-Relation (SRR) triplet paradigm &amp; Nanonets-OCR-s a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction and other experimental document OCR models, is combined into a single space.</title><link>https://huggingface.co/posts/prithivMLmods/854747069091698</link><description>The demo for the MonkeyOCR Recognition model, which adopts a Structure-Recognition-Relation (SRR) triplet paradigm &amp; Nanonets-OCR-s a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction and other experimental document OCR models, is combined into a single space. ‚ú¶ Try the demo here : prithivMLmods/core-OCR ‚ú¶ Try Nanonets-OCR-s demo here : prithivMLmods/Multimodal-OCR ‚§∑ MonkeyOCR Recognition : echo840/MonkeyOCR ‚§∑ docscopeOCR-7B-050425-exp : prithivMLmods/docscopeOCR-7B-050425-exp ‚§∑ coreOCR-7B-050325-preview : prithivMLmods/coreOCR-7B-050325-preview ‚§∑ Nanonets-OCR-s : nanonets/Nanonets-OCR-s ‚§∑ Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Also, include a sample OCR test using the VisionOCR-3B-061125 model and the Qwen2-VL-OCR-2B-Instruct model. ‚§∑ Blog : https://huggingface.co/blog/prithivMLmods/visionocr-3b-061125-vs-qwen2-vl-ocr-2b-instruct To know more about it, visit the model card...</description><pubDate>Thu, 19 Jun 2025 05:23:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/854747069091698</guid></item><item><title>Releases of the past week are here</title><link>https://huggingface.co/posts/merve/828394748215710</link><description>Releases of the past week are here merve/releases-june-13-6852c3c1eaf1e0c24c958860 Here's our picks ü§ì So many interesting models released past week in open AI! ü§ñ üñºÔ∏è Computer Vision/VLMs &gt; nanonets/Nanonets-OCR-s is the new state-of-the-art OCR model that can handle checkboxes, watermarks, tables (OS) &gt; Meta released facebook/v-jepa-2-6841bad8413014e185b497a6 , new sota video embeddings with two new classification models (OS) &gt; ByteDance-Seed/SeedVR2-3B is a new 3B video restoration model (OS) Audio &gt; Stepfun released stepfun-ai/Step-Audio-AQAA , new large (137B ü§Ø) audio language model that takes in audio and generates audio (OS) ü§ñ Robotics &gt; nvidia released nvidia/GR00T-N1.5-3B , new open foundation vision language action model 3D &gt; tencent/Hunyuan3D-2.1 is the new version of Hunyuan by Tencent that can generate 3D assets from text and image prompts See translation</description><pubDate>Thu, 19 Jun 2025 05:23:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/828394748215710</guid></item><item><title>Extending datasets just got a whole lot easier! üöÄ With Sheets, I was able to create a Spanish version of the popular fka/awesome-chatgpt-prompts dataset in just a few minutes ‚è±Ô∏è.</title><link>https://huggingface.co/posts/frascuchon/832207442100886</link><description>Extending datasets just got a whole lot easier! üöÄ With Sheets, I was able to create a Spanish version of the popular fka/awesome-chatgpt-prompts dataset in just a few minutes ‚è±Ô∏è. Check out the resulting dataset: frascuchon/fka_awesome_chatgpt_es üìä Want to try it out for yourself? Head over to the Sheets space and see how easy it is to extend and modify existing datasets ü§Ø. The possibilities are endless! üåê See translation</description><pubDate>Thu, 19 Jun 2025 05:23:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/frascuchon/832207442100886</guid></item><item><title># Reinforcement Learning societal impact: A Deep Dive</title><link>https://huggingface.co/posts/ghostai1/378992241286931</link><description># Reinforcement Learning societal impact: A Deep Dive Artificial Intelligence, or AI, is revolutionizing the way we live, work, and interact with our environment. With advancements in Reinforcement Learning (RL), machines are becoming increasingly intelligent and capable of making decisions autonomously. This shift is having a significant impact on society as we know it. One of the most notable aspects of RL is its ability to learn from experience. By observing and interacting with its surroundings, an AI-driven RL system can adapt to new situations and make decisions based on real-world data. This has huge implications for industries like healthcare, where AI can be used to analyze patient data and provide personalized treatment plans, or finance, where it can help predict market trends and make more informed investment decisions. Furthermore, RL is driving innovation in robotics and automation. Autonomous vehicles, for example, rely on RL to navigate complex environments safely...</description><pubDate>Thu, 19 Jun 2025 05:23:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ghostai1/378992241286931</guid></item><item><title>Kimi-Dev üíª New coding model by Moonshot AI</title><link>https://huggingface.co/posts/AdinaY/465653536234983</link><description>Kimi-Dev üíª New coding model by Moonshot AI moonshotai/Kimi-Dev-72B ‚ú® 72B - MIT license ‚ú® 60.4% on SWE-bench Verified ‚ú® RL-trained to patch real repos in Docker ‚ú® Only rewarded if full test suite passes See translation</description><pubDate>Thu, 19 Jun 2025 05:23:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/465653536234983</guid></item><item><title>üéØ Open GAMMA - AI PPT Generator 'GamJa'</title><link>https://huggingface.co/posts/openfree/667885440717862</link><description>üéØ Open GAMMA - AI PPT Generator 'GamJa' üöÄ Project Introduction Revolutionary AI presentation generator presented by OpenFree AI Community! Create professional-level PPTs with just a few clicks. üÜì Completely FREE! Create Premium PPTs with Free GAMMA! üéâ DEMO: openfree/Open-GAMMA ‚ú® Key Features ü§ñ Powered by FACTS Grounding Leaderboard 2nd RANK LLM Base Model: vidraft/gemma-3-R1984-27B Perfect support for English/Korean/Multi-language Automatic speaker notes generation üé® Premium Visuals 3D style AI image generation 5 design themes (Professional, Modern, Nature, Creative, Minimal) FLUX style diagram images Automatic emoji bullet points üìä Smart Diagrams Process Flow, Concept Map, WBS, Radial, Synoptic Chart Content analysis-based automatic diagram generation Perfect Korean font support üí° Main Features üìù Intelligent Content Generation Auto-generate 3-20 slides just by entering a topic Latest information through web search Reference PDF, CSV, TXT files üñºÔ∏è Visual Automation 3D images for...</description><pubDate>Thu, 19 Jun 2025 05:23:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/667885440717862</guid></item><item><title>stop writing CUDA kernels yourself</title><link>https://huggingface.co/posts/merve/593764353844896</link><description>stop writing CUDA kernels yourself we have launched Kernel Hub: easy optimized kernels for all models on Hugging Face üî• use them right away! it's where the community populates optimized kernels ü§ù this release comes in three parts &gt; Kernel Hub: contains (as of now) 14 kernels &gt; kernels: Python library to load kernels from Kernel Hub &gt; kernel-builder: Nix package to build kernels for PyTorch (made using PyTorch C++ frontend) when building models, your regular workflow should be pulling kernels from Hub and building your model with them ü§ó here's a practical example with RMSNorm: 1. pull the kernel from Hub with get_kernel 2. decorate with use_kernel_forward_from_hub 3. inject it to your model we'd love to hear your feedback! üôèüèª we also welcome kernel contributions by community ü•πüíó - request kernels here: kernels-community/README#1 - check out this org: kernels-community - read the blog: https://huggingface.co/blog/hello-hf-kernels See translation</description><pubDate>Thu, 19 Jun 2025 05:23:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/593764353844896</guid></item><item><title>#CVPR2025 Paper Picks #1</title><link>https://huggingface.co/posts/merve/840226106342254</link><description>#CVPR2025 Paper Picks #1 VisionZip is a compression technique that reduces number of visual tokens to improve performance AND prefill time for vision language models demo: Senqiao/VisionZip paper: VisionZip: Longer is Better but Not Necessary in Vision Language Models (2412.04467) most of the image tokens are redundant for the LLM, so the authors ask "are all visual tokens necessary?" the method is simple: find which tokens have the highest attention score, merge rest of the tokens based on similarity, then merge both their method is both training-free and for fine-tuning the authors report 5 point improvement on average of vision language tasks + 8x improvement in prefilling time for Llava-Next 7B and 13B ü§Ø removing redundant tokens improve image token quality too ü•π See translation</description><pubDate>Thu, 19 Jun 2025 05:23:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/840226106342254</guid></item><item><title>WAN 2.1 FusionX + Self Forcing LoRA are the New Best of Local Video Generation with Only 8 Steps + FLUX Upscaling Guide :</title><link>https://huggingface.co/posts/MonsterMMORPG/682969091190201</link><description>WAN 2.1 FusionX + Self Forcing LoRA are the New Best of Local Video Generation with Only 8 Steps + FLUX Upscaling Guide : https://www.youtube.com/watch?v=Xbn93GRQKsQ Tutorial : https://www.youtube.com/watch?v=Xbn93GRQKsQ Video Chapters 0:00 Introduction to the New FusionX Video Model &amp; FLUX Upscaling 0:30 One-Click Presets &amp; The SwarmUI Model Downloader Explained 1:07 Achieving Hyper-Realism with the FLUX 2x Latent Upscale Preset 1:58 How to Download &amp; Install the SwarmUI Model Downloader 2:49 Downloading Full Models vs. Downloading Just The LoRAs 3:48 Final Setup: Updating SwarmUI &amp; Importing The New Presets 4:32 Generating a Video: Applying the FusionX Image-to-Video Preset 5:03 Critical Step: Correcting The Model's Native Resolution Metadata 5:55 Finalizing Image-to-Video Settings (Frame Count &amp; RIFE Interpolation) 6:49 Troubleshooting Performance: Identifying Low GPU Usage &amp; Shared VRAM Bug 8:35 The Solution: Disabling Sage Attention for Image-to-Video Models 10:02 Final Result:...</description><pubDate>Thu, 19 Jun 2025 05:23:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/682969091190201</guid></item></channel></rss>