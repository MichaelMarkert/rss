<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board — including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. 🤗🔥</title><link>https://huggingface.co/posts/prithivMLmods/844227545389355</link><description>The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board — including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. 🤗🔥 ⚡ Space / App: prithivMLmods/Qwen3-VL-HF-Demo ⚡ Github: https://github.com/prithivsakthiur/qwen3-vl-hf-demo The model’s demo supports a wide range of tasks, including; Image Inference, Video Inference, PDF Inference, Image Captioning (VLA), GIF Inference. ⚡ Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Thanks for granting the blazing-fast Zero GPU access, @ merve 🙏 ⚡ Other Pages &gt; Multimodal VLMs July'25 : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 &gt; VL caption — &lt; Sep 15 ’25 : prithivMLmods/vl-caption-sep-15-25-68c7f6d737985c63c13e2391 &gt; Multimodal...</description><pubDate>Mon, 13 Oct 2025 05:23:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/844227545389355</guid></item><item><title>Arm will be @ PyTorch Conference, Join Us!</title><link>https://huggingface.co/posts/sondhiArm/422203128655094</link><description>Arm will be @ PyTorch Conference, Join Us! Join us on site October 22-23 to see how Arm empowers developers to build and deploy AI applications with ease using PyTorch and ExecuTorch. Learn about the latest AI technologies from Arm and our ecosystem while expanding your professional network alongside like-minded AI engineers. Learn more here: https://huggingface.co/blog/Arm/arm-at-pytorch-conference See translation</description><pubDate>Mon, 13 Oct 2025 05:23:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sondhiArm/422203128655094</guid></item><item><title>Ovi is Local Version of VEO 3 &amp; SORA 2 - The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer on Windows even with a 6GB GPUs - Full Tutorial for Windows, RunPod and Massed Compute - Gradio App &gt;</title><link>https://huggingface.co/posts/MonsterMMORPG/350754844731753</link><description>Ovi is Local Version of VEO 3 &amp; SORA 2 - The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer on Windows even with a 6GB GPUs - Full Tutorial for Windows, RunPod and Massed Compute - Gradio App &gt; https://youtu.be/T00VmkMQRPQ Tutorial : https://youtu.be/T00VmkMQRPQ Forget waiting lists and expensive APIs. The era of closed-off, corporate-controlled AI video generation is soon over. This is Ovi : The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer—even with a 6GB GPU! This isn't just a demo; it's a full, step-by-step revolution. Tutorial Info In this ultimate A-Z guide, I'll show you EVERYTHING you need to know to install and master this Sora 2 and VEO3 like AI. We'll go from zero to generating incredible talking videos from text or a single image. 🔥 In This Tutorial, You Will Learn To: 🎓 Master the Ultimate SORA 2 and VEO 3...</description><pubDate>Mon, 13 Oct 2025 05:23:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/350754844731753</guid></item><item><title>🚀 No invite. No watermark. Just pure AI magic.</title><link>https://huggingface.co/posts/Ethank01/902315574816069</link><description>🚀 No invite. No watermark. Just pure AI magic. Experience Sora 2 on iMini — free for members 👉 https://imini.com/ See translation</description><pubDate>Mon, 13 Oct 2025 05:23:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ethank01/902315574816069</guid></item><item><title>Benchmarking xLLM and Specialized Language Models: New Approach &amp; Results</title><link>https://huggingface.co/posts/vincentg64/231735241059410</link><description>Benchmarking xLLM and Specialized Language Models: New Approach &amp; Results https://mltblog.com/4nzaKUb Standard benchmarking techniques using LLM as a judge have strong limitations. First it creates a circular loop and reflects the flaws present in the AI judges. Then, the perceived quality depends on the end user: an enterprise LLM appeals to professionals and business people, while a generic one appeals to laymen. The two have almost opposite criteria to assess the value. Finally, benchmarking metrics currently in use fail to capture many of the unique features of specialized LLMs, such as exhaustivity, or the quality of the relevancy and trustworthiness scores attached to each element in the response. In fact, besides xLLM, very few if any LLMs display such scores to the user. I now discuss these points, as well as the choice of test prompts, and preliminary results about xLLM, compared to others. -- Structured output vs standard response -- A peculiarity of xLLM is that if offers...</description><pubDate>Mon, 13 Oct 2025 05:23:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/vincentg64/231735241059410</guid></item><item><title>No invitation code needed — create AI videos with one click!</title><link>https://huggingface.co/posts/Ethank01/441451500060564</link><description>No invitation code needed — create AI videos with one click! Experience Sora 2, Veo 3, and Wan 2.2 all in one place on iMini. 👉 Try it here: https://imini.com/ See translation</description><pubDate>Mon, 13 Oct 2025 05:23:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ethank01/441451500060564</guid></item><item><title>Super nice intro to fine-tuning with TRL, just dropped by</title><link>https://huggingface.co/posts/sergiopaniego/617301570898525</link><description>Super nice intro to fine-tuning with TRL, just dropped by @ google (runs free on Colab)! They use SFT + QLoRA to fine-tune the tiny Gemma 3 270M model for emoji generation Here’s what the fine-tuned model generates for the prompt: “I'm learning to tweet” → 🐦🗣💻 Colab: https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb Try it out: google/emoji-gemma Learn more: https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/ See translation</description><pubDate>Mon, 13 Oct 2025 05:23:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/617301570898525</guid></item><item><title>✅ New Article: *Envy — The Structural Law of Relative Evaluation*</title><link>https://huggingface.co/posts/kanaria007/689097395708339</link><description>✅ New Article: *Envy — The Structural Law of Relative Evaluation* Title: 🧭 Envy, Comparison, and the Structural Law of Relative Evaluation: Why “They Got More” Hurts — and How Protocols Contain It 🔗 https://huggingface.co/blog/kanaria007/envy-structural-law-of-relative-evaluation --- Summary: Envy isn’t a moral glitch — it’s a *relative-evaluation loop*. When identity meets comparison, the mind computes *delta-to-others* (status, attention, resources). If that delta breaches a threshold without a repair path, the loop escalates into resentment. Structured Intelligence makes this computable — and containable. &gt; Envy compares levels. &gt; *Integrity compares trajectories.* --- Why It Matters: • Turns envy from shame into an *auditable signal* (where, when, and why comparison spikes) • Provides *de-escalation protocols* for individuals, teams, and platforms • Guides product/organization design to reduce perceived unfairness and burnout --- What’s Inside: • The Envy Loop: *trigger →...</description><pubDate>Mon, 13 Oct 2025 05:23:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/689097395708339</guid></item><item><title>🌊 Create Your Own Underwater World with iMini AI – No Invite, No Watermark!</title><link>https://huggingface.co/posts/Monica997/949496784214909</link><description>🌊 Create Your Own Underwater World with iMini AI – No Invite, No Watermark! Just tried something incredible on iMini AI — I used the nano banana model to generate a stunning underwater world image 🐠, then turned it into a dynamic video using the latest Sora 2 model. The result? It looks so real, like I actually filmed it while scuba diving! 🤿✨ The water ripples, the light rays, even the fish movement — all generated by AI in seconds. The best part? ✅ No invite code required ✅ No watermark ✅ Members can generate without using credits If you’re into AI visuals or creative video experiments, this one’s a must-try. 👉 Experience it now at https://imini.com/ See translation</description><pubDate>Mon, 13 Oct 2025 05:23:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Monica997/949496784214909</guid></item><item><title>Another abliteration by huihui which had big positive impact!</title><link>https://huggingface.co/posts/etemiz/180950130032944</link><description>Another abliteration by huihui which had big positive impact! huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF @ huihui-ai See translation</description><pubDate>Mon, 13 Oct 2025 05:23:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/180950130032944</guid></item></channel></rss>