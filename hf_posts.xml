<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Since when are H200s on ZeroGPU?</title><link>https://huggingface.co/posts/nroggendorff/580094805938772</link><description>Since when are H200s on ZeroGPU? See translation</description><pubDate>Fri, 11 Jul 2025 13:35:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/580094805938772</guid></item><item><title>LiquidAI</title><link>https://huggingface.co/posts/mlabonne/882001725108546</link><description>LiquidAI open-sources a new generation of edge LLMs! ü•≥ Based on a new hybrid architecture, these 350M, 700M, and 1.2B models are both fast and performant, ideal for on-device deployment. I recommend fine-tuning them to power your next edge application. We already provide Colab notebooks to guide you. More to come soon! üìù Blog post: https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models ü§ó Models: LiquidAI/lfm2-686d721927015b2ad73eaa38 See translation</description><pubDate>Fri, 11 Jul 2025 13:35:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/882001725108546</guid></item><item><title>How to achieve 100% Pass Rate on HumanEval ? üî•</title><link>https://huggingface.co/posts/YerbaPage/789674491872223</link><description>How to achieve 100% Pass Rate on HumanEval ? üî• Meet MGDebugger if you are tired of LLMs failing on complex bugs ü§î Our MGDebugger, just hit 100% accuracy on HumanEval using the DeepSeek-R1 model. üöÄ ‚ú® Demo: learnmlf/MGDebugger üìù Paper: From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging (2410.01215) üíª Code: https://github.com/YerbaPage/MGDebugger HumanEval may be retired, we're ready for the next challenge In more complex scenarios! You may also take look at this repo for a collection of awesome repo-level coding tasks! üñ•Ô∏è https://github.com/YerbaPage/Awesome-Repo-Level-Code-Generation See translation</description><pubDate>Fri, 11 Jul 2025 13:35:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/789674491872223</guid></item><item><title>Test SmolLM3, the newest fully open model released by</title><link>https://huggingface.co/posts/sergiopaniego/321784447889589</link><description>Test SmolLM3, the newest fully open model released by @ HuggingFaceTB ! It's smol (3B), multilingual (6 languages), comes with dual mode reasoning (think/no_think modes) and supports long-context (128k). Try it now in the notebook below!! ‚¨áÔ∏è Colab notebook: https://colab.research.google.com/github/sergiopaniego/samples/blob/main/smollm3_3b_inference.ipynb notebook: https://github.com/sergiopaniego/samples/blob/main/smollm3_3b_inference.ipynb blog: https://huggingface.co/blog/smollm3 See translation</description><pubDate>Fri, 11 Jul 2025 13:35:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/321784447889589</guid></item><item><title>Because hackathons are often the starting point for many AI projects, I've created a Python-backend template incorporating my feedback to streamline collaboration and urgent deployments üèéÔ∏è</title><link>https://huggingface.co/posts/louisbrulenaudet/165456039014439</link><description>Because hackathons are often the starting point for many AI projects, I've created a Python-backend template incorporating my feedback to streamline collaboration and urgent deployments üèéÔ∏è Within a year, I had the opportunity to participate in hackathons organized by Mistral, OpenAI, and DeepMind and this GitHub template is structured around several fundamental building blocks and recommendations I offer developers eager to participate in their first hackathon, whether as part of a team or individually. Its emphasis is on rapid setup and deployment through: - uv as a package manager, simplifying usage via a series of pre-configured make commands. - FastAPI for API management, structured in a modular architecture designed to minimize branch conflicts during merges to main branches (using minimal health-check and ping routes to verify Docker‚Äôs proper execution and backend accessibility on the local network). - Pydantic for validation and type handling, which simplifies debugging and...</description><pubDate>Fri, 11 Jul 2025 13:35:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/louisbrulenaudet/165456039014439</guid></item><item><title>GitHub refuses to render notebooks for a long time now üíî</title><link>https://huggingface.co/posts/merve/713910696313243</link><description>GitHub refuses to render notebooks for a long time now üíî so smol-vision now lives in Hugging Face model repository ü§ó merve/smol-vision See translation</description><pubDate>Fri, 11 Jul 2025 13:35:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/713910696313243</guid></item><item><title>I am happy to announce that Ark now supports the following robots:</title><link>https://huggingface.co/posts/hba123/547491071894207</link><description>I am happy to announce that Ark now supports the following robots: 1. Franka Panda 2. Kuka LWR 3. UFactory XArm 4. Husky Robot Everything is done in Python. You can even control your robot from a Jupiter notebook. Check out the tutorials: https://arkrobotics.notion.site/ARK-Home-22be053d9c6f8096bcdbefd6276aba61 Check out the code: https://github.com/orgs/Robotics-Ark/repositories Check out the documentation: https://robotics-ark.github.io/ark_robotics.github.io/docs/html/index.html Check out the paper: https://robotics-ark.github.io/ark_robotics.github.io/static/images/ark_framework_2025.pdf Hope you find it useful. Let us know if you want a specific feature! We would love to support you üòÑ See translation</description><pubDate>Fri, 11 Jul 2025 13:35:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/547491071894207</guid></item><item><title>Are you looking to run a robot simulator, maybe run long robot policy training tasks, but you don't have the GPU at home?</title><link>https://huggingface.co/posts/jbilcke-hf/967618646971244</link><description>Are you looking to run a robot simulator, maybe run long robot policy training tasks, but you don't have the GPU at home? Well.. you can run MuJoCo inside a Hugging Face space! All you have to do is to clone this space: jbilcke-hf/train-robots-with-mujoco Don't forget to a pick a Nvidia GPU for your space, to be able to get some nice OpenGL renders! Are you new to MuJoCo and/or JupyterLab notebooks? You can get started with this tutorial (select "Open from URL" then paste the URL to this notebook): jbilcke-hf/train-robots-with-mujoco Happy robot hacking! ü¶æ See translation</description><pubDate>Fri, 11 Jul 2025 13:35:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jbilcke-hf/967618646971244</guid></item><item><title>Caching is an essential technique used in diffusion inference serving for speeding up image/video generations. Diffusers just added support for another caching method: First Block Cache - a technique developed by</title><link>https://huggingface.co/posts/a-r-r-o-w/278025275110164</link><description>Caching is an essential technique used in diffusion inference serving for speeding up image/video generations. Diffusers just added support for another caching method: First Block Cache - a technique developed by @ chengzeyi building upon the ideas of TeaCache. The idea in short is: if the model predictions do not vary much over successive inference steps, we can skip certain steps where the prediction difference is small. To figure out whether an inference step will make a significant improvement to the overall velocity/noise prediction, we calculate the relative difference of the output of the first transformer block at timestep $t$ with $t-1$, and compare it against a selected threshold. If the difference is lower than the threshold, we skip the step. A higher threshold will lead to more steps being skipped. However, skipping many steps is bad because it can throw off the model predictions, and so we need to test and select the threshold based on level of quality-speed tradeoff...</description><pubDate>Fri, 11 Jul 2025 13:35:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/a-r-r-o-w/278025275110164</guid></item><item><title>I've been posting bits and pieces about this research, but now I can finally say: new paper alert üö®</title><link>https://huggingface.co/posts/giadap/958744591263435</link><description>I've been posting bits and pieces about this research, but now I can finally say: new paper alert üö® My colleague @ brunatrevelin and I just shared a paper exploring why traditional consent frameworks are breaking down in AI contexts (forthcoming chapter in a collective book). The current model places impossible burdens on users to manage countless consent decisions. Meanwhile, AI systems learn to mimic our voices and writing styles from data we unknowingly provided years ago. What's next? We need to shift from individual responsibility to collective accountability. This means: - Organizations designing systems that respect human agency by default - Developers building ethics into models from the start - Policymakers creating frameworks beyond minimal compliance Blog post: https://huggingface.co/blog/giadap/consentful-ai Paper: Can AI be Consentful? (2507.01051) See translation</description><pubDate>Fri, 11 Jul 2025 13:35:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/958744591263435</guid></item></channel></rss>