<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Reverse Engineering a $500M Mystery: From HashHop to Memory-Augmented Language Models</title><link>https://huggingface.co/posts/codelion/260857666290873</link><description>Reverse Engineering a $500M Mystery: From HashHop to Memory-Augmented Language Models I wrote a deep dive into how Magic AI's 100M token context window might work, starting from their HashHop benchmark and building up to MALM - a Memory-Augmented Language Model. Key insight: treating each key as a single token enables perfect retrieval at unlimited context lengths. The article covers: - How HashHop works and why its perfect accuracy is suspicious - Building a tokenized solver that achieves 100% accuracy - Scaling to MALM for real code search tasks - Why this approach could handle 100M+ tokens Read the full article: https://huggingface.co/blog/codelion/reverse-engineering-magic-hashhop Try the model: codelion/malm-165m Code: https://github.com/codelion/hash-hop See translation</description><pubDate>Sun, 25 Jan 2026 17:22:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/260857666290873</guid></item><item><title>We‚Äôve released two conversational speech datasets from oto on Hugging Face ü§ó</title><link>https://huggingface.co/posts/consome2/284818675921006</link><description>We‚Äôve released two conversational speech datasets from oto on Hugging Face ü§ó Both are based on real, casual, full-duplex conversations, but with slightly different focuses. Dataset 1: Processed / curated subset otoearth/otoSpeech-full-duplex-processed-141h * Full-duplex, spontaneous multi-speaker conversations * Participants filtered for high audio quality * PII removal and audio enhancement applied * Designed for training and benchmarking S2S or dialogue models Dataset 2: Larger raw(er) release otoearth/otoSpeech-full-duplex-280h * Same collection pipeline, with broader coverage * More diversity in speakers, accents, and conversation styles * Useful for analysis, filtering, or custom preprocessing experiments We intentionally split the release to support different research workflows: clean and ready-to-use vs. more exploratory and research-oriented use. The datasets are currently private, but we‚Äôre happy to approve access requests ‚Äî feel free to request access if you‚Äôre interested....</description><pubDate>Sun, 25 Jan 2026 17:22:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/consome2/284818675921006</guid></item><item><title>ü§ó Just released Rain-100M, an experimental ~97M-parameter Qwen3-style language model trained from random initialization.</title><link>https://huggingface.co/posts/raincandy-u/660562661454335</link><description>ü§ó Just released Rain-100M, an experimental ~97M-parameter Qwen3-style language model trained from random initialization. Repo: raincandy-u/Rain-100M Data: HuggingFaceFW/fineweb-edu , ~3B tokens, English only Tokenizer: custom 16k BPE, context length 4096 Architecture: 12 Transformer layers, hidden size 768, 12 heads, MLP 2048, SiLU, bf16 Rain-100M is a raw base model (not instruction-tuned or safety-aligned), aimed at small-scale research, debugging training pipelines, and CPU/edge experiments. If you run evaluations, finetunes, or visualizations with it, I would be very interested in your results! See translation</description><pubDate>Sun, 25 Jan 2026 17:22:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/raincandy-u/660562661454335</guid></item><item><title>Uncensored, Heretic GGUF quants of GLM 4.7 (30B-A3B) with correct Llamacpp and all updates ; NEO-CODE Imatrix W 16 bit OTs.</title><link>https://huggingface.co/posts/DavidAU/879154559678091</link><description>Uncensored, Heretic GGUF quants of GLM 4.7 (30B-A3B) with correct Llamacpp and all updates ; NEO-CODE Imatrix W 16 bit OTs. Also specialized quants (balanced for this model), and all quants are NEO-CODE Imatrix W 16 bit output tensor. DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF "Reg quants, non-heretic" : Also 16 bit ot, NEO-CODE Imatrix and specialized: DavidAU/GLM-4.7-Flash-NEO-CODE-Imatrix-MAX-GGUF See translation</description><pubDate>Sun, 25 Jan 2026 17:22:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DavidAU/879154559678091</guid></item><item><title>Introducing QIE-2511-Zoom-Master for highlight-guided area zoom-in, enabling lossless zooming within a drawn square area, and QIE-2511-Object-Remover-v2 for precise object or highlight-guided area cleanup. These experimental adapters are trained based on QIE-2511. Find the adapters below.</title><link>https://huggingface.co/posts/prithivMLmods/575874119029258</link><description>Introducing QIE-2511-Zoom-Master for highlight-guided area zoom-in, enabling lossless zooming within a drawn square area, and QIE-2511-Object-Remover-v2 for precise object or highlight-guided area cleanup. These experimental adapters are trained based on QIE-2511. Find the adapters below. üïπÔ∏èQIE-2511-Zoom-Master : prithivMLmods/QIE-2511-Zoom-Master üïπÔ∏èQIE-2511-Object-Remover-v2: prithivMLmods/QIE-2511-Object-Remover-v2 ü§óDemo: prithivMLmods/Qwen-Image-Edit-Object-Manipulator üìÇCollection: https://huggingface.co/collections/prithivMLmods/qwen-image-edit-exps To learn more, visit the app page or the respective model pages. See translation</description><pubDate>Sun, 25 Jan 2026 17:22:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/575874119029258</guid></item><item><title>After 2 months of refinement, I'm happy to announce that a lot of Transformers' modeling code is now significantly more torch-compile &amp; export-friendly üî•</title><link>https://huggingface.co/posts/IlyasMoutawwakil/703555138750194</link><description>After 2 months of refinement, I'm happy to announce that a lot of Transformers' modeling code is now significantly more torch-compile &amp; export-friendly üî• Why it had to be done üëá PyTorch's Dynamo compiler is increasingly becoming the default interoperability layer for ML systems. Anything that relies on torch.export or torch.compile, from model optimization to cross-framework integrations, benefits directly when models can be captured as a single dynamo-traced graph ! Transformers models are now easier to: ‚öôÔ∏è Compile end-to-end with torch.compile backends üì¶ Export reliably via torch.export and torch.onnx.export üöÄ Deploy to ONNX / ONNX Runtime, Intel Corporation's OpenVINO, NVIDIA AutoDeploy (TRT-LLM), AMD's Quark, Meta's Executorch and more hardware-specific runtimes. This work aims at unblocking entire TorchDynamo-based toolchains that rely on exporting Transformers across runtimes and accelerators. We are doubling down on Transformers commitment to be a first-class citizen of the...</description><pubDate>Sun, 25 Jan 2026 17:22:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/IlyasMoutawwakil/703555138750194</guid></item><item><title>‚úÖ New Article: *Jumps as Atomic Moves* (v0.1)</title><link>https://huggingface.co/posts/kanaria007/389082518772877</link><description>‚úÖ New Article: *Jumps as Atomic Moves* (v0.1) Title: üß† Jumps: Atomic Moves in Structured Intelligence (and How to Make Them Safe) üîó https://huggingface.co/blog/kanaria007/jumps-atomic-moves-in-si --- Summary: In SI-Core, a *Jump* is the smallest *effectful* unit of reasoning+action: a move that consumes observations, proposes/chooses an action, and (optionally) commits results + memory updates. This article makes Jumps operational: *what a Jump must declare*, how it is gated (OBS/ETH/RML), how it produces auditable traces, and how to keep it safe under uncertainty‚Äîwithout collapsing into ‚Äújust prompt chaining.‚Äù &gt; If you can‚Äôt name the Jump, you can‚Äôt audit it. &gt; If you can‚Äôt gate it, you can‚Äôt ship it. --- Why It Matters: ‚Ä¢ Stops hidden behavior: every effectful move becomes *declared + inspectable* ‚Ä¢ Prevents ‚Äújumping in the dark‚Äù via *OBS gating + sandbox-only paths* ‚Ä¢ Makes policy enforceable: ETH overlay can *allow/modify/block/escalate* per Jump type ‚Ä¢ Improves rollback...</description><pubDate>Sun, 25 Jan 2026 17:22:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/389082518772877</guid></item><item><title>You can now fine-tune embedding models in our free Unsloth notebook! ü§ó</title><link>https://huggingface.co/posts/danielhanchen/579968620456275</link><description>You can now fine-tune embedding models in our free Unsloth notebook! ü§ó Fine-tuning embedding models improves retrieval &amp; RAG by aligning vectors to your domain-specific notion of similarity, improving search, clustering, and recommendations on your data. ‚≠ê Blog + Notebooks: https://unsloth.ai/docs/new/embedding-finetuning Unsloth trains embedding models 1.8-3.3x faster with 20% less VRAM, 2x longer context &amp; no accuracy loss vs. FA2 setups. We'd like to thank Hugging Face and Unsloth contributor: electroglyph for making this possible! See translation</description><pubDate>Sun, 25 Jan 2026 17:22:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/579968620456275</guid></item><item><title>‚úÖ New Article: *Genius Replay Protocol* (v0.1)</title><link>https://huggingface.co/posts/kanaria007/334353113368335</link><description>‚úÖ New Article: *Genius Replay Protocol* (v0.1) Title: üß† Genius Replay Protocol: Capturing and Replaying High-Value Jumps üîó https://huggingface.co/blog/kanaria007/genius-replay-protocol --- Summary: ‚ÄúGenius‚Äù isn‚Äôt magic‚Äîit‚Äôs a **high-value Jump (or short Jump sequence)** that reliably produces outsized GCS gains with strong robustness and reuse potential. This article defines the **Genius Replay Protocol (GRP)**: a safe way to **capture, validate, store, and replay** those exceptional moves as reusable macro-intelligence‚Äîwithout turning them into copy-pasted folklore. At the core is a **GeniusTrace** bundle: *ContextSignature* (where it applies) + *JumpSequence* (what was done) + *EvalSummary* (why it worked) + *EthicsTrace* (why it‚Äôs allowed). &gt; Don‚Äôt replay outcomes. &gt; Replay **structure**, under today‚Äôs constraints. --- Why It Matters: ‚Ä¢ Turns ‚Äúit worked once‚Äù into a **reproducible asset** ‚Ä¢ Prevents unsafe cargo-culting by replaying **structure**, not brittle outputs ‚Ä¢ Ensures...</description><pubDate>Sun, 25 Jan 2026 17:22:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/334353113368335</guid></item><item><title>Meet FluxLailah;</title><link>https://huggingface.co/posts/AbstractPhil/216348621155018</link><description>Meet FluxLailah; AbstractPhil/tiny-flux-deep ; 220m Flux variant currently pretraining at BF16. She is experimental, does not produce solid images yet - and yet she is producing. There is both an EMA and a raw weights pair producing different images. The EMA is particularly interesting at times. Lailah uses flan-t5-base, clip-vit-l-14, and BlackForestLabs Flux1s VAE. SEQ limit 128, images 512x512 for now. Lailah's early form is based on three variants. TinyFlux's weights were carefully planted into a deeper structure and trained yet again - dubbed TinyFlux-Deep. This variant has 15 dual-stream blocks and 25 single-stream blocks, nearly identical weight code as Flux with a similar attention mechanism - but intentionally deviant and compacted with careful consideration to scaling and purpose of mechanisms. She went through quite a few growing pains with her earlier attention mechanism which required a reimagining today and careful consideration of the consequences, and now I present...</description><pubDate>Sun, 25 Jan 2026 17:22:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AbstractPhil/216348621155018</guid></item></channel></rss>