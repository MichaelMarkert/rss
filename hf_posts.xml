<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AI Just Made My Cat the King of Emojis 👑🐱😂</title><link>https://huggingface.co/posts/Monica997/874620000877286</link><description>AI Just Made My Cat the King of Emojis 👑🐱😂 Never thought I’d see this — but with iMini’s nano banana model, my cat is now a full emoji + sticker pack 🎨✨ Used the 9-grid meme template + cartoon sticker generator, and in just ONE click 👉 my ordinary cat photo turned into a hilarious, cute, and super shareable set of stickers 💬🔥 No need to master complicated nano banana prompts — iMini handles everything. Perfect for chats, socials, or just showing off your pet’s new “digital identity.” 👉 Try it here: https://imini.com/nano-banana Who else wants their pet to be the next emoji star? 🌟 See translation</description><pubDate>Thu, 25 Sep 2025 13:32:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Monica997/874620000877286</guid></item><item><title>I'm sorry, what?</title><link>https://huggingface.co/posts/nroggendorff/916862110503909</link><description>I'm sorry, what?</description><pubDate>Thu, 25 Sep 2025 13:32:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/916862110503909</guid></item><item><title>Photo-Mate-i2i – a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages!</title><link>https://huggingface.co/posts/prithivMLmods/355225487543965</link><description>Photo-Mate-i2i – a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages! ⚡Demo: prithivMLmods/Photo-Mate-i2i ⚙️How to Use: prithivMLmods/Photo-Mate-i2i#2 👨‍🔧i2i-Kontext(Experimental LoRAs): prithivMLmods/i2i-kontext-exp-68ce573b5c0623476b636ec7 See translation</description><pubDate>Thu, 25 Sep 2025 13:32:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/355225487543965</guid></item><item><title>🎯 RetinaFace On-Device Deployment Study: NPU Acceleration Breakthrough!</title><link>https://huggingface.co/posts/yeonseok-zeticai/752870941871415</link><description>🎯 RetinaFace On-Device Deployment Study: NPU Acceleration Breakthrough! (Check details at :https://mlange.zetic.ai/p/Steve/RetinaFace) TL;DR: Successfully deployed RetinaFace with ZETIC.MLange achieving 1.43ms inference on mobile NPU! 🔍 Complete Performance Analysis: Latency Comparison: - NPU: 1.43ms (Winner! 🏆) - GPU: 3.75ms - CPU: 21.42ms Accuracy Metrics - SNR: - FP16: 56.98 dB - Integer Quantized: 48.03 dB (Precision-Performance: Excellent trade-off maintained) Memory Footprint: - Model Size: 2.00 MB (highly compressed) - Runtime Memory: 14.58 MB peak - Deployment Ready: ✅ Production optimized 🛠 Technical Implementation: (Runnable with Copy &amp; Paste at the MLange link!) 📊 Device Compatibility Matrix: Tested on 50+ devices including Samsung Galaxy series, Google Pixel lineup, and Xiaomi devices, iPhones and iPads. Consistent sub-5ms performance across the board! 🚀 Applications Unlocked: - Real-time AR/VR face tracking - Privacy-preserving edge authentication - Live video...</description><pubDate>Thu, 25 Sep 2025 13:32:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/752870941871415</guid></item><item><title>YOLOv11 Complete On-device Study</title><link>https://huggingface.co/posts/yeonseok-zeticai/506441566129403</link><description>YOLOv11 Complete On-device Study - {NPU vs GPU vs CPU} Across All Model Variants We've just completed comprehensive benchmarking of the entire YOLOv11 family on ZETIC.MLange. Here's what every ML engineer needs to know. 📊 Key Findings Across 5 Model Variants (XL to Nano): 1. NPU Dominance in Efficiency: - YOLOv11n: 1.72ms on NPU vs 53.60ms on CPU (31x faster) - Memory footprint: 0-65MB across all variants - Consistent sub-10ms inference even on XL models 2. The Sweet Spot - YOLOv11s: - NPU: 3.23ms @ 95.57% mAP - Perfect balance: 36MB model, production-ready speed - 10x faster than GPU, 30x faster than CPU 3. Surprising Discovery: Medium models (YOLOv11m) show unusual GPU performance patterns - NPU outperforms GPU by 4x (9.55ms vs 35.82ms), suggesting current GPU kernels aren't optimized for mid-size architectures. 4. Production Insights: - XL/Large: GPU still competitive for batch processing - Small/Nano: NPU absolutely crushes everything else - Memory scaling: Linear from 10MB...</description><pubDate>Thu, 25 Sep 2025 13:32:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/506441566129403</guid></item><item><title>BAAI has released ROME🔥 evaluating 30+ large reasoning models on text &amp; visual reasoning</title><link>https://huggingface.co/posts/AdinaY/290924120685458</link><description>BAAI has released ROME🔥 evaluating 30+ large reasoning models on text &amp; visual reasoning FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions (2509.17177) ✨Tests visual reasoning, not just recognition ✨Covers capability × alignment × safety × efficiency ✨More transparent &amp; reliable (less data contamination) ✨Helps make real-world deployment choices See translation</description><pubDate>Thu, 25 Sep 2025 13:32:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/290924120685458</guid></item><item><title>Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.🤗✨</title><link>https://huggingface.co/posts/prithivMLmods/322831563234696</link><description>Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.🤗✨ prithivMLmods/PhotoCleanser-i2i : Remove objects while preserving the rest of the image. prithivMLmods/Photo-Restore-i2i : Restore old photos into moderately colorized, detailed images. prithivMLmods/Polaroid-Warm-i2i : Seamless vintage Polaroid-style images with warm, faded tones. prithivMLmods/Yarn-Photo-i2i : Convert images into yarn-stitched artwork while retaining key details. prithivMLmods/Monochrome-Pencil : Turn images into monochrome pencil sketches while keeping original features. ✨Note: All the above models share the same auto-labeling multimodal VLM captioning model, prithivMLmods/DeepCaption-VLA-7B , which is used...</description><pubDate>Thu, 25 Sep 2025 13:32:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/322831563234696</guid></item><item><title>🌞 Our Summer of Workflows finale! New release Embedding Generation for Videos.</title><link>https://huggingface.co/posts/dmoxy/581267174720703</link><description>🌞 Our Summer of Workflows finale! New release Embedding Generation for Videos. With our latest ApertureDB AI workflow, you can now generate embeddings for video frames &amp; clips and store them directly in a multimodal database—ready for semantic search, RAG, or agentic use cases. 🎬 See It In Action: https://youtu.be/X2ZXE0EEAkk 🔎 Example use cases: Natural language search across video libraries Highlight reel creation &amp; scene retrieval Safer content moderation Lecture indexing + video-to-text alignment Multimodal RAG (text + images + video) ✨ Check out the demo notebook to see how you can: Import videos from S3 Generate embeddings per frame/clip Query videos with natural language (“show me a baby”) 👉 Try it here: https://shorturl.at/jSNtu Turn your video library into a fully searchable knowledge base. No scrubbing—just instant, semantic video results. We’d love your feedback and ideas on how you would use it. See translation</description><pubDate>Thu, 25 Sep 2025 13:32:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dmoxy/581267174720703</guid></item><item><title>We're joining the</title><link>https://huggingface.co/posts/salma-remyx/828425996651513</link><description>We're joining the @ ag2 team in discord to present a deep-dive into how we've used the framework to build GitRank in their Community Talks The GitRank pipeline is used to: 📰 power personalized paper recommendations 🐳 build environments as Docker Images 🎯 implement core-methods as PRs for your target repo Don't miss it! Tomorrow, Sept 25 at 9:00 am PST: https://calendar.app.google/3soCpuHupRr96UaF8 See translation</description><pubDate>Thu, 25 Sep 2025 13:32:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/828425996651513</guid></item><item><title>💥 Tons of new material just landed in the smol-course! 🧑‍💻</title><link>https://huggingface.co/posts/sergiopaniego/566597314485869</link><description>💥 Tons of new material just landed in the smol-course! 🧑‍💻 &gt; evaluation &gt; alignment &gt; VLMs &gt; quizzes &gt; assignments! &gt; certificates!👩‍🎓 go learn! 👉 https://huggingface.co/learn/smol-course/unit0/1 See translation</description><pubDate>Thu, 25 Sep 2025 13:32:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/566597314485869</guid></item></channel></rss>