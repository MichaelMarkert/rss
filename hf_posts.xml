<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Why I think local, open-source models will eventually win.</title><link>https://huggingface.co/posts/abidlabs/941146046599374</link><description>Why I think local, open-source models will eventually win. The most useful AI applications are moving toward multi-turn agentic behavior: systems that take hundreds or even thousands of iterative steps to complete a task, e.g. Claude Code, computer-control agents that click, type, and test repeatedly. In these cases, the power of the model is not how smart it is per token, but in how quickly it can interact with its environment and tools across many steps. In that regime, model quality becomes secondary to latency. An open-source model that can call tools quickly, check that the right thing was clicked, or verify that a code change actually passes tests can easily outperform a slightly â€œsmarterâ€ closed model that has to make remote API calls for every move. Eventually, the balance tips: it becomes impractical for an agent to rely on remote inference for every micro-action. Just as no one would tolerate a keyboard that required a network request per keystroke, users wonâ€™t accept...</description><pubDate>Wed, 05 Nov 2025 17:23:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/abidlabs/941146046599374</guid></item><item><title>fine-tuning a 14B model with TRL + SFT on a free Colab (T4 GPU)?</title><link>https://huggingface.co/posts/sergiopaniego/990279445625588</link><description>fine-tuning a 14B model with TRL + SFT on a free Colab (T4 GPU)? thanks to the latest TRL optimizations, you actually can! sharing a new notebook showing how to do it ğŸ˜ colab: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_trl_lora_qlora.ipynb notebooks in TRL: https://github.com/huggingface/trl/tree/main/examples/notebooks See translation</description><pubDate>Wed, 05 Nov 2025 17:23:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/990279445625588</guid></item><item><title>Experience the future of fashion with our AI-powered virtual try-on technology. See how clothes look on anyone instantly, create realistic outfit visualizations, and mix-and-match styles with unprecedented accuracy.</title><link>https://huggingface.co/posts/wang12390/323744389614625</link><description>Experience the future of fashion with our AI-powered virtual try-on technology. See how clothes look on anyone instantly, create realistic outfit visualizations, and mix-and-match styles with unprecedented accuracy. https://miragic.ai/products/virtual-try-on See translation</description><pubDate>Wed, 05 Nov 2025 17:23:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wang12390/323744389614625</guid></item><item><title>Some weeks ago, i've just decide its time to leave LinkedIn for me.</title><link>https://huggingface.co/posts/flozi00/890663421107803</link><description>Some weeks ago, i've just decide its time to leave LinkedIn for me. It got silent around my open source activities the last year, so i thought something has to change. That's why my focus will move to share experiences and insights about hardware, drivers, kernels and linux. I won't post about how to use models, built agents or do prompting. I want to share about some deeper layers the actual hypes are built on. I will start posting summarizations of my articles here on the hub. English version: https://flozi.net/en German translated version: https://flozi.net/de Feel free to reach me if you want to read something specific. See translation</description><pubDate>Wed, 05 Nov 2025 17:23:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/flozi00/890663421107803</guid></item><item><title>Pixcribe â€” AI-Powered Social Media Caption Generator ğŸ“¸âœ¨</title><link>https://huggingface.co/posts/DawnC/897621817995080</link><description>Pixcribe â€” AI-Powered Social Media Caption Generator ğŸ“¸âœ¨ Transform your images into compelling stories with intelligent multi-model analysis! What can Pixcribe do? ğŸ“¸ Upload a photo to get instant AI-generated captions in Traditional Chinese and English. - ğŸ·ï¸ Brand Recognition â€” Detects logos and brand elements through visual detection, semantic analysis, and OCR verification. - ğŸ¨ Scene Understanding â€” Analyzes composition, lighting conditions, and visual aesthetics to capture your image's mood and context. - ğŸ” Smart Text Extraction â€” Identifies and incorporates text from your images into captions seamlessly. - âš¡ Multi-Model Intelligence â€” Combines YOLOv11 object detection, OpenCLIP semantic understanding, EasyOCR text recognition, U2-Net saliency detection, and Qwen2.5-VL-7B caption generation. What's next? ğŸ¬ Video processing capabilities ğŸŒ Enhanced multilingual support ğŸ¯ Interactive caption refinement with user feedback âš¡ Real-time processing optimizations - Current Status: Under...</description><pubDate>Wed, 05 Nov 2025 17:23:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/897621817995080</guid></item><item><title>We just released Maya-1-Voice, an open source voice AI model with voice design and emotions.</title><link>https://huggingface.co/posts/DheemanthReddy/522487783365937</link><description>We just released Maya-1-Voice, an open source voice AI model with voice design and emotions. Describe voices in natural language. Add 20+ emotions like &lt;laugh&gt;, &lt;cry&gt;, &lt;whisper&gt; inline. 3B parameters, production-ready, runs on single GPU with vLLM. Apache 2.0. Built on Llama backbone, predicts SNAC codec tokens for real-time streaming. Model: https://huggingface.co/maya-research/maya-1-voice See translation</description><pubDate>Wed, 05 Nov 2025 17:23:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DheemanthReddy/522487783365937</guid></item><item><title>11 Fascinating new Policy Optimization techniques</title><link>https://huggingface.co/posts/Kseniase/468043722468280</link><description>11 Fascinating new Policy Optimization techniques Policy optimization (PO) algorithms are central to training AI models with preference-based feedback. In recent weeks, numerous new PO methods have emerged that build on or replace the popular PPO and GRPO, solving their issues. Here are 11 of them: 1. BAlanced Policy Optimization (BAPO) â†’ BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping (2510.18927) Dynamically adjusting the clipping bounds in PPO-style updates to balance positive and negative gradients and prevent entropy collapse 2. Training-Free GRPO â†’ Training-Free Group Relative Policy Optimization (2510.08191) Instead of using numeric rewards, it compares rollouts semantically to distill useful knowledge as a token prior, which is then applied during inference to guide the modelâ€™s behavior 3. Asymmetric Importance Sampling Policy Optimization (ASPO) â†’ ASPO: Asymmetric Importance Sampling Policy Optimization...</description><pubDate>Wed, 05 Nov 2025 17:23:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/468043722468280</guid></item><item><title>On this day in 2019, OpenAI released the final GPT-2 model as part of their staged release. I still remember that November well - so much was happening, but GPT-2's release felt like a watershed moment for the field. It showed us what was possible with carefully trained language models.</title><link>https://huggingface.co/posts/codelion/382097318111878</link><description>On this day in 2019, OpenAI released the final GPT-2 model as part of their staged release. I still remember that November well - so much was happening, but GPT-2's release felt like a watershed moment for the field. It showed us what was possible with carefully trained language models. To recreate some of that GPT-2 magic, I recently tackled an interesting challenge: can you pretrain a language model with just 1 billion tokens - roughly 1/10th of what GPT-2 used - and still get comparable performance? After 50+ systematic experiments testing different dataset mixtures, the answer is yes. The result is **codelion/gpt-2-70m** ( codelion/gpt-2-70m ), which achieves over 90% of GPT-2's benchmark performance despite being trained on 10x less data. The key was finding the optimal dataset composition: 50% high-quality textbook PDFs, 30% filtered web content, and 20% educational resources. It even beats GPT-2 on TruthfulQA (47.31% vs 40.69%). If you're interested in the full story of how...</description><pubDate>Wed, 05 Nov 2025 17:23:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/382097318111878</guid></item><item><title>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥</title><link>https://huggingface.co/posts/nouamanetazi/972464132222376</link><description>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥ Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious ğğ‚ğ‚ğ‹ ğğ«ğ«ğ¨ğ«ğ¬, or when your expensive GPU cluster is running at ğŸ”ğŸ% ğğŸğŸğ¢ğœğ¢ğğ§ğœğ², the problem isn't your model. It's most probably a ğ¦ğ¢ğ¬ğ®ğ¬ğ ğ¨ğŸ ğ­ğ¡ğ ğ¡ğšğ«ğğ°ğšğ«ğ. ğŸ› ï¸ Questions that seemed simple but had no clear answers: Why is ğŒğ¨ğ„ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğ¬ğ¥ğ¨ğ°ğğ« ğ­ğ¡ğšğ§ ğğğ§ğ¬ğ ğ¦ğ¨ğğğ¥ğ¬? Which ğğ‚ğ‚ğ‹ ğŸğ¥ğšğ ğ¬ should we actually set? How often should we checkpoint without killing throughput? That's why we built ğ“ğ¡ğ ğ’ğ¦ğ¨ğ¥ ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğğ¥ğšğ²ğ›ğ¨ğ¨ğ¤ ğŸ“–: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¥ğšğ²ğğ« that most teams get wrong. We validated real vs...</description><pubDate>Wed, 05 Nov 2025 17:23:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nouamanetazi/972464132222376</guid></item><item><title>ğŸš€ğŸ‘ï¸ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ‘ï¸ğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/716491468051168</link><description>ğŸš€ğŸ‘ï¸ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ‘ï¸ğŸš€ ğŸ“„ Title: Diving into the Fusion of Monocular Priors for Generalized Stereo Matching ğŸ” ğŸ“ Description: The proposed method enhances stereo matching by efficiently combining unbiased monocular priors from vision foundation models. This method addresses misalignment and local optima issues using a binary local ordering map and pixel-wise linear regression. ğŸ‘¥ Authors: Chengtang Yao, Lidong Yu, Zhidan Liu, Jiaxi Zeng, Yuwei Wu, and Yunde Jia ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: Diving into the Fusion of Monocular Priors for Generalized Stereo Matching (2505.14414) ğŸ“ Repository: https://github.com/YaoChengTang/Diving-into-the-Fusion-of-Monocular-Priors-for-Generalized-Stereo-Matching ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the 3D Pose Understanding Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/3d-pose-...</description><pubDate>Wed, 05 Nov 2025 17:23:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/716491468051168</guid></item></channel></rss>