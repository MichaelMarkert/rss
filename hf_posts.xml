<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>made a few improvements on custom grpo trainer:</title><link>https://huggingface.co/posts/Jaward/982484477481896</link><description>made a few improvements on custom grpo trainer: - added sequence similarity reward (seems to work) - improved vllm support (5x inference speed) - adjusted reward scores (this helped with format/accuracy) - can now push to hf hub (already pushed mine lol: Jaward/smollm2_360m_grpo_gsm8k_reasoner ) Code: https://github.com/Jaykef/ai-algorithms/blob/main/smollm2_360M_135M_grpo_gsm8k.ipynb See translation</description><pubDate>Sun, 02 Mar 2025 09:21:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/982484477481896</guid></item><item><title>Dear HF Community!</title><link>https://huggingface.co/posts/lingvanex-mt/205421793754165</link><description>Dear HF Community! Our company open-sourced machine translation models for 12 rare languages under MIT license. You can use them freely with OpenNMT translation framework. Each model is about 110 mb and has an excellent performance, ( about 40000 characters / s on Nvidia RTX 3090 ) Download models there https://huggingface.co/lingvanex You can test translation quality there: https://lingvanex.com/translate/ See translation</description><pubDate>Sun, 02 Mar 2025 09:21:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lingvanex-mt/205421793754165</guid></item><item><title>What if AI becomes as ubiquitous as the internet, but runs locally and transparently on our devices?</title><link>https://huggingface.co/posts/fdaudens/113970261627099</link><description>What if AI becomes as ubiquitous as the internet, but runs locally and transparently on our devices? Fascinating TED talk by @ thomwolf on open source AI and its future impact. Imagine this for AI: instead of black box models running in distant data centers, we get transparent AI that runs locally on our phones and laptops, often without needing internet access. If the original team moves on? No problem - resilience is one of the beauties of open source. Anyone (companies, collectives, or individuals) can adapt and fix these models. This is a compelling vision of AI's future that solves many of today's concerns around AI transparency and centralized control. Watch the full talk here: https://www.ted.com/talks/thomas_wolf_what_if_ai_just_works See translation</description><pubDate>Sun, 02 Mar 2025 09:21:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/113970261627099</guid></item><item><title>Dropping some of the custom fine-tunes based on SigLIP2,</title><link>https://huggingface.co/posts/prithivMLmods/305640045790864</link><description>Dropping some of the custom fine-tunes based on SigLIP2, with a single-label classification problem type! ðŸŒ€ðŸ§¤ - AI vs Deepfake vs Real : prithivMLmods/AI-vs-Deepfake-vs-Real-Siglip2 - Deepfake Detect : prithivMLmods/Deepfake-Detect-Siglip2 - Fire Detection : prithivMLmods/Fire-Detection-Siglip2 - Deepfake Quality Assess : prithivMLmods/Deepfake-Quality-Assess-Siglip2 - Guard Against Unsafe Content : prithivMLmods/Guard-Against-Unsafe-Content-Siglip2 ðŸŒ Collection : prithivMLmods/siglip2-custom-67bcdb2de8fe96b99fb4e19e See translation</description><pubDate>Sun, 02 Mar 2025 09:21:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/305640045790864</guid></item><item><title>ðŸ“Š Introducing "Hugging Face Dataset Spotlight" ðŸ“Š</title><link>https://huggingface.co/posts/davanstrien/119757489156561</link><description>ðŸ“Š Introducing "Hugging Face Dataset Spotlight" ðŸ“Š I'm excited to share the first episode of our AI-generated podcast series focusing on nice datasets from the Hugging Face Hub! This first episode explores mathematical reasoning datasets: - SynthLabsAI/Big-Math-RL-Verified : Over 250,000 rigorously verified problems spanning multiple difficulty levels and mathematical domains - open-r1/OpenR1-Math-220k : 220,000 math problems with multiple reasoning traces, verified for accuracy using Math Verify and Llama-3.3-70B models. - facebook/natural_reasoning : 1.1 million general reasoning questions carefully deduplicated and decontaminated from existing benchmarks, showing superior scaling effects when training models like Llama3.1-8B-Instruct. Plus a bonus segment on bespokelabs/bespoke-manim ! https://www.youtube.com/watch?v=-TgmRq45tW4 See translation</description><pubDate>Sun, 02 Mar 2025 09:21:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/davanstrien/119757489156561</guid></item><item><title>After running some 3DMark and FurMark benchmarks on Windows to make sure that my new 5090 is not causing melting cables [1] and some nice shots with a thermal camera (I don't think that's too much), running some fine-tuning experiments with my favorite Flair &amp; Transformers libraries are very easy to perform.</title><link>https://huggingface.co/posts/stefan-it/989004577410802</link><description>After running some 3DMark and FurMark benchmarks on Windows to make sure that my new 5090 is not causing melting cables [1] and some nice shots with a thermal camera (I don't think that's too much), running some fine-tuning experiments with my favorite Flair &amp; Transformers libraries are very easy to perform. Important steps: Good idea is to start with a fresh Ubuntu 24.04 installation with latest CUDA 12.8 and the open NVIDIA driver - follow more advices from [2]: sudo apt -y install cuda-toolkit-12-8 nvidia-open I tried update from an existing Ubuntu installation with an older CUDA and driver version and it resulted in a non-startable system. If you are using PyTorch 2.6 with built CUDA 12.6 it will result in: NVIDIA Graphics Device with CUDA capability sm_120 is not compatible with the current PyTorch installation. The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90. But no worries! For PyTorch you need just to use a nightly 2.7 version...</description><pubDate>Sun, 02 Mar 2025 09:21:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/stefan-it/989004577410802</guid></item><item><title>Quick POC: Turn a Hugging Face dataset card into a short podcast introducing the dataset using all open models.</title><link>https://huggingface.co/posts/davanstrien/855251141208457</link><description>Quick POC: Turn a Hugging Face dataset card into a short podcast introducing the dataset using all open models. I think I'm the only weirdo who would enjoy listening to something like this though ðŸ˜… Here is an example for eth-nlped/stepverify See translation</description><pubDate>Sun, 02 Mar 2025 09:21:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/davanstrien/855251141208457</guid></item><item><title>I made a real time voice agent with FastRTC, smolagents, and hugging face inference providers. Check it out in this space:</title><link>https://huggingface.co/posts/burtenshaw/742649076372470</link><description>I made a real time voice agent with FastRTC, smolagents, and hugging face inference providers. Check it out in this space: ðŸ”— burtenshaw/coworking_agent See translation</description><pubDate>Sun, 02 Mar 2025 09:21:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/742649076372470</guid></item><item><title>Has OpenGVLab Lumina Outperformed OpenAIâ€™s Model?</title><link>https://huggingface.co/posts/jasoncorkill/366338815946143</link><description>Has OpenGVLab Lumina Outperformed OpenAIâ€™s Model? Weâ€™ve just released the results from a large-scale human evaluation (400k annotations) of OpenGVLabâ€™s newest text-to-image model, Lumina. Surprisingly, Lumina outperforms OpenAIâ€™s DALL-E 3 in terms of alignment, although it ranks #6 in our overall human preference benchmark. To support further development in text-to-image models, weâ€™re making our entire human-annotated dataset publicly available. If youâ€™re working on model improvements and need high-quality data, feel free to explore. We welcome your feedback and look forward to any insights you might share! Rapidata/OpenGVLab_Lumina_t2i_human_preference See translation</description><pubDate>Sun, 02 Mar 2025 09:21:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/366338815946143</guid></item><item><title>We're using RLHF on diffusion models, right? Just making sure..</title><link>https://huggingface.co/posts/nroggendorff/209736816535732</link><description>We're using RLHF on diffusion models, right? Just making sure.. See translation</description><pubDate>Sun, 02 Mar 2025 09:21:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/209736816535732</guid></item></channel></rss>