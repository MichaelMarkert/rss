<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>8 Emerging trends in Reinforcement Learning</title><link>https://huggingface.co/posts/Kseniase/610901061800000</link><description>8 Emerging trends in Reinforcement Learning Reinforcement learning is having a moment - and not just this week. Some of its directions are already showing huge promise, while others are still early but exciting. Here‚Äôs a look at what‚Äôs happening right now in RL: 1. Reinforcement Pre-Training (RPT) ‚Üí Reinforcement Pre-Training (2506.08007) Reframes next-token pretraining as RL with verifiable rewards, yielding scalable reasoning gains 2. Reinforcement Learning from Human Feedback (RLHF) ‚Üí Deep reinforcement learning from human preferences (1706.03741) The top approach. It trains a model using human preference feedback, building a reward model and then optimizing the policy to generate outputs people prefer 3. Reinforcement Learning with Verifiable Rewards (RLVR) ‚Üí Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs (2506.14245) Moves from subjective (human-labeled) rewards to objective ones that can be automatically verified, like in...</description><pubDate>Mon, 06 Oct 2025 17:20:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/610901061800000</guid></item><item><title>Ovi - Generate Videos With Audio Like VEO 3 or SORA 2 - Run Locally - Open Source for Free</title><link>https://huggingface.co/posts/MonsterMMORPG/329376915558167</link><description>Ovi - Generate Videos With Audio Like VEO 3 or SORA 2 - Run Locally - Open Source for Free Download and install : https://www.patreon.com/posts/140393220 Quick demo tutorial : https://youtu.be/uE0QabiHmRw Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation Project page : https://aaxwaz.github.io/Ovi/ SECourses Ovi Pro Premium App Features Full scale ultra advanced app for Ovi - an open source project that can generate videos from both text prompts and image + text prompts with real audio. Project page is here : https://aaxwaz.github.io/Ovi/ I have developed an ultra advanced Gradio app and much better pipeline that fully supports block swapping Now we can generate full quality videos with as low as 8.2 GB VRAM Hopefully I will work on dynamic on load FP8_Scaled tomorrow to improve VRAM even further So more VRAM optimizations will come hopefully tomorrow Our implemented block swapping is the very best one out there - I took the approach from famous Kohya Musubi tuner The...</description><pubDate>Mon, 06 Oct 2025 17:20:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/329376915558167</guid></item><item><title>A few days ago, Thinking Machines Lab released ‚ÄúLoRA Without Regret‚Äù, showing that LoRA can match full fine-tuning performance when configured right.</title><link>https://huggingface.co/posts/sergiopaniego/862106039351008</link><description>A few days ago, Thinking Machines Lab released ‚ÄúLoRA Without Regret‚Äù, showing that LoRA can match full fine-tuning performance when configured right. Naturally, we decided to reproduce the results with TRL and release a guide! https://huggingface.co/docs/trl/main/en/lora_without_regret See translation</description><pubDate>Mon, 06 Oct 2025 17:20:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/862106039351008</guid></item><item><title>Ever wanted an open‚Äësource deep research agent? Meet Deepresearch‚ÄëAgent üîçü§ñ</title><link>https://huggingface.co/posts/Parveshiiii/260243419840768</link><description>Ever wanted an open‚Äësource deep research agent? Meet Deepresearch‚ÄëAgent üîçü§ñ 1. Multi‚Äëstep reasoning: Reflects between steps, fills gaps, iterates until evidence is solid. 2. Research‚Äëaugmented: Generates queries, searches, synthesizes, and cites sources. 3. Fullstack + LLM‚Äëfriendly: React/Tailwind frontend, LangGraph/FastAPI backend; works with OpenAI/Gemini. üîó GitHub: https://github.com/Parveshiiii/Deepresearch-Agent See translation</description><pubDate>Mon, 06 Oct 2025 17:20:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Parveshiiii/260243419840768</guid></item><item><title>Introducing a Hugging Face Tutorial on Regression</title><link>https://huggingface.co/posts/SelmaNajih001/436109607618945</link><description>Introducing a Hugging Face Tutorial on Regression While Hugging Face offers extensive tutorials on classification and NLP tasks, there is very little guidance on performing regression tasks with Transformers. In my latest article, I provide a step-by-step guide to running regression using Hugging Face, applying it to financial news data to predict stock returns. In this tutorial, you will learn how to: -Prepare and preprocess textual and numerical data for regression -Configure a Transformer model for regression tasks -Apply the model to real-world financial datasets with fully reproducible code Read the full article here: https://huggingface.co/blog/SelmaNajih001/how-to-run-a-regression-using-hugging-face The dataset used: SelmaNajih001/FinancialClassification See translation</description><pubDate>Mon, 06 Oct 2025 17:20:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/SelmaNajih001/436109607618945</guid></item><item><title>Sora 2 invitation code, share your Sora 2 invitation code!</title><link>https://huggingface.co/posts/404Zen/898612621867670</link><description>Sora 2 invitation code, share your Sora 2 invitation code! Also, you can use the imini AI platform directly: https://imini.com/ See translation</description><pubDate>Mon, 06 Oct 2025 17:20:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/404Zen/898612621867670</guid></item><item><title>Finally, I uploaded the model I developed for my master‚Äôs thesis! Given a financial event, it provides explained predictions based on a dataset of past news and central bank speeches.</title><link>https://huggingface.co/posts/SelmaNajih001/201829706503917</link><description>Finally, I uploaded the model I developed for my master‚Äôs thesis! Given a financial event, it provides explained predictions based on a dataset of past news and central bank speeches. Try it out here: SelmaNajih001/StockPredictionExplanation (Just restart the space and wait a minute) The dataset used for RAG can be found here: SelmaNajih001/FinancialNewsAndCentralBanksSpeeches-Summary-Rag While the dataset used for the training is: SelmaNajih001/FinancialClassification I also wrote an article to explain how I've done the training. You can find it here https://huggingface.co/blog/SelmaNajih001/explainable-financial-predictions See translation</description><pubDate>Mon, 06 Oct 2025 17:20:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/SelmaNajih001/201829706503917</guid></item><item><title>a senior engineer at google just dropped a 400-page free book on docs for review: agentic design patterns.</title><link>https://huggingface.co/posts/hesamation/792197182072762</link><description>a senior engineer at google just dropped a 400-page free book on docs for review: agentic design patterns. the table of contents looks like everything you need to know about agents + code: &gt; advanced prompt techniques &gt; multi-agent patterns &gt; tool use and MCP &gt; you name it read it here: https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/edit?tab=t.0#heading=h.pxcur8v2qagu you can also pre-order on Amazon (published by Springer) and the royalties goes to Save the Children: https://www.amazon.com/Agentic-Design-Patterns-Hands-Intelligent/dp/3032014018/ See translation</description><pubDate>Mon, 06 Oct 2025 17:20:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/792197182072762</guid></item><item><title>AI for Scientific Discovery Won't Work Without Fixing How We Collaborate.</title><link>https://huggingface.co/posts/evijit/733521758778057</link><description>AI for Scientific Discovery Won't Work Without Fixing How We Collaborate. My co-author @ cgeorgiaw and I just published a paper challenging a core assumption: that the main barriers to AI in science are technical. They're not. They're social. Key findings: üö® The "AI Scientist" myth delays progress: Waiting for AGI devalues human expertise and obscures science's real purpose: cultivating understanding, not just outputs. üìä Wrong incentives: Datasets have 100x longer impact than models, yet data curation is undervalued. ‚ö†Ô∏è Broken collaboration: Domain scientists want understanding. ML researchers optimize performance. Without shared language, projects fail. üîç Fragmentation costs years: Harmonizing just 9 cancer files took 329 hours. Why this matters: Upstream bottlenecks like efficient PDE solvers could accelerate discovery across multiple sciences. CASP mobilized a community around protein structure, enabling AlphaFold. We need this for dozens of challenges. Thus, we're launching...</description><pubDate>Mon, 06 Oct 2025 17:20:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/evijit/733521758778057</guid></item><item><title>üöÄ New blog: Maintain the unmaintainable ‚Äì 1M+ Python LOC, 400+ models</title><link>https://huggingface.co/posts/Molbap/149398104991391</link><description>üöÄ New blog: Maintain the unmaintainable ‚Äì 1M+ Python LOC, 400+ models How do you stop a million-line library built by thousands of contributors from collapsing under its own weight? At ü§ó Transformers, we do it with explicit software-engineering tenets, principles that make the codebase hackable at scale. üîç Inside the post: ‚Äì One Model, One File: readability first ‚Äî you can still open a modeling file and see the full logic, top to bottom. ‚Äì Modular Transformers: visible inheritance that cuts maintenance cost by ~15√ó while keeping models readable. ‚Äì Config-Driven Performance: FlashAttention, tensor parallelism, and attention scheduling are config-level features, not rewrites. Written with @ lysandre ,@pcuenq and @ yonigozlan , this is a deep dive into how Transformers stays fast, open, and maintainable. Read it here ‚Üí transformers-community/Transformers-tenets See translation</description><pubDate>Mon, 06 Oct 2025 17:20:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Molbap/149398104991391</guid></item></channel></rss>