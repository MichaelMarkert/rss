<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AGENTS + FINETUNING! This week Hugging Face learn has a whole pathway on finetuning for agentic applications. You can follow these two courses to get knowledge on levelling up your agent game beyond prompts:</title><link>https://huggingface.co/posts/burtenshaw/189514834246661</link><description>AGENTS + FINETUNING! This week Hugging Face learn has a whole pathway on finetuning for agentic applications. You can follow these two courses to get knowledge on levelling up your agent game beyond prompts: 1️⃣ New Supervised Fine-tuning unit in the NLP Course https://huggingface.co/learn/nlp-course/en/chapter11/1 2️⃣New Finetuning for agents bonus module in the Agents Course https://huggingface.co/learn/agents-course/bonus-unit1/introduction Fine-tuning will squeeze everything out of your model for how you’re using it, more than any prompt. See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/189514834246661</guid></item><item><title>Google just released PaliGemma 2 Mix: new versatile instruction vision language models 🔥</title><link>https://huggingface.co/posts/merve/467807900895850</link><description>Google just released PaliGemma 2 Mix: new versatile instruction vision language models 🔥 &gt; Three new models: 3B, 10B, 28B with res 224, 448 💙 &gt; Can do vision language tasks with open-ended prompts, understand documents, and segment or detect anything 🤯 Read more https://huggingface.co/blog/paligemma2mix Try the demo google/paligemma2-10b-mix All models are here google/paligemma-2-mix-67ac6a251aaf3ee73679dcc4 See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/467807900895850</guid></item><item><title>UIGEN for Tailwind v4 is coming soon!</title><link>https://huggingface.co/posts/smirki/311150694603392</link><description>UIGEN for Tailwind v4 is coming soon! See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/smirki/311150694603392</guid></item><item><title>🚀🎭🌟 New Research Alert - WACV 2025 (Avatars Collection)! 🌟🎭🚀</title><link>https://huggingface.co/posts/DmitryRyumin/189065722993769</link><description>🚀🎭🌟 New Research Alert - WACV 2025 (Avatars Collection)! 🌟🎭🚀 📄 Title: EmoVOCA: Speech-Driven Emotional 3D Talking Heads 🔝 📝 Description: EmoVOCA is a data-driven method for generating emotional 3D talking heads by combining speech-driven lip movements with expressive facial dynamics. This method has been developed to overcome the limitations of corpora and to achieve state-of-the-art animation quality. 👥 Authors: @ FedeNoce , Claudio Ferrari, and Stefano Berretti 📅 Conference: WACV, 28 Feb – 4 Mar, 2025 | Arizona, USA 🇺🇸 📄 Paper: https://arxiv.org/abs/2403.12886 🌐 Github Page: https://fedenoce.github.io/emovoca/ 📁 Repository: https://github.com/miccunifi/EmoVOCA 🚀 CVPR-2023-24-Papers: https://github.com/DmitryRyumin/CVPR-2023-24-Papers 🚀 WACV-2024-Papers: https://github.com/DmitryRyumin/WACV-2024-Papers 🚀 ICCV-2023-Papers: https://github.com/DmitryRyumin/ICCV-2023-Papers 📚 More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers...</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/189065722993769</guid></item><item><title>🔥 Meet Muse: that can generate a game environment based on visuals or players’ controller actions. It was developed by Microsoft Research in collaboration with Ninja Theory (Hellblade developer). It’s built on something called the World and Human Action Model (WHAM-1.6B model). They trained on 7 years of Bleeding Edge gameplay and it can generate 2 minute long 3D game sequences with consistent physics and character behaviors all from just a second of input. They’ve gone and open-sourced it too. Open weights, the WHAM Demonstrator, and sample data on Azure AI Foundry for anyone to play with. Hope so soon on Hugging Face 🤗.</title><link>https://huggingface.co/posts/merterbak/134010141714846</link><description>🔥 Meet Muse: that can generate a game environment based on visuals or players’ controller actions. It was developed by Microsoft Research in collaboration with Ninja Theory (Hellblade developer). It’s built on something called the World and Human Action Model (WHAM-1.6B model). They trained on 7 years of Bleeding Edge gameplay and it can generate 2 minute long 3D game sequences with consistent physics and character behaviors all from just a second of input. They’ve gone and open-sourced it too. Open weights, the WHAM Demonstrator, and sample data on Azure AI Foundry for anyone to play with. Hope so soon on Hugging Face 🤗. 📄 Paper: https://www.nature.com/articles/s41586-025-08600-3 Blog Post: https://www.microsoft.com/en-us/research/blog/introducing-muse-our-first-generative-ai-model-designed-for-gameplay-ideation/ See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/134010141714846</guid></item><item><title>🎯 Perplexity drops their FIRST open-weight model on Hugging Face: A decensored DeepSeek-R1 with full reasoning capabilities. Tested on 1000+ examples for unbiased responses.</title><link>https://huggingface.co/posts/fdaudens/121352437859372</link><description>🎯 Perplexity drops their FIRST open-weight model on Hugging Face: A decensored DeepSeek-R1 with full reasoning capabilities. Tested on 1000+ examples for unbiased responses. Check it out: perplexity-ai/r1-1776 Blog post: https://perplexity.ai/hub/blog/open-sourcing-r1-1776 See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/121352437859372</guid></item><item><title>Integrating human feedback is vital for evolving AI models. Boost quality, scalability, and cost-effectiveness with our crowdsourcing tool!</title><link>https://huggingface.co/posts/jasoncorkill/138106605710984</link><description>Integrating human feedback is vital for evolving AI models. Boost quality, scalability, and cost-effectiveness with our crowdsourcing tool! ..Or run A/B tests and gather thousands of responses in minutes. Upload two images, ask a question, and watch the insights roll in! Check it out here and let us know your feedback: https://app.rapidata.ai/compare See translation</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/138106605710984</guid></item><item><title>🚀 Excited to share our technical report on the Southeast Asian multilingual model Sailor2 and its latest updates!</title><link>https://huggingface.co/posts/dreamerdeo/426313569382827</link><description>🚀 Excited to share our technical report on the Southeast Asian multilingual model Sailor2 and its latest updates! Our 49-page report details Sailor2's development journey, including multilingual data cleaning, small model data mixture simulations, multi-stage continual pre-training, multi-stage post-training, and multi-cultural multi-lingual evaluations. Sailor2 aims to streamline the multilingual model pre-training process efficiently for the community. 🧭 We highlight Sailor2's impressive performance in low-resource language translation scenarios and its cultural understanding advantages in Southeast Asia, promoting practical applications for regional languages. Model updates include: 💡 More precise outputs: Reduced redundancy in model outputs through refined post-training data and optimization techniques. 🌈 Handling longer texts: Expanded to handle up to 128K context length in Southeast Asian languages through long-text training. ⚡️ Faster inference: Achieved 2.5x faster inference...</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dreamerdeo/426313569382827</guid></item><item><title>Me and my team have performed an in-depth investigation comparing o1 to R1 (and other reasoning models)</title><link>https://huggingface.co/posts/cogwheelhead/360341725112136</link><description>Me and my team have performed an in-depth investigation comparing o1 to R1 (and other reasoning models) Link: https://toloka.ai/blog/r1-is-not-on-par-with-o1-and-the-difference-is-qualitative-not-quantitative It started with us evaluating them on our own university-math benchmarks: U-MATH for problem-solving and μ-MATH for judging solution correctness (see the HF leaderboard: toloka/u-math-leaderboard ) tl;dr: R1 sure is amazing, but what we find is that it lags behind in novelty adaptation and reliability: * performance drops when updating benchmarks with fresh unseen tasks (e.g. AIME 2024 -&gt; 2025) * R1-o1 gap widens when evaluating niche subdomains (e.g. university-specific math instead of the more common Olympiad-style contests) * same with going into altogether unconventional domains (e.g. chess) or skills (e.g. judgment instead of problem-solving) * R1 also runs into failure modes way more often (e.g. making illegal chess moves or falling into endless generation loops) Our...</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/cogwheelhead/360341725112136</guid></item><item><title>What’s in a name? More than you might think, especially for AI.</title><link>https://huggingface.co/posts/frimelle/972648838018664</link><description>What’s in a name? More than you might think, especially for AI. Whenever I introduce myself, people often start speaking French to me, even though my French is très basic. It turns out that AI systems do something similar: Large language models infer cultural identity from names, shaping their responses based on presumed backgrounds. But is this helpful personalization or a reinforcement of stereotypes? In our latest paper, we explored this question by testing DeepSeek, Llama, Aya, Mistral-Nemo, and GPT-4o-mini on how they associate names with cultural identities. We analysed 900 names from 30 cultures and found strong assumptions baked into AI responses: some cultures were overrepresented, while others barely registered. For example, a name like "Jun" often triggered Japan-related responses, while "Carlos" was linked primarily to Mexico, even though these names exist in multiple countries. Meanwhile, names from places like Ireland led to more generic answers, suggesting weaker...</description><pubDate>Fri, 21 Feb 2025 13:25:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/frimelle/972648838018664</guid></item></channel></rss>