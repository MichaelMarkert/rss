<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>y'all have been asking my opinion on how OCR models compare to each other üëÄ</title><link>https://huggingface.co/posts/merve/300457326273979</link><description>y'all have been asking my opinion on how OCR models compare to each other üëÄ I will leave three apps to compare newest models by @ prithivMLmods instead ‚§µÔ∏è &gt; compare Nanonets-OCR-s, Qwen2-VL-OCR-2B-Instruct, RolmOCR, Aya-Vision prithivMLmods/Multimodal-OCR &gt; SmolDocling, Nanonets-OCR-s, MonkeyOCR, Typhoon-OCR-7B prithivMLmods/Multimodal-OCR2 &gt; docscopeOCR, MonkeyOCR, coreOCR prithivMLmods/core-OCR See translation</description><pubDate>Sun, 22 Jun 2025 13:30:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/300457326273979</guid></item><item><title>ü§î Ready to build better AI models with synthetic data, but don't know where to start? Why go at it alone?üí°</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/352866416610611</link><description>ü§î Ready to build better AI models with synthetic data, but don't know where to start? Why go at it alone?üí° üëã Join Duality AI‚Äôs Falcon community! It is one of the best resources for support, creativity, and growth as you move along your synthetic data journey. ‚û°Ô∏èOur current Kaggle competition is the easiest way to get started: https://www.kaggle.com/competitions/multi-instance-object-detection-challenge When you join, you'll meet some of the rising stars in our community, such as: üåü Sergio Sanz, @ sergio-sanz-rodriguez , who took 1st and 2nd place in recent computer vision Kaggle competitions and shared his process of using R-CNN and Falcon-generated images in this article: https://www.duality.ai/blog/leveraging-synthetic-data-for-real-world-object-detection üåüMohana pavan Bezawada, @ mohanapavan , who has risen in the ranks from the top 25 in the first competition all the way to top scorer in our current competition! His journey illustrates how dedication + Falcon can take you far in...</description><pubDate>Sun, 22 Jun 2025 13:30:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/352866416610611</guid></item><item><title>Hello everyone, happy to share with you my experimentation of a Deep Research Assistant, using 7 agents and a quality assurance pipeline :</title><link>https://huggingface.co/posts/mallocode200/873699618032778</link><description>Hello everyone, happy to share with you my experimentation of a Deep Research Assistant, using 7 agents and a quality assurance pipeline : ü§ñ What makes this special: ‚úÖ Agent-Based Architecture - 7 specialised AI agents working together: - Planner Agent - Strategic search planning - Search Agent - Multi-source web research - Writer Agent - Comprehensive report generation - Evaluator Agent - Automatic quality assessment - Optimiser Agent - Iterative improvement when needed - Email Agent - Professional report delivery - Clarifier Agent - Interactive query refinement ‚úÖ Quality Assurance Pipeline - Every report is scored (1-10) and automatically improved if it scores below 7/10 ‚úÖ Multiple Research Modes - From quick queries to deep, clarification-driven analysis ‚úÖ Production-Ready - Deployed on Hugging Face Spaces with comprehensive documentation üîß Technical Stack: - Frontend: Gradio with theme-adaptive UI - Backend: OpenAI Agents framework - Integration: SendGrid for email delivery -...</description><pubDate>Sun, 22 Jun 2025 13:30:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mallocode200/873699618032778</guid></item><item><title>Adaptive Classifier: Dynamic Text Classification with Strategic Learning</title><link>https://huggingface.co/posts/codelion/825512802076392</link><description>Adaptive Classifier: Dynamic Text Classification with Strategic Learning New text classification system that learns continuously without catastrophic forgetting. Achieved 22.2% robustness improvement on adversarial datasets while maintaining clean data performance. üéØ THE PROBLEM Traditional classifiers require complete retraining when adding new classes. Expensive and time-consuming, especially with adversarial users trying to game the system. üöÄ KEY INNOVATIONS ‚Ä¢ Hybrid memory-neural architecture (prototype-based + neural adaptation) ‚Ä¢ Strategic classification using game theory to predict and defend against manipulation ‚Ä¢ Elastic Weight Consolidation prevents catastrophic forgetting üìä RESULTS Tested on AI-Secure/adv_glue dataset: ‚Ä¢ Clean data: 80.0% ‚Üí 82.2% (+2.2%) ‚Ä¢ Manipulated data: 60.0% ‚Üí 82.2% (+22.2%) ‚Ä¢ Zero performance drop under adversarial attacks üî¨ APPLICATIONS ‚Ä¢ Hallucination detection: 80.7% recall for RAG safety ‚Ä¢ LLM routing: 26.6% cost optimization improvement ‚Ä¢...</description><pubDate>Sun, 22 Jun 2025 13:30:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/825512802076392</guid></item><item><title>Trinity-Synthesis: A Multi-Agent Architecture for AI Agents That Think Before They Speak</title><link>https://huggingface.co/posts/brainhome/128948549150065</link><description>Trinity-Synthesis: A Multi-Agent Architecture for AI Agents That Think Before They Speak Ever felt your AI agent is "shooting from the hip"? It latches onto a single line of thought and fails to produce a robust, well-rounded plan. This is a common struggle I've called the "AI Reasoning Paradox." To tackle this, I developed Trinity-Synthesis, a multi-agent architecture designed to force reflection and synthesis before delivering a final answer. The philosophy is simple: constructive conflict between different perspectives leads to better solutions. Here‚Äôs the core idea: Instead of one agent, it uses four agents running on the same base model but with different "personalities" defined by their system prompts and temperature settings: üß† The Visionary: Thinks outside the box (high temp: 1.0). üìä The Analyst: Focuses on logic, data, and structure (low temp: 0.3). üõ†Ô∏è The Pragmatist: Evaluates feasibility, costs, and risks (mid temp: 0.5). These three "thinkers" work in parallel on the...</description><pubDate>Sun, 22 Jun 2025 13:30:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/brainhome/128948549150065</guid></item><item><title>üöÄ  Try deep-research without Network Connection and Data Leak, Jan-Nano production-ready on-device AI in 6 hours!</title><link>https://huggingface.co/posts/yeonseok-zeticai/509797148011121</link><description>üöÄ Try deep-research without Network Connection and Data Leak, Jan-Nano production-ready on-device AI in 6 hours! Jan-Nano has been making waves as one of HuggingFace's most trending 4B parameter models, outperforming even 671B models on SimpleQA benchmarks. But here's what changes everything: ZETIC.MLange just transformed Jan-Nano into a blazing-fast on-device AI solution. ‚ú® 6-hour deployment from huggingface to production-ready library! Zero cloud dependency - complete privacy and offline capability While others struggle with complex on-device deployments taking weeks or months, ZETIC.MLange's automated pipeline makes it effortless. No manual optimization, no vendor-specific coding, no compromise on performance. üì± Ready to transform your AI models? Try ZETIC.MLange, it is totally free now! The future of AI is on-device. Make it happen in hours, not months. #OnDeviceAI #EdgeAI #MLOptimization #NPU #PrivacyFirst See translation</description><pubDate>Sun, 22 Jun 2025 13:30:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/509797148011121</guid></item><item><title>üó£Ô∏è Whose voice do we hear when AI speaks?</title><link>https://huggingface.co/posts/giadap/887797954746272</link><description>üó£Ô∏è Whose voice do we hear when AI speaks? Every language carries its own cultural values and worldviews. So, when we build AI systems, we're not just deciding how they speak but also whose perspectives they represent. Even choosing which dialect to train on in Norway becomes a question of inclusion and power. In Kenya, will AI speak Swahili from Nairobi or coastal regions? What about indigenous languages with rich oral traditions but limited written text, like Quechua in Peru or Cherokee in North America? The path forward? Building WITH communities, not just FOR them. Working with local partners (libraries, universities, civil society), testing for cultural alignment, and asking hard questions about representation. Just published some thoughts on this after my keynote in Norway a few weeks ago: https://huggingface.co/blog/giadap/when-ai-speaks See translation</description><pubDate>Sun, 22 Jun 2025 13:30:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/887797954746272</guid></item><item><title>Love the subtle retro style of this sculptural piece! Took a bit of fine tuning the style but one of the more satisfying things I have created recently.</title><link>https://huggingface.co/posts/BFFree/305893036494056</link><description>Love the subtle retro style of this sculptural piece! Took a bit of fine tuning the style but one of the more satisfying things I have created recently. See translation</description><pubDate>Sun, 22 Jun 2025 13:30:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/BFFree/305893036494056</guid></item><item><title>Now you can make Flux.1 your own within just 10GBs of VRAM. In our new blog post we walk you through the process step by step.</title><link>https://huggingface.co/posts/derekl35/148956797193810</link><description>Now you can make Flux.1 your own within just 10GBs of VRAM. In our new blog post we walk you through the process step by step. Check it out here: https://huggingface.co/blog/flux-qlora See translation</description><pubDate>Sun, 22 Jun 2025 13:30:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/derekl35/148956797193810</guid></item><item><title>Self-Forcing - a real-time video distilled model from Wan 2.1 by</title><link>https://huggingface.co/posts/multimodalart/420236527922092</link><description>Self-Forcing - a real-time video distilled model from Wan 2.1 by @ adobe is out, and they open sourced it üêê I've built a live real time demo on Spaces üìπüí® multimodalart/self-forcing See translation</description><pubDate>Sun, 22 Jun 2025 13:30:11 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/multimodalart/420236527922092</guid></item></channel></rss>