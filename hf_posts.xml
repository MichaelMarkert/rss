<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>8 Free Sources about AI Agents:</title><link>https://huggingface.co/posts/Kseniase/557379551700019</link><description>8 Free Sources about AI Agents: Agents seem to be everywhere and this collection is for a deep dive into the theory and practice: 1. "Agents" Google's whitepaper by Julia Wiesinger, Patrick Marlow and Vladimir Vuskovic -&gt; https://www.kaggle.com/whitepaper-agents Covers agents, their functions, tool use and how they differ from models 2. "Agents in the Long Game of AI. Computational Cognitive Modeling for Trustworthy, Hybrid AI" book by Marjorie McShane, Sergei Nirenburg, and Jesse English -&gt; https://direct.mit.edu/books/oa-monograph/5833/Agents-in-the-Long-Game-of-AIComputational Explores building AI agents, using Hybrid AI, that combines ML with knowledge-based reasoning 3. "AI Engineer Summit 2025: Agent Engineering" 8-hour video -&gt; https://www.youtube.com/watch?v=D7BzTxVVMuw Experts' talks that share insights on the freshest Agent Engineering advancements, such as Google Deep Research, scaling tips and more 4. AI Agents Course from Hugging Face -&gt;...</description><pubDate>Tue, 25 Feb 2025 09:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/557379551700019</guid></item><item><title>Datasets Convertor üöÄ</title><link>https://huggingface.co/posts/openfree/251902401084370</link><description>Datasets Convertor üöÄ openfree/Datasets-Convertor Welcome to Datasets Convertor, the cutting-edge solution engineered for seamless and efficient data format conversion. Designed with both data professionals and enthusiasts in mind, our tool simplifies the transformation process between CSV, Parquet, and JSONL, XLS file formats, ensuring that your data is always in the right shape for your next analytical or development challenge. üíª‚ú® Why Choose Datasets Convertor? In today‚Äôs data-driven world, managing and converting large datasets can be a daunting task. Our converter is built on top of robust technologies like Pandas and Gradio, delivering reliable performance with a modern, intuitive interface. Whether you‚Äôre a data scientist, analyst, or developer, Datasets Convertor empowers you to effortlessly switch between formats while maintaining data integrity and optimizing storage. Key Features and Capabilities: CSV ‚áÜ Parquet Conversion: Easily transform your CSV files into the highly...</description><pubDate>Tue, 25 Feb 2025 09:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/251902401084370</guid></item><item><title>She arrived üòç</title><link>https://huggingface.co/posts/stefan-it/765581033311913</link><description>She arrived üòç [Expect more models soon...] See translation</description><pubDate>Tue, 25 Feb 2025 09:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/stefan-it/765581033311913</guid></item><item><title>It's really interesting about the deployment of a new state of matter in Majorana 1: the world‚Äôs first quantum processor powered by topological qubits. If you missed this news this week, here are some links for you:</title><link>https://huggingface.co/posts/prithivMLmods/319015417669347</link><description>It's really interesting about the deployment of a new state of matter in Majorana 1: the world‚Äôs first quantum processor powered by topological qubits. If you missed this news this week, here are some links for you: üÖ±Ô∏èTopological qubit arrays: https://arxiv.org/pdf/2502.12252 ‚öõÔ∏è Quantum Blog: https://azure.microsoft.com/en-us/blog/quantum/2025/02/19/microsoft-unveils-majorana-1-the-worlds-first-quantum-processor-powered-by-topological-qubits/ üìñ Read the story: https://news.microsoft.com/source/features/innovation/microsofts-majorana-1-chip-carves-new-path-for-quantum-computing/ üìù Majorana 1 Intro: https://youtu.be/Q4xCR20Dh1E?si=Z51DbEYnZFp_88Xp üåÄThe Path to a Million Qubits: https://youtu.be/wSHmygPQukQ?si=TS80EhI62oWiMSHK See translation</description><pubDate>Tue, 25 Feb 2025 09:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/319015417669347</guid></item><item><title>We now have a Deep Research for academia: SurveyX automatically writes academic surveys nearly indistinguishable from human-written ones üî•</title><link>https://huggingface.co/posts/m-ric/788543954757847</link><description>We now have a Deep Research for academia: SurveyX automatically writes academic surveys nearly indistinguishable from human-written ones üî• Researchers from Beijing and Shanghai just published the first application of a deep research system to academia: their algorithm, given a question, can give you a survey of all papers on the subject. To make a research survey, you generally follow two steps, preparation (collect and organize papers) and writing (outline creation, writing, polishing). Researchers followed the same two steps and automated them. üéØ For the preparation part, a key part is find all the important references on the given subject. Researchers first cast a wide net of all relevant papers. But then finding the really important ones is like distilling knowledge from a haystack of information. To solve this challenge, they built an ‚ÄúAttributeTree‚Äù object that structures key information from citations. Ablating these AttributeTrees significantly decreased structure and...</description><pubDate>Tue, 25 Feb 2025 09:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/788543954757847</guid></item><item><title>Testing Training on AMD/ROCm the first time!</title><link>https://huggingface.co/posts/csabakecskemeti/967959031973508</link><description>Testing Training on AMD/ROCm the first time! I've got my hands on an AMD Instinct MI100. It's about the same price used as a V100 but on paper has more TOPS (V100 14TOPS vs MI100 23TOPS) also the HBM has faster clock so the memory bandwidth is 1.2TB/s. For quantized inference it's a beast (MI50 was also surprisingly fast) For LORA training with this quick test I could not make the bnb config works so I'm running the FT on the fill size model. Will share all the install, setup and setting I've learned in a blog post, together with the cooling shroud 3D design. See translation</description><pubDate>Tue, 25 Feb 2025 09:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/csabakecskemeti/967959031973508</guid></item><item><title>I'd like to draw your attention to a Lamarck-based experiment which uses Arcee AI's newly published arcee_fusion merge method for three out of its four merges.  Yes, just four.  This is a simple one, and its recipe is fully open:</title><link>https://huggingface.co/posts/sometimesanotion/952202239440445</link><description>I'd like to draw your attention to a Lamarck-based experiment which uses Arcee AI's newly published arcee_fusion merge method for three out of its four merges. Yes, just four. This is a simple one, and its recipe is fully open: sometimesanotion/Lamarck-14B-v0.7-Fusion It unifies three branches, all of which feature models which bring Lamarck-14B-v0.7 and Qwenvergence-14B-v12-Prose together. One side features @ jpacifico 's jpacifico/Chocolatine-2-14B-Instruct-v2.0.3 and the other features @ suayptalha 's suayptalha/Lamarckvergence-14B paired with my models which were their merge ancestors. A fusion merge - of a fusion merge and a SLERP of a fusion and older merge - should demonstrate the new merge method's behavior in interesting ways, especially in the first 1/4th of the model where the SLERP has less impact. I welcome you to kick the tires and learn from it. It has prose quality near Qwenvergence v12's - as you'd expect. Thank you, @ mradermacher and @ MaziyarPanahi , for the...</description><pubDate>Tue, 25 Feb 2025 09:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sometimesanotion/952202239440445</guid></item><item><title>üöÄ Just launched: A toolkit of 20 powerful AI tools that journalists can use right now - transcribe, analyze, create. 100% free &amp; open-source.</title><link>https://huggingface.co/posts/fdaudens/982146976081521</link><description>üöÄ Just launched: A toolkit of 20 powerful AI tools that journalists can use right now - transcribe, analyze, create. 100% free &amp; open-source. Been testing all these tools myself and created a searchable collection of the most practical ones - from audio transcription to image generation to document analysis. No coding needed, no expensive subscriptions. Some highlights I've tested personally: - Private, on-device transcription with speaker ID in 100+ languages using Whisper - Website scraping that just works - paste a URL, get structured data - Local image editing with tools like Finegrain (impressive results) - Document chat using Qwen 2.5 72B (handles technical papers well) Sharing this early because the best tools come from the community. Drop your favorite tools in the comments or join the discussion on what to add next! üëâ JournalistsonHF/ai-toolkit See translation</description><pubDate>Tue, 25 Feb 2025 09:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/982146976081521</guid></item><item><title>The past few years have been a blast for artificial intelligence, with large language models (LLMs) stunning everyone with their capabilities and powering everything from chatbots to code assistants. However, not all applications demand the massive size and complexity of LLMs, the computational power required makes them impractical for many use cases. This is why Small Language Models (SLMs) entered the scene to make powerful AI models more accessible by shrinking in size.</title><link>https://huggingface.co/posts/jjokah/811590134220421</link><description>The past few years have been a blast for artificial intelligence, with large language models (LLMs) stunning everyone with their capabilities and powering everything from chatbots to code assistants. However, not all applications demand the massive size and complexity of LLMs, the computational power required makes them impractical for many use cases. This is why Small Language Models (SLMs) entered the scene to make powerful AI models more accessible by shrinking in size. In this article we went through what SLMs are, how they are made small, their benefits and limitations, real-world use cases, and how they can be used on mobile and desktop devices. https://huggingface.co/blog/jjokah/small-language-model See translation</description><pubDate>Tue, 25 Feb 2025 09:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jjokah/811590134220421</guid></item><item><title>Try QwQ-Max-Preview, Qwen's reasoning model hereüëâ</title><link>https://huggingface.co/posts/AdinaY/443551611964291</link><description>Try QwQ-Max-Preview, Qwen's reasoning model hereüëâ https://chat.qwen.ai Can't wait for the model weights to drop on the Hugging Face Hub üî• See translation</description><pubDate>Tue, 25 Feb 2025 09:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/443551611964291</guid></item></channel></rss>