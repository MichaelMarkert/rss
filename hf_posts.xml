<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸ™‹ğŸ»â€â™‚ï¸ Normalize adding compute &amp; runtime traces to your model cards</title><link>https://huggingface.co/posts/Tonic/414083244384754</link><description>ğŸ™‹ğŸ»â€â™‚ï¸ Normalize adding compute &amp; runtime traces to your model cards See translation</description><pubDate>Mon, 14 Jul 2025 17:23:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Tonic/414083244384754</guid></item><item><title>Kimi-K2 is now available on the hubğŸ”¥ğŸš€</title><link>https://huggingface.co/posts/AdinaY/423045666935241</link><description>Kimi-K2 is now available on the hubğŸ”¥ğŸš€ This is a trillion-parameter MoE model focused on long context, code, reasoning, and agentic behavior. moonshotai/kimi-k2-6871243b990f2af5ba60617d âœ¨ Base &amp; Instruct âœ¨ 1T total / 32B active - Modified MIT License âœ¨ 128K context length âœ¨ Muon optimizer for stable trillion-scale training See translation</description><pubDate>Mon, 14 Jul 2025 17:23:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/423045666935241</guid></item><item><title>13 New types of LoRA</title><link>https://huggingface.co/posts/Kseniase/384482543815919</link><description>13 New types of LoRA LoRA (Low-Rank Adaptation) is a popular lightweight method for fine-tuning AI models. It doesn't update the full model, it adds small trainable components, low-rank matrices, while keeping the original weights frozen. Only these adapters are trained. Recently, many interesting new LoRA variations came out, so itâ€™s a great time to take a look at these 13 clever approaches: 1. T-LoRA â†’ T-LoRA: Single Image Diffusion Model Customization Without Overfitting (2507.05964) A timestep-dependent LoRA method for adapting diffusion models with a single image. It dynamically adjusts updates and uses orthogonal initialization to reduce overlap, achieving better fidelityâ€“alignment balance than standard LoRA 2. SingLoRA â†’ SingLoRA: Low Rank Adaptation Using a Single Matrix (2507.05566) Simplifies LoRA by using only one small matrix instead of usual two, and multiplying it by its own transpose (like A Ã— Aáµ€). It uses half the parameters of LoRA and avoids scale mismatch between...</description><pubDate>Mon, 14 Jul 2025 17:23:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/384482543815919</guid></item><item><title>in case you didnâ€™t know, Claude now has a developer training course with certificates,</title><link>https://huggingface.co/posts/hesamation/850768471232119</link><description>in case you didnâ€™t know, Claude now has a developer training course with certificates, this is better than anything you can find on Coursera. covers Claude Code, MCP and its advanced topics and even more: https://www.anthropic.com/learn/build-with-claude See translation</description><pubDate>Mon, 14 Jul 2025 17:23:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/850768471232119</guid></item><item><title>ğŸ¯ Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science!</title><link>https://huggingface.co/posts/DawnC/760622875415705</link><description>ğŸ¯ Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science! This isn't just another computer vision project. VisionScout represents a fundamental shift from simple object detection to genuine scene understanding, where four specialized AI models work together to interpret what's actually happening in an image. ğŸ—ï¸ Part 1: Architecture Foundation How careful system design transforms independent models into collaborative intelligence through proper layering and coordination strategies. âš™ï¸ Part 2: Deep Technical Implementation The five core algorithms powering the system: dynamic weight adjustment, attention mechanisms, statistical methods, lighting analysis, and CLIP's zero-shot learning. ğŸŒ Part 3: Real-World Validation Concrete case studies from indoor spaces to cultural landmarks, demonstrating how integrated systems deliver insights no single model could achieve. What makes this valuable:...</description><pubDate>Mon, 14 Jul 2025 17:23:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/760622875415705</guid></item><item><title>Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. ğŸ¤—ğŸ“„</title><link>https://huggingface.co/posts/prithivMLmods/700925755780035</link><description>Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. ğŸ¤—ğŸ“„ Spaces &amp; Models : &gt; Doc-VLMs-OCR : prithivMLmods/Doc-VLMs-OCR &gt; core-OCR : prithivMLmods/core-OCR &gt; Megalodon-OCR (3B) : prithivMLmods/Megalodon-OCR-Sync-0713 &gt; Perseus-Doc-vl (7B): prithivMLmods/Perseus-Doc-vl-0712 Datasets Caption Mix : &gt; Corvus-OCR-Caption-Mix : prithivMLmods/Corvus-OCR-Caption-Mix &gt; Corvus-OCR-Caption-Mini-Mix : prithivMLmods/Corvus-OCR-Caption-Mini-Mix Collections : &gt; Corvus OCR Caption Mix: prithivMLmods/corvus-ocr-caption-mix-687349bfaceffbd10976f0cc &gt; Captioning / OCR / DocTable : prithivMLmods/captioning-ocr-doctable-687382e1da822008bb5c06f2 GitHub : &gt; OCR-ReportLab :...</description><pubDate>Mon, 14 Jul 2025 17:23:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/700925755780035</guid></item><item><title>In our latest paper, Bourbaki (7b), we show how one can achieve state-of-the-art 7B theorem provers on PutnamBench by applying MCTS to what we call self-generated and goal-conditioned MDPs. I started a series of Blogs on this!</title><link>https://huggingface.co/posts/hba123/150628355666585</link><description>In our latest paper, Bourbaki (7b), we show how one can achieve state-of-the-art 7B theorem provers on PutnamBench by applying MCTS to what we call self-generated and goal-conditioned MDPs. I started a series of Blogs on this! Why a series of Blogs ğŸ˜? I want to try to make everyone understand what Bourbaki (7b) is and what it does. I don't want to just give you a ChatGPT summary with some result hype. I think there are many things to improve, and I am hoping with more exposure to this, beyond experiments and codes, some people would be interested and help us improve it! In this first blog, we will be talking basics: 1) MCTS and why it should be applied to LLMs so that the whole world is not just fine-tuning a 100000000000000000000000 b model on 10 data points (not that i have not done it before ğŸ¤ªğŸ¤ª), 2) the basics of MDPs, and 3) the Vanilla MCTS algorithm. Check it out: https://huggingface.co/blog/hba123/bourbaki7b If you find it useful, consider upvoting and sharing this post and...</description><pubDate>Mon, 14 Jul 2025 17:23:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/150628355666585</guid></item><item><title>ğŸš€ Announcing the Synthetic-to-Real Multi-Class Object Detection Challenge!</title><link>https://huggingface.co/posts/3LC/827651059369427</link><description>ğŸš€ Announcing the Synthetic-to-Real Multi-Class Object Detection Challenge! Weâ€™re excited to announce the launch of the Synthetic-to-Real Multi-Class Object Detection Challengeâ€”now live on Kaggle! This exciting competition is brought to you by 3LC in partnership with Duality AI, creators of the powerful FalconCloud tool for generating targeted synthetic data. Together, we're offering a unique opportunity to push the boundaries of object detection through high-fidelity, simulation-to-real workflows. ğŸ§ª What Makes This Challenge Special? ğŸ’» Create customized training data with Dualityâ€™s cloud-based scenario ğŸ§  Analyze data weaknesses and take precise, data-driven actions using 3LC's robust tooling âš™ï¸ Optimize data for peak model training ğŸ† Why Join? â€¢ Win cash prizes, certificates, and global recognition â€¢ Gain exposure to real-world simulation workflows used in top AI companies â€¢ Collaborate and compete with leading minds in computer vision, ML, and AI Whether you're a student,...</description><pubDate>Mon, 14 Jul 2025 17:23:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/3LC/827651059369427</guid></item><item><title>I loved the idea of the Boxing by</title><link>https://huggingface.co/posts/Quazim0t0/506664241355208</link><description>I loved the idea of the Boxing by sergiopaniego/vlm_object_understanding And webml-community/fastvlm-webgpu So I tried to combine the two idea, unfortunately I canâ€™t seem to get it consistent and I only worked on the File Upload side. You may have to change the prompt a bit to suite the video you upload but it seems to semi work. If anyone knows a better way to fix this, I really wanted to use this for a project but I canâ€™t seem to figure it out. Quazim0t0/FastVLMBoxes I used videos from here and uploaded them to try it out. https://pixabay.com/videos/search/branch+birds/ See translation</description><pubDate>Mon, 14 Jul 2025 17:23:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Quazim0t0/506664241355208</guid></item><item><title>ğŸ“¢ The Kaggle Synthetic-to-Real Object Detection Challenge is back, and it's a multi-class challenge ğŸ˜ !</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/395314150122330</link><description>ğŸ“¢ The Kaggle Synthetic-to-Real Object Detection Challenge is back, and it's a multi-class challenge ğŸ˜ ! ğŸ’µThis time we made a harder, multi-class challenge and are offering a larger prize pool. Sign up here: https://www.kaggle.com/competitions/multi-class-object-detection-challenge ğŸ¤ Duality AI is partnering in this competition with 3LC.AI, a cutting-edge platform which enables users to capture per sample metrics and take meaningful action leading to better, faster, and smaller AI models. Competitors will be challenged to: âœ¨ Create customized training data with Dualityâ€™s cloud-based scenario âœ¨Analyze data weaknesses and make calculated changes using 3LCâ€™s robust software âœ¨Optimize data for peak model training Compete for prizes, certificates, and recognition from peer competitors around the world. Whether youâ€™re a student, researcher, or industry pro, this challenge offers hands-on experience customizing high-fidelity synthetic data for robust models. Ready to bridge the Sim2Real...</description><pubDate>Mon, 14 Jul 2025 17:23:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/395314150122330</guid></item></channel></rss>