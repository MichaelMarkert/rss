<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>üöÄ Introducing MOUSE: Space Research Thinking on HuggingFace Spaces</title><link>https://huggingface.co/posts/ginipick/923354082387927</link><description>üöÄ Introducing MOUSE: Space Research Thinking on HuggingFace Spaces üöÄ How to Get Started ginipick/spaces-research-think Welcome to **MOUSE: Space Research Thinking** ‚Äì an innovative HuggingFace Spaces project designed to transform how you analyze and interact with Python code. Whether you're a developer, researcher, or simply passionate about coding, this tool provides state-of-the-art analysis, summarization, and usage guidance, all powered by advanced AI. --- ## üåü Key Features - **Real-Time Code Analysis** Instantly dissect your Python code to reveal its structure, functionality, and potential applications. Our tool delivers: - **Background &amp; Necessity**: Understand the context behind the code. - **Functional Utility &amp; Value**: Highlight core functionalities and benefits. - **Distinctive Features**: Discover what sets the project apart. - **Target Audience &amp; Applications**: Identify who can benefit and how. - **Expected Impact**: Envision the improvements and innovations the code...</description><pubDate>Thu, 27 Feb 2025 17:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/923354082387927</guid></item><item><title>Now the Hugging Face agent course is getting real! With frameworks like smolagents, LlamaIndex, and LangChain.</title><link>https://huggingface.co/posts/burtenshaw/352638065928004</link><description>Now the Hugging Face agent course is getting real! With frameworks like smolagents, LlamaIndex, and LangChain. üîó Follow the org for updates https://huggingface.co/agents-course This week we are releasing the first framework unit in the course and it‚Äôs on smolagents. This is what the unit covers: - why should you use smolagents vs another library? - how to build agents that use code - build multiagents systems - use vision language models for browser use The team has been working flat out on this for a few weeks. Led by @ sergiopaniego and supported by smolagents author @ m-ric . See translation</description><pubDate>Thu, 27 Feb 2025 17:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/352638065928004</guid></item><item><title>Dropping some of the custom fine-tunes based on SigLIP2,</title><link>https://huggingface.co/posts/prithivMLmods/305640045790864</link><description>Dropping some of the custom fine-tunes based on SigLIP2, with a single-label classification problem type! üåÄüß§ - AI vs Deepfake vs Real : prithivMLmods/AI-vs-Deepfake-vs-Real-Siglip2 - Deepfake Detect : prithivMLmods/Deepfake-Detect-Siglip2 - Fire Detection : prithivMLmods/Fire-Detection-Siglip2 - Deepfake Quality Assess : prithivMLmods/Deepfake-Quality-Assess-Siglip2 - Guard Against Unsafe Content : prithivMLmods/Guard-Against-Unsafe-Content-Siglip2 üå†Collection : prithivMLmods/siglip2-custom-67bcdb2de8fe96b99fb4e19e See translation</description><pubDate>Thu, 27 Feb 2025 17:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/305640045790864</guid></item><item><title>Getting WebRTC and Websockets right in python is very tricky. If you've tried to wrap an LLM in a real-time audio layer then you know what I'm talking about.</title><link>https://huggingface.co/posts/freddyaboulton/628292960317038</link><description>Getting WebRTC and Websockets right in python is very tricky. If you've tried to wrap an LLM in a real-time audio layer then you know what I'm talking about. That's where FastRTC comes in! It makes WebRTC and Websocket streams super easy with minimal code and overhead. Check out our org: hf.co/fastrtc See translation</description><pubDate>Thu, 27 Feb 2025 17:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/freddyaboulton/628292960317038</guid></item><item><title>üî• Agents can do anything!</title><link>https://huggingface.co/posts/alvarobartt/393660009896131</link><description>üî• Agents can do anything! @ microsoft Research just announced the release of Magma 8B! Magma is a new Visual Language Model (VLM) with 8B parameters for multi-modal agents designed to handle complex interactions across virtual and real environments; and it's MIT licensed! Magma comes with exciting new features such as: - Introduces the Set-of-Mark and Trace-of-Mark techniques for fine-tuning - Leverages a large amount of unlabeled video data to learn the spatial-temporal grounding and planning - A strong generalization and ability to be fine-tuned for other agentic tasks - SOTA in different multi-modal benchmarks spanning across UI navigation, robotics manipulation, image / video understanding and spatial understanding and reasoning - Generates goal-driven visual plans and actions for agentic use cases Model: microsoft/Magma-8B Technical Report: Magma: A Foundation Model for Multimodal AI Agents (2502.13130) See translation</description><pubDate>Thu, 27 Feb 2025 17:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/alvarobartt/393660009896131</guid></item><item><title>Necessity is mother of invention. To understand ‚ö°FlashMLA‚ö° by</title><link>https://huggingface.co/posts/onekq/116274779602090</link><description>Necessity is mother of invention. To understand ‚ö°FlashMLA‚ö° by üêãDeepSeek üêã, the first question to ask is why. The keyword here is H800, a lower-end product tailored for export control. The purpose here is to squeeze out as much performance as possible. But here is the most important takeaway: this invention benefits EVERYONE. See translation</description><pubDate>Thu, 27 Feb 2025 17:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/116274779602090</guid></item><item><title>Wan2.1 üî•üìπ new OPEN video model by Alibaba Wan team!</title><link>https://huggingface.co/posts/AdinaY/200256238569001</link><description>Wan2.1 üî•üìπ new OPEN video model by Alibaba Wan team! Model: Wan-AI/Wan2.1-T2V-14B Demo: Wan-AI/Wan2.1 ‚ú®Apache 2.0 ‚ú®8.19GB VRAM, runs on most GPUs ‚ú®Multi-Tasking: T2V, I2V, Video Editing, T2I, V2A ‚ú®Text Generation: Supports Chinese &amp; English ‚ú®Powerful Video VAE: Encode/decode 1080P w/ temporal precision See translation</description><pubDate>Thu, 27 Feb 2025 17:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/200256238569001</guid></item><item><title>üéâ Exciting news, everyone! I've just released **Thespis-Llama-3.1-8B**, a new language model designed for enhanced roleplaying! ‚ú®Ô∏è</title><link>https://huggingface.co/posts/Locutusque/390800785751051</link><description>üéâ Exciting news, everyone! I've just released **Thespis-Llama-3.1-8B**, a new language model designed for enhanced roleplaying! ‚ú®Ô∏è It's built on Llama-3.1 and fine-tuned with a focus on Theory of Mind reasoning to create more believable and engaging characters. It even learned a few tricks on its own, like adding in-character thought processes! üß† Check it out here: Locutusque/Thespis-Llama-3.1-8B Give it a try and let me know what you think! I'm especially interested in feedback on how well the characters stay in role and if the responses feel natural. Looking forward to seeing what amazing stories you create! ‚úçÔ∏è See translation</description><pubDate>Thu, 27 Feb 2025 17:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Locutusque/390800785751051</guid></item><item><title>Wan 2.1 Ultra Advanced Gradio APP for - Works as low as 4GB VRAM - 1-Click Installers for Windows, RunPod, Massed Compute - Batch Processing - T2V - I2V - V2V</title><link>https://huggingface.co/posts/MonsterMMORPG/268157671870814</link><description>Wan 2.1 Ultra Advanced Gradio APP for - Works as low as 4GB VRAM - 1-Click Installers for Windows, RunPod, Massed Compute - Batch Processing - T2V - I2V - V2V Installer and APP : https://www.patreon.com/posts/123105403 Download from here : https://www.patreon.com/posts/123105403 I have been working 14 hours today to make this APP before sleeping for you guys :) We have all the features of Wan 2.1 model Text to Video 1.3B (as low as 3.5 GB VRAM) - Really fast - 480x832px or 832x480px Video to Video 1.3B (as low as 3.5 GB VRAM) - Really fast - 480x832px or 832x480px Text to Video 14B (as low as 17 GB VRAM) - still may work at below VRAM but slower - 720x1280px or 1280x720px Image to Video 14B (as low as 17 GB VRAM) - still may work at below VRAM but slower - 720x1280px or 1280x720px When you analyze the above and below images First video is animated from the input image with following prompt A hooded wraith stands motionless in a torrential downpour, lightning cracking across the...</description><pubDate>Thu, 27 Feb 2025 17:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/268157671870814</guid></item><item><title>She arrived üòç</title><link>https://huggingface.co/posts/stefan-it/765581033311913</link><description>She arrived üòç [Expect more models soon...] See translation</description><pubDate>Thu, 27 Feb 2025 17:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/stefan-it/765581033311913</guid></item></channel></rss>