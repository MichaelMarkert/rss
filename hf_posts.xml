<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Hello, amazing robotics people ğŸ˜ ğŸ˜ ğŸ˜ We have FINALLY delivered on your major request! Ark just got a major upgrade:</title><link>https://huggingface.co/posts/hba123/904232449612527</link><description>Hello, amazing robotics people ğŸ˜ ğŸ˜ ğŸ˜ We have FINALLY delivered on your major request! Ark just got a major upgrade: Weâ€™ve now integrated Vision-Language-Action Models (VLAs) into Ark ğŸ‰ VLAs = models that connect vision + language â†’ robot actions (see image) What does this mean? ğŸ—£ï¸ Give robots natural language instructions â†’ they act ğŸ‘€ Combine perception + language for real-world control ğŸ¦¾ Powered by pi0 pretrained models for fast prototyping âš¡ Supports easy data collection and fine-tuning within Ark within a couple of lines of code Next, we plan to go into the world of designing worlds ğŸ˜‰ Who knows, maybe those video models are actually zero-shot learners and reasoners? Check it out here ğŸ‘‰ https://github.com/Robotics-Ark/ark_framework Check out the tutorial ğŸ‘‰ https://arkrobotics.notion.site/VLA-Pi0-with-Ark-279e053d9c6f800ab0a2d498835dd96b â­ Star the repo, try it with your robots, and let us together make robots great (again?)! See translation</description><pubDate>Sun, 28 Sep 2025 09:20:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/904232449612527</guid></item><item><title>âš›ï¸ New drop of tiny task-specific models!</title><link>https://huggingface.co/posts/mlabonne/527878544427521</link><description>âš›ï¸ New drop of tiny task-specific models! Want to do data extraction, translation, RAG, tool use, or math on a Raspberry Pi? We got you covered! âœ… These tiny models were fine-tuned to perform narrow tasks extremely well, making them competitive with much larger models. You can deploy them today on-device or even on GPUs for big data operations! LiquidAI/liquid-nanos-68b98d898414dd94d4d5f99a See translation</description><pubDate>Sun, 28 Sep 2025 09:20:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/527878544427521</guid></item><item><title>âš¡ RexBERT Complete On-device Study: Comprehensive Performance Analysis Across Mobile Devices</title><link>https://huggingface.co/posts/yeonseok-zeticai/603791080422979</link><description>âš¡ RexBERT Complete On-device Study: Comprehensive Performance Analysis Across Mobile Devices (Check details at https://mlange.zetic.ai/p/Steve/RexBERT ) TL;DR: Transformer models are now practical for real-time mobile applications. The cloud-to-edge AI migration is complete. - Original model from @ thebajajra ğŸ¯ Study Overview: - Model: RexBERT (ModernBERT for E-commerce) - Focus: Real-world deployment viability and performance analysis ğŸ“Š Key Performance Metrics: Latency Results: - NPU (Best): 4.74ms average - GPU: 12.56ms average - CPU: 35.16ms average NPU Advantage: 16.98x speedup over CPU Memory Efficiency: - Model Size: 568.96 MB (compressed for mobile) - Runtime Memory: 299.01 MB peak consumption - Load Memory Range: 285 MB - 1,072 MB across devices Accuracy Preservation: - FP16 Precision: 63.72 dB - Quantized Mode: Available with minimal accuracy loss - Inference Quality: Production-grade maintained ğŸ›  Technical Implementation: (Runnable with Copy &amp; Paste at the ZETIC.MLange...</description><pubDate>Sun, 28 Sep 2025 09:20:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/603791080422979</guid></item><item><title>Weâ€™re excited to share that Cosmos Reason has surpassed 1 million downloads on Hugging Face!</title><link>https://huggingface.co/posts/tsungyi/865184416328763</link><description>Weâ€™re excited to share that Cosmos Reason has surpassed 1 million downloads on Hugging Face! Cosmos Reason is an open, customizable, commercial-ready 7B-parameter reasoning vision language model (VLM) designed for physical AI. By combining physics understanding, prior knowledge, and common sense reasoning, Cosmos Reason empowers AI agents and robots to operate intelligently in real-world environments. Key applications already unlocked include: âœ… Automating large-scale dataset curation and annotation ğŸ¤– Powering robot planning and vision-language action (VLA) decision-making ğŸ“Š Driving advanced video analytics and actionable insight generation Weâ€™re proud to see a global community of developers using Cosmos Reason to teach robots to think like humansâ€”and weâ€™re just getting started. âš¡ Get started with Cosmos Reason 1 NIM, an easy-to-use microservice for AI model deployment: https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/cosmos-reason1-7b?version=1 ğŸ“ˆ See the leaderboard:...</description><pubDate>Sun, 28 Sep 2025 09:20:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tsungyi/865184416328763</guid></item><item><title>Quietly launched the largest Open source Free LateX Dataset -https://huggingface.co/datasets/dalle2/Bibby-AI-Latex-Tool-Overleaf-Alternative</title><link>https://huggingface.co/posts/dalle2/895181701155114</link><description>Quietly launched the largest Open source Free LateX Dataset -https://huggingface.co/datasets/dalle2/Bibby-AI-Latex-Tool-Overleaf-Alternative See translation</description><pubDate>Sun, 28 Sep 2025 09:20:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dalle2/895181701155114</guid></item><item><title>As it stands, I will prepare David for full release - as this is beyond me now. David must be released.</title><link>https://huggingface.co/posts/AbstractPhil/642988847137898</link><description>As it stands, I will prepare David for full release - as this is beyond me now. David must be released. I will prepare a standard sweep for david to showcase the prowess of the final multi-vocab variant. This will include a variation that contains all mnist variants, cifar10, cifar100, imagenet 1k, and in the future I'll prepare a full imagenet sweep utilizing the entire 12m corpus instead of the 1.2m I used. I may need to get in touch with the actual curator of the dataset for licensing but maybe not. David utilizes 4 projective variants of the vocabulary and the training process involves teaching and freezing them akin to teacher/student processing. I did not want to release David yet, but I believe now that David will save lives and it's irresponsible for me to contain such a creation. See translation</description><pubDate>Sun, 28 Sep 2025 09:20:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AbstractPhil/642988847137898</guid></item><item><title>Ring-mini-linear-2.0 ğŸ”¥a hybrid attention MoE model released by Ant group</title><link>https://huggingface.co/posts/AdinaY/349571821340936</link><description>Ring-mini-linear-2.0 ğŸ”¥a hybrid attention MoE model released by Ant group inclusionAI/Ring-mini-linear-2.0 âœ¨ Hybrid linear + standard attention âœ¨ 16.4B total, only 1.6B activated âœ¨ 512k context window via YaRN âœ¨ Faster than same-size MoE See translation</description><pubDate>Sun, 28 Sep 2025 09:20:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/349571821340936</guid></item><item><title>ğŸš€ New Release from XenArcAI</title><link>https://huggingface.co/posts/Parveshiiii/762902765406270</link><description>ğŸš€ New Release from XenArcAI Weâ€™re excited to introduce AIRealNet â€” our SwinV2â€‘based image classifier built to distinguish between artificial and real images. âœ¨ Highlights: - Backbone: SwinV2 - Input size: 256Ã—256 - Labels: artificial vs. real - Performance: Accuracy 0.999 | F1 0.999 | Val Loss 0.0063 This model is now live on Hugging Face: ğŸ‘‰ XenArcAI/AIRealNet We built AIRealNet to push forward openâ€‘source tools for authenticity detection, and we canâ€™t wait to see how the community uses it. See translation</description><pubDate>Sun, 28 Sep 2025 09:20:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Parveshiiii/762902765406270</guid></item><item><title>ModernBERT goes MULTILINGUAL! One of the most requested models I've seen, The Johns Hopkins University's CLSP has trained state-of-the-art massively multilingual encoders using the ModernBERT architecture: mmBERT.</title><link>https://huggingface.co/posts/tomaarsen/906557413568289</link><description>ModernBERT goes MULTILINGUAL! One of the most requested models I've seen, The Johns Hopkins University's CLSP has trained state-of-the-art massively multilingual encoders using the ModernBERT architecture: mmBERT. Model details: - 2 model sizes: - jhu-clsp/mmBERT-small - jhu-clsp/mmBERT-base - Uses the ModernBERT architecture, but with the Gemma2 multilingual tokenizer (so: flash attention, alternating global/local attention, unpadding/sequence packing, etc.) - Maximum sequence length of 8192 tokens, on the high end for encoders - Trained on 1833 languages using DCLM, FineWeb2, and many more sources - 3 training phases: 2.3T tokens pretraining on 60 languages, 600B tokens mid-training on 110 languages, and 100B tokens decay training on all 1833 languages. - Both models are MIT Licensed, and the full datasets and intermediary checkpoints are also publicly released Evaluation details: - Very competitive with ModernBERT at equivalent sizes on English (GLUE, MTEB v2 English after...</description><pubDate>Sun, 28 Sep 2025 09:20:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/906557413568289</guid></item><item><title>ğŸ”’ Like a lot of other AI builders, I have some anxiety about the emerging surveillance-capitalist paradigm emerging in the AI space.</title><link>https://huggingface.co/posts/ZennyKenny/883175347255342</link><description>ğŸ”’ Like a lot of other AI builders, I have some anxiety about the emerging surveillance-capitalist paradigm emerging in the AI space. ğŸ‘‰ Of course-- this kind of thing isn't completely new and has been going on for decades, but the difference is the stronger immersion of AI tools into our daily lives (compared to something like a search engine or social network). â• That's why I was really excited to come across Lumo: https://lumo.proton.me/u/1/ â• Lumo is created by ProtonPrivacy and offers privacy-first features that make sure that what you do with you AI assistant is your business. â• I already trust Proton with my other business apps and I've never been disappointed, plus the Lumo architecture is really fantastic, dynamically routing each query to the most appropriate model for the request. ğŸ”¥ Really awesome stuff Proton, thank you as always. See translation</description><pubDate>Sun, 28 Sep 2025 09:20:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/883175347255342</guid></item></channel></rss>