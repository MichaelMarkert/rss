<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Hi, I just published research paper that's introducing my Reactive Transformer (RxT) architecture. I would be grateful if you could check it and upvote on HuggingFace Daily Papers -</title><link>https://huggingface.co/posts/AdamF92/854077189361039</link><description>Hi, I just published research paper that's introducing my Reactive Transformer (RxT) architecture. I would be grateful if you could check it and upvote on HuggingFace Daily Papers - Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models (2510.03561) Architecture is based on stateful real-time processing with innovational asynchronous memory update. Instead of reprocessing all the conversation history for each message, it's processing only single query with all the context moved to dedicated memory layers. Memory is updated after generating the answer, so it's not influencing latency - in tests, time to first token was almost the same as generating a single token. It has also better quality/accuracy in multi-turn dialogue than the same size stateless decoder-only model. Initial experiments were small scale (12M to 160M params models trained on simple synthetic datasets), but just now I'm starting training of bigger 270M params model on...</description><pubDate>Thu, 09 Oct 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdamF92/854077189361039</guid></item><item><title>Have built the new Image Studio with the Gemini Image Gen models for the following multiple tasks:</title><link>https://huggingface.co/posts/prithivMLmods/886559259732378</link><description>Have built the new Image Studio with the Gemini Image Gen models for the following multiple tasks: imagen-4.0-fast-generate-001 model for Image Generation (Text-to-Image) and Multi-Image Editing (Image-to-Image), and Draw-to-Image powered by gemini-2.5-flash-image (aka Nano Banana). ‚≠ê Gemini-Image-Studio: prithivMLmods/Gemini-Image-Studio (Latest) ü§û Old-App: prithivMLmods/Nano-Banana-AIO ü•ä GitHub: https://github.com/prithivsakthiur/gemini-image-studio-hf To proceed, you need to add your Gemini API key. Your API key is stored only for the duration of your session and will be lost when you reload or exit the page. It will not be shared or exposed anywhere. See translation</description><pubDate>Thu, 09 Oct 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/886559259732378</guid></item><item><title>ü§ñ What if building your own robot arm costs less than ¬£220?</title><link>https://huggingface.co/posts/hba123/315319549896319</link><description>ü§ñ What if building your own robot arm costs less than ¬£220? For years, robotics has been locked behind high prices and complex systems. So we decided to change that. Today, we‚Äôre open-sourcing Ark-Bot ‚Äî a fully 3D-printed, 6-DOF robot arm that works seamlessly with our Python robotics library, Ark. And yes‚Ä¶ It‚Äôs only ¬£215.86 to build. üß†ArkBot Specs üß† 1Ô∏è‚É£ Reach: 1 meter 2Ô∏è‚É£ Weight: 2.6 kg 3Ô∏è‚É£ Payload: 1.8 kg üí™ 4Ô∏è‚É£ DOF: 6 5Ô∏è‚É£ Input Voltage: DC 12V ü§üFully 3D-printable &amp; open-source ü§üIntegrated with Ark ‚Äî no ROS required üìπ We‚Äôve also released a video showing the full assembly process ‚Äî because robotics should be something everyone can learn, build, and improve on. üë©‚Äçüéì With Ark-Bot, anyone ‚Äî from students to AI researchers ‚Äî can experiment with embodied AI, robot learning, and control algorithms on real hardware, affordably. If you could control a 1-meter robot arm from your laptop for under ¬£220‚Ä¶ üëâ What would you build first? üîóhttps://github.com/Robotics-Ark/ark_bot üé•...</description><pubDate>Thu, 09 Oct 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/315319549896319</guid></item><item><title>STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! üö®</title><link>https://huggingface.co/posts/m-ric/175050207181959</link><description>STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! üö® A lone scientist just proposed Tiny Recursive Model (TRM), and it is literally the most impressive model that I've seen this year. ‚û°Ô∏è Tiny Recursive Model is 7M parameters ‚û°Ô∏è On ARC-AGI, it beats flagship models like Gemini-2.5-pro Consider how wild this is: Gemini-2.5-pro must be over 10,000x bigger and had 1,000 as many authors üòÇ (Alexia is alone on the paper) What's this sorcery? In short: it's a very tiny Transformers, but it loops over itself at two different frequencies, updating two latent variables: one for the proposed answer and one for the reasoning. @ AlexiaJM started from the paper Hierarchical Reasoning Model, published a few months ago, that already showed breakthrough improvement on AGI for its small size (27M) Hierarchical Reasoning Model had introduced one main feature: üîé Deep supervision In their model, one part (here one layer) would run at high frequency, and...</description><pubDate>Thu, 09 Oct 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/175050207181959</guid></item><item><title>Finally, I uploaded the model I developed for my master‚Äôs thesis! Given a financial event, it provides explained predictions based on a dataset of past news and central bank speeches.</title><link>https://huggingface.co/posts/SelmaNajih001/201829706503917</link><description>Finally, I uploaded the model I developed for my master‚Äôs thesis! Given a financial event, it provides explained predictions based on a dataset of past news and central bank speeches. Try it out here: SelmaNajih001/StockPredictionExplanation (Just restart the space and wait a minute) The dataset used for RAG can be found here: SelmaNajih001/FinancialNewsAndCentralBanksSpeeches-Summary-Rag While the dataset used for the training is: SelmaNajih001/FinancialClassification I also wrote an article to explain how I've done the training. You can find it here https://huggingface.co/blog/SelmaNajih001/explainable-financial-predictions See translation</description><pubDate>Thu, 09 Oct 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/SelmaNajih001/201829706503917</guid></item><item><title>I found it excellent and very well done.</title><link>https://huggingface.co/posts/SelmaNajih001/229668569857091</link><description>I found it excellent and very well done. One of the best explanations of embedding I've ever read. Well done, @ hesamation ! Had to share this: hesamation/primer-llm-embedding See translation</description><pubDate>Thu, 09 Oct 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/SelmaNajih001/229668569857091</guid></item><item><title>Online training methods (e.g., GRPO) require real-time generation, a compute- and memory-heavy bottleneck.</title><link>https://huggingface.co/posts/sergiopaniego/573302448191586</link><description>Online training methods (e.g., GRPO) require real-time generation, a compute- and memory-heavy bottleneck. TRL has built-in vLLM support and in this new recipe, we show how to leverage it for efficient online training. Run on Colab ‚ö°, scale to multi-GPU/multi-node! üßë‚Äçüç≥ recipe: https://huggingface.co/learn/cookbook/grpo_vllm_online_training See translation</description><pubDate>Thu, 09 Oct 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/573302448191586</guid></item><item><title>NEW RELEASE: Esper 3.1!</title><link>https://huggingface.co/posts/sequelbox/166564347957621</link><description>NEW RELEASE: Esper 3.1! - Esper is our full-stack, full-cycle coding, DevOps, and architecture specialist! - Our newest, best DeepSeek technical datasets emphasize more challenging queries and tough real-world coding tasks across a variety of programming languages and development paradigms: - Titanium 3 for coding and reasoning in DevOps and architecture: sequelbox/Titanium3-DeepSeek-V3.1-Terminus - Tachibana 3 for high-difficulty code production in a variety of topics and programming languages: - sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus - sequelbox/Tachibana3-Part2-DeepSeek-V3.2 - Mitakihara for MLOps, AI building, use, expertise, and research: sequelbox/Mitakihara-DeepSeek-R1-0528 Our first release in the Esper 3.1 series is built on Qwen3-4B-Thinking-2507. GET IT NOW, FOR EVERYONE: ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1 We'll be bringing Esper 3.1 to more, larger models as soon as we can; you can help this happen faster with a donation: sequelbox/SupportOpenSource...</description><pubDate>Thu, 09 Oct 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sequelbox/166564347957621</guid></item><item><title>üöÄ New blog: Maintain the unmaintainable ‚Äì 1M+ Python LOC, 400+ models</title><link>https://huggingface.co/posts/Molbap/149398104991391</link><description>üöÄ New blog: Maintain the unmaintainable ‚Äì 1M+ Python LOC, 400+ models How do you stop a million-line library built by thousands of contributors from collapsing under its own weight? At ü§ó Transformers, we do it with explicit software-engineering tenets, principles that make the codebase hackable at scale. üîç Inside the post: ‚Äì One Model, One File: readability first ‚Äî you can still open a modeling file and see the full logic, top to bottom. ‚Äì Modular Transformers: visible inheritance that cuts maintenance cost by ~15√ó while keeping models readable. ‚Äì Config-Driven Performance: FlashAttention, tensor parallelism, and attention scheduling are config-level features, not rewrites. Written with @ lysandre ,@pcuenq and @ yonigozlan , this is a deep dive into how Transformers stays fast, open, and maintainable. Read it here ‚Üí transformers-community/Transformers-tenets See translation</description><pubDate>Thu, 09 Oct 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Molbap/149398104991391</guid></item><item><title>A few days ago, Thinking Machines Lab released ‚ÄúLoRA Without Regret‚Äù, showing that LoRA can match full fine-tuning performance when configured right.</title><link>https://huggingface.co/posts/sergiopaniego/862106039351008</link><description>A few days ago, Thinking Machines Lab released ‚ÄúLoRA Without Regret‚Äù, showing that LoRA can match full fine-tuning performance when configured right. Naturally, we decided to reproduce the results with TRL and release a guide! https://huggingface.co/docs/trl/main/en/lora_without_regret See translation</description><pubDate>Thu, 09 Oct 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/862106039351008</guid></item></channel></rss>