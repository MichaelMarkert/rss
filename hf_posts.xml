<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Are you familiar with reverse residual connections or looping in language models?</title><link>https://huggingface.co/posts/Sunny111/812628018751645</link><description>Are you familiar with reverse residual connections or looping in language models? Excited to share my Looped-GPT blog post and codebase ðŸš€ https://github.com/sanyalsunny111/Looped-GPT TL;DR: looping during pre-training improves generalization. Plot shows GPT2 LMs pre-trained with 15.73B OWT tokens P.S. This is my first post here â€” I have ~4 followers and zero expectations for reach ðŸ˜„ See translation</description><pubDate>Sun, 18 Jan 2026 13:32:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Sunny111/812628018751645</guid></item><item><title>Compared Quality and Speed Difference (with CUDA 13 &amp; Sage Attention) of BF16 vs GGUF Q8 vs FP8 Scaled vs NVFP4 for Z Image Turbo, FLUX Dev, FLUX SRPO, FLUX Kontext, FLUX 2 - Full 4K step by step tutorial also published</title><link>https://huggingface.co/posts/MonsterMMORPG/425042812655046</link><description>Compared Quality and Speed Difference (with CUDA 13 &amp; Sage Attention) of BF16 vs GGUF Q8 vs FP8 Scaled vs NVFP4 for Z Image Turbo, FLUX Dev, FLUX SRPO, FLUX Kontext, FLUX 2 - Full 4K step by step tutorial also published Full 4K tutorial : https://youtu.be/XDzspWgnzxI Check above full 4K tutorial to learn more and see uncompressed original quality and size images It was always wondered how much quality and speed difference exists between BF16, GGUF, FP8 Scaled and NVFP4 precisions. In this tutorial I have compared all these precision and quantization variants for both speed and quality. The results are pretty surprising. Moreover, we have developed and published NVFP4 model quant generator app and FP8 Scaled quant generator apps. The links of the apps are below if you want to use them. Furthermore, upgrading ComfyUI to CUDA 13 with properly compiled libraries is now very much recommended. We have observed some noticeable performance gains with CUDA 13. So for both SwarmUI and ComfyUI...</description><pubDate>Sun, 18 Jan 2026 13:32:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/425042812655046</guid></item><item><title>My First MCP Server: DataView</title><link>https://huggingface.co/posts/efecelik/312692243758686</link><description>My First MCP Server: DataView Browse HuggingFace datasets directly from your AI assistant. -Search &amp; filter datasets -View rows &amp; stats -SQL queries &amp; Parquet export efecelik/dataview-mcp See translation</description><pubDate>Sun, 18 Jan 2026 13:32:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/efecelik/312692243758686</guid></item><item><title>Tired of paying a bunch of money for large LLMs? Then read this notebook and/or watch this video on how to distill knowledge into tiny LLMs.</title><link>https://huggingface.co/posts/davidmezzetti/675473701665593</link><description>Tired of paying a bunch of money for large LLMs? Then read this notebook and/or watch this video on how to distill knowledge into tiny LLMs. Notebook: https://github.com/neuml/txtai/blob/master/examples/80_Distilling_Knowledge_into_Tiny_LLMs.ipynb Video: https://www.youtube.com/watch?v=Ol560ktgkf0 See translation</description><pubDate>Sun, 18 Jan 2026 13:32:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/davidmezzetti/675473701665593</guid></item><item><title>After a VLM, StepFun dropped a new audio model: Step-Audio-R1.1, enabling thinking while speaking ðŸ”¥</title><link>https://huggingface.co/posts/AdinaY/484665846337512</link><description>After a VLM, StepFun dropped a new audio model: Step-Audio-R1.1, enabling thinking while speaking ðŸ”¥ stepfun-ai/Step-Audio-R1.1 âœ¨ Apache 2.0 âœ¨ Combines dual-brain architecture and acoustic-grounded reasoning to enable real-time dialogue with SOTA-level reasoning See translation</description><pubDate>Sun, 18 Jan 2026 13:32:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/484665846337512</guid></item><item><title>ðŸ¥‡ 23 Kaggle Gold Medals. One Agent Framework.</title><link>https://huggingface.co/posts/FreshmanD/375261612088441</link><description>ðŸ¥‡ 23 Kaggle Gold Medals. One Agent Framework. Introducing LoongFlow: A Thinking &amp; Learning Framework for Expert-Grade AI Agents. Unlike traditional evolve agents(like OpenEvolve-Style), LoongFlow implements the PES (Plan-Execute-Summary) paradigm to learn from mistakes and avoid local optima. ðŸš€ Highlights: * SOTA: Surpassed human mathematicians on 11 geometry/algebra problems. * 23 Kaggle Gold Medals on MLE Bench. * Efficiency: 60% more efficient than current baselines. ðŸ”— Code &amp; Paper: https://github.com/baidu-baige/LoongFlow LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm (2512.24077) #AutoML #Kaggle #Agents #OpenSource #LLM See translation</description><pubDate>Sun, 18 Jan 2026 13:32:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/FreshmanD/375261612088441</guid></item><item><title>We've open-sourced a bilingual Semantic Highlighting model that can power multiple production scenarios:</title><link>https://huggingface.co/posts/zc277584121/729228721948980</link><description>We've open-sourced a bilingual Semantic Highlighting model that can power multiple production scenarios: 1) RAG Answer Highlighting â€” Automatically highlight the exact sentences that answer user queries, improving interpretability and helping users quickly locate relevant information. 2) RAG Noise Filtering â€” Prune irrelevant context before sending to LLMs, achieving 70-80% token cost reduction while improving answer quality by letting the model focus on what matters. 3) Search System Highlighting â€” Add semantic highlighting features to recommendation systems, e-commerce search, or any retrieval system where users need to see why a result is relevant. Try it out: zilliz/semantic-highlight-bilingual-v1 Read our article: https://huggingface.co/blog/zilliz/zilliz-semantic-highlight-model See translation</description><pubDate>Sun, 18 Jan 2026 13:32:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/zc277584121/729228721948980</guid></item><item><title>You can now do reinforcement learning training with 7Ã— longer context and no accuracy loss, via our new batching algorithms.</title><link>https://huggingface.co/posts/danielhanchen/641905091288769</link><description>You can now do reinforcement learning training with 7Ã— longer context and no accuracy loss, via our new batching algorithms. Long reasoning chains in RL are costly, but now we enable you to train gpt-oss with GRPO &amp; reach 380K context on a 192GB GPU. Blog: https://unsloth.ai/docs/new/grpo-long-context See translation</description><pubDate>Sun, 18 Jan 2026 13:32:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/641905091288769</guid></item><item><title>I built a crazy ultraâ€“low latency voice assistant agent using Pipecat, NVIDIA Riva, NVIDIA NIM, and an MCPâ€‘powered tool stack. It can talk in real time, search the web, and manage your project directory files, document your code and docs handsâ€‘free (create, read, summarise, and clean up).</title><link>https://huggingface.co/posts/RakshitAralimatti/271506431516041</link><description>I built a crazy ultraâ€“low latency voice assistant agent using Pipecat, NVIDIA Riva, NVIDIA NIM, and an MCPâ€‘powered tool stack. It can talk in real time, search the web, and manage your project directory files, document your code and docs handsâ€‘free (create, read, summarise, and clean up). Link - https://github.com/rakshit2020/Voice-Agent-using-Nvidia-Riva-NIM-Pipecat I put everything into a small demo repo with the full architecture diagram and a short demo video so you can see exactly how it works and adapt it to your own projects. Check out the GitHub, play with the agent, and let me know if itâ€™s useful or if you want a breakdown of any part of the setup. See translation</description><pubDate>Sun, 18 Jan 2026 13:32:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/271506431516041</guid></item><item><title>NEW MODEL:</title><link>https://huggingface.co/posts/unmodeled-tyler/270028211989897</link><description>NEW MODEL: vanta-research/mox-tiny-1 Excited to share the first model from the new Mox-1 model family from VANTA Research! Mox-Tiny-1 is finetuned from Llama's 3.1 8B architecture specifically for thinking partnership and high-fidelity AI interaction. Training domains were carefully selected with this goal in mind, and all datasets used were synthetically generated for the Mox-1 models specifically. Domains include but not limited to: - Direct opinions (clear stances on various topics) - Constructive disagreement (Pushing back on flawed premises) - Epistemic confidence (calibrated certainty and uncertainty) - Warmth &amp; presence (Empathetic, human responses) - Wonder &amp; puzzlement (Intellectual curiosity) Interaction Examples: Direct opinions: User: What 's the best programming language? Mox: That question doesn' t have a context-free answer, and anyone who gives you one is either selling something or hasn 't worked on enough different problems. But I' ll give you my actual take across...</description><pubDate>Sun, 18 Jan 2026 13:32:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/270028211989897</guid></item></channel></rss>