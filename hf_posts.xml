<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Awesome intro to LLM course "Language Modeling from Scratch" by stanford. love the aesthetics behind the lecture notes, notes-in-code genius ideaüëç</title><link>https://huggingface.co/posts/Jaward/445538723467397</link><description>Awesome intro to LLM course "Language Modeling from Scratch" by stanford. love the aesthetics behind the lecture notes, notes-in-code genius ideaüëç Course site: https://stanford-cs336.github.io/spring2025/ Repo: https://github.com/stanford-cs336/spring2025-lectures Videos: https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_ See translation</description><pubDate>Thu, 26 Jun 2025 13:35:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/445538723467397</guid></item><item><title>Hi everyone,</title><link>https://huggingface.co/posts/yeonseok-zeticai/727857049396772</link><description>Hi everyone, I‚Äôve been running small language models (SLLMs) directly on smartphones ‚Äî completely offline, with no cloud backend or server API calls. I wanted to share: 1. ‚ö° Tokens/sec performance across several SLLMs 2. ü§ñ Observations on hardware utilization (where the workload actually runs) 3. üìè Trade-offs between model size, latency, and feasibility for mobile apps There are reports for below models - QWEN3 0.6B - NVIDIA/Nemotron QWEN 1.5B - SimpleScaling S1 - TinyLlama - Unsloth tuned Llama 3.2 1B - Naver HyperClova 0.5B üìúComparable Benchmark reports (no cloud, all on-device): I‚Äôd really value your thoughts on: - Creative ideas to further optimize inference under these hardware constraints - Other compact LLMs worth testing on-device - Experiences you‚Äôve had trying to deploy LLMs at the edge If there‚Äôs interest, I‚Äôm happy to share more details on the test setup, hardware specs, or the tooling we used for these comparisons. Thanks for taking a look, and you can build your own...</description><pubDate>Thu, 26 Jun 2025 13:35:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/727857049396772</guid></item><item><title>Introducing Dhanishtha 2.0: World's first Intermediate Thinking Model</title><link>https://huggingface.co/posts/Abhaykoul/997219525730173</link><description>Introducing Dhanishtha 2.0: World's first Intermediate Thinking Model Dhanishtha 2.0 is the world's first LLM designed to think between the responses. Unlike other Reasoning LLMs, which think just once. Dhanishtha can think, rethink, self-evaluate, and refine in between responses using multiple &lt;think&gt; blocks. This technique makes it Hinghlt Token efficient it Uses up to 79% fewer tokens than DeepSeek R1 --- You can try our model from: https://helpingai.co/chat Also, we're gonna Open-Source Dhanistha on July 1st. --- For Devs: üîë Get your API key at https://helpingai.co/dashboard from HelpingAI import HAI # pip install HelpingAI ==1.1.1 from rich import print hai = HAI( api_key = "hl-***********************" ) response = hai.chat.completions.create( model = "Dhanishtha-2.0-preview" , messages=[{ "role" : "user" , "content" : "What is the value of ‚à´0‚àûùë•3/ùë•‚àí1ùëëùë• ?" }], stream = True , hide_think = False # Hide or show models thinking ) for chunk in response: print...</description><pubDate>Thu, 26 Jun 2025 13:35:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/997219525730173</guid></item><item><title>Hackathons in Paris on July 5th and 6th!</title><link>https://huggingface.co/posts/pagezyhf/610118153924016</link><description>Hackathons in Paris on July 5th and 6th! Hugging Face just wrapped 4 months of deep work with AMD to push kernel-level optimization on their MI300X GPUs. Now, it's time to share everything we learned. Join us in Paris at STATION F for a hands-on weekend of workshops and a hackathon focused on making open-source LLMs faster and more efficient on AMD. Prizes, amazing host speakers, ... if you want more details, navigate to https://lu.ma/fmvdjmur ! See translation</description><pubDate>Thu, 26 Jun 2025 13:35:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/pagezyhf/610118153924016</guid></item><item><title>Was going to post this on /r/LocalLLaMa, but apparently it's without moderation at this time :')</title><link>https://huggingface.co/posts/bartowski/460622149989234</link><description>Was going to post this on /r/LocalLLaMa, but apparently it's without moderation at this time :') bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF Was able to use previous mistral chat templates, some hints from Qwen templates, and Claude to piece together a seemingly working chat template, tested it with llama.cpp server and got perfect results, though lmstudio still seems to be struggling for some reason (don't know how to specify a jinja file there) Outlined the details of the script and results in my llama.cpp PR to add the jinja template: https://github.com/ggml-org/llama.cpp/pull/14349 Start server with a command like this: ./llama-server -m /models/mistralai_Mistral-Small -3 .2 -24 B-Instruct -2506 -Q4_K_M.gguf --jinja --chat-template-file /models/Mistral-Small -3 .2 -24 B-Instruct -2506 .jinja and it should be perfect! Hoping it'll work for ALL tools if lmstudio gets an update or something, not just llama.cpp, but very happy to see it works flawlessly in llama.cpp...</description><pubDate>Thu, 26 Jun 2025 13:35:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/bartowski/460622149989234</guid></item><item><title>Working on some chess set concepts. I went towards minimal sculpted shapes then returned to some traditionalism.</title><link>https://huggingface.co/posts/BFFree/190836494252067</link><description>Working on some chess set concepts. I went towards minimal sculpted shapes then returned to some traditionalism. See translation</description><pubDate>Thu, 26 Jun 2025 13:35:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/BFFree/190836494252067</guid></item><item><title>Huge new bio datasets just dropped!!!</title><link>https://huggingface.co/posts/cgeorgiaw/508678634223532</link><description>Huge new bio datasets just dropped!!! Check out them out @ ginkgo-datapoints Read the blog for more info: https://huggingface.co/blog/cgeorgiaw/gdp See translation</description><pubDate>Thu, 26 Jun 2025 13:35:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/cgeorgiaw/508678634223532</guid></item><item><title>üöÄ SmolAgents v1.19.0 is live!</title><link>https://huggingface.co/posts/albertvillanova/188022025451209</link><description>üöÄ SmolAgents v1.19.0 is live! This release brings major improvements to agent flexibility, UI usability, streaming architecture, and developer experience: making it easier than ever to build smart, interactive AI agents. Here's what's new: üîß Agent Upgrades - Support for managed agents in ToolCallingAgent - Context manager support for cleaner agent lifecycle handling - Output formatting now uses XML tags for consistency üñ•Ô∏è UI Enhancements - GradioUI now supports reset_agent_memory: perfect for fresh starts in dev &amp; demos. üîÑ Streaming Refactor - Streaming event aggregation moved off the Model class - ‚û°Ô∏è Better architecture &amp; maintainability üì¶ Output Tracking - CodeAgent outputs are now stored in ActionStep - ‚úÖ More visibility and structure to agent decisions üêõ Bug Fixes - Smarter planning logic - Cleaner Docker logs - Better prompt formatting for additional_args - Safer internal functions and final answer matching üìö Docs Improvements - Added quickstart examples with tool usage - One-...</description><pubDate>Thu, 26 Jun 2025 13:35:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/albertvillanova/188022025451209</guid></item><item><title>The new</title><link>https://huggingface.co/posts/freddyaboulton/509614991477853</link><description>The new multimodalart/self-forcing model and demo are truly impressive! See translation</description><pubDate>Thu, 26 Jun 2025 13:35:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/freddyaboulton/509614991477853</guid></item><item><title>LongWriter-Zero üî• A Purely RL trained LLM handles 10K+ token coherent passages by Tsinghua University</title><link>https://huggingface.co/posts/AdinaY/313388068506036</link><description>LongWriter-Zero üî• A Purely RL trained LLM handles 10K+ token coherent passages by Tsinghua University Model: THU-KEG/LongWriter-Zero-32B Dataset: THU-KEG/LongWriter-Zero-RLData Paper: LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning (2506.18841) ‚ú® 32B ‚ú® Multi-reward GRPO: length, fluency, structure, non-redundancy ‚ú® Enforces &lt;think&gt;&lt;answer&gt; format via Format RM ‚ú® Build on Qwen2.5-32B-base See translation</description><pubDate>Thu, 26 Jun 2025 13:35:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/313388068506036</guid></item></channel></rss>