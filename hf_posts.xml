<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AGENTS + FINETUNING! This week Hugging Face learn has a whole pathway on finetuning for agentic applications. You can follow these two courses to get knowledge on levelling up your agent game beyond prompts:</title><link>https://huggingface.co/posts/burtenshaw/189514834246661</link><description>AGENTS + FINETUNING! This week Hugging Face learn has a whole pathway on finetuning for agentic applications. You can follow these two courses to get knowledge on levelling up your agent game beyond prompts: 1ï¸âƒ£ New Supervised Fine-tuning unit in the NLP Course https://huggingface.co/learn/nlp-course/en/chapter11/1 2ï¸âƒ£New Finetuning for agents bonus module in the Agents Course https://huggingface.co/learn/agents-course/bonus-unit1/introduction Fine-tuning will squeeze everything out of your model for how youâ€™re using it, more than any prompt. See translation</description><pubDate>Thu, 20 Feb 2025 09:22:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/189514834246661</guid></item><item><title>ğŸ¯ Perplexity drops their FIRST open-weight model on Hugging Face: A decensored DeepSeek-R1 with full reasoning capabilities. Tested on 1000+ examples for unbiased responses.</title><link>https://huggingface.co/posts/fdaudens/121352437859372</link><description>ğŸ¯ Perplexity drops their FIRST open-weight model on Hugging Face: A decensored DeepSeek-R1 with full reasoning capabilities. Tested on 1000+ examples for unbiased responses. Check it out: perplexity-ai/r1-1776 Blog post: https://perplexity.ai/hub/blog/open-sourcing-r1-1776 See translation</description><pubDate>Thu, 20 Feb 2025 09:22:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/121352437859372</guid></item><item><title>ğŸš€ StepFuné˜¶è·ƒæ˜Ÿè¾° is making BIG open moves!</title><link>https://huggingface.co/posts/AdinaY/709023807759284</link><description>ğŸš€ StepFuné˜¶è·ƒæ˜Ÿè¾° is making BIG open moves! Last year, their GOT-OCR 2.0 took the community by storm ğŸ”¥but many didnâ€™t know they were also building some amazing models. Now, theyâ€™ve just dropped something huge on the hub! ğŸ“º Step-Video-T2V: a 30B bilingual open video model that generates 204 frames (8-10s) at 540P resolution with high information density &amp; consistency. stepfun-ai/stepvideo-t2v ğŸ”Š Step-Audio-TTS-3B : a TTS trained with the LLM-Chat paradigm on a large synthetic dataset, capable of generating RAP &amp; Humming stepfun-ai/step-audio-67b33accf45735bb21131b0b See translation</description><pubDate>Thu, 20 Feb 2025 09:22:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/709023807759284</guid></item><item><title>Dino: The Minimalist Multipurpose Chat System ğŸŒ </title><link>https://huggingface.co/posts/prithivMLmods/874083632338295</link><description>Dino: The Minimalist Multipurpose Chat System ğŸŒ  Agent-Dino : prithivMLmods/Agent-Dino By default, it performs the following tasks: {Text-to-Text Generation}, {Image-Text-Text Generation} @image : Generates an image using Stable Diffusion xL. @3d : Generates a 3D mesh. @web : Web search agents. @rAgent : Initiates a reasoning chain using Llama mode for coding explanations. @tts1-â™€ , @tts2-â™‚ : Voice generation (Female and Male voices). See translation</description><pubDate>Thu, 20 Feb 2025 09:22:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/874083632338295</guid></item><item><title>Google just released PaliGemma 2 Mix: new versatile instruction vision language models ğŸ”¥</title><link>https://huggingface.co/posts/merve/467807900895850</link><description>Google just released PaliGemma 2 Mix: new versatile instruction vision language models ğŸ”¥ &gt; Three new models: 3B, 10B, 28B with res 224, 448 ğŸ’™ &gt; Can do vision language tasks with open-ended prompts, understand documents, and segment or detect anything ğŸ¤¯ Read more https://huggingface.co/blog/paligemma2mix Try the demo google/paligemma2-10b-mix All models are here google/paligemma-2-mix-67ac6a251aaf3ee73679dcc4 See translation</description><pubDate>Thu, 20 Feb 2025 09:22:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/467807900895850</guid></item><item><title>Less is More for Reasoning (LIMO): a 32B model fine-tuned with 817 examples can beat o1-preview on math reasoning! ğŸ¤¯</title><link>https://huggingface.co/posts/m-ric/436586297766836</link><description>Less is More for Reasoning (LIMO): a 32B model fine-tuned with 817 examples can beat o1-preview on math reasoning! ğŸ¤¯ Do we really need o1's huge RL procedure to see reasoning emerge? It seems not. Researchers from Shanghai Jiaotong University just demonstrated that carefully selected examples can boost math performance in large language models using SFT â€”no huge datasets or RL procedures needed. Their procedure allows Qwen2.5-32B-Instruct to jump from 6.5% to 57% on AIME and from 59% to 95% on MATH, while using only 1% of the data in previous approaches. âš¡ The Less-is-More Reasoning Hypothesis: â€£ Minimal but precise examples that showcase optimal reasoning patterns matter more than sheer quantity â€£ Pre-training knowledge plus sufficient computational resources at inference levels up math skills â¡ï¸ Core techniques: â€£ High-quality reasoning chains with self-verification steps â€£ 817 handpicked problems that encourage deeper reasoning â€£ Enough inference-time computation to allow...</description><pubDate>Thu, 20 Feb 2025 09:22:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/436586297766836</guid></item><item><title>ğŸš€ FLUX Workflow Canvas</title><link>https://huggingface.co/posts/ginipick/141662077994282</link><description>ğŸš€ FLUX Workflow Canvas Welcome to Workflow Canvas, your ultimate AI-driven platform for crafting stunning design concepts and intricate workflow diagrams that empower your business! ğŸ¤–âœ¨ ginigen/Workflow-Canvas Features Product Design ğŸ› ï¸ Transform your ideas into reality with sleek, industrial product designs that blend modern aesthetics with advanced technology. Mindmap ğŸ§  Generate vibrant, educational mind maps that outline your strategies and processes in a clear, visually engaging layout. Mockup ğŸ“± Quickly prototype intuitive app interfaces and web designs using clean, hand-drawn wireframes that capture your vision. Infographic ğŸ“Š Build polished, data-rich infographics that communicate complex corporate metrics and trends with style and clarity. Diagram ğŸ“ˆ Illustrate comprehensive, end-to-end business workflowsâ€”from market analysis to implementationâ€”with detailed and organized diagrams. Flowchart ğŸ”„ Design easy-to-follow, hand-drawn style flowcharts that map out your operational...</description><pubDate>Thu, 20 Feb 2025 09:22:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/141662077994282</guid></item><item><title>ğŸ”¥ Meet Muse: that can generate a game environment based on visuals or playersâ€™ controller actions. It was developed by Microsoft Research in collaboration with Ninja Theory (Hellblade developer). Itâ€™s built on something called the World and Human Action Model (WHAM-1.6B model). They trained on 7 years of Bleeding Edge gameplay and it can generate 2 minute long 3D game sequences with consistent physics and character behaviors all from just a second of input. Theyâ€™ve gone and open-sourced it too. Open weights, the WHAM Demonstrator, and sample data on Azure AI Foundry for anyone to play with. Hope so soon on Hugging Face ğŸ¤—.</title><link>https://huggingface.co/posts/merterbak/134010141714846</link><description>ğŸ”¥ Meet Muse: that can generate a game environment based on visuals or playersâ€™ controller actions. It was developed by Microsoft Research in collaboration with Ninja Theory (Hellblade developer). Itâ€™s built on something called the World and Human Action Model (WHAM-1.6B model). They trained on 7 years of Bleeding Edge gameplay and it can generate 2 minute long 3D game sequences with consistent physics and character behaviors all from just a second of input. Theyâ€™ve gone and open-sourced it too. Open weights, the WHAM Demonstrator, and sample data on Azure AI Foundry for anyone to play with. Hope so soon on Hugging Face ğŸ¤—. ğŸ“„ Paper: https://www.nature.com/articles/s41586-025-08600-3 Blog Post: https://www.microsoft.com/en-us/research/blog/introducing-muse-our-first-generative-ai-model-designed-for-gameplay-ideation/ See translation</description><pubDate>Thu, 20 Feb 2025 09:22:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/134010141714846</guid></item><item><title>What are the best organizations to follow on</title><link>https://huggingface.co/posts/clem/219492053181381</link><description>What are the best organizations to follow on @ huggingface ? On top of my head: - Deepseek (35,000 followers): https://huggingface.co/deepseek-ai - Meta Llama (27,000 followers): https://huggingface.co/meta-llama - Black Forrest Labs (11,000 followers): https://huggingface.co/black-forest-labs - OpenAI (5,000 followers): https://huggingface.co/openai - Nvidia (16,000 followers): https://huggingface.co/nvidia - MIcrosoft (9,000 followers): https://huggingface.co/microsoft - AllenAI (2,000 followers): https://huggingface.co/allenai - Mistral (5,000 followers): https://huggingface.co/mistralai - XAI (600 followers): https://huggingface.co/xai-org - Stability AI (16,000 followers): https://huggingface.co/stabilityai - Qwen (16,000 followers): https://huggingface.co/Qwen - GoogleAI (8,000 followers): https://huggingface.co/google - Unsloth (3,000 followers): https://huggingface.co/unsloth - Bria AI (4,000 followers): https://huggingface.co/briaai - NousResearch (1,300 followers):...</description><pubDate>Thu, 20 Feb 2025 09:22:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/219492053181381</guid></item><item><title>I built an AI agent app in less than 8 hoursğŸ¤¯</title><link>https://huggingface.co/posts/as-cle-bert/260283430473510</link><description>I built an AI agent app in less than 8 hoursğŸ¤¯ And, believe me, this is ğ—»ğ—¼ğ˜ clickbaitâŒ GitHub ğŸ‘‰ https://github.com/AstraBert/PapersChat Demo ğŸ‘‰ as-cle-bert/PapersChat The app is called ğğšğ©ğğ«ğ¬ğ‚ğ¡ğšğ­, and it is aimed at ğ—ºğ—®ğ—¸ğ—¶ğ—»ğ—´ ğ—°ğ—µğ—®ğ˜ğ˜ğ—¶ğ—»ğ—´ ğ˜„ğ—¶ğ˜ğ—µ ğ˜€ğ—°ğ—¶ğ—²ğ—»ğ˜ğ—¶ğ—³ğ—¶ğ—° ğ—½ğ—®ğ—½ğ—²ğ—¿ğ˜€ ğ—²ğ—®ğ˜€ğ—¶ğ—²ğ—¿. ğ‡ğğ«ğ ğ¢ğ¬ ğ°ğ¡ğšğ­ ğ­ğ¡ğ ğšğ©ğ© ğğ¨ğğ¬: ğŸ“„ Parses the papers that you upload thanks to LlamaIndexğŸ¦™ (either with LlamaParse or with simpler, local methods) ğŸ“„ Embeds documents both with a sparse and with a dense encoder to enable hybrid search ğŸ“„ Uploads the embeddings to Qdrant âš™ï¸ Activates an Agent based on mistralai/Mistral-Small-24B-Instruct-2501 that will reply to your prompt ğŸ§  Retrieves information relevant to your question from the documents ğŸ§  If no relevant information is found, it searches PubMed and arXiv databases ğŸ§  Returns a grounded answer to your prompt ğ‡ğ¨ğ° ğğ¢ğ ğˆ ğ¦ğšğ§ğšğ ğ ğ­ğ¨ ğ¦ğšğ¤ğ ğ­ğ¡ğ¢ğ¬ ğšğ©ğ©ğ¥ğ¢ğœğšğ­ğ¢ğ¨ğ§ ğ¢ğ§ ğŸ– ğ¡ğ¨ğ®ğ«ğ¬? Three key points: - LlamaIndexğŸ¦™ provides countless integrations with LLM providers, text embedding models and vectorstore...</description><pubDate>Thu, 20 Feb 2025 09:22:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/as-cle-bert/260283430473510</guid></item></channel></rss>