<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸ¤— Just released Rain-100M, an experimental ~97M-parameter Qwen3-style language model trained from random initialization.</title><link>https://huggingface.co/posts/raincandy-u/660562661454335</link><description>ğŸ¤— Just released Rain-100M, an experimental ~97M-parameter Qwen3-style language model trained from random initialization. Repo: raincandy-u/Rain-100M Data: HuggingFaceFW/fineweb-edu , ~3B tokens, English only Tokenizer: custom 16k BPE, context length 4096 Architecture: 12 Transformer layers, hidden size 768, 12 heads, MLP 2048, SiLU, bf16 Rain-100M is a raw base model (not instruction-tuned or safety-aligned), aimed at small-scale research, debugging training pipelines, and CPU/edge experiments. If you run evaluations, finetunes, or visualizations with it, I would be very interested in your results! See translation</description><pubDate>Tue, 27 Jan 2026 09:38:32 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/raincandy-u/660562661454335</guid></item><item><title>Tencent HunyuanImage 3.0-Instruct is seriously impressive</title><link>https://huggingface.co/posts/Benedictat/483321622062325</link><description>Tencent HunyuanImage 3.0-Instruct is seriously impressive skyrocketed to 2nd place globally on the LMArena leaderboard, only trailing Google Nano-banana Pro. What excites me most is its newly launched image editing and multi-image fusion capabilities its semantic understanding is rock-solid this Instruct-following capability basically enables one-sentence end-to-end workflows, delivering a dimensionality-reducing boost in efficiency. Frankly, it nails the pain points of frontline creators: old photo restoration, text modification, even extracting people from multiple images to create group shots. Previously, tweaking the fusion quality took tons of effort, but now the out-of-the-box realism and emotional expression are top-tier zero cheap AI artifacts ğŸ‘‰ Repo: https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&amp;modelId=Hunyuan-Image-3.0-Instruct technical reportï¼šhttps://arxiv.org/abs/2509.23951 See translation</description><pubDate>Tue, 27 Jan 2026 09:38:32 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Benedictat/483321622062325</guid></item><item><title>I'm absolutely stunned by the aesthetics of HunyuanImage-3.0</title><link>https://huggingface.co/posts/kelsend/790764867019535</link><description>I'm absolutely stunned by the aesthetics of HunyuanImage-3.0 The visual effects of this model are simply beyond imagination itâ€™s every bit as good as NanoBanana, no compromise at all. I fine-tuned my micro-scene prompts by adding text overlays and background effects, and its adaptability is truly breathtaking. With just one prompt, you can generate scene posters for any movie or novel. Every detail, from scene design to text style and atmospheric effects, perfectly aligns with the tone of the original material. No forced elements, just seamless, film-grade visual effects that exactly match what I envisioned. ğŸ‘‰ Repo: https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&amp;modelId=Hunyuan-Image-3.0-Instruct See translation</description><pubDate>Tue, 27 Jan 2026 09:38:32 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kelsend/790764867019535</guid></item><item><title>HunyuanImage 3.0-Instruct just dropped</title><link>https://huggingface.co/posts/wangbuer999/572194851679348</link><description>HunyuanImage 3.0-Instruct just dropped fresh -sourceImage 3.0model! Spent 20 mins testing it on a Messi + retro scrambler fusion case Ran on diffusers v0.26.3 + CUDA 12.1 | 8B MoE params (1.3B activated) | zero VRAM issues strength=0.9 Messi #10 kit/tattoo sharp, motoâ€™s rusted metal texture blurred (classic open-source pain) strength=0.7 Moto/cobblestone background crisp, Messiâ€™s jersey details faded completely strength=0.75 + prompt "Blend seamlessly, keep all original details": both subject &amp; background sharp No ControlNet, no manual masking the modelâ€™s chain-of-thought reasoning parses image+prompt first Already outperforms Qwen-Image-Edit 2511 (GSB eval +25.7% on single-image edits) | 100% open-source ğŸ‘‰ Repo: https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&amp;modelId=Hunyuan-Image-3.0-Instruct technical reportï¼šhttps://arxiv.org/abs/2509.23951 Anyone else struggled with strength tweaks for fusion? This fixed it for my Messi+moto case did it work as well for yours?...</description><pubDate>Tue, 27 Jan 2026 09:38:32 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wangbuer999/572194851679348</guid></item><item><title>ovi054/LTX-2-19b-Squish-LoRA</title><link>https://huggingface.co/posts/ovi054/176731931119322</link><description>ovi054/LTX-2-19b-Squish-LoRA âš¡ I trained a Squish LoRA for LTX-2. Upload an image and give prompt "squish it" to get the squish video. Demo output videos are attached. ğŸ‘‰Try it now: ovi054/LTX-2-19b-Squish-LoRA ovi054/ltx-2-Audio-to-Video See translation</description><pubDate>Tue, 27 Jan 2026 09:38:32 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/176731931119322</guid></item><item><title>ğŸ‘€Just published a first-look at Tencent HY-Image-v3.0-I2I!</title><link>https://huggingface.co/posts/imnotkitty/991984513173348</link><description>ğŸ‘€Just published a first-look at Tencent HY-Image-v3.0-I2I! Tested its multi-image fusion and single-reference consistency. The results on complex prompts are quite impressive. Whatâ€™s the most creative image task youâ€™d give it? ğŸ‘‰ Read the full analysis: https://huggingface.co/blog/imnotkitty/tencent-hy-image-v30-i2i See translation</description><pubDate>Tue, 27 Jan 2026 09:38:32 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/imnotkitty/991984513173348</guid></item><item><title>SecureCode: security-aware code models (3Bâ€“20B), trained for review + remediation</title><link>https://huggingface.co/posts/scthornton/483996346337767</link><description>SecureCode: security-aware code models (3Bâ€“20B), trained for review + remediation Iâ€™ve been frustrated by how often code assistants recommend patterns that pass tests but fail security review (e.g., string-built SQL, brittle auth logic, unsafe parsing, insecure defaults, etc.). So I built **SecureCode**: a collection of **8 code models (3B â†’ 20B)** trained to behave more like a security reviewer. What you should expect from SecureCode: - identify likely vuln patterns and explain *why* theyâ€™re risky - outline plausible abuse paths (defensive framing) - propose a secure rewrite (drop-in where possible) - include defense-in-depth guidance + regression tests/checks Links: - **Models:** https://huggingface.co/collections/scthornton/securecode - **Dataset:** scthornton/securecode-v2 - **Paper:** https://arxiv.org/html/2512.18542v1 SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models (2512.18542) **How to test it (copy/paste prompt):** &gt; You are a...</description><pubDate>Tue, 27 Jan 2026 09:38:32 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/scthornton/483996346337767</guid></item><item><title>Weâ€™ve released two conversational speech datasets from oto on Hugging Face ğŸ¤—</title><link>https://huggingface.co/posts/consome2/284818675921006</link><description>Weâ€™ve released two conversational speech datasets from oto on Hugging Face ğŸ¤— Both are based on real, casual, full-duplex conversations, but with slightly different focuses. Dataset 1: Processed / curated subset otoearth/otoSpeech-full-duplex-processed-141h * Full-duplex, spontaneous multi-speaker conversations * Participants filtered for high audio quality * PII removal and audio enhancement applied * Designed for training and benchmarking S2S or dialogue models Dataset 2: Larger raw(er) release otoearth/otoSpeech-full-duplex-280h * Same collection pipeline, with broader coverage * More diversity in speakers, accents, and conversation styles * Useful for analysis, filtering, or custom preprocessing experiments We intentionally split the release to support different research workflows: clean and ready-to-use vs. more exploratory and research-oriented use. The datasets are currently private, but weâ€™re happy to approve access requests â€” feel free to request access if youâ€™re interested....</description><pubDate>Tue, 27 Jan 2026 09:38:32 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/consome2/284818675921006</guid></item><item><title>ğŸ”¥ SWE-Pruner can save up to 40% of your Claude Code cost without sacrificing performance. Try it out!</title><link>https://huggingface.co/posts/YerbaPage/493697712808947</link><description>ğŸ”¥ SWE-Pruner can save up to 40% of your Claude Code cost without sacrificing performance. Try it out! ğŸ“š SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents (2601.16746) ğŸ’» https://github.com/Ayanami1314/swe-pruner Drawing inspiration from how human programmers â€œselectively skimâ€ source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. See translation</description><pubDate>Tue, 27 Jan 2026 09:38:32 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/493697712808947</guid></item><item><title>ğŸ“¢ New release! World_events Dataset now available featuring global events spanning 2023 through 2025</title><link>https://huggingface.co/posts/Reubencf/777019745383250</link><description>ğŸ“¢ New release! World_events Dataset now available featuring global events spanning 2023 through 2025 ğŸŒ https://huggingface.co/collections/Reubencf/world-events ğŸš€ 2026 dataset dropping soon See translation</description><pubDate>Tue, 27 Jan 2026 09:38:32 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Reubencf/777019745383250</guid></item></channel></rss>