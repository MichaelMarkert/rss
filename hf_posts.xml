<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>2025: The Year of Agents.</title><link>https://huggingface.co/posts/daavoo/629869039060445</link><description>2025: The Year of Agents. 2026: The Year of Local Agents? Relying on cloud-hosted LLMs is often overkill. While frontier models still lead in complex coding, local models are now more than capable of handling many agentic workflowsâ€”with zero latency and total privacy. To help bridge the gap between local inference and usable agents, Iâ€™m releasing agent.cpp: https://github.com/mozilla-ai/agent.cpp It provides minimal, high-performance building blocks for agents in C++, built directly around the awesome llama.cpp ecosystem. Stop sending your data to a remote API. Start building and running agents on your own hardware. See translation</description><pubDate>Sun, 21 Dec 2025 13:31:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/daavoo/629869039060445</guid></item><item><title>Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... ğŸ¤—ğŸ”¥</title><link>https://huggingface.co/posts/prithivMLmods/787095126804028</link><description>Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... ğŸ¤—ğŸ”¥ âœ¨ TRELLIS.2-Text-to-3D [Demo]: prithivMLmods/TRELLIS.2-Text-to-3D âœ¨ Multimodal Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations âœ¨ Github: https://github.com/PRITHIVSAKTHIUR/TRELLIS.2-Text-to-3D To know more about it, visit the app page or the respective model page! See translation</description><pubDate>Sun, 21 Dec 2025 13:31:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/787095126804028</guid></item><item><title>Next week, we will release full documentation for the SO ARM 101 with a parallel gripper, featuring leader and follower arms and support for widely used stereo cameras.</title><link>https://huggingface.co/posts/branikita/131682304514806</link><description>Next week, we will release full documentation for the SO ARM 101 with a parallel gripper, featuring leader and follower arms and support for widely used stereo cameras. See translation</description><pubDate>Sun, 21 Dec 2025 13:31:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/131682304514806</guid></item><item><title>looks like the best way to incorporate truth in AI is to use some kind of RAG.</title><link>https://huggingface.co/posts/etemiz/592416201142301</link><description>looks like the best way to incorporate truth in AI is to use some kind of RAG. what are the state of the art ways to consume knowledge graphs? and what is the best way to build a knowledge graph using AI? See translation</description><pubDate>Sun, 21 Dec 2025 13:31:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/592416201142301</guid></item><item><title>Google releases FunctionGemma, a new 270M parameter model that runs on just 0.5 GB RAM.âœ¨</title><link>https://huggingface.co/posts/danielhanchen/264398594064230</link><description>Google releases FunctionGemma, a new 270M parameter model that runs on just 0.5 GB RAM.âœ¨ Built for tool-calling, run locally on your phone at 50+ tokens/s, or fine-tune with Unsloth &amp; deploy to your phone. GGUF: unsloth/functiongemma-270m-it-GGUF Docs + Notebook: https://docs.unsloth.ai/models/functiongemma See translation</description><pubDate>Sun, 21 Dec 2025 13:31:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/264398594064230</guid></item><item><title>Introducing demos for new SOTA models from AI2: SAGE-MM (Smart Any-Horizon Agents for Long-Video Reasoning) and Molmo-2, an open vision-language model that supports multi-image (QA and pointing) and video (QA, pointing, and tracking). The respective demo-related collections are listed below. ğŸƒğŸ”¥</title><link>https://huggingface.co/posts/prithivMLmods/761836377624422</link><description>Introducing demos for new SOTA models from AI2: SAGE-MM (Smart Any-Horizon Agents for Long-Video Reasoning) and Molmo-2, an open vision-language model that supports multi-image (QA and pointing) and video (QA, pointing, and tracking). The respective demo-related collections are listed below. ğŸƒğŸ”¥ âœ¨ SAGE-MM [Video-Reasoning]: prithivMLmods/SAGE-MM-Video-Reasoning âœ¨ Molmo2 [Demo]: prithivMLmods/Molmo2-HF-Demo ğŸƒ GitHub[SAGE-MM]: https://github.com/PRITHIVSAKTHIUR/SAGE-MM-Video-Reasoning ğŸƒ GitHub[Molmo2]: https://github.com/PRITHIVSAKTHIUR/Molmo2-HF-Demo ğŸƒ Multimodal Implementations: https://huggingface.co/collections/prithivMLmods/multimodal-implementations To know more about it, visit the app page or the respective model page! See translation</description><pubDate>Sun, 21 Dec 2025 13:31:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/761836377624422</guid></item><item><title>æˆ‘å³å°†è¾¾åˆ°å…¬å…±å­˜å‚¨ç©ºé—´ä¸Šé™ã€‚æˆ‘å‘ç°æˆ‘çš„ä»“åº“ John1604/Kimi-K2-Thinking-q6K-gguf æ²¡æœ‰è·å¾—è¶³å¤Ÿçš„ä¸‹è½½é‡ï¼Œå‡ ä¹å ç”¨äº† 1T å­˜å‚¨ç©ºé—´ã€‚å°½ç®¡æˆ‘å–œçˆ± Kimi K2 çš„æ€è€ƒæ–¹å¼ï¼Œä½†å¯èƒ½ä¸å¾—ä¸åˆ é™¤è¿™ä¸ªæ¨¡å‹ã€‚å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªçœŸæ­£çš„å¼€æº 1T LLMï¼Œä¸ä»»ä½•å‰æ²¿çš„ LLM æ¨¡å‹ç›¸åª²ç¾ã€‚åœ¨ AI ç«äº‰ä¸­ï¼Œç¾å›½æœ‰å››å®¶å…¬å¸æ‹¥æœ‰1T+æ¨¡å‹ï¼šxAI,  OpenAI, è°·æ­Œå’ŒAnthropologieã€‚ä¸­å›½ä¹Ÿæœ‰å››å®¶å…¬å¸æ‹¥æœ‰1T+æ¨¡å‹ï¼šé˜¿é‡Œå·´å·´, Kimi, DeepSeekå’ŒGLMã€‚ç›®å‰åŒæ–¹åŠ¿å‡åŠ›æ•Œã€‚</title><link>https://huggingface.co/posts/John1604/712509372180068</link><description>æˆ‘å³å°†è¾¾åˆ°å…¬å…±å­˜å‚¨ç©ºé—´ä¸Šé™ã€‚æˆ‘å‘ç°æˆ‘çš„ä»“åº“ John1604/Kimi-K2-Thinking-q6K-gguf æ²¡æœ‰è·å¾—è¶³å¤Ÿçš„ä¸‹è½½é‡ï¼Œå‡ ä¹å ç”¨äº† 1T å­˜å‚¨ç©ºé—´ã€‚å°½ç®¡æˆ‘å–œçˆ± Kimi K2 çš„æ€è€ƒæ–¹å¼ï¼Œä½†å¯èƒ½ä¸å¾—ä¸åˆ é™¤è¿™ä¸ªæ¨¡å‹ã€‚å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªçœŸæ­£çš„å¼€æº 1T LLMï¼Œä¸ä»»ä½•å‰æ²¿çš„ LLM æ¨¡å‹ç›¸åª²ç¾ã€‚åœ¨ AI ç«äº‰ä¸­ï¼Œç¾å›½æœ‰å››å®¶å…¬å¸æ‹¥æœ‰1T+æ¨¡å‹ï¼šxAI, OpenAI, è°·æ­Œå’ŒAnthropologieã€‚ä¸­å›½ä¹Ÿæœ‰å››å®¶å…¬å¸æ‹¥æœ‰1T+æ¨¡å‹ï¼šé˜¿é‡Œå·´å·´, Kimi, DeepSeekå’ŒGLMã€‚ç›®å‰åŒæ–¹åŠ¿å‡åŠ›æ•Œã€‚ I'm about to reach my public storage limit. I've discovered that my repository John1604/Kimi-K2-Thinking-q6K-gguf isn't getting enough downloads and is nearly consuming 1TB of storage. While I love Kimi K2's way of thinking, I have to delete this model because it's a true open-source 1TB LLM, comparable to any cutting-edge LLM model. In the AI â€‹â€‹race, four US companies have 1TB+ models: xAI, OpenAI, Google, and Anthropic. China also has four companies with 1TB+ models: Alibaba, Kimi, DeepSeek, and GLM. Currently, the two sides are evenly matched. Only American team and Chinese team have LLM with 1T+ parameters. Let's cheer for them to reach AGI in next 5 to 10 years. Maybe a 64T chinese model will do it -- Human and cat brain...</description><pubDate>Sun, 21 Dec 2025 13:31:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/John1604/712509372180068</guid></item><item><title>ğŸ”¥Check out Project Los Angeles new SOTA searchable MIDI dataset! ğŸ”¥</title><link>https://huggingface.co/posts/projectlosangeles/440962027896970</link><description>ğŸ”¥Check out Project Los Angeles new SOTA searchable MIDI dataset! ğŸ”¥ projectlosangeles/Discover-MIDI-Dataset The dataset features over 6.74M+ unique searchable MIDIs and is tailored for MIDI music discovery and symbolic music AI! If you like the dataset, pleaseâ¤ï¸ Sincerely, Alex Project Los Angeles Tegridy Code 2025 See translation</description><pubDate>Sun, 21 Dec 2025 13:31:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/projectlosangeles/440962027896970</guid></item><item><title>NEW MODEL:</title><link>https://huggingface.co/posts/unmodeled-tyler/429910399522120</link><description>NEW MODEL: vanta-research/scout-8b VANTA Research is excited to share our new model, Scout-8B! This iteration of Scout is based on the RNJ-1 Instruct architecture from Essential AI, and not only improves but expands on the capabilities from vanta-research/scout-4b Scout is specifically designed for: Tactical Intelligence Analysis - Systematic problem decomposition - Structured reconnaissance approach - Data-driven assessment methodology Operational Planning - Multi-phase operation planning - Risk assessment and mitigation - Resource allocation guidance Technical Assessment - Architecture evaluation and analysis - Performance optimization recommendations - Security perimeter assessment This model is great for anyone that works in security, IT, DevOps, or anyone looking for a unique, but functional AI collaborator. Check it out! See translation</description><pubDate>Sun, 21 Dec 2025 13:31:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/429910399522120</guid></item><item><title>Introducing PTS Visualizer - an interactive tool for exploring how language models reason!</title><link>https://huggingface.co/posts/codelion/327583349449116</link><description>Introducing PTS Visualizer - an interactive tool for exploring how language models reason! Visualize pivotal tokens, thought anchors, and reasoning circuits. See which tokens and sentences significantly impact success probability, explore embedding clusters, and trace reasoning step-by-step. Try it: codelion/pts-visualizer Explore PTS datasets: - Qwen3-0.6B: codelion/Qwen3-0.6B-pts - DeepSeek-R1: codelion/DeepSeek-R1-Distill-Qwen-1.5B-pts Or upload your own JSONL files! GitHub: https://github.com/codelion/pts See translation</description><pubDate>Sun, 21 Dec 2025 13:31:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/327583349449116</guid></item></channel></rss>