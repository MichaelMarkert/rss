<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Say hello to</title><link>https://huggingface.co/posts/Wauplin/921235032674409</link><description>Say hello to hf : a faster, friendlier Hugging Face CLI âœ¨ We are glad to announce a long-awaited quality-of-life improvement: the Hugging Face CLI has been officially renamed from huggingface-cli to hf! So... why this change? Typing huggingface-cli constantly gets old fast. More importantly, the CLIâ€™s command structure became messy as new features were added over time (upload, download, cache management, repo management, etc.). Renaming the CLI is a chance to reorganize commands into a clearer, more consistent format. We decided not to reinvent the wheel and instead follow a well-known CLI pattern: hf &lt;resource&gt; &lt;action&gt;. Isn't hf auth login easier to type and remember? The full rationale, implementation details, and migration notes are in the blog post: https://huggingface.co/blog/hf-cli See translation</description><pubDate>Sat, 26 Jul 2025 17:23:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Wauplin/921235032674409</guid></item><item><title>I run Qwen3-Coder 480B locally on my Z8, with a 1-million token context window. Itâ€™s the equivalent of parallel-parking a Nimitz-class carrier in a kiddie pool. Thanks to whatever dark pact the llama.cpp, CUDA, and kernel folks signed, hybrid inferencing + VRAMâ†”RAM offload let me stream the modelâ€™s synapses across Xeon, RAM, and four lonely A6000s without summoning either the OOM killer or a small house fire.</title><link>https://huggingface.co/posts/mitkox/940300076081193</link><description>I run Qwen3-Coder 480B locally on my Z8, with a 1-million token context window. Itâ€™s the equivalent of parallel-parking a Nimitz-class carrier in a kiddie pool. Thanks to whatever dark pact the llama.cpp, CUDA, and kernel folks signed, hybrid inferencing + VRAMâ†”RAM offload let me stream the modelâ€™s synapses across Xeon, RAM, and four lonely A6000s without summoning either the OOM killer or a small house fire. See translation</description><pubDate>Sat, 26 Jul 2025 17:23:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/940300076081193</guid></item><item><title>Introducing Voxtral WebGPU: State-of-the-art audio transcription directly in your browser! ğŸ¤¯</title><link>https://huggingface.co/posts/Xenova/793837995432659</link><description>Introducing Voxtral WebGPU: State-of-the-art audio transcription directly in your browser! ğŸ¤¯ ğŸ—£ï¸ Transcribe videos, meeting notes, songs and more ğŸ” Runs on-device, meaning no data is sent to a server ğŸŒ Multilingual (8 languages) ğŸ¤— Completely free (forever) &amp; open source That's right, we're running Mistral's new Voxtral-Mini-3B model 100% locally in-browser on WebGPU, powered by Transformers.js and ONNX Runtime Web! ğŸ”¥ Try it out yourself! ğŸ‘‡ webml-community/Voxtral-WebGPU See translation</description><pubDate>Sat, 26 Jul 2025 17:23:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Xenova/793837995432659</guid></item><item><title>longer context doesn't generate better responses. it can even hurt your llm/agent.  1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it.</title><link>https://huggingface.co/posts/hesamation/830297477341251</link><description>longer context doesn't generate better responses. it can even hurt your llm/agent. 1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it. here are 4 types of context failure and why each one happens: 1. context poisoning: if hallucination finds its way into your context, the agent will rely on that false information to make its future moves. for example if the agent hallucinates about the "task description", all of its planning to solve the task would also be corrupt. 2. context distraction: when the context becomes too bloated, the model focuses too much on it rather than come up with novel ideas or to follow what it has learned during training. as Gemini 2.5 Pro technical report points out, as context grows significantly from 100K tokens, "the agent showed a tendency toward favoring repeating actions from its vast history rather than synthesizing novel plans". 3. context confusion: everyone lost it when MCPs became popular, it...</description><pubDate>Sat, 26 Jul 2025 17:23:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/830297477341251</guid></item><item><title>ğŸš€ Deca 3 Ultra Alpha is coming in the next 72 hours! ğŸš€</title><link>https://huggingface.co/posts/Blazgo/275361717193984</link><description>ğŸš€ Deca 3 Ultra Alpha is coming in the next 72 hours! ğŸš€ We're on the verge of something monumental. Right now, we're in the final stages of testing, and we're about to drop a game-changing milestone in the open-source AI community. ğŸ‰ In just two weeks, we've managed to almost 4x the size of the largest open-source LLM at that time (and we are still 2.6x bigger than the largest LLM). This is unprecedented and a testament to the power of collaboration, innovation, and the relentless pursuit of pushing AI to its limits. The future of open-source AI is now. Stay tuned for the release â€“ weâ€™re just getting started. - Model testing finishes: 24hrs from now - Model gets uploaded: 30hrs from now - Related code/inference stack gets published: 70-90hrs from now See translation</description><pubDate>Sat, 26 Jul 2025 17:23:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Blazgo/275361717193984</guid></item><item><title>Big respect to the Qwen team! They just dropped another modelğŸ”¥</title><link>https://huggingface.co/posts/AdinaY/243707766122533</link><description>Big respect to the Qwen team! They just dropped another modelğŸ”¥ Qwen3-235B-A22B-Thinking-2507 ğŸ§  new reasoning model by Qwen Qwen/Qwen3-235B-A22B-Thinking-2507 âœ¨ 235B total / 22B active (8 experts) âœ¨ 256K context window âœ¨ Agent-ready with tool use &amp; &lt;think&gt; reasoning mode Hope the team gets some well-deserved rest this weekend after all the massive releases ğŸ™Œ See translation</description><pubDate>Sat, 26 Jul 2025 17:23:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/243707766122533</guid></item><item><title>Excited to introduce the new experimental model "Qwen2.5-VL-7B-Abliterated-Caption-it", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.ğŸ§ªğŸ¤—</title><link>https://huggingface.co/posts/prithivMLmods/432897219160306</link><description>Excited to introduce the new experimental model "Qwen2.5-VL-7B-Abliterated-Caption-it", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.ğŸ§ªğŸ¤— âœ¨ Try the demo here : prithivMLmods/Qwen2.5-VL âœ¨ Qwen2.5-VL-7B-Abliterated-Caption-it : prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it âœ¨ Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 âœ¨ Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Sat, 26 Jul 2025 17:23:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/432897219160306</guid></item><item><title>NEW ARTICLE: "Detecting Beyond Sight: Building AI-Enabled SAR Intelligence with Synthetic Data"</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/822376760397275</link><description>NEW ARTICLE: "Detecting Beyond Sight: Building AI-Enabled SAR Intelligence with Synthetic Data" Synthetic Aperture Radar (SAR) reveals what optical sensors canâ€™t. AI can turn that information into actionable intelligenceâ€”but only with the right training data. In our latest blog, we explore how Falconâ€™s new virtual SAR sensor solves the SAR data bottleneck for AI development. As the newest addition to Falconâ€™s sensor library, it models radar returns with precisionâ€”including azimuth, range resolution, signal intensity, and noise. This Falcon-specific, GPU-accelerated raytraced SAR model is exposed via Falconâ€™s Python API, giving teams precise, control over radar wave propagation and enabling physically grounded, highly customizable, and user-friendly SAR simulation. The result? High-fidelity, automatically labeled synthetic SAR imagery from any scenarioâ€”on demand. No custom setup. No external workflows. Just mission-ready data for building AI models across defense, disaster response,...</description><pubDate>Sat, 26 Jul 2025 17:23:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/822376760397275</guid></item><item><title>Qwen is on fire this week ğŸ”¥</title><link>https://huggingface.co/posts/AdinaY/225572236442446</link><description>Qwen is on fire this week ğŸ”¥ They just released Qwen3-MT ğŸŒ a translation model supports 92 languages. Demo is available on the hub. Qwen/Qwen3-MT-Demo âœ¨ Highly Customizable: Supports custom terms, domain prompts, and translation memory for accurate, context-aware results. âœ¨ Fast and affordable: $0.5 per million tokens. See translation</description><pubDate>Sat, 26 Jul 2025 17:23:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/225572236442446</guid></item><item><title>Excuse the lag, it's from the real-time inference from the webcam ğŸ‘€ . Did you know that YOLOv11 added Streamlit for live object detection straight from your webcam?</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/878234498759324</link><description>Excuse the lag, it's from the real-time inference from the webcam ğŸ‘€ . Did you know that YOLOv11 added Streamlit for live object detection straight from your webcam? ğŸ“¸ Learn about live inference with YOLOv11 and the Streamlit Application - https://docs.ultralytics.com/guides/streamlit-live-inference/ ğŸ¥« Join the mentioned Kaggle competition here - https://www.kaggle.com/competitions/multi-class-object-detection-challenge/overview ğŸ‘€ Checkout Duality AI - https://www.duality.ai/?utm_source=hf&amp;utm_medium=post&amp;utm_campaign=video ğŸ«¡ Checkout 3LC.AI - https://3lc.ai/ See translation</description><pubDate>Sat, 26 Jul 2025 17:23:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/878234498759324</guid></item></channel></rss>