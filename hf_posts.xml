<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The Hugging Face agents course is finally out!</title><link>https://huggingface.co/posts/burtenshaw/457613029588941</link><description>The Hugging Face agents course is finally out! ğŸ‘‰ https://huggingface.co/agents-course This first unit of the course sets you up with all the fundamentals to become a pro in agents. - What's an AI Agent? - What are LLMs? - Messages and Special Tokens - Understanding AI Agents through the Thought-Action-Observation Cycle - Thought, Internal Reasoning and the Re-Act Approach - Actions, Enabling the Agent to Engage with Its Environment - Observe, Integrating Feedback to Reflect and Adapt See translation</description><pubDate>Fri, 14 Feb 2025 05:19:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/457613029588941</guid></item><item><title>Runway Gen-3 Alpha: The Style and Coherence Champion</title><link>https://huggingface.co/posts/jasoncorkill/476446672223675</link><description>Runway Gen-3 Alpha: The Style and Coherence Champion Runway's latest video generation model, Gen-3 Alpha, is something special. It ranks #3 overall on our text-to-video human preference benchmark, but in terms of style and coherence, it outperforms even OpenAI Sora. However, it struggles with alignment, making it less predictable for controlled outputs. We've released a new dataset with human evaluations of Runway Gen-3 Alpha: Rapidata's text-2-video human preferences dataset. If you're working on video generation and want to see how your model compares to the biggest players, we can benchmark it for you. ğŸš€ DM us if youâ€™re interested! Dataset: Rapidata/text-2-video-human-preferences-runway-alpha See translation</description><pubDate>Fri, 14 Feb 2025 05:19:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/476446672223675</guid></item><item><title>"ğŸ®ğŸ¬ğŸ®ğŸ± ğ˜„ğ—¶ğ—¹ğ—¹ ğ—¯ğ—² ğ˜ğ—µğ—² ğ˜†ğ—²ğ—®ğ—¿ ğ—¼ğ—³ ğ—”ğ—œ ğ—®ğ—´ğ—²ğ—»ğ˜ğ˜€": this statement has often been made, here are numbers to support it.</title><link>https://huggingface.co/posts/m-ric/116861695030454</link><description>"ğŸ®ğŸ¬ğŸ®ğŸ± ğ˜„ğ—¶ğ—¹ğ—¹ ğ—¯ğ—² ğ˜ğ—µğ—² ğ˜†ğ—²ğ—®ğ—¿ ğ—¼ğ—³ ğ—”ğ—œ ğ—®ğ—´ğ—²ğ—»ğ˜ğ˜€": this statement has often been made, here are numbers to support it. I've plotted the progress of AI agents on GAIA test set, and it seems they're headed to catch up with the human baseline in early 2026. And that progress is still driven mostly by the improvement of base LLMs: progress would be even faster with fine-tuned agentic models. See translation</description><pubDate>Fri, 14 Feb 2025 05:19:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/116861695030454</guid></item><item><title>I've completed the first unit of the just-launched Hugging Face Agents Course. I would highly recommend it, even for experienced builders, because it is a great walkthrough of the smolagents library and toolkit.</title><link>https://huggingface.co/posts/ZennyKenny/584467772865203</link><description>I've completed the first unit of the just-launched Hugging Face Agents Course. I would highly recommend it, even for experienced builders, because it is a great walkthrough of the smolagents library and toolkit. See translation</description><pubDate>Fri, 14 Feb 2025 05:19:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/584467772865203</guid></item><item><title>Some things are simple</title><link>https://huggingface.co/posts/etemiz/440192103698875</link><description>Some things are simple See translation</description><pubDate>Fri, 14 Feb 2025 05:19:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/440192103698875</guid></item><item><title>Toward the end of last year, the Xet team provided an inside look into the foundations of how we plan to enable rapid experimentation and iteration for the AI builders on the Hub:</title><link>https://huggingface.co/posts/jsulz/515376333515070</link><description>Toward the end of last year, the Xet team provided an inside look into the foundations of how we plan to enable rapid experimentation and iteration for the AI builders on the Hub: https://huggingface.co/blog/from-files-to-chunks But it turns out chunks aren't all you need! Our goal is to bring: ğŸš€ Faster uploads â¬ Speedy downloads ğŸ’ª All without sacrificing your workflow To do that, we need the infrastructure and system and design to back it up. As we prepare to roll out the first Xet-backed repositories on the Hub, we wrote up a post explaining the nitty gritty details of the decisions that bring this to life https://huggingface.co/blog/from-chunks-to-blocks Complete with an interactive visualization that shows the power of deduplication in action - taking a 191GB repo to ~97GB and shaving a few hours off upload speeds. The darker each block in the heatmap, the more we dedupe, the less we have to transfer. Clicking on a file's blocks shows all other files that share blocks. Check it...</description><pubDate>Fri, 14 Feb 2025 05:19:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/515376333515070</guid></item><item><title>InspireMusic ğŸµğŸ”¥ an open music generation framework by Alibaba FunAudio Lab</title><link>https://huggingface.co/posts/AdinaY/720327371561767</link><description>InspireMusic ğŸµğŸ”¥ an open music generation framework by Alibaba FunAudio Lab Model: FunAudioLLM/InspireMusic-1.5B-Long Demo: FunAudioLLM/InspireMusic âœ¨ Music, songs, audio - ALL IN ONE âœ¨ High quality audio: 24kHz &amp; 48kHz sampling rates âœ¨ Long-Form Generation: enables extended audio creation âœ¨ Efficient Fine-Tuning: precision (BF16, FP16, FP32) with user-friendly scripts See translation</description><pubDate>Fri, 14 Feb 2025 05:19:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/720327371561767</guid></item><item><title>ğŸ˜Š Panorama X3 Image</title><link>https://huggingface.co/posts/fantos/903293233196599</link><description>ğŸ˜Š Panorama X3 Image an innovative system that leverages a Stable Diffusion XL-based tiling pipeline to generate unique and vibrant panoramic images by applying different prompts to the left, center, and right sections of a single image. Key Features &amp; Strengths Multi-Area Prompt Support Input distinct descriptions for the left, center, and right regions (e.g., "dense forest" for the left, "calm lake" for the center, and "majestic mountains" for the right). This allows the system to seamlessly blend multiple scenes into one stunning panoramic image. ğŸŒ„ Automatic Korean-to-English Translation If your prompt contains Korean text, it will be automatically translated into English before image generation. (For example, "ì•ˆê°œ ë‚€ ì‚°" becomes "Misty mountain") ğŸ”„ This feature ensures that you can effortlessly use both English and Korean prompts. Advanced Tiling Technology The project uses a sophisticated tiling approach that manages overlapping regions to produce natural transitions and high-...</description><pubDate>Fri, 14 Feb 2025 05:19:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fantos/903293233196599</guid></item><item><title>â­ï¸ The AI Energy Score project just launched - this is a game-changer for making informed decisions about AI deployment.</title><link>https://huggingface.co/posts/fdaudens/212771868233348</link><description>â­ï¸ The AI Energy Score project just launched - this is a game-changer for making informed decisions about AI deployment. You can now see exactly how much energy your chosen model will consume, with a simple 5-star rating system. Think appliance energy labels, but for AI. Looking at transcription models on the leaderboard is fascinating: choosing between whisper-tiny or whisper-large-v3 can make a 7x difference. Real-time data on these tradeoffs changes everything. 166 models already evaluated across 10 different tasks, from text generation to image classification. The whole thing is public and you can submit your own models to test. Why this matters: - Teams can pick efficient models that still get the job done - Developers can optimize for energy use from day one - Organizations can finally predict their AI environmental impact If you're building with AI at any scale, definitely worth checking out. ğŸ‘‰ leaderboard: https://lnkd.in/esrSxetj ğŸ‘‰ blog post: https://lnkd.in/eFJvzHi8 Huge...</description><pubDate>Fri, 14 Feb 2025 05:19:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/212771868233348</guid></item><item><title>I am excited to share that Iâ€™ve successfully completed Unit 1: Foundations of Agents in the Hugging Face Agents Course.</title><link>https://huggingface.co/posts/lukmanaj/991201381303884</link><description>I am excited to share that Iâ€™ve successfully completed Unit 1: Foundations of Agents in the Hugging Face Agents Course. Exploring the fundamentals of AI agents has been an insightful journey, and Iâ€™m looking forward to applying these concepts in real-world applications. Big thanks to the Hugging Face team for this amazing learning opportunity! ğŸ¤— Check out the course here: https://huggingface.co/learn/agents-course/ See translation</description><pubDate>Fri, 14 Feb 2025 05:19:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lukmanaj/991201381303884</guid></item></channel></rss>