<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>🧠👁️ Can AI visualize solutions?</title><link>https://huggingface.co/posts/andito/535116877809522</link><description>🧠👁️ Can AI visualize solutions? Humans often solve visual problems by sketching ideas in our minds. What if Vision-Language Models (VLMs) could do something similar, not by generating full images, but by using internal “mental sketches”? That’s the idea behind Mirage, a new framework that empowers VLMs to reason using latent visual tokens. Instead of just thinking in words, Mirage mixes in abstract visual representations that help the model solve complex tasks. These aren't photorealistic images. They're compact, internal representations optimized purely to support reasoning. 🔧 Mirage is trained in two phases: 1) Grounding: It learns to produce latent tokens anchored in real images. 2) Refinement: The model drops the images and learns to generate visual tokens on its own. 📈 And yes, it works! On challenging benchmarks like Visual Spatial Planning, Jigsaw puzzles, and Spatial Attention Tasks, Mirage clearly outperforms GPT-4o and other strong baselines. Smart sketches &gt; empty words....</description><pubDate>Fri, 04 Jul 2025 17:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/andito/535116877809522</guid></item><item><title>If you're using any HF libraries, you should enable the Hub MCP in your agentic coding tool!</title><link>https://huggingface.co/posts/m-ric/145810386001131</link><description>If you're using any HF libraries, you should enable the Hub MCP in your agentic coding tool! The brand new Docs Semantic Search tool is intravenous caffeine supply for Cursor, enables to correct API errors in a few seconds, gj @ mishig ⚡️⚡️ 👉 To enable Hub MCP, head to your account setting, under MCP, and it will give you everything you need! See translation</description><pubDate>Fri, 04 Jul 2025 17:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/145810386001131</guid></item><item><title>‼️Sentence Transformers v5.0 is out! The biggest update yet introduces Sparse Embedding models, encode methods improvements, Router module for asymmetric models &amp; much more. Sparse + Dense = 🔥 hybrid search performance! Details:</title><link>https://huggingface.co/posts/tomaarsen/190568030432786</link><description>‼️Sentence Transformers v5.0 is out! The biggest update yet introduces Sparse Embedding models, encode methods improvements, Router module for asymmetric models &amp; much more. Sparse + Dense = 🔥 hybrid search performance! Details: 1️⃣ Sparse Encoder Models Brand new support for sparse embedding models that generate high-dimensional embeddings (30,000+ dims) where &lt;1% are non-zero: - Full SPLADE, Inference-free SPLADE, and CSR architecture support - 4 new modules, 12 new losses, 9 new evaluators - Integration with @ elastic-co , @ opensearch-project , @ NAVER LABS Europe, @ qdrant , @ IBM , etc. - Decode interpretable embeddings to understand token importance - Hybrid search integration to get the best of both worlds 2️⃣ Enhanced Encode Methods &amp; Multi-Processing - Introduce encode_query &amp; encode_document automatically use predefined prompts - No more manual pool management - just pass device list directly to encode() - Much cleaner and easier to use than the old multi-process approach...</description><pubDate>Fri, 04 Jul 2025 17:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/190568030432786</guid></item><item><title>Updated my HF Space for vibe testing smol VLMs on object detection, visual grounding, keypoint detection &amp; counting! 👓</title><link>https://huggingface.co/posts/sergiopaniego/776509639548053</link><description>Updated my HF Space for vibe testing smol VLMs on object detection, visual grounding, keypoint detection &amp; counting! 👓 🆕 Compare Qwen2.5 VL 3B vs Moondream 2B side-by-side with annotated images &amp; text outputs. Try examples or test your own images! 🏃 📱Space: sergiopaniego/vlm_object_understanding See translation</description><pubDate>Fri, 04 Jul 2025 17:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/776509639548053</guid></item><item><title>🔥 June highlights from China’s open source ecosystem.</title><link>https://huggingface.co/posts/AdinaY/923543336919000</link><description>🔥 June highlights from China’s open source ecosystem. zh-ai-community/june-2025-open-works-from-the-chinese-community-683d66c188f782dc5570ba15 ✨Baidu &amp; MiniMax both launched open foundation models - Baidu: Ernie 4.5 ( from 0.3B -424B ) 🤯 - MiniMax: MiniMax -M1 ( Hybrid MoE reasoning model ) ✨Multimodal AI is moving from fusion to full-stack reasoning: unified Any-to-Any pipelines across text, vision, audio, and 3D - Baidu: ERNIE-4.5-VL-424B - Moonshot AI: Kimi-VL-A3B - Alibaba: Ovis-U1 - BAAI: Video-XL-2/OmniGen2 - AntGroup: Ming-Lite-Omni - Chinese Academy of Science: Stream-Omni - Bytedance: SeedVR2-3B - Tencent: Hunyuan 3D 2.1/ SongGeneration - FishAudio: Openaudio-s1-mini ✨Domain specific models are rapidly emerging - Alibaba DAMO: Lingshu-7B (medical MLLM) - BAAI: RoboBrain (Robotics) ✨ So many small models! - OpenBMB: MiciCPM4 ( on device ) - Qwen: Embedding/Reranker (0.6B) - Alibaba: Ovis-U1-3B - Moonshot AI: Kimi-VL-A3B - Bytedance: SeedVR2-3B See translation</description><pubDate>Fri, 04 Jul 2025 17:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/923543336919000</guid></item><item><title>✅ New Article on Hugging Face: Teaching AI to Learn from Its Own Thinking</title><link>https://huggingface.co/posts/kanaria007/563668515609459</link><description>✅ New Article on Hugging Face: Teaching AI to Learn from Its Own Thinking Title: 🔁 Understanding the Pattern Learning Bridge: Adaptive Learning from Problem-Solving Experience 🔗 Read it here: https://huggingface.co/blog/kanaria007/understanding-the-pattern-learning-bridge Summary: After exploring structural selfhood in the Identity-Construct Protocol, this new piece introduces a next step in cognitive development: learning from one’s own problem-solving patterns. The Pattern Learning Bridge equips AI with the ability to reflect structurally — not just on results, but on *why* certain reasoning paths succeed or fail. This protocol enables agents to: • Log reasoning attempts in a structured format • Analyze success/failure correlations across problem types • Extract reusable frame-jump patterns with confidence scoring • Proactively adapt future reasoning choices It’s not just about having memory — it’s about having *experience*. Key Features: • Detects recurring reasoning traps •...</description><pubDate>Fri, 04 Jul 2025 17:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/563668515609459</guid></item><item><title>🔥 HuggingFace Heatmap Leaderboard</title><link>https://huggingface.co/posts/aiqtech/840192921912390</link><description>🔥 HuggingFace Heatmap Leaderboard Visualizing AI ecosystem activity at a glance aiqtech/Heatmap-Leaderboard 🎯 Introduction A leaderboard that visualizes the vibrant HuggingFace community activity through heatmaps. ✨ Key Features 📊 Real-time Tracking - Model/dataset/app releases from AI labs and developers 🏆 Auto Ranking - Rankings based on activity over the past year 🎨 Responsive UI - Unique colors per organization, mobile optimized ⚡ Auto Updates - Hourly data refresh for latest information 🌍 Major Participants Big Tech: OpenAI, Google, Meta, Microsoft, Apple, NVIDIA AI Startups: Anthropic, Mistral, Stability AI, Cohere, DeepSeek Chinese Companies: Tencent, Baidu, ByteDance, Qwen HuggingFace Official: HuggingFaceH4, HuggingFaceM4, lerobot, etc. Active Developers: prithivMLmods, lllyasviel, multimodalart and many more 🚀 Value Trend Analysis 📈 Real-time open source contribution insights Inspiration 💪 Learn from other developers' activity patterns Ecosystem Growth 🌱 Visualize AI...</description><pubDate>Fri, 04 Jul 2025 17:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aiqtech/840192921912390</guid></item><item><title>The bunch of comparable demos for Multimodal VLMs (excels in OCR, cinematography understanding, spatial reasoning, etc.) now up on the Hub 🤗 — max recent till Jun'25.</title><link>https://huggingface.co/posts/prithivMLmods/861488499348917</link><description>The bunch of comparable demos for Multimodal VLMs (excels in OCR, cinematography understanding, spatial reasoning, etc.) now up on the Hub 🤗 — max recent till Jun'25. ✦ Demo Spaces — &gt; [Nanonets-OCR-s, MonkeyOCR, Typhoon-OCR-7B, SmolDocling] : prithivMLmods/Multimodal-OCR2 &gt; [GLM-4.1v, docscopeOCR-7B, MonkeyOCR, coreOCR-7B] : prithivMLmods/core-OCR &gt; [Camel-Doc-OCR, ViLaSR-7B, OCRFlux-3B, ShotVL-7B] : prithivMLmods/Doc-VLMs-v2-Localization &gt; [SkyCaptioner-V1, SpaceThinker-3B, coreOCR-7B, SpaceOm-3B] : prithivMLmods/VisionScope-R2 &gt; [RolmOCR-7B, Qwen2-VL-OCR-2B, Aya-Vision-8B, Nanonets-OCR-s] : prithivMLmods/Multimodal-OCR &gt; [DREX-062225-7B, Typhoon-OCR-3B, olmOCR-7B-0225, VIREX-062225-7B] : prithivMLmods/Doc-VLMs-OCR &gt; [Cosmos-Reason1-7B, docscopeOCR-7B, Captioner-7B, visionOCR-3B] : prithivMLmods/DocScope-R1 ✦ Space Collection : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See...</description><pubDate>Fri, 04 Jul 2025 17:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/861488499348917</guid></item><item><title>The Chinese Open Source Heatmap is live 🔥</title><link>https://huggingface.co/posts/AdinaY/711789989544718</link><description>The Chinese Open Source Heatmap is live 🔥 You can now track the companies/ research labs/ communities powering China’s open source AI movement. zh-ai-community/model-release-heatmap-zh Some highlights: ✨Giant Tech are investing more in open source. -Alibaba: Full stack open ecosystem -Tecent: Hunyuan image/video/3D -Bytedance: Catching up fast in 2025 -Baidu: New player in open LLM ✨New players emerging post–DeepSeek moment. -Xiaomi -Red Note -Bilibili -MiniMax -Moonshot AI ✨Startup list is shifting fast! Those who find a direction aligned with their strengths are the ones who endure. -DeepSeek -MiniMax -StepFun -Moonshot AI -Zhipu AI -OpenBMB ✨Research Lab &amp; Community are making key contributions. -BAAI -Shanghai AI Lab -OpenMOSS -MAP See translation</description><pubDate>Fri, 04 Jul 2025 17:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/711789989544718</guid></item><item><title>visual reasoning is now in  transformers 🔥</title><link>https://huggingface.co/posts/merve/119393817203316</link><description>visual reasoning is now in transformers 🔥 THUDM/GLM-4.1V-9B-Thinking is just released and merged into transformers, we gave it a vibe test run 🤠 it's very good, comes with 64k context length and MIT license 😍 it supports 4k image tokens and any aspect ratio as well! Notebook: http://colab.research.google.com/drive/1atODIiV57hOZLv16Bjzwd6fwx0yoTorj?usp=sharing Demo: THUDM/GLM-4.1V-9B-Thinking-Demo See translation</description><pubDate>Fri, 04 Jul 2025 17:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/119393817203316</guid></item></channel></rss>