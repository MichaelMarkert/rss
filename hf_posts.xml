<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ModernBERT goes MULTILINGUAL! One of the most requested models I've seen, The Johns Hopkins University's CLSP has trained state-of-the-art massively multilingual encoders using the ModernBERT architecture: mmBERT.</title><link>https://huggingface.co/posts/tomaarsen/906557413568289</link><description>ModernBERT goes MULTILINGUAL! One of the most requested models I've seen, The Johns Hopkins University's CLSP has trained state-of-the-art massively multilingual encoders using the ModernBERT architecture: mmBERT. Model details: - 2 model sizes: - jhu-clsp/mmBERT-small - jhu-clsp/mmBERT-base - Uses the ModernBERT architecture, but with the Gemma2 multilingual tokenizer (so: flash attention, alternating global/local attention, unpadding/sequence packing, etc.) - Maximum sequence length of 8192 tokens, on the high end for encoders - Trained on 1833 languages using DCLM, FineWeb2, and many more sources - 3 training phases: 2.3T tokens pretraining on 60 languages, 600B tokens mid-training on 110 languages, and 100B tokens decay training on all 1833 languages. - Both models are MIT Licensed, and the full datasets and intermediary checkpoints are also publicly released Evaluation details: - Very competitive with ModernBERT at equivalent sizes on English (GLUE, MTEB v2 English after...</description><pubDate>Fri, 12 Sep 2025 09:22:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/906557413568289</guid></item><item><title>ğŸš€ Ever dreamed of training your own Large Language Model from scratch? What if I told you it doesn't require a supercomputer or PhD in ML? ğŸ¤¯</title><link>https://huggingface.co/posts/Abhaykoul/356298508659280</link><description>ğŸš€ Ever dreamed of training your own Large Language Model from scratch? What if I told you it doesn't require a supercomputer or PhD in ML? ğŸ¤¯ Introducing LLM Trainer - the educational framework that makes LLM training accessible to EVERYONE! Whether you're on a CPU-only laptop or scaling to distributed GPUs, we've got you covered. ğŸ’»â¡ï¸ğŸ–¥ï¸ Why LLM Trainer? Because existing tools are either too simplistic (hiding the magic) or too complex (requiring expert knowledge). We bridge the gap with: ğŸ“ Educational transparency - every component built from scratch with clear code ğŸ’» CPU-first approach - start training immediately, no GPU needed ğŸ”§ Full customization - modify anything you want ğŸ“ˆ Seamless scaling - from laptop to cluster without code changes ğŸ¤ HuggingFace integration - works with existing models &amp; tokenizers Key highlights: âœ… Built-in tokenizers (BPE, WordPiece, HF wrappers) âœ… Complete Transformer implementation from scratch âœ… Optimized for CPU training âœ… Advanced features: mixed...</description><pubDate>Fri, 12 Sep 2025 09:22:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/356298508659280</guid></item><item><title>Hunyuan Image 2.1 by Tencent Full Tutorial and 1-Click to Install Ultra Advanced App to Use Locally :</title><link>https://huggingface.co/posts/MonsterMMORPG/330916531424120</link><description>Hunyuan Image 2.1 by Tencent Full Tutorial and 1-Click to Install Ultra Advanced App to Use Locally : https://youtu.be/dNeA5mJ36hA Tutorial video : https://youtu.be/dNeA5mJ36hA Check the below screenshots Hunyuan Image 2.1 just published by Tencent and I have been working on developing the very best app to let you use HunyuanImage-2.1 with easiest and most accurate way. In this tutorial video, I will show you how to literally 1-click to install this model and our app on Windows (locally), Massed Compute (cloud) and RunPod (cloud). The images are all raw 2560x1440 pixels with 8-steps Refiner of Hunyuan Image 2.1 model This model native resolution is 2048x2048 pixels See translation</description><pubDate>Fri, 12 Sep 2025 09:22:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/330916531424120</guid></item><item><title>Hello everyone,</title><link>https://huggingface.co/posts/drvsbrkcn/494150147788454</link><description>Hello everyone, It is my first time using Hugging Face. It is hella nice. Take care. See translation</description><pubDate>Fri, 12 Sep 2025 09:22:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/drvsbrkcn/494150147788454</guid></item><item><title>gpt-oss was possible thanks to new engineering efforts in ğŸ¤— transformers. We just dropped a blog covering them:</title><link>https://huggingface.co/posts/sergiopaniego/319778709690075</link><description>gpt-oss was possible thanks to new engineering efforts in ğŸ¤— transformers. We just dropped a blog covering them: - Kernels from the Hub - MXFP4 Quantization - Tensor &amp; Expert Parallelism - Dynamic Sliding Window &amp; Cache - Continuous Batching &amp; Paged Attention Grab a coffee &amp; dive in! â˜•ï¸ https://huggingface.co/blog/faster-transformers See translation</description><pubDate>Fri, 12 Sep 2025 09:22:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/319778709690075</guid></item><item><title>Smol course has a distinctive approach to teaching post-training, so I'm posting about how itâ€™s different to other post-training courses, including the llm course thatâ€™s already available.</title><link>https://huggingface.co/posts/burtenshaw/833511511767176</link><description>Smol course has a distinctive approach to teaching post-training, so I'm posting about how itâ€™s different to other post-training courses, including the llm course thatâ€™s already available. In short, the smol course is just more direct that any of the other course, and intended for semi-pro post trainers. - Itâ€™s a minimal set of instructions on the core parts. - Itâ€™s intended to bootstrap real projects you're working on. - The material handsover to existing documentation for details - Likewise, it handsover to the LLM course for basics. - Assessment is based on a leaderboard, without reading all the material. To start the smol course, follow here: smol-course See translation</description><pubDate>Fri, 12 Sep 2025 09:22:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/833511511767176</guid></item><item><title>Build something cool with Nano Banana aka Gemini 2.5 Flash Image AIO [All-in-One]. Draw and transform on canvas, edit images, and generate imagesâ€”all in one place!ğŸŒ</title><link>https://huggingface.co/posts/prithivMLmods/195813965636174</link><description>Build something cool with Nano Banana aka Gemini 2.5 Flash Image AIO [All-in-One]. Draw and transform on canvas, edit images, and generate imagesâ€”all in one place!ğŸŒ âœ¦ï¸ Constructed with the Gemini API (GCP). Try it here: https://nano-banana-aio-op72ohwdda-uw.a.run.app/ See translation</description><pubDate>Fri, 12 Sep 2025 09:22:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/195813965636174</guid></item><item><title>Excited to share our Unified Multimodal Models new work Reconstruction Alignment (RecA)! ğŸš€ Just 6 Ã— 80GB A100s Ã— 4.5 hours to boost BAGEL performance across all tasks! Outperforms FLUX-Kontext in image editing capabilities!</title><link>https://huggingface.co/posts/sanaka87/107769277937246</link><description>Excited to share our Unified Multimodal Models new work Reconstruction Alignment (RecA)! ğŸš€ Just 6 Ã— 80GB A100s Ã— 4.5 hours to boost BAGEL performance across all tasks! Outperforms FLUX-Kontext in image editing capabilities! ğŸ“„ Paper: https://alphaxiv.org/abs/2509.07295 ğŸ’» Code: https://github.com/HorizonWind2004/reconstruction-alignment ğŸ¤— HF Models: sanaka87/reca-68ad2176380355a3dcedc068 âœï¸ DEMO: sanaka87/BAGEL-RecA ğŸŒ Project Page: https://reconstruction-alignment.github.io ğŸ”¥ X: https://x.com/XDWang101/status/1965908302581420204 ğŸ“° Zhihu: https://zhuanlan.zhihu.com/p/1947584568187159814 ğŸ¤— HF Daily Paper: Reconstruction Alignment Improves Unified Multimodal Models (2509.07295) âš¡ &lt;10k images &amp; 27 GPU hours (no-arch-changes) â†’ SOTA, surpassing much larger open-source &amp; private models: ğŸ“Š GenEval: 0.73 â†’ 0.90 | ğŸ“Š DPGBench: 80.93 â†’ 88.15 ğŸ–¼ï¸ ImgEdit: 3.38 â†’ 3.75 | ğŸ–Œï¸ GEdit: 6.94 â†’ 7.25 âœ… RecA trains UMMs to reconstruct images from their own visual understanding encoder embeddings â†’ big gains...</description><pubDate>Fri, 12 Sep 2025 09:22:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sanaka87/107769277937246</guid></item><item><title>new smol course</title><link>https://huggingface.co/posts/burtenshaw/724732252831042</link><description>new smol course If youâ€™re building with or learning about post training AI models right now, we have a new FREE and CERTIFIED course. ğŸ”— Follow the org to join in smol-course The course builds on smol course v1 which was the fastest way to learn to train your custom AI models. It now has: - A leaderboard for students to submit models to - Certification based on exams and leaderboards - Prizes based on Leaderboards - Up to date content on TRL and SmolLM3 - Deep integration with the Hubâ€™s compute for model training and evaluation We will release chapters every few weeks, so you can follow the org to stay updated. See translation</description><pubDate>Fri, 12 Sep 2025 09:22:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/724732252831042</guid></item><item><title>Most apps don't have great full-text search over their assets.</title><link>https://huggingface.co/posts/salma-remyx/853424776483426</link><description>Most apps don't have great full-text search over their assets. We've developed an agent to automate the environment building and testing of experimental codebases sourced from arXiv. We push these containerized reproductions daily to Docker Hub: https://hub.docker.com/u/remyxai However, searching for them can be challenging unless you know the specific arXiv ID associated with each paper. We are currently working on implementing a search feature in Remyx, which will make these assets easily discoverable and ready for testing ğŸ” Stay tuned! Discover your next best idea to experiment with here: https://engine.remyx.ai See translation</description><pubDate>Fri, 12 Sep 2025 09:22:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/853424776483426</guid></item></channel></rss>