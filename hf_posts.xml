<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>üöÄ ZeroGPU now supports PyTorch native quantization via</title><link>https://huggingface.co/posts/cbensimon/565026286160860</link><description>üöÄ ZeroGPU now supports PyTorch native quantization via torchao While it hasn‚Äôt been battle-tested yet, Int8WeightOnlyConfig is already working flawlessly in our tests. Let us know if you run into any issues ‚Äî and we‚Äôre excited to see what the community will build! import spaces from diffusers import FluxPipeline from torchao.quantization.quant_api import Int8WeightOnlyConfig, quantize_ pipeline = FluxPipeline.from_pretrained(...).to( 'cuda' ) quantize_(pipeline.transformer, Int8WeightOnlyConfig()) # Or any other component(s) @spaces.GPU def generate ( prompt: str ): return pipeline(prompt).images[ 0 ] See translation</description><pubDate>Fri, 13 Jun 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/cbensimon/565026286160860</guid></item><item><title>ü§ó I'm leading 'Openfree AI', Korea's most prominent AI open-source community. First and foremost, I'd like to express my deepest gratitude for Hugging Face's continuous support and efforts. üíô</title><link>https://huggingface.co/posts/openfree/428786122279500</link><description>ü§ó I'm leading 'Openfree AI', Korea's most prominent AI open-source community. First and foremost, I'd like to express my deepest gratitude for Hugging Face's continuous support and efforts. üíô Our Openfree AI collaborates with various AI communities across Korea, contributing to knowledge sharing and ecosystem development. ü§ù I've been actively promoting the critical importance of Hugging Face as Korea's AI infrastructure backbone, engaging with senior government officials, National Assembly members, university leaders, and media executives to emphasize how Hugging Face represents Korea's AI future at a national policy level. I consider myself a 'voluntary Korean ambassador for Hugging Face'. üá∞üá∑‚ú® Let me share our community's achievements on the Hugging Face platform over the past year: üéØ üöÄ Published hundreds of models and spaces üë• Surpassed 10 million cumulative visitors üìà Achieved 1.7 million Monthly Active Users (MAU) üé® Generated over 1 million images/videos per month These...</description><pubDate>Fri, 13 Jun 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/428786122279500</guid></item><item><title>Has anyone ever backed up a model to a sequential tape drive, or I'm the world first? :D</title><link>https://huggingface.co/posts/csabakecskemeti/762115035937109</link><description>Has anyone ever backed up a model to a sequential tape drive, or I'm the world first? :D Just played around with my retro PC that has got a tape drive‚Äîdid it just because I can. See translation</description><pubDate>Fri, 13 Jun 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/csabakecskemeti/762115035937109</guid></item><item><title>Imagine you could have an Image Arena score equivalent at each checkpoint during training. We released the first version of just that:</title><link>https://huggingface.co/posts/jasoncorkill/871941197791232</link><description>Imagine you could have an Image Arena score equivalent at each checkpoint during training. We released the first version of just that: Crowd-Eval Add one line of code to your training loop and you will have a new real human loss curve in your W&amp;B dashboard. Thousands of real humans from around the world rating your model in real time at the cost of a few dollars per checkpoint is a game changer. Check it out here: https://github.com/RapidataAI/crowd-eval First 5 people to put it in their loop get 100'000 human responses for free! (ping me) See translation</description><pubDate>Fri, 13 Jun 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/871941197791232</guid></item><item><title>üó£Ô∏è üì¢  New article alert!</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/462704497331407</link><description>üó£Ô∏è üì¢ New article alert! "Integrity Threats in AI: When Data Poisoning Undermines Model Effectiveness" from Duality AI is now on HuggingFace here: https://huggingface.co/blog/DualityAI-RebekahBogdanoff/integrity-threats-in-ai Significant threats to AI model performance aren‚Äôt always loud or obvious. Integrity violations‚Äîlike subtle data poisoning attacks‚Äîcan quietly erode your model‚Äôs reliability, long before anyone notices. These attacks can be surprisingly effective with minimal changes to the dataset. At Duality, our work in high-stakes sectors like defense has driven us to tackle this threat head-on. In our latest blog from Duality's Director of Infrastructure and Security at Duality, David Strout, we unpack how data poisoning works, why it‚Äôs so dangerous, and how organizations can secure their AI pipelines with clear provenance, regular performance auditing, and a trusted synthetic data supply chain. Whether you're building AI models for finance, healthcare, manufacturing, or...</description><pubDate>Fri, 13 Jun 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/462704497331407</guid></item><item><title>Recently, I've been focusing my learning on the following topics:</title><link>https://huggingface.co/posts/a-r-r-o-w/231008365980283</link><description>Recently, I've been focusing my learning on the following topics: - Pytorch internals, specifically the inductor system (roughly ~1 month of experience) - Triton internals (~8 moe) - CUDA (~3 moe) - Understanding fusion patterns in compilers and how to improve them (~1 moe) - Parallelism strategies for large scale inference optimization (~6-7 moe) I thought it would be nice to document it somewhere for no particular reason. Maybe someone will find it useful? It's also because I want to get into the habit of writing, but had no motivation to do so. Maybe writing short informal posts will help build the habit. Since I don't have a personal site, and don't plan to create one in the near future, I think HF posts are best suited for short and informal documentation to share my little discoveries and learnings. If you're interested, strap in! First post in this series will be on basic study of Pytorch's float32 matmuls and their Triton implementation (nothing much, just the tutorial...</description><pubDate>Fri, 13 Jun 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/a-r-r-o-w/231008365980283</guid></item><item><title>Mistral releases Magistral, their new reasoning models! üî•</title><link>https://huggingface.co/posts/danielhanchen/426556210957370</link><description>Mistral releases Magistral, their new reasoning models! üî• GGUFs to run: unsloth/Magistral-Small-2506-GGUF Magistral-Small-2506 excels at mathematics and coding. You can run the 24B model locally with just 32GB RAM by using our Dynamic GGUFs. See translation</description><pubDate>Fri, 13 Jun 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/426556210957370</guid></item><item><title>this repo is gold! a collection of LLM apps with multi-agents, MCP, RAG and so much more.</title><link>https://huggingface.co/posts/hesamation/842061188959684</link><description>this repo is gold! a collection of LLM apps with multi-agents, MCP, RAG and so much more. the best way to learn is by building, and this repo provides the blueprint. Repo: https://github.com/Shubhamsaboo/awesome-llm-apps See translation</description><pubDate>Fri, 13 Jun 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/842061188959684</guid></item><item><title>Super excited to release Autotrain MCP. This is an MCP server for training AI models, so you can use your AI tools to train your AI models ü§Ø.</title><link>https://huggingface.co/posts/burtenshaw/642764546410723</link><description>Super excited to release Autotrain MCP. This is an MCP server for training AI models, so you can use your AI tools to train your AI models ü§Ø. üîó burtenshaw/autotrain-mcp Use this MCP server with tools like Claude Desktop, Cursor, VSCode, or Continue to do this: - Define an ML problem like Image Classification, LLM fine-tuning, Text Classification, etc. - The AI can retrieve models and datasets from the hub using the hub MCP. - Training happens on a Hugging Face space, so no worries about hardware restraints. - Models are pushed to the hub to be used inference tools like Llama.cpp, vLLM, MLX, etc. - Built on top of the AutoTrain library, so it has full integration with transformers and other libraries. Everything is still under active development, but I‚Äôm super excited to hear what people build, and I‚Äôm open to contributions! See translation</description><pubDate>Fri, 13 Jun 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/642764546410723</guid></item><item><title>Me: This function is too slow. Find a faster algorithm.</title><link>https://huggingface.co/posts/Narsil/207015264611430</link><description>Me: This function is too slow. Find a faster algorithm. Cursor: Hold my beer. Me: *Slacking off with colleagues* Cursor: Ping. Me: ü§Ø See translation</description><pubDate>Fri, 13 Jun 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Narsil/207015264611430</guid></item></channel></rss>