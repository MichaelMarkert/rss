<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Some things are simple</title><link>https://huggingface.co/posts/etemiz/440192103698875</link><description>Some things are simple See translation</description><pubDate>Sat, 15 Feb 2025 09:20:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/440192103698875</guid></item><item><title>InspireMusic ğŸµğŸ”¥ an open music generation framework by Alibaba FunAudio Lab</title><link>https://huggingface.co/posts/AdinaY/720327371561767</link><description>InspireMusic ğŸµğŸ”¥ an open music generation framework by Alibaba FunAudio Lab Model: FunAudioLLM/InspireMusic-1.5B-Long Demo: FunAudioLLM/InspireMusic âœ¨ Music, songs, audio - ALL IN ONE âœ¨ High quality audio: 24kHz &amp; 48kHz sampling rates âœ¨ Long-Form Generation: enables extended audio creation âœ¨ Efficient Fine-Tuning: precision (BF16, FP16, FP32) with user-friendly scripts See translation</description><pubDate>Sat, 15 Feb 2025 09:20:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/720327371561767</guid></item><item><title>Your weekly recap of open AI is here, and it's packed with models!</title><link>https://huggingface.co/posts/merve/171433424068357</link><description>Your weekly recap of open AI is here, and it's packed with models! merve/feb-14-releases-67af876b404cc27c6d837767 ğŸ‘€ Multimodal &gt; OpenGVLab released InternVideo 2.5 Chat models, new video LMs with long context &gt; AIDC released Ovis2 model family along with Ovis dataset, new vision LMs in different sizes (1B, 2B, 4B, 8B, 16B, 34B), with video and OCR support &gt; ColQwenStella-2b is a multilingual visual retrieval model that is sota in it's size &gt; Hoags-2B-Exp is a new multilingual vision LM with contextual reasoning, long context video understanding ğŸ’¬ LLMs A lot of math models! &gt; Open-R1 team released OpenR1-Math-220k large scale math reasoning dataset, along with Qwen2.5-220K-Math fine-tuned on the dataset, OpenR1-Qwen-7B &gt; Nomic AI released new Nomic Embed multilingual retrieval model, a MoE with 500 params with 305M active params, outperforming other models &gt; DeepScaleR-1.5B-Preview is a new DeepSeek-R1-Distill fine-tune using distributed RL on math &gt; LIMO is a new fine-tune of...</description><pubDate>Sat, 15 Feb 2025 09:20:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/171433424068357</guid></item><item><title>â­ï¸ The AI Energy Score project just launched - this is a game-changer for making informed decisions about AI deployment.</title><link>https://huggingface.co/posts/fdaudens/212771868233348</link><description>â­ï¸ The AI Energy Score project just launched - this is a game-changer for making informed decisions about AI deployment. You can now see exactly how much energy your chosen model will consume, with a simple 5-star rating system. Think appliance energy labels, but for AI. Looking at transcription models on the leaderboard is fascinating: choosing between whisper-tiny or whisper-large-v3 can make a 7x difference. Real-time data on these tradeoffs changes everything. 166 models already evaluated across 10 different tasks, from text generation to image classification. The whole thing is public and you can submit your own models to test. Why this matters: - Teams can pick efficient models that still get the job done - Developers can optimize for energy use from day one - Organizations can finally predict their AI environmental impact If you're building with AI at any scale, definitely worth checking out. ğŸ‘‰ leaderboard: https://lnkd.in/esrSxetj ğŸ‘‰ blog post: https://lnkd.in/eFJvzHi8 Huge...</description><pubDate>Sat, 15 Feb 2025 09:20:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/212771868233348</guid></item><item><title>RTX 5090 Tested Against FLUX DEV, SD 3.5 Large, SD 3.5 Medium, SDXL, SD 1.5 with AMD 9950X CPU and RTX 5090 compared against RTX 3090 TI in all benchmarks. Moreover, compared FP8 vs FP16 and changing prompt impact as well</title><link>https://huggingface.co/posts/MonsterMMORPG/156738682781025</link><description>RTX 5090 Tested Against FLUX DEV, SD 3.5 Large, SD 3.5 Medium, SDXL, SD 1.5 with AMD 9950X CPU and RTX 5090 compared against RTX 3090 TI in all benchmarks. Moreover, compared FP8 vs FP16 and changing prompt impact as well Video Link : https://youtu.be/jHlGzaDLkto In this video I have intensively compared RTX 5090 speed on FLUX DEV, FLUX Fill, SD 3.5 Large, SD 3.5 Medium, Stable Diffusion XL (SDXL) and Stable Diffusion 1.5 (SD 1.5) models. For each benchmark, I have compared RTX 5090 against RTX 3090 TI so we see the speed improvement. Moreover, I have tested FP8 vs 16-bit precision for FLUX and SD 3.5 Large and SD 3.5 Medium models. Furthermore, I have tested the speed impact of changing prompt on FLUX DEV model since one of the follower had requested. Full specs of the system provided below. I have used SwarmUI with ComfyUI backend so these benchmarks are literally done on ComfyUI you can think as. Currently no other interface / UI supporting RTX 5000 series as far as i know. Video...</description><pubDate>Sat, 15 Feb 2025 09:20:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/156738682781025</guid></item><item><title>"ğŸ®ğŸ¬ğŸ®ğŸ± ğ˜„ğ—¶ğ—¹ğ—¹ ğ—¯ğ—² ğ˜ğ—µğ—² ğ˜†ğ—²ğ—®ğ—¿ ğ—¼ğ—³ ğ—”ğ—œ ğ—®ğ—´ğ—²ğ—»ğ˜ğ˜€": this statement has often been made, here are numbers to support it.</title><link>https://huggingface.co/posts/m-ric/116861695030454</link><description>"ğŸ®ğŸ¬ğŸ®ğŸ± ğ˜„ğ—¶ğ—¹ğ—¹ ğ—¯ğ—² ğ˜ğ—µğ—² ğ˜†ğ—²ğ—®ğ—¿ ğ—¼ğ—³ ğ—”ğ—œ ğ—®ğ—´ğ—²ğ—»ğ˜ğ˜€": this statement has often been made, here are numbers to support it. I've plotted the progress of AI agents on GAIA test set, and it seems they're headed to catch up with the human baseline in early 2026. And that progress is still driven mostly by the improvement of base LLMs: progress would be even faster with fine-tuned agentic models. See translation</description><pubDate>Sat, 15 Feb 2025 09:20:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/116861695030454</guid></item><item><title>Hey, Iâ€™m Ben and I work at Hugging Face.</title><link>https://huggingface.co/posts/burtenshaw/555593095351748</link><description>Hey, Iâ€™m Ben and I work at Hugging Face. Right now, Iâ€™m focusing on educational stuff and getting loads of new people to build open AI models using free and open source tools. Iâ€™ve made a collection of some of the tools Iâ€™m building and using for teaching. Stuff like quizzes, code challenges, and certificates. burtenshaw/tools-for-learning-ai-6797453caae193052d3638e2 See translation</description><pubDate>Sat, 15 Feb 2025 09:20:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/555593095351748</guid></item><item><title>Final upgrade to the Multi-Agent Task Completion Space:</title><link>https://huggingface.co/posts/CultriX/110138158069042</link><description>Final upgrade to the Multi-Agent Task Completion Space: CultriX/MultiAgent-CodeTask . It now includes : - a live stream of the progress being made on the task (see included video), - The following components: 1. Automatic prompt optimization 2. An orchestrator deciding which agent to call dynamically including feedback from a human (human-in-the-loop) 3. A coding agent to complete the task 4. A code reviewing agent to iteratively provide feedback to improve the code generated by the coding agent until the code meets the required criteria after which it is approved. 5. A testing agent that tests the approved code or provides information on how to test it. 6. A documentation agent that provides documentation and a help message for the approved and tested code. See translation</description><pubDate>Sat, 15 Feb 2025 09:20:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/CultriX/110138158069042</guid></item><item><title>ğ—šğ—¿ğ—²ğ—®ğ˜ ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—®ğ—¹ğ—²ğ—¿ğ˜: you can now share agents to the Hub! ğŸ¥³ğŸ¥³</title><link>https://huggingface.co/posts/m-ric/668305263865285</link><description>ğ—šğ—¿ğ—²ğ—®ğ˜ ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—®ğ—¹ğ—²ğ—¿ğ˜: you can now share agents to the Hub! ğŸ¥³ğŸ¥³ And any agent pushed to Hub get a cool Space interface to directly chat with it. This was a real technical challenge: for instance, serializing tools to export them meant that you needed to get all the source code for a tool, verify that it was standalone (not relying on external variables), and gathering all the packages required to make it run. Go try it out! ğŸ‘‰ https://github.com/huggingface/smolagents See translation</description><pubDate>Sat, 15 Feb 2025 09:20:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/668305263865285</guid></item><item><title>For those who haven't come across it yet, here's a handy trick to discuss an entire GitHub repo with an LLM:</title><link>https://huggingface.co/posts/m-ric/573811868569071</link><description>For those who haven't come across it yet, here's a handy trick to discuss an entire GitHub repo with an LLM: =&gt; Just replace "github" with "gitingest" in the url, and you get the whole repo as a single string that you can then paste in your LLMs See translation</description><pubDate>Sat, 15 Feb 2025 09:20:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/573811868569071</guid></item></channel></rss>