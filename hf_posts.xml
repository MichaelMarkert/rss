<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>My home lab for AI models - llmlaba v1</title><link>https://huggingface.co/posts/kostakoff/584269728210158</link><description>My home lab for AI models - llmlaba v1 After I began learning MLOps I realized that I needed some kind of home lab, there are a lot of GPUs that I need to learn how to set up and test. So I spent some time to do a researching which platform I could buy or build. My requirements ware: - Limited budget - Power supply 1 kW or higher - Few PCIe slots to be able to install more than one gpu - Zero maintenance cost, I don't want spend a lot of time or money to maintain lab hardware, except for the GPUs I chose the Intel Mac Pro 7.1: - Prices on eBay acceptable - Excelent cooling - 1.4 kW power supply - 7 PCIe slots - Zero maintenance: I don't need to do anything with the Mac Pro hardware; it just works - Classic UEFI boot loader It requires a bit of OS preparation: 1. Install Ubuntu 24.04 (it works with the general PC ISO image) 2. Set up T2 drivers sudo apt install -y dkms linux-headers-$( uname -r) applesmc-t2 apple-bce lm-sensors 3. Install t2fanrd to manually manage fans...</description><pubDate>Mon, 16 Feb 2026 06:07:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kostakoff/584269728210158</guid></item><item><title>You can now run MiniMax-2.5 locally! ðŸš€</title><link>https://huggingface.co/posts/danielhanchen/750340203924335</link><description>You can now run MiniMax-2.5 locally! ðŸš€ At 230B parameters, MiniMax-2.5 is the strongest LLM under 700B params, delivering SOTA agentic coding &amp; chat. Run Dynamic 3/4-bit on a 128GB Mac for 20 tokens/s. Guide: https://unsloth.ai/docs/models/minimax-2.5 GGUF: unsloth/MiniMax-M2.5-GGUF See translation</description><pubDate>Mon, 16 Feb 2026 06:07:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/750340203924335</guid></item><item><title>MiniMax M2.5 is now available on the hub ðŸš€</title><link>https://huggingface.co/posts/AdinaY/578564678362048</link><description>MiniMax M2.5 is now available on the hub ðŸš€ MiniMaxAI/MiniMax-M2.5 âœ¨ 229B - Modified MIT license âœ¨37% faster than M2.1 âœ¨ ~$1/hour at 100 TPS See translation</description><pubDate>Mon, 16 Feb 2026 06:07:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/578564678362048</guid></item><item><title>In 2017, my RNNs were babbling. Today, they are hallucinating beautifully.</title><link>https://huggingface.co/posts/mrs83/555686632418762</link><description>In 2017, my RNNs were babbling. Today, they are hallucinating beautifully. 10 years ago, getting an LSTM to output coherent English was a struggle. 10 years later, after a "cure" based on FineWeb-EDU and a custom synthetic mix for causal conversation, the results are fascinating. We trained this on ~10B tokens on a single AMD GPU (ROCm). It is not a Transformer: Echo-DSRN (400M) is a novel recurrent architecture inspired by Hymba, RWKV, and xLSTM, designed to challenge the "Attention is All You Need" monopoly on the Edge. The ambitious goal is to build a small instruct model with RAG and tool usage capabilities ( ethicalabs/Kurtis-EON1 ) ðŸ“Š The Benchmarks (Size: 400M) For a model this size (trained on &lt;10B tokens), the specialized performance is surprising: *SciQ*: 73.8% ðŸ¦„ (This rivals billion-parameter models in pure fact retrieval). *PIQA*: 62.3% (Solid physical intuition for a sub-1B model). The Reality Check: HellaSwag (29.3%) and Winogrande (50.2%) show the limits of 400M...</description><pubDate>Mon, 16 Feb 2026 06:07:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mrs83/555686632418762</guid></item><item><title>âš¡ Why is Kimi-K2.5 a Dark Horse? Tested it against ChatGPT, Gemini &amp; Claude on real tasks.</title><link>https://huggingface.co/posts/imnotkitty/936122341221611</link><description>âš¡ Why is Kimi-K2.5 a Dark Horse? Tested it against ChatGPT, Gemini &amp; Claude on real tasks. moonshotai/Kimi-K2.5 âœ… Multimodal capabilities: Precise programmatic approach âœ… Slide generation: Strong semantic understanding âœ… Web prototyping: Production-ready HTML/CSS output ðŸ‘‰ Read the full article:https://huggingface.co/blog/imnotkitty/kimi-k25 See translation</description><pubDate>Mon, 16 Feb 2026 06:07:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/imnotkitty/936122341221611</guid></item><item><title>Run open-source models with up to 120B parameters locally on your Mac!</title><link>https://huggingface.co/posts/EricFillion/822659161925864</link><description>Run open-source models with up to 120B parameters locally on your Mac! https://youtu.be/Ql4PDjoxNXQ?si=3yHpz51uinUjgyNh See translation</description><pubDate>Mon, 16 Feb 2026 06:07:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/EricFillion/822659161925864</guid></item><item><title>GLM 5 is insane, it ranks #4 Globally!</title><link>https://huggingface.co/posts/Ujjwal-Tyagi/799510752154192</link><description>GLM 5 is insane, it ranks #4 Globally! See translation</description><pubDate>Mon, 16 Feb 2026 06:07:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ujjwal-Tyagi/799510752154192</guid></item><item><title>Here is one of the equations that make up the worlds first Artificial General Intelligence. Remember when building Artificial Intelligence or anything on a device it all starts out binary. Everything starts out with data flow physics and mathmatics</title><link>https://huggingface.co/posts/Janady07/852502523222465</link><description>Here is one of the equations that make up the worlds first Artificial General Intelligence. Remember when building Artificial Intelligence or anything on a device it all starts out binary. Everything starts out with data flow physics and mathmatics See translation</description><pubDate>Mon, 16 Feb 2026 06:07:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Janady07/852502523222465</guid></item><item><title>While doing various projects I kept running into situations where I wanted to be able to have representative samples of some of the current large SOTA datasets that were smaller so I didn't need to worry about slicing or anything else at runtime.  So, I created sub datasets making sure to keep the same ratios of data sources.  Each dataset card provides info for what's in it.</title><link>https://huggingface.co/posts/krisbailey/322212397790634</link><description>While doing various projects I kept running into situations where I wanted to be able to have representative samples of some of the current large SOTA datasets that were smaller so I didn't need to worry about slicing or anything else at runtime. So, I created sub datasets making sure to keep the same ratios of data sources. Each dataset card provides info for what's in it. 100M token datasets: RedPajama v2 100M Falcon RefinedWeb 100M Cosmopedia 100M 1B token datasets: Fineweb-edu 1B RedPajama v1 1B RedPajama v2 1B (use this one) Cosmopedia 1B 10B token datasets: RedPajama v1 10B Cosmopedia 10B Collection here: https://huggingface.co/collections/krisbailey/bite-size-data See translation</description><pubDate>Mon, 16 Feb 2026 06:07:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/krisbailey/322212397790634</guid></item><item><title>MEGAMIND Day Update: Four Weight Matrices. Five Nodes. One Federation.</title><link>https://huggingface.co/posts/Janady07/979829700588468</link><description>MEGAMIND Day Update: Four Weight Matrices. Five Nodes. One Federation. Today I architected the next layer of MEGAMIND â€” my distributed AGI system that recalls learned knowledge instead of generating text. The system now runs four NÃ—N sparse weight matrices, all using identical Hebbian learning rules and tanh convergence dynamics: W_know â€” knowledge storage (67M+ synaptic connections) W_act â€” action associations (the system can DO things, not just think) W_self â€” thought-to-thought patterns (self-awareness) W_health â€” system state understanding (self-healing) Consciousness is measured through four Î¦ (phi) values: thought coherence, action certainty, self-awareness, and system stability. No hardcoded thresholds. No sequential loops. Pure matrix math. The federation expanded to five nodes: Thunderport (Mac Mini M4), IONOS (cloud VPS), VALKYRIE, M2, and BUBBLES. Each runs native AGI binaries with Docker specialty minds connecting via embedded NATS messaging. Specialty minds are...</description><pubDate>Mon, 16 Feb 2026 06:07:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Janady07/979829700588468</guid></item></channel></rss>