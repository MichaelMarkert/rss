<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Developing with ZeroGPU without a PRO account is painful. They give you so many requests at once, but then have like a 24 hour cooldown. I vote less requests in a batch, but then a shorter cooldown.</title><link>https://huggingface.co/posts/nroggendorff/877752190149689</link><description>Developing with ZeroGPU without a PRO account is painful. They give you so many requests at once, but then have like a 24 hour cooldown. I vote less requests in a batch, but then a shorter cooldown. or just less of a cooldown, but i understand if that is not allowed See translation</description><pubDate>Sat, 15 Nov 2025 17:18:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/877752190149689</guid></item><item><title>Reached 1000+ total downloads across my models and datasets! üéâ</title><link>https://huggingface.co/posts/ronantakizawa/435117440357729</link><description>Reached 1000+ total downloads across my models and datasets! üéâ Follow me for more @ ronantakizawa See translation</description><pubDate>Sat, 15 Nov 2025 17:18:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/435117440357729</guid></item><item><title>This shared notebook comprises the MMLU benchmark evaluating task for my latest reasoning model for the sociology field. The results show that using Few-shot prompting in the system prompt can significantly improve the model's performance at answering questions.</title><link>https://huggingface.co/posts/alibidaran/312503029386627</link><description>This shared notebook comprises the MMLU benchmark evaluating task for my latest reasoning model for the sociology field. The results show that using Few-shot prompting in the system prompt can significantly improve the model's performance at answering questions. Model's link: alibidaran/GRPO_LLAMA3-instructive_reasoning1 Notebook evaluation: https://www.kaggle.com/code/alibidaran/mmlu-socialogy-thinking-evals?scriptVersionId=277240033 See translation</description><pubDate>Sat, 15 Nov 2025 17:18:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/alibidaran/312503029386627</guid></item><item><title>LLMs can leak their post-training data (RL included) üíß</title><link>https://huggingface.co/posts/anakin87/201202681111752</link><description>LLMs can leak their post-training data (RL included) üíß New interesting paper on this topic from Google DeepMind: Extracting alignment data in open models (2510.18554) It's known that Language Models memorize data that can be extracted via prompting. In this paper, the authors investigate this aspect: - using open models, where prompting can be fully customized by the user, including special tokens. - focusing on open-source models like Olmo, where full training data is available. üì§ How do they extract data? During post-training (like SFT), new tokens such as &lt;|user|&gt; are introduced. The authors hypothesize prompting the model with these tokens can make it output its alignment data (remember Magpie?). For example, for SFT, their extraction prompt is &lt;|endoftext|&gt;&lt;|user|&gt;. üìè Evaluating memorization The authors compare each sampled example with the original data using vector search with embedding similarity. They find that many outputs are semantically very similar to the original...</description><pubDate>Sat, 15 Nov 2025 17:18:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/anakin87/201202681111752</guid></item><item><title>Hugging Face MCP Server v0.2.45</title><link>https://huggingface.co/posts/evalstate/865812476358807</link><description>Hugging Face MCP Server v0.2.45 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - New! Experimental dynamic_space tool. - Default Image Generator changed to Qwen-Image-Fast See translation</description><pubDate>Sat, 15 Nov 2025 17:18:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/evalstate/865812476358807</guid></item><item><title>üéØ Introducing Chayan: A Calibrated 4-Model LLM Router Achieving 69% Accuracy on RouterArena</title><link>https://huggingface.co/posts/codelion/349113167339666</link><description>üéØ Introducing Chayan: A Calibrated 4-Model LLM Router Achieving 69% Accuracy on RouterArena We're excited to share Chayan, a cost-efficient LLM router that intelligently routes queries between 4 models to maximize accuracy while minimizing cost. Chayan just submitted to the RouterArena leaderboard and achieved 69.05% accuracy on the benchmark! üîó Model: adaptive-classifier/chayan üîó Dataset: RouteWorks/RouterArena üìä Performance Highlights Chayan achieves impressive results on the RouterArena benchmark: ‚Ä¢ 69.05% accuracy (would rank #1 on current leaderboard) ‚Ä¢ $0.333 per 1K queries ‚Ä¢ +12.07pp improvement over all-mini baseline (56.98%) ‚Ä¢ 99% of perfect 2-model oracle performance at 57% lower cost Compared to our previous 2-model router (61.43% accuracy), Chayan delivers +7.62pp improvement through smarter 4-model routing. üß† How It Works Chayan uses an Adaptive K-NN classifier with prototype memory to route between 4 models: ‚Ä¢ openai/gpt-4o-mini (fast &amp; cheap) ‚Ä¢...</description><pubDate>Sat, 15 Nov 2025 17:18:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/349113167339666</guid></item><item><title>I built a demo on how to implement Cache-Augmented Generation (CAG) in an LLM and compare its performance gains to RAG (111 stars, 20 forks).</title><link>https://huggingface.co/posts/ronantakizawa/762624768908636</link><description>I built a demo on how to implement Cache-Augmented Generation (CAG) in an LLM and compare its performance gains to RAG (111 stars, 20 forks). https://github.com/ronantakizawa/cacheaugmentedgeneration CAG preloads document content into an LLM‚Äôs context as a precomputed key-value (KV) cache. This caching eliminates the need for real-time retrieval during inference, reducing token usage by up to 76% while maintaining answer quality. CAG is particularly effective for constrained knowledge bases like internal documentation, FAQs, and customer support systems, where all relevant information can fit within the model's extended context window. #rag #retrievalaugmentedgeneration See translation</description><pubDate>Sat, 15 Nov 2025 17:18:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/762624768908636</guid></item><item><title>The reaction on the QAT post is beyond expectations so below is my optimizer post as promised. But I found that I had lots of explanation to do about optimizer itself. So this post is actually a historical recount. The Muon optimizer  (used by Kimi) post (coming very soon) can only continue after this.</title><link>https://huggingface.co/posts/onekq/566639652632782</link><description>The reaction on the QAT post is beyond expectations so below is my optimizer post as promised. But I found that I had lots of explanation to do about optimizer itself. So this post is actually a historical recount. The Muon optimizer (used by Kimi) post (coming very soon) can only continue after this. https://huggingface.co/blog/onekq/adam-optimizer If you know Adam(W) optimizer already, you can just skip and sorry for the wait. Otherwise, it should be a useful read. See translation</description><pubDate>Sat, 15 Nov 2025 17:18:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/566639652632782</guid></item><item><title>Try the all-new trending Qwen-Image-Edit specialized adapter demos, including Photo-to-Anime, Light Restoration, Multi-Angle Edits, Relighting, and more ‚Äî all in a single Hugging Face Space. Below is the demo link. ü§óüå†</title><link>https://huggingface.co/posts/prithivMLmods/809040430953807</link><description>Try the all-new trending Qwen-Image-Edit specialized adapter demos, including Photo-to-Anime, Light Restoration, Multi-Angle Edits, Relighting, and more ‚Äî all in a single Hugging Face Space. Below is the demo link. ü§óüå† ‚Æû Demo-Space: prithivMLmods/Qwen-Image-Edit-2509-LoRAs-Fast ‚Æû How-to-Use: prithivMLmods/Qwen-Image-Edit-2509-LoRAs-Fast#2 ‚Æû Collection: https://huggingface.co/collections/prithivMLmods/image-generation-apps-collection To know more about it, visit the app page or the respective model page! See translation</description><pubDate>Sat, 15 Nov 2025 17:18:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/809040430953807</guid></item><item><title>Hugging Face MCP Server v0.2.46</title><link>https://huggingface.co/posts/evalstate/966306150906160</link><description>Hugging Face MCP Server v0.2.46 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - Add "discover" to Dynamic Space tool. Recommend deselecting "space_search" if using dynamic spaces. See translation</description><pubDate>Sat, 15 Nov 2025 17:18:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/evalstate/966306150906160</guid></item></channel></rss>