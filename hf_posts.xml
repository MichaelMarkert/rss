<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸ”¥ AgenticAI: The Ultimate Multimodal AI with 16 MBTI Girlfriend Personas! ğŸ”¥</title><link>https://huggingface.co/posts/seawolf2357/796388354612946</link><description>ğŸ”¥ AgenticAI: The Ultimate Multimodal AI with 16 MBTI Girlfriend Personas! ğŸ”¥ Hello AI community! Today, our team is thrilled to introduce AgenticAI, an innovative open-source AI assistant that combines deep technical capabilities with uniquely personalized interaction. ğŸ’˜ ğŸ› ï¸ MBTI 16 Types SPACES Collections link seawolf2357/heartsync-mbti-67f793d752ef1fa542e16560 âœ¨ 16 MBTI Girlfriend Personas Complete MBTI Implementation: All 16 MBTI female personas modeled after iconic characters (Dana Scully, Lara Croft, etc.) Persona Depth: Customize age groups and thinking patterns for hyper-personalized AI interactions Personality Consistency: Each MBTI type demonstrates consistent problem-solving approaches, conversation patterns, and emotional expressions ğŸš€ Cutting-Edge Multimodal Capabilities Integrated File Analysis: Deep analysis and cross-referencing of images, videos, CSV, PDF, and TXT files Advanced Image Understanding: Interprets complex diagrams, mathematical equations, charts, and...</description><pubDate>Fri, 11 Apr 2025 17:19:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/seawolf2357/796388354612946</guid></item><item><title>Google published a 69-page whitepaper on Prompt Engineering and its best practices, a must-read if you are using LLMs in production:</title><link>https://huggingface.co/posts/hesamation/789492772324435</link><description>Google published a 69-page whitepaper on Prompt Engineering and its best practices, a must-read if you are using LLMs in production: &gt; zero-shot, one-shot, few-shot &gt; system prompting &gt; chain-of-thought (CoT) &gt; ReAct LINK: https://www.kaggle.com/whitepaper-prompt-engineering &gt; code prompting &gt; best practices See translation</description><pubDate>Fri, 11 Apr 2025 17:19:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/789492772324435</guid></item><item><title>ğŸ¨ Designers, meet OmniSVG! This new model helps you create professional vector graphics from text/images, generate editable SVGs from icons to detailed characters, convert rasters to vectors, maintain style consistency with references, and integrate into your workflow.</title><link>https://huggingface.co/posts/fdaudens/513864434208106</link><description>ğŸ¨ Designers, meet OmniSVG! This new model helps you create professional vector graphics from text/images, generate editable SVGs from icons to detailed characters, convert rasters to vectors, maintain style consistency with references, and integrate into your workflow. @ OmniSVG See translation</description><pubDate>Fri, 11 Apr 2025 17:19:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/513864434208106</guid></item><item><title>Qwen 3 can launch very soon. ğŸ‘€</title><link>https://huggingface.co/posts/merterbak/235850739835485</link><description>Qwen 3 can launch very soon. ğŸ‘€ https://github.com/ggml-org/llama.cpp/pull/12828 See translation</description><pubDate>Fri, 11 Apr 2025 17:19:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/235850739835485</guid></item><item><title>Hi All,  I recently released two Audio datasets  which are generated using my earlier released dataset:</title><link>https://huggingface.co/posts/ajibawa-2023/282296415348325</link><description>Hi All, I recently released two Audio datasets which are generated using my earlier released dataset: ajibawa-2023/Children-Stories-Collection First Audio Dataset:https://huggingface.co/datasets/ajibawa-2023/Audio-Children-Stories-Collection-Large has 5600++ stories in .mp3 format. Second Audio Dataset:https://huggingface.co/datasets/ajibawa-2023/Audio-Children-Stories-Collection has 600 stories in .mp3 format. See translation</description><pubDate>Fri, 11 Apr 2025 17:19:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ajibawa-2023/282296415348325</guid></item><item><title>ğŸš€ We tried something new!</title><link>https://huggingface.co/posts/jasoncorkill/225557458891562</link><description>ğŸš€ We tried something new! We just published a dataset using a new (for us) preference modality: direct ranking based on aesthetic preference. We ranked a couple of thousand images from most to least preferred, all sampled from the Open Image Preferences v1 dataset by the amazing @ data-is-better-together team. ğŸ“Š Check it out here: Rapidata/2k-ranked-images-open-image-preferences-v1 We're really curious to hear your thoughts! Is this kind of ranking interesting or useful to you? Let us know! ğŸ’¬ If it is, please consider leaving a â¤ï¸ and if we hit 30 â¤ï¸s, weâ€™ll go ahead and rank the full 17k image dataset! See translation</description><pubDate>Fri, 11 Apr 2025 17:19:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/225557458891562</guid></item><item><title>We desperately need GPU for model inference. CPU can't replace GPU.</title><link>https://huggingface.co/posts/onekq/215048320445842</link><description>We desperately need GPU for model inference. CPU can't replace GPU. I will start with the basics. GPU is designed to serve predictable workloads with many parallel units (pixels, tensors, tokens). So a GPU allocates as much transistor budget as possible to build thousands of compute units (Cuda cores in NVidia or execution units in Apple Silicon), each capable of running a thread. But CPU is designed to handle all kinds of workloads. CPU cores are much larger (hence a lot fewer) with branch prediction and other complex things. In addition, more and more transistors are allocated to build larger cache (~50% now) to house the unpredictable, devouring the compute budget. Generalists can't beat specialists. See translation</description><pubDate>Fri, 11 Apr 2025 17:19:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/215048320445842</guid></item><item><title>OpenAI has released BrowseComp an open source benchmark designed to evaluate the web browsing capabilities of AI agents. This dataset comprising 1,266 questions challenges AI models to navigate the web and uncover complex and obscure information. Crafted by human trainers, the questions are intentionally difficult. (unsolvable by another person in under ten minutes and beyond the reach of existing models like ChatGPT with and without browsing and an early version of OpenAI's Deep Research tool.)</title><link>https://huggingface.co/posts/merterbak/566061805679533</link><description>OpenAI has released BrowseComp an open source benchmark designed to evaluate the web browsing capabilities of AI agents. This dataset comprising 1,266 questions challenges AI models to navigate the web and uncover complex and obscure information. Crafted by human trainers, the questions are intentionally difficult. (unsolvable by another person in under ten minutes and beyond the reach of existing models like ChatGPT with and without browsing and an early version of OpenAI's Deep Research tool.) Blog Post: https://openai.com/index/browsecomp/ Paper: https://cdn.openai.com/pdf/5e10f4ab-d6f7-442e-9508-59515c65e35d/browsecomp.pdf Code in simple eval repo: https://github.com/openai/simple-evals See translation</description><pubDate>Fri, 11 Apr 2025 17:19:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/566061805679533</guid></item><item><title>Moonshot AI æœˆä¹‹æš—é¢ ğŸŒ› @Kimi_Moonshotis just dropped an MoE VLM  and an MoE Reasoning VLM on the hub!!</title><link>https://huggingface.co/posts/AdinaY/423063846745216</link><description>Moonshot AI æœˆä¹‹æš—é¢ ğŸŒ› @Kimi_Moonshotis just dropped an MoE VLM and an MoE Reasoning VLM on the hub!! Model:https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85 âœ¨3B with MIT license âœ¨Long context windows up to 128K âœ¨Strong multimodal reasoning (36.8% on MathVision, on par with 10x larger models) and agent skills (34.5% on ScreenSpot-Pro) See translation</description><pubDate>Fri, 11 Apr 2025 17:19:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/423063846745216</guid></item><item><title>Do chatbots lie about CÃ©line Dion? We now have answers, not speculation.</title><link>https://huggingface.co/posts/fdaudens/177759662135997</link><description>Do chatbots lie about CÃ©line Dion? We now have answers, not speculation. Ai2 just released OLMoTrace and it's a game-changer for transparency. You can literally see where an AI's responses come from in its training data - in real time. The demo shows results about CÃ©line. So I tried it out myself! Watch what happens in the video. For journalists, researchers studying hallucinations and anyone who needs to trust their AI, this is like getting X-ray vision into AI systems. When the model made claims, I could instantly verify them against original sources. When it hallucinated, I could see why. You can finally 1) understand how LLMs actually work and 2) verify if what they're saying is true. No more blind trust. This pushes the open data movement to the next level. ğŸ‘‰ Blog post: https://allenai.org/blog/olmotrace ğŸ‘‰ Paper: https://www.datocms-assets.com/64837/1743890415-olmotrace.pdf P.S.: A word of caution: never use a chatbot as a knowledge base. It's not Google. Better use it with a...</description><pubDate>Fri, 11 Apr 2025 17:19:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/177759662135997</guid></item></channel></rss>