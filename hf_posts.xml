<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Today, we're unveiling two new open-source AI robots! HopeJR for $3,000 &amp; Reachy Mini for $300 🤖🤖🤖</title><link>https://huggingface.co/posts/clem/522668354429256</link><description>Today, we're unveiling two new open-source AI robots! HopeJR for $3,000 &amp; Reachy Mini for $300 🤖🤖🤖 Let's go open-source AI robotics! See translation</description><pubDate>Fri, 30 May 2025 09:25:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/522668354429256</guid></item><item><title>VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration</title><link>https://huggingface.co/posts/DawnC/538322807718464</link><description>VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration I'm excited to share significant improvements to VisionScout that substantially enhance accuracy and analytical capabilities. ⭐️ Key Enhancements - CLIP Zero-Shot Landmark Detection: The system now identifies famous landmarks and architectural features without requiring specific training data, expanding scene understanding beyond generic object detection. - Places365 Environmental Classification: Integration of MIT's Places365 model provides robust scene baseline classification across 365 categories, significantly improving lighting analysis accuracy and overall scene identification precision. - Enhanced Multi-Modal Fusion: Advanced algorithms now dynamically combine insights from YOLOv8, CLIP, and Places365 to optimize accuracy across diverse scenarios. - Refined LLM Narratives: Llama 3.2 integration continues to transform analytical data into fluent, contextually rich descriptions while maintaining...</description><pubDate>Fri, 30 May 2025 09:25:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/538322807718464</guid></item><item><title>🤗👨🏻‍🎓</title><link>https://huggingface.co/posts/darkc0de/635443238112929</link><description>🤗👨🏻‍🎓</description><pubDate>Fri, 30 May 2025 09:25:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/darkc0de/635443238112929</guid></item><item><title>deepseek-ai/DeepSeek-R1-0528</title><link>https://huggingface.co/posts/AtAndDev/639250895656011</link><description>deepseek-ai/DeepSeek-R1-0528 This is the end See translation</description><pubDate>Fri, 30 May 2025 09:25:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AtAndDev/639250895656011</guid></item><item><title>🎵 Dream come true for content creators! TIGER AI can extract voice, effects &amp; music from ANY audio file 🤯</title><link>https://huggingface.co/posts/fdaudens/323840314242853</link><description>🎵 Dream come true for content creators! TIGER AI can extract voice, effects &amp; music from ANY audio file 🤯 This lightweight model uses frequency band-split technology to separate speech like magic. Kudos to @ fffiloni for the amazing demo! fffiloni/TIGER-audio-extraction See translation</description><pubDate>Fri, 30 May 2025 09:25:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/323840314242853</guid></item><item><title>🔥 New benchmark &amp; dataset for Subject-to-Video generation</title><link>https://huggingface.co/posts/AdinaY/228294422537840</link><description>🔥 New benchmark &amp; dataset for Subject-to-Video generation OPENS2V-NEXUS by Pekin University ✨ Fine-grained evaluation for subject consistency BestWishYsh/OpenS2V-Eval ✨ 5M-scale dataset: BestWishYsh/OpenS2V-5M ✨ New metrics – automatic scores for identity, realism, and text match See translation</description><pubDate>Fri, 30 May 2025 09:25:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/228294422537840</guid></item><item><title>🧠  AI Brand Naming with 15 Specialized Theories</title><link>https://huggingface.co/posts/openfree/518842941778923</link><description>🧠 AI Brand Naming with 15 Specialized Theories 🎯 Core Features 15 Expert Theories for professional brand naming Bilingual Support Korean/English for global brands Unified Evaluation System creativity/memorability/relevance scores Real-time Visualization theory-specific custom designs openfree/Naming 🔬 Applied Theories Cognitive Theories (4) 🟦 Square Theory - Semantic square structure with 4-word relationships 🔊 Sound Symbolism - Psychological connections between phonemes and meaning 🧠 Cognitive Load - Minimized processing for instant recognition 👁️ Gestalt Theory - Perceptual principles where whole exceeds parts Creative Theories (3) 🔀 Conceptual Blending - Merging concepts to create new meanings 🔧 SCAMPER Method - 7 creative transformation techniques 🌿 Biomimicry - Nature-inspired wisdom from 3.8 billion years of evolution Strategic Theories (2) ✅ Jobs-to-be-Done - Customer-centric problem-solving focus 💭 Design Thinking - Human-centered innovation methodology Cultural Theories (3)...</description><pubDate>Fri, 30 May 2025 09:25:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/518842941778923</guid></item><item><title>I am so happy  to share to all that I’ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I’m doing my best to keep up. Excited for what’s ahead!</title><link>https://huggingface.co/posts/lukmanaj/495766537273785</link><description>I am so happy to share to all that I’ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I’m doing my best to keep up. Excited for what’s ahead! See translation</description><pubDate>Fri, 30 May 2025 09:25:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lukmanaj/495766537273785</guid></item><item><title>I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book.</title><link>https://huggingface.co/posts/hesamation/260011784391977</link><description>I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book. It gives an overview, then goes into detail for each stage, even providing best practices. It’s 115 pages on arxiv, definitely worth a read. Check it out: https://arxiv.org/abs/2408.13296 See translation</description><pubDate>Fri, 30 May 2025 09:25:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/260011784391977</guid></item><item><title>VEO 3 FLOW Full Tutorial - How To Use VEO3 in FLOW Guide :</title><link>https://huggingface.co/posts/MonsterMMORPG/279762403721253</link><description>VEO 3 FLOW Full Tutorial - How To Use VEO3 in FLOW Guide : https://youtu.be/AoEmQPU2gtg Tutorial link : https://youtu.be/AoEmQPU2gtg VEO 3 AI is rocking generative AI field right now. FLOW is the platform that lets you use VEO 3 with so many cool features. This is an official tutorial and guide made by Google team. I edited it slightly. I hope this be helpful. FLOW : https://labs.google/flow/about Veo 3 is Google DeepMind’s most advanced video generation model to date. It allows users to create high-quality, cinematic video clips from simple text prompts, making it one of the most powerful AI tools for video creation. What sets Veo 3 apart is its ability to generate videos with native audio. This means that along with stunning visuals, Veo 3 can produce synchronized dialogue, ambient sounds, and background music—all from a single prompt. For filmmakers, this is a significant leap forward, as it eliminates the need for separate audio generation or complex syncing processes. Veo 3...</description><pubDate>Fri, 30 May 2025 09:25:15 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/279762403721253</guid></item></channel></rss>