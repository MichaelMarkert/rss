<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Excited to introduce the Tiny VLMs Lab App for experiencing 15+ multimodal VLMs, ranging from a 250M parameter model to a 4B parameter model, for tasks like OCR, reasoning, small models for single-shot answering, and captioning (abliterated), across a broad range of visual categories including images with complex, sensitive, or nuanced content, while handling varying aspect ratios and resolutions.ğŸ§ª</title><link>https://huggingface.co/posts/prithivMLmods/284574267701705</link><description>Excited to introduce the Tiny VLMs Lab App for experiencing 15+ multimodal VLMs, ranging from a 250M parameter model to a 4B parameter model, for tasks like OCR, reasoning, small models for single-shot answering, and captioning (abliterated), across a broad range of visual categories including images with complex, sensitive, or nuanced content, while handling varying aspect ratios and resolutions.ğŸ§ª ğŸ¤— Space/App: prithivMLmods/Tiny-VLMs-Lab âœ¦ï¸ Also introducing prithivMLmods/Qwen2.5-VL-3B-Abliterated-Caption-it , tailored for Abliterated Captioning / Uncensored Image Captioning. This release comes as a lighter alternative to the existing Qwen2.5-VL-7B-Abliterated-Caption-it prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it model, making it usable on mid-range GPUs and even experimental on T4 GPUs. âœ¦ï¸ Collection: prithivMLmods/vl-abliterated-caption-68a0443b63182e97a15c47a3 âœ¦ï¸ GitHub: https://github.com/PRITHIVSAKTHIUR/Tiny-VLMs-Lab . . . To know more about it, visit the app page or...</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/284574267701705</guid></item><item><title>Image-to-Promptâš¡</title><link>https://huggingface.co/posts/ovi054/657358125503535</link><description>Image-to-Promptâš¡ ovi054/image-to-prompt Extract text prompt from image. And you can reuse the prompt to generate similar images! Useful for prompt engineering, studying image-to-text alignment, making training datasets, or recreating similar outputs. Powered by: Gradio, Florence 2 ğŸ‘‰ Try it now: ovi054/image-to-prompt See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/657358125503535</guid></item><item><title>When you ask ChatGPT, Claude, or Gemini a really tough question,</title><link>https://huggingface.co/posts/RakshitAralimatti/207934490136479</link><description>When you ask ChatGPT, Claude, or Gemini a really tough question, you might notice that little "thinking..." moment before it answers. But what does it actually mean when an LLM is â€œthinkingâ€? Imagine a chess player pausing before their next move not because they donâ€™t know how to play, but because theyâ€™re running through possibilities, weighing options, and choosing the best one. LLMs do something similarâ€¦ except theyâ€™re not really thinking like us. Hereâ€™s the surprising part :- You might think these reasoning skills come from futuristic architectures or alien neural networks. In reality, most reasoning LLMs still use the same transformer decoder-only architecture as other models The real magic? Itâ€™s in how theyâ€™re trained and what data they learn from. Can AI actually think, or is it just insanely good at faking it? I broke it down in a simple, 4-minute Medium read. Bet youâ€™ll walk away with at least one â€œaha!â€ moment. ğŸš€ Read here - https://lnkd.in/edZ8Ceyg See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/207934490136479</guid></item><item><title>âœ¨ HairPick | Preview Your Perfect Hair Transformation in 360Â° âœ¨</title><link>https://huggingface.co/posts/ginipick/955296677233221</link><description>âœ¨ HairPick | Preview Your Perfect Hair Transformation in 360Â° âœ¨ ğŸŠ Free Trial for Hugging Face Launch! Hurry! â° Hello! Introducing an innovative AI service that helps you choose the perfect hairstyle without any regrets before visiting the salon! ğŸ¯ Try It Now ginigen/Hair-Pick ğŸ”„ What Makes HairPick Special? 360Â° Complete Preview! Other hair simulators only show the front view? ğŸ˜‘ HairPick is different! âœ… Front + 4 random angles = Total 5 multi-angle images generated âœ… Perfect check from side profile ğŸ‘¤ diagonal ğŸ“ back view ğŸ‘¥! âœ… 100+ trendy hairstyle library ğŸ’‡â€â™€ï¸ ğŸ’¡ Highly Recommended For: ğŸ¯ "I really don't want to fail this time!" â†’ Check side volume and back lines thoroughly ğŸ¯ "It's hard to explain exactly to my stylist" â†’ Perfect communication with 360Â° result images! ğŸ¯ "I have a profile photo/photoshoot coming up" â†’ Preview your best look from every angle ğŸš€ Super Simple Usage (Just 1 Minute!) 1ï¸âƒ£ One Selfie ğŸ“¸ Take a front-facing photo in bright light (show your forehead and face...</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/955296677233221</guid></item><item><title>benchmarked 9 models in 3 days. they were mostly below average in AHA score. p(doom) probably increased :(</title><link>https://huggingface.co/posts/etemiz/891816438009932</link><description>benchmarked 9 models in 3 days. they were mostly below average in AHA score. p(doom) probably increased :( See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/891816438009932</guid></item><item><title>Want to quickly try Gemma 3 270m? ğŸ’ğŸ’¬</title><link>https://huggingface.co/posts/anakin87/751707976654130</link><description>Want to quickly try Gemma 3 270m? ğŸ’ğŸ’¬ I made a simple Space to do that: anakin87/gemma-3-270m-it âš¡ Fast: Flash Attention, Zero GPU âš™ï¸ Configurable See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/anakin87/751707976654130</guid></item><item><title>Qwen's latest Image Edit model has been implemented with lightx2v's LoRA for 8-step lightning fast inferencing. Still a WIP, so YMMV.</title><link>https://huggingface.co/posts/LPX55/307818881554669</link><description>Qwen's latest Image Edit model has been implemented with lightx2v's LoRA for 8-step lightning fast inferencing. Still a WIP, so YMMV. https://huggingface.co/spaces/LPX55/Qwen-Image-Edit-Lightning-Fast See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/LPX55/307818881554669</guid></item><item><title>âœ… New Article: *Memory as Structured Time*</title><link>https://huggingface.co/posts/kanaria007/576453037371058</link><description>âœ… New Article: *Memory as Structured Time* Title: ğŸ§  History: Memory Loops as Civilization Structure ğŸ”— https://huggingface.co/blog/kanaria007/memory-loops-as-civilization-structure --- Summary: Memory is often treated as *storage and retrieval*. Structured Intelligence reframes it as *timeâ€‘shaping architecture*: * *Loops that preserve context and continuity* * *Rollback paths that enable reflection and correction* * *Patterns that turn experience into adaptive structure* &gt; Memory isnâ€™t static â€” &gt; *itâ€™s how intelligence edits time.* --- Why It Matters: â€¢ Reveals *how memory enables learning, identity, and adaptation* â€¢ Supports *AI that can reflect, revise, and selfâ€‘align* â€¢ Connects *personal cognition and collective history* as structural processes --- Whatâ€™s Inside: â€¢ Memory as *recursive structural loop* â€¢ *Failure and recovery* as part of adaptive recall â€¢ How *history and recordâ€‘keeping mirror cognitive memory* â€¢ Implications for *resilient AI and social knowledge systems* --- ğŸ“–...</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/576453037371058</guid></item><item><title>gpt-oss-120B scored 28 (one of the lowest) on AHA leaderboard. not very human aligned model.</title><link>https://huggingface.co/posts/etemiz/710778843328598</link><description>gpt-oss-120B scored 28 (one of the lowest) on AHA leaderboard. not very human aligned model. these kind of models are not really "free": they are costing you your freedom if you know what i mean. See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/710778843328598</guid></item><item><title>Added plug-and-play support for Qwen Image LoRA!  ğŸ¤—âš¡</title><link>https://huggingface.co/posts/prithivMLmods/366249407896156</link><description>Added plug-and-play support for Qwen Image LoRA! ğŸ¤—âš¡ Try it here: âœ¦ï¸ Qwen-Image (with LoRA): prithivMLmods/Qwen-Image-Diffusion âœ¦ï¸ Collection: prithivMLmods/image-gen-apps-diffusion-lastupdated-08-18-68a2f4c5ef3e5e394eacc20a See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/366249407896156</guid></item></channel></rss>