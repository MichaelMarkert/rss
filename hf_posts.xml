<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Access requests enabled for latest GLM models</title><link>https://huggingface.co/posts/bartowski/160920719239523</link><description>Access requests enabled for latest GLM models While a fix is being implemented ( https://github.com/ggml-org/llama.cpp/pull/12957 ) I want to leave the models up for visibility and continued discussion, but want to prevent accidental downloads of known broken models (even though there are settings that could fix it at runtime for now) With this goal, I've enabled access requests. I don't really want your data, so I'm sorry that I don't think there's a way around that? But that's what I'm gonna do for now, and I'll remove the gate when a fix is up and verified and I have a chance to re-convert and quantize! Hope you don't mind in the mean time :D See translation</description><pubDate>Thu, 17 Apr 2025 09:25:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/bartowski/160920719239523</guid></item><item><title>Agentic AI Era: Analyzing MCP vs MCO üöÄ</title><link>https://huggingface.co/posts/openfree/305569626054328</link><description>Agentic AI Era: Analyzing MCP vs MCO üöÄ Hello everyone! With the rapid advancement of AI agent technology, two architectures have come into the spotlight: MCP (Model Context Protocol) and MCO (Model Context Open-json). Today, we‚Äôll introduce the key features and differences of these two approaches. VIDraft/Agentic-AI-CHAT MCP: The Traditional Approach üèõÔ∏è Centralized Function Registry: All functions are hardcoded into the core system. Static Function Definitions &amp; Tight Coupling: New features require changes to the core application code, limiting scalability. Monolithic Design: Complex deployment and version management can cause a single error to affect the whole system. Code Example: '''py FUNCTION_REGISTRY = { "existing_function": existing_function, "new_function": new_function # Adding a new function } ''' MCO: A Revolutionary Approach üÜï JSON-based Function Definitions: Function details are stored in external JSON files, enabling dynamic module loading. Loose Coupling &amp;...</description><pubDate>Thu, 17 Apr 2025 09:25:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/305569626054328</guid></item><item><title>this paper has been blowing up</title><link>https://huggingface.co/posts/hesamation/690372011092552</link><description>this paper has been blowing up they train an open-source multimodal LLM (InternVL3) that can compete with GPT-4o and Claude 3.5 Sonnet by: &gt; training text and vision on a single stage &gt; a novel V2PE positional encoding &gt; SFT &amp; mixed preference optimization Paper: InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models (2504.10479) &gt; test-time scaling See translation</description><pubDate>Thu, 17 Apr 2025 09:25:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/690372011092552</guid></item><item><title>I know Hunyuan Video is yesterday's jam, but in case you're looking for some cinematic LoRA's (and don't like civitai for some reason), I've uploaded my most popular ones to hf. They are:</title><link>https://huggingface.co/posts/neph1/196685851091384</link><description>I know Hunyuan Video is yesterday's jam, but in case you're looking for some cinematic LoRA's (and don't like civitai for some reason), I've uploaded my most popular ones to hf. They are: 1980s fantasy: neph1/1980s_Fantasy_Movies_Hunyuan_Video_Lora 1950s scifi: neph1/50s_scifi_hunyuan_video_lora 1920s horror: neph1/1920s_horror_hunyuan_video_lora See translation</description><pubDate>Thu, 17 Apr 2025 09:25:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/neph1/196685851091384</guid></item><item><title>Try out the demo for Multimodal OCR featuring the implementation of models including</title><link>https://huggingface.co/posts/prithivMLmods/125773384088431</link><description>Try out the demo for Multimodal OCR featuring the implementation of models including RolmOCR and Qwen2VL OCR . The use case showcases image-text-to-text conversion and video understanding support for the RolmOCR model ! üöÄ ü§óMultimodal OCR Space : prithivMLmods/Multimodal-OCR üì¶The models implemented in this Space are: + RolmOCR : reducto/RolmOCR + Qwen2VL OCR : prithivMLmods/Qwen2-VL-OCR-2B-Instruct [ or ] + Qwen2VL OCR2 : prithivMLmods/Qwen2-VL-OCR2-2B-Instruct Qwen2VL OCR supports only image-text-to-text in the space. See translation</description><pubDate>Thu, 17 Apr 2025 09:25:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/125773384088431</guid></item><item><title>Today in Privacy &amp; AI Tooling - introducing a nifty new tool to examine where data goes in open-source apps on  ü§ó</title><link>https://huggingface.co/posts/yjernite/333157611385452</link><description>Today in Privacy &amp; AI Tooling - introducing a nifty new tool to examine where data goes in open-source apps on ü§ó HF Spaces have tons (100Ks!) of cool demos leveraging or examining AI systems - and because most of them are OSS we can see exactly how they handle user data üìöüîç That requires actually reading the code though, which isn't always easy or quick! Good news: code LMs have gotten pretty good at automatic review, so we can offload some of the work - here I'm using Qwen/Qwen2.5-Coder-32B-Instruct to generate reports and it works pretty OK üôå The app works in three stages: 1. Download all code files 2. Use the Code LM to generate a detailed report pointing to code where data is transferred/(AI-)processed (screen 1) 3. Summarize the app's main functionality and data journeys (screen 2) 4. Build a Privacy TLDR with those inputs It comes with a bunch of pre-reviewed apps/Spaces, great to see how many process data locally or through (private) HF endpoints ü§ó Note that this is a POC,...</description><pubDate>Thu, 17 Apr 2025 09:25:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yjernite/333157611385452</guid></item><item><title>Made a workable program that uses IREE runtime using Rust to inference wav2vec2-bert model for Automatic Speech Recognition.</title><link>https://huggingface.co/posts/Yehor/373485901957909</link><description>Made a workable program that uses IREE runtime using Rust to inference wav2vec2-bert model for Automatic Speech Recognition. See translation</description><pubDate>Thu, 17 Apr 2025 09:25:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Yehor/373485901957909</guid></item><item><title>We are excited to announce the release of our paper, "Cobra: Efficient Line Art COlorization with BRoAder References," along with the official code! Cobra is a novel efficient long-context fine-grained ID preservation framework for line art colorization, achieving high precision, efficiency, and flexible usability for comic colorization. By effectively integrating extensive contextual references, it transforms black-and-white line art into vibrant illustrations.</title><link>https://huggingface.co/posts/JunhaoZhuang/767916607687281</link><description>We are excited to announce the release of our paper, "Cobra: Efficient Line Art COlorization with BRoAder References," along with the official code! Cobra is a novel efficient long-context fine-grained ID preservation framework for line art colorization, achieving high precision, efficiency, and flexible usability for comic colorization. By effectively integrating extensive contextual references, it transforms black-and-white line art into vibrant illustrations. We invite you to explore Cobra and share your feedback! You can access the paper and code via the following links: [PDF]( https://arxiv.org/abs/2504.12240 ) and [Project page]( https://zhuang2002.github.io/Cobra/ ). We eagerly anticipate your engagement and support! Thank you for your interest! See translation</description><pubDate>Thu, 17 Apr 2025 09:25:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/JunhaoZhuang/767916607687281</guid></item><item><title>Want to see machine learning algorithms training?</title><link>https://huggingface.co/posts/gavinkhung/300399121852584</link><description>Want to see machine learning algorithms training? I made a website: https://gavinkhung.github.io/machine-learning-visualized/ The website implements, visualizes, and mathematically derives machine learning algorithms from first-principles. Feel free to contribute to this open-source resource: https://github.com/gavinkhung/machine-learning-visualized See translation</description><pubDate>Thu, 17 Apr 2025 09:25:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/gavinkhung/300399121852584</guid></item><item><title>BREAKING NEWS! üöÄ OpenAI‚Äôs GPT-4.1 API Models Are Here ‚Äì Built for Developers</title><link>https://huggingface.co/posts/luigi12345/402431491640847</link><description>BREAKING NEWS! üöÄ OpenAI‚Äôs GPT-4.1 API Models Are Here ‚Äì Built for Developers OpenAI has launched GPT-4.1, GPT-4.1 Mini, and GPT-4.1 Nano‚Äîmodels engineered for real-world coding, instruction following, and long-context tasks. Ôøº üîß Key Dev Features ‚Ä¢ Coding Performance: GPT-4.1 scores 54.6% on SWE-bench Verified, outperforming GPT-4o by 21.4% and GPT-4.5 by 26.6%. It handles diffs more precisely, reduces unnecessary edits, and adheres to formatting constraints. Ôøº ‚Ä¢ Long Context: All models support up to 1 million tokens‚Äî8x more than GPT-4o‚Äîenabling full repo analysis and deep document comprehension. Ôøº ‚Ä¢ Instruction Following: Improved multi-step reasoning and formatting accuracy, with a 10.5% gain over GPT-4o on MultiChallenge. Ôøº ‚Ä¢ Latency &amp; Cost: GPT-4.1 is 40% faster and 80% cheaper per query than GPT-4o. Mini and Nano versions offer even greater speed and affordability. Ôøº üß† Model Lineup Model Context Window Use Case Cost per 1M Tokens GPT-4.1 1M tokens Production-grade coding &amp;...</description><pubDate>Thu, 17 Apr 2025 09:25:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/luigi12345/402431491640847</guid></item></channel></rss>