<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>"Why did the bee get married?"</title><link>https://huggingface.co/posts/jasoncorkill/847126227827487</link><description>"Why did the bee get married?" "Because he found his honey!" This was the "funniest" joke out of 10'000 jokes we generated with LLMs. With 68% of respondents rating it as "funny". Original jokes are particularly hard for LLMs, as jokes are very nuanced and a lot of context is needed to understand if something is "funny". Something that can only reliably be measured using humans. LLMs are not equally good at generating jokes in every language. Generated English jokes turned out to be way funnier than the Japanese ones. 46% of English-speaking voters on average found the generated joke funny. The same statistic for other languages: Vietnamese: 44% Portuguese: 40% Arabic: 37% Japanese: 28% There is not much variance in generation quality among models for any fixed language. But still Claude Sonnet 4 slightly outperforms others in Vietnamese, Arabic and Japanese and Gemini 2.5 Flash in Portuguese and English We have release the 1 Million (!) native speaker ratings and the 10'000 jokes...</description><pubDate>Thu, 17 Jul 2025 13:40:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/847126227827487</guid></item><item><title>I couldn't watch innocent people get their rights trampled anymore. So I built something to help.</title><link>https://huggingface.co/posts/Severian/232302079144255</link><description>I couldn't watch innocent people get their rights trampled anymore. So I built something to help. Stories of families torn apart, U.S. citizens detained for hours, people arrested just for speaking Spanish. This isn't the America I believe in. Instead of doom-scrolling, I spent a few days building FIREWATCH - a free civil rights protection app. What it does: ‚Ä¢ Real-time ICE raid alerts ‚Ä¢ Know Your Rights education in 10+ languages ‚Ä¢ Secure evidence recording ‚Ä¢ Emergency panic button ‚Ä¢ Legal hotlines and resources ‚Ä¢ 100% private, no tracking The catch? There isn't one. You just need a free Google API key that stays on your device. Works completely offline. https://firewatch-ice.vercel.app/ I built this because everyone deserves constitutional protection. The 4th Amendment doesn't have an asterisk. If this helps one family stay safe, every sleepless night was worth it. Please share with anyone who needs it. Stay safe. See translation</description><pubDate>Thu, 17 Jul 2025 13:40:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Severian/232302079144255</guid></item><item><title>We've moved over 20PB from Git LFS to Xet on the Hub without downtime or data loss. Having things "just work" on a migration of this scale is about as good as it gets.</title><link>https://huggingface.co/posts/jsulz/304869821441099</link><description>We've moved over 20PB from Git LFS to Xet on the Hub without downtime or data loss. Having things "just work" on a migration of this scale is about as good as it gets. Now, we're migrating the rest of the Hub https://huggingface.co/blog/migrating-the-hub-to-xet But how did we get here? In the early days of joining Hugging Face, we made a few key design decisions: * There would be no "hard cut-over" from Git LFS to Xet * A Xet-enabled repository should be able to contain both Xet and LFS files * Repository migrations from LFS to Xet can run in the background without disrupting downloads or uploads These were largely driven by our desire to ensure the community could keep working without interruption. We cover the infrastructure making this all go in this post, specifically: * An integral piece of infrastructure known internally as the Git LFS Bridge * Background content migrations that run around the clock To skip the wait and join Xet now, sign up here...</description><pubDate>Thu, 17 Jul 2025 13:40:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/304869821441099</guid></item><item><title>You might not have heard of Moonshot AI ‚Äî but within 24 hours, their new model Kimi K2 shot to the top of Hugging Face‚Äôs trending leaderboard.</title><link>https://huggingface.co/posts/fdaudens/102813247813198</link><description>You might not have heard of Moonshot AI ‚Äî but within 24 hours, their new model Kimi K2 shot to the top of Hugging Face‚Äôs trending leaderboard. So‚Ä¶ who are they, and why does it matter? Had a lot of fun co-writing this blog post with @ xianbao , with key insights translated from Chinese, to unpack how this startup built a model that outperforms GPT-4.1, Claude Opus, and DeepSeek V3 on several major benchmarks. üßµ A few standout facts: 1. From zero to $3.3B in 18 months: Founded in March 2023, Moonshot is now backed by Alibaba, Tencent, Meituan, and HongShan. 2. A CEO who thinks from the end: Yang Zhilin (31) previously worked at Meta AI, Google Brain, and Carnegie Mellon. His vision? Nothing less than AGI ‚Äî still a rare ambition among Chinese AI labs. 3. A trillion-parameter model that‚Äôs surprisingly efficient: Kimi K2 uses a mixture-of-experts architecture (32B active params per inference) and dominates on coding/math benchmarks. 4. The secret weapon: Muon optimizer: A new training...</description><pubDate>Thu, 17 Jul 2025 13:40:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/102813247813198</guid></item><item><title>Join us in Austin tomorrow for AI Camp‚Äôs monthly meetup.</title><link>https://huggingface.co/posts/GeorgiaArm/326406743119964</link><description>Join us in Austin tomorrow for AI Camp‚Äôs monthly meetup. Arm‚Äôs Zach Lasiuk and Geremy Cohen will dive into ‚ÄúFrom Model to Product: Right-Sizing Infrastructure for Real-World Use Cases.‚Äù RSVP here üëâ https://www.aicamp.ai/event/eventdetails/W2025071616 See translation</description><pubDate>Thu, 17 Jul 2025 13:40:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/GeorgiaArm/326406743119964</guid></item><item><title>Is your code written by a human or an AI? ü§ñ</title><link>https://huggingface.co/posts/YerbaPage/335085486816500</link><description>Is your code written by a human or an AI? ü§ñ With the rise of AI coding assistants, this question is more critical than ever. Our new tool, DetectCodeGPT, effectively identifies AI-generated code, outperforming SOTA methods with a 7.6% increase in AUC! How? By analyzing unique stylistic and syntactic patterns in code, not just the text. üëá Explore more: Paper (ICSE 2025): https://arxiv.org/html/2401.06461v2 Code: https://github.com/YerbaPage/DetectCodeGPT #AI #Programming #DeveloperTools #LLM #GenAI See translation</description><pubDate>Thu, 17 Jul 2025 13:40:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/335085486816500</guid></item><item><title>Made some 245GB (80% size reduction) 1.8bit quants for Kimi K2!</title><link>https://huggingface.co/posts/danielhanchen/383539577783045</link><description>Made some 245GB (80% size reduction) 1.8bit quants for Kimi K2! unsloth/Kimi-K2-Instruct-GGUF See translation</description><pubDate>Thu, 17 Jul 2025 13:40:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/383539577783045</guid></item><item><title>Fine-tune Gemma3n on videos with audios inside with Colab A100 üî•</title><link>https://huggingface.co/posts/merve/535700058492148</link><description>Fine-tune Gemma3n on videos with audios inside with Colab A100 üî• Just dropped the notebook where you can learn how to fine-tune Gemma3n on images+audio+text at the same time! keep in mind, it's made for educational purposes ü´° we do LoRA, audio resampling &amp; video downsampling to be able to train &lt;40GB VRAM stretch modalities and unfreeze layers as you wish! üôèüèª merve/smol-vision See translation</description><pubDate>Thu, 17 Jul 2025 13:40:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/535700058492148</guid></item><item><title>üéâ Dhanishtha-2.0-preview-0725 is Now Live</title><link>https://huggingface.co/posts/Abhaykoul/633416293671295</link><description>üéâ Dhanishtha-2.0-preview-0725 is Now Live The Intermediate Thinking Model just got even better. With the new update, Dhanishtha is now sharper, smarter, and trained further on tool use üß† What Makes Dhanishtha Different? Unlike standard COT models that give one-shot responses, Dhanishtha thinks in layers: &gt; Think ‚Üí Answer ‚Üí Rethink ‚Üí Improve ‚Üí Rethink again if needed. HelpingAI/Dhanishtha-2.0-preview-0725 See translation</description><pubDate>Thu, 17 Jul 2025 13:40:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/633416293671295</guid></item><item><title>üéØ AGI NOVEL Generator: The First Step Toward True AI Creativity</title><link>https://huggingface.co/posts/openfree/484414010135985</link><description>üéØ AGI NOVEL Generator: The First Step Toward True AI Creativity openfree/AGI-NOVEL Can AI Write a 100,000-Word Novel? What's the ultimate test for AGI (Artificial General Intelligence)? Calculation? Logic? Or creativity? We tackled the hardest creative challenge: A single AI writing a full-length novel with consistent voice from beginning to end. üöÄ Core Innovations Single Writer System: Not fragmented texts from multiple AIs, but a genuine novel by one author Immediate Critique System: Real-time literary critique and revision for each part 170 Quadrillion Themes: Infinite creative possibilities (4.6 million years at 100 novels/day!) Philosophical Depth: Nobel Prize-level existential exploration and social insight üé≤ Infinite Possibilities "The day my father died, I discovered he had another family he'd hidden all his life." One random click generates a powerful opening sentence and a completely new story begins. üìä Technical Achievements 8,000-word novella auto-generation...</description><pubDate>Thu, 17 Jul 2025 13:40:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/484414010135985</guid></item></channel></rss>