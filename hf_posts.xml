<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>I'm a Hugging Face Fellow now, guys!ü§ó‚ù§Ô∏è</title><link>https://huggingface.co/posts/prithivMLmods/245628043869780</link><description>I'm a Hugging Face Fellow now, guys!ü§ó‚ù§Ô∏è With the same passion, trust, and momentum to contribute to the community, I‚Äôm excited to do some amazing things to wrap up Q3 and Q4 of 2025. And importantly, I‚Äôve been lucky enough to receive some knowledge and guidance from @ merve to build open-source demos and stuff. Thank you for the belief. Thank you ‚Äî much love. Long live open source! ‚Äî Prithiv See translation</description><pubDate>Thu, 18 Sep 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/245628043869780</guid></item><item><title>Reproducing research code shouldn't take longer than reading the paper.</title><link>https://huggingface.co/posts/salma-remyx/494327094243098</link><description>Reproducing research code shouldn't take longer than reading the paper. For papers that include code, setting up the right environment often means hours of dependency hell and configuration debugging. At Remyx AI, we built an agent that automatically creates and tests Docker images for research papers, then shares them publicly so anyone can reproduce results with a single command. We just submitted PR #908 to integrate this directly into arXiv Labs. If you believe in making reproducible research accessible to everyone, give it a bump!: https://github.com/arXiv/arxiv-browse/pull/908 See translation</description><pubDate>Thu, 18 Sep 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/494327094243098</guid></item><item><title>IBM just released small swiss army knife for the document models: granite-docling-258M on Hugging Face üî•</title><link>https://huggingface.co/posts/merve/445414862119862</link><description>IBM just released small swiss army knife for the document models: granite-docling-258M on Hugging Face üî• &gt; not only a document converter but also can do document question answering, understand multiple languages ü§Ø &gt; best part: released with Apache 2.0 license üëè use it with your commercial projects! &gt; it supports transformers, vLLM and MLX from the get-go! ü§ó &gt; built on SigLIP2 &amp; granite-165M model: ibm-granite/granite-docling-258M demo: ibm-granite/granite-docling-258m-demo üíó See translation</description><pubDate>Thu, 18 Sep 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/445414862119862</guid></item><item><title>We're kick-starting the process of Transformers v5, with</title><link>https://huggingface.co/posts/lysandre/194539610907979</link><description>We're kick-starting the process of Transformers v5, with @ ArthurZ and @ cyrilvallez ! v5 should be significant: we're using it as a milestone for performance optimizations, saner defaults, and a much cleaner code base worthy of 2025. Fun fact: v4.0.0-rc-1 came out on Nov 19, 2020, nearly five years ago! See translation</description><pubDate>Thu, 18 Sep 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lysandre/194539610907979</guid></item><item><title>Search is such a fundamental part of content discovery, yet ends up overlooked or poorly implemented in so many apps we use every day.</title><link>https://huggingface.co/posts/salma-remyx/227310661152992</link><description>Search is such a fundamental part of content discovery, yet ends up overlooked or poorly implemented in so many apps we use every day. We built hundreds of Docker images for arXiv papers with a codebase - it's tough to find what you're looking for unless you happen to have the arXiv id handy using DockerHub's search. So we added full text search over these resources so that you're that much closer to testing a new promising idea. More resources to be indexed soon! Full Demo: https://www.youtube.com/watch?v=GjYReWbQZw8 Try it here!: https://engine.remyx.ai/resources Join us at Experiment 2025!: https://experiment.remyx.ai See translation</description><pubDate>Thu, 18 Sep 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/227310661152992</guid></item><item><title>ü§ñ As AI-generated content is shared in movies/TV/across the web, there's one simple low-hanging fruit üçá to help know what's real: Visible watermarks. With the Gradio team, I've made sure it's trivially easy to add this disclosure to images, video, chatbot text. See how:</title><link>https://huggingface.co/posts/meg/340948346361550</link><description>ü§ñ As AI-generated content is shared in movies/TV/across the web, there's one simple low-hanging fruit üçá to help know what's real: Visible watermarks. With the Gradio team, I've made sure it's trivially easy to add this disclosure to images, video, chatbot text. See how: https://huggingface.co/blog/watermarking-with-gradio Thanks to the code collab in particular from @ abidlabs and Yuvraj Sharma. See translation</description><pubDate>Thu, 18 Sep 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/meg/340948346361550</guid></item><item><title>Are LLMs just memorizing benchmarks?</title><link>https://huggingface.co/posts/YerbaPage/794883696625717</link><description>Are LLMs just memorizing benchmarks? We developed LastingBench to stop the "cheating" and ensure fair AI evaluation. Our LastingBench has been accepted to #EMNLP2025 Findings! üéâ Paper: https://arxiv.org/abs/2506.21614 Code: https://github.com/Seriousss/LastingBench #NLP #EMNLP2025 See translation</description><pubDate>Thu, 18 Sep 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/794883696625717</guid></item><item><title>Many of 'em pinged me asking to make the</title><link>https://huggingface.co/posts/prithivMLmods/792537677776345</link><description>Many of 'em pinged me asking to make the nano-banana-aio to available on hf.co/spaces, so I‚Äôve transferred the app‚Äôs tech stack to make it compatible for deployment on Spaces.ü§ó‚≠êÔ∏è ‚ú¶ Yes, it is now available on Spaces: prithivMLmods/Nano-Banana-AIO Nano Banana AIO (All-in-One) App, which offers seamless image manipulation features, including single/multiple image adaptation, a canvas for free-style drawing to creative image generation, and standard text-to-image generation. GitHub: https://github.com/PRITHIVSAKTHIUR/Nano-Banana-AIO-HF About: https://huggingface.co/spaces/prithivMLmods/Nano-Banana-AIO/blob/main/README.md (Remember, it only supports SFW content.) All in One Banana for you! üòâ See translation</description><pubDate>Thu, 18 Sep 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/792537677776345</guid></item><item><title>Gradio 6.0 is launching this year!</title><link>https://huggingface.co/posts/freddyaboulton/832825198188069</link><description>Gradio 6.0 is launching this year! We're revamping the core to give you performance improvements and unprecedented customization. Build better, faster. Check out the GitHub milestone to learn what's planned under the hood! https://github.com/gradio-app/gradio/issues?q=is:issue%20state:open%20milestone:%22Gradio%206%22 See translation</description><pubDate>Thu, 18 Sep 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/freddyaboulton/832825198188069</guid></item><item><title>COMPUTER CONTROL IS ON-DEVICE !</title><link>https://huggingface.co/posts/Tonic/265427502664617</link><description>COMPUTER CONTROL IS ON-DEVICE ! üè°ü§ñ 78 % of EU smart-home owners DON‚ÄôT trust cloud voice assistants. So we killed the cloud. Meet Ext√©: a palm-sized Android device that sees, hears &amp; speaks your language - 100 % offline, 0 % data sent anywhere. üîì We submitted our technologies for consideration to the Liquid AI hackathon. üìä Dataset: 79 k UI-action pairs on Hugging Face (largest Android-control corpus ever) Tonic/android-operator-episodes ‚ö° Model: 98 % task accuracy, 678MB compressed , fits on existing android devices ! Tonic/l-android-control üõ§Ô∏è Experiment Tracker : check out the training on our TrackioApp Tonic/l-android-control üéÆ Live Model Demo: Upload an Android Screenshot and instructions to see the model in action ! Tonic/l-operator-demo Built in a garage, funded by pre-orders, no VC. Now we‚Äôre scaling to 1 k installer units. We‚Äôre giving 50 limited-edition prototypes to investors , installers &amp; researchers who want to co-design the sovereign smart home. üëá Drop ‚ÄúEUSKERA‚Äù in the...</description><pubDate>Thu, 18 Sep 2025 17:19:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Tonic/265427502664617</guid></item></channel></rss>