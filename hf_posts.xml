<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥</title><link>https://huggingface.co/posts/nouamanetazi/972464132222376</link><description>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥ Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious ğğ‚ğ‚ğ‹ ğğ«ğ«ğ¨ğ«ğ¬, or when your expensive GPU cluster is running at ğŸ”ğŸ% ğğŸğŸğ¢ğœğ¢ğğ§ğœğ², the problem isn't your model. It's most probably a ğ¦ğ¢ğ¬ğ®ğ¬ğ ğ¨ğŸ ğ­ğ¡ğ ğ¡ğšğ«ğğ°ğšğ«ğ. ğŸ› ï¸ Questions that seemed simple but had no clear answers: Why is ğŒğ¨ğ„ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğ¬ğ¥ğ¨ğ°ğğ« ğ­ğ¡ğšğ§ ğğğ§ğ¬ğ ğ¦ğ¨ğğğ¥ğ¬? Which ğğ‚ğ‚ğ‹ ğŸğ¥ğšğ ğ¬ should we actually set? How often should we checkpoint without killing throughput? That's why we built ğ“ğ¡ğ ğ’ğ¦ğ¨ğ¥ ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğğ¥ğšğ²ğ›ğ¨ğ¨ğ¤ ğŸ“–: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¥ğšğ²ğğ« that most teams get wrong. We validated real vs...</description><pubDate>Sun, 02 Nov 2025 09:20:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nouamanetazi/972464132222376</guid></item><item><title>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the</title><link>https://huggingface.co/posts/sergiopaniego/207791817757812</link><description>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the @ TogetherAgent x @ huggingface workshop we hosted in our Paris office ğŸƒ! Link: https://github.com/sergiopaniego/talks/blob/main/fine_tuning_with_trl/Fine%20tuning%20with%20TRL%20(Oct%2025).pdf See translation</description><pubDate>Sun, 02 Nov 2025 09:20:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/207791817757812</guid></item><item><title>Starts erasing! ğŸ‰ ğŸ‰ ğŸ‰</title><link>https://huggingface.co/posts/piercus/167394123498038</link><description>Starts erasing! ğŸ‰ ğŸ‰ ğŸ‰ This is made with a one-step SD1.5 LBM [1] eraser ! Data is open. Data pipeline is open. Training code is open. On our LBM fork : https://github.com/finegrain-ai/LBM [1] LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) See translation</description><pubDate>Sun, 02 Nov 2025 09:20:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/piercus/167394123498038</guid></item><item><title>ğŸš€ğŸ‘ŒğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ¤ŒğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/744756733617336</link><description>ğŸš€ğŸ‘ŒğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ¤ŒğŸš€ ğŸ“„ Title: Understanding Co-speech Gestures in-the-wild ğŸ” ğŸ“ Description: JEGAL is a tri-modal model that learns from gestures, speech and text simultaneously, enabling devices to interpret co-speech gestures in the wild. ğŸ‘¥ Authors: @ sindhuhegde , K R Prajwal, Taein Kwon, and Andrew Zisserman ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: Understanding Co-speech Gestures in-the-wild (2503.22668) ğŸŒ Web Page: https://www.robots.ox.ac.uk/~vgg/research/jegal ğŸ“ Repository: https://github.com/Sindhu-Hegde/jegal ğŸ“º Video: https://www.youtube.com/watch?v=TYFOLKfM-rM ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the Human Modeling Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/human-modeling.md ğŸ“š More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers curated by @ DmitryRyumin ğŸ” Keywords:...</description><pubDate>Sun, 02 Nov 2025 09:20:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/744756733617336</guid></item><item><title>A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on â†—ï¸</title><link>https://huggingface.co/posts/prithivMLmods/710644146568512</link><description>A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on â†—ï¸ https://huggingface.co/blog/prithivMLmods/multimodal-ocr-vlms on behalf of strangervisionhf It discusses the latest trends in OCR models, the multilingual support offered by modern OCR systems, their unique capabilities, OCR benchmark model comparisons, transformer-based implementations, and strategies for streamlining transformers compatibility. See translation</description><pubDate>Sun, 02 Nov 2025 09:20:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/710644146568512</guid></item><item><title>*** Happy Halloween - Embrace the Horror ! ***</title><link>https://huggingface.co/posts/DavidAU/433692013361833</link><description>*** Happy Halloween - Embrace the Horror ! *** Unsloth fine tunes using in house horror dataset. Gemma 3 - 1B, 4B, two 12Bs and 27B (uploaded yesterday) Qwen 3 - 1.7B [two] - new today... and , 4B, 6B, 42B ... And 32 MORE horror models: https://huggingface.co/DavidAU/models?search=horror Collection: https://huggingface.co/collections/DavidAU/grand-horror-165b-horror-and-fiction-generation Enjoy ; See translation</description><pubDate>Sun, 02 Nov 2025 09:20:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DavidAU/433692013361833</guid></item><item><title>Introducing the Medical-o1-Reasoning-SFT-Japanese dataset ğŸ‰</title><link>https://huggingface.co/posts/ronantakizawa/591564562942305</link><description>Introducing the Medical-o1-Reasoning-SFT-Japanese dataset ğŸ‰ This dataset is a Japanese dataset consisting questions, reasoning, and answer results for complex medical topics. #japanese #medical #dataset ronantakizawa/Medical-o1-Reasoning-SFT-Japanese See translation</description><pubDate>Sun, 02 Nov 2025 09:20:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/591564562942305</guid></item><item><title>Kimi K2 is a bit disappointing by my expectations. It is on a par with Codex mini.</title><link>https://huggingface.co/posts/onekq/456763679689481</link><description>Kimi K2 is a bit disappointing by my expectations. It is on a par with Codex mini. onekq-ai/WebApp1K-models-leaderboard See translation</description><pubDate>Sun, 02 Nov 2025 09:20:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/456763679689481</guid></item><item><title>At</title><link>https://huggingface.co/posts/branikita/910220398337791</link><description>At Robonine , we applied topology optimization to enhance the stiffness and efficiency of a robotic manipulator. Using HyperMesh with the OptiStruct solver, we defined the design space where each element had a pseudo-density coefficient (0â€“1) controlling stiffness. This allowed the algorithm to continuously redistribute material toward regions with higher strain energy â€” much like how a fluid naturally flows to balance pressure. Results: - Aluminum bracket: displacement reduced by 0.16 mm - Steel bracket: displacement reduced from 1.05 mm â†’ 0.63 mm - Steel clamp: displacement reduced by 0.14 mm - Final structure: optimized geometry with improved load distribution and reduced deformation This project highlights how advanced structural optimization can significantly improve performance while minimizing material usage â€” shaping the next generation of robotic design. See translation</description><pubDate>Sun, 02 Nov 2025 09:20:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/910220398337791</guid></item><item><title>I am dedicating this weekend to practicing/reading the latest b(ook)log from hugging face. It is meant to be a guide for anyone trying to go from â€œwe have a great dataset and GPUsâ€ to â€œwe built a really strong model.â€ Will share thoughts upon completion.</title><link>https://huggingface.co/posts/Shivansh000/941986646578616</link><description>I am dedicating this weekend to practicing/reading the latest b(ook)log from hugging face. It is meant to be a guide for anyone trying to go from â€œwe have a great dataset and GPUsâ€ to â€œwe built a really strong model.â€ Will share thoughts upon completion. Thanks for the treat @ eliebak @ ThomasWolf and HF team! HuggingFaceTB/smol-training-playbook See translation</description><pubDate>Sun, 02 Nov 2025 09:20:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Shivansh000/941986646578616</guid></item></channel></rss>