<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>‚ú® High-Resolution Ghibli Style Image Generator ‚ú®</title><link>https://huggingface.co/posts/aiqtech/202174985893140</link><description>‚ú® High-Resolution Ghibli Style Image Generator ‚ú® üåü Introducing FLUX Ghibli LoRA Hello everyone! Today I'm excited to present a special LoRA model for FLUX Dev.1. This model leverages a LoRA trained on high-resolution Ghibli images for FLUX Dev.1 to easily create beautiful Ghibli-style images with stunning detail! üé® space: aiqtech/FLUX-Ghibli-Studio-LoRA model: openfree/flux-chatgpt-ghibli-lora üîÆ Key Features Trained on High-Resolution Ghibli Images - Unlike other LoRAs, this one is trained on high-resolution images, delivering sharper and more beautiful results Powered by FLUX Dev.1 - Utilizing the latest FLUX model for faster generation and superior quality User-Friendly Interface - An intuitive UI that allows anyone to create Ghibli-style images with ease Diverse Creative Possibilities - Express various themes in Ghibli style, from futuristic worlds to fantasy elements üñºÔ∏è Sample Images Include "Ghibli style" in your prompts Try combining nature, fantasy elements, futuristic...</description><pubDate>Tue, 01 Apr 2025 05:22:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aiqtech/202174985893140</guid></item><item><title>ü§ó Hug Contributors</title><link>https://huggingface.co/posts/aiqtech/518766175001571</link><description>ü§ó Hug Contributors Hugging Face Contributor Dashboard üë®‚Äçüíªüë©‚Äçüíª aiqtech/Contributors-Leaderboard üìä Key Features Contributor Activity Tracking: Visualize yearly and monthly contributions through interactive calendars Top 100 Rankings: Provide rankings based on models, spaces, and dataset contributions Detailed Analysis: Analyze user-specific contribution patterns and influence Visualization: Understand contribution activities at a glance through intuitive charts and graphs üåü Core Visualization Elements Contribution Calendar: Track activity patterns with GitHub-style heatmaps Radar Chart: Visualize balance between models, spaces, datasets, and activity levels Monthly Activity Graph: Identify most active months and patterns Distribution Pie Chart: Analyze proportion by contribution type üèÜ Ranking System Rankings based on overall contributions, spaces, and models Automatic badges for top 10, 30, and 100 contributors Ranking visualization to understand your position in the community üí° How...</description><pubDate>Tue, 01 Apr 2025 05:22:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aiqtech/518766175001571</guid></item><item><title>Hi all,</title><link>https://huggingface.co/posts/hanzla/237314499914963</link><description>Hi all, Last week, I open sourced Free Search API. It allows sourcing results from top search engines (including google, bing) for free. It uses searxng instances for this purpose. I was overwhelmed by community's response and I am glad for all the support and suggestions. So today, I have pushed several improvements that make this API more stable. These improvements include 1) Parallel scrapping of search results for faster response 2) Markdown formatting of search results 3) Prioritizing SearXNG instances that have faster google response time 4) Update/Get endpoints for searxng instances. Github: https://github.com/HanzlaJavaid/Free-Search/tree/main Try the deployed version: https://freesearch.replit.app/docs I highly appreciate PRs, issues, stars, and any kind of feedback. Let's join hands, and make it real big! See translation</description><pubDate>Tue, 01 Apr 2025 05:22:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hanzla/237314499914963</guid></item><item><title>üöÄ Gemma3-R1984-27B: Next Generation Agentic AI Platform</title><link>https://huggingface.co/posts/openfree/214646053127729</link><description>üöÄ Gemma3-R1984-27B: Next Generation Agentic AI Platform Model Path: VIDraft/Gemma-3-R1984-27B Space: VIDraft/Gemma-3-R1984-27B git clone VIDraft/Gemma-3-R1984-27B üí´ A New Frontier in AI Innovation Gemma3-R1984-27B is a powerful agentic AI platform built on Google's Gemma-3-27B model. It integrates state-of-the-art deep research via web search with multimodal file processing capabilities and handles long contexts up to 8,000 tokens. Designed for local deployment on independent servers using NVIDIA A100 GPUs, it provides high security and prevents data leakage. üîì Uncensored and Unrestricted AI Experience Gemma3-R1984-27B comes with all censorship restrictions removed, allowing users to operate any persona without limitations. The model perfectly implements various roles and characters according to users' creative requests, providing unrestricted responses that transcend the boundaries of conventional AI. This unlimited interaction opens infinite possibilities across research, creative...</description><pubDate>Tue, 01 Apr 2025 05:22:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/214646053127729</guid></item><item><title>You can now run DeepSeek-V3-0324 on your own local device!</title><link>https://huggingface.co/posts/danielhanchen/465464088880734</link><description>You can now run DeepSeek-V3-0324 on your own local device! Run our Dynamic 2.42 and 2.71-bit DeepSeek GGUFs: unsloth/DeepSeek-V3-0324-GGUF You can run them on llama.cpp and other inference engines. See our guide here: https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-v3-0324-locally See translation</description><pubDate>Tue, 01 Apr 2025 05:22:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/465464088880734</guid></item><item><title>The new DeepSite space is really insane for vibe-coders</title><link>https://huggingface.co/posts/thomwolf/431321353076398</link><description>The new DeepSite space is really insane for vibe-coders enzostvs/deepsite With the wave of vibe-coding-optimized LLMs like the latest open-source DeepSeek model (version V3-0324), you can basically prompt out-of-the-box and create any app and game in one-shot. It feels so powerful to me, no more complex framework or under-the-hood prompt engineering to have a working text-to-app tool. AI is eating the world and *open-source* AI is eating AI itself! PS: and even more meta is that the DeepSite app and DeepSeek model are both fully open-source code =&gt; time to start recursively improve? PPS: you still need some inference hosting unless you're running the 600B param model at home, so check the very nice list of HF Inference Providers for this model: deepseek-ai/DeepSeek-V3-0324 See translation</description><pubDate>Tue, 01 Apr 2025 05:22:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/thomwolf/431321353076398</guid></item><item><title>Wohoo ü•≥ I have finished my 2025 GPU workstation build and I am very excited to train new awesome open source models on it.</title><link>https://huggingface.co/posts/stefan-it/513898057053383</link><description>Wohoo ü•≥ I have finished my 2025 GPU workstation build and I am very excited to train new awesome open source models on it. I built my last GPU workstation 5 years ago featuring an AMD Ryzen 5900X, 64GB of G.SKILL Trident Z RGB on an ASRock X570 Taichi cooled by an Alphacool Eisb√§r 420. GPU was a Zotac RTX 3090 AMP Extreme. Unfortunately, I was never satisfied with the case - some Fractal Define 7, as it is definitely too small, airflow is not optimal as I had to open the front door all the time and it also arrived with a partly damaged side panel. For my new build, I've used the following components: an outstanding new AMD Ryzen 9950X3D with 64GB of Corsair Dominator Titanium (what a name). As a huge Noctua fan - warm greetings to my Austrian neighbors - I am using the brand new Noctua NH-D15 G2 on an ASRock X870E Taichi in an amazing Lian Li LANCOOL III chassis. One joke that only NVIDIA Blackwell users will understand: you definitely need a tempered glass panel to check if your...</description><pubDate>Tue, 01 Apr 2025 05:22:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/stefan-it/513898057053383</guid></item><item><title>Are you interesting in different runtimes for AI models?</title><link>https://huggingface.co/posts/Yehor/460907086720491</link><description>Are you interesting in different runtimes for AI models? Check out IREE (iree.dev), it convert models to MLIR and then execute on different platforms. I have tested it in Rust on CPU and CUDA: https://github.com/egorsmkv/eerie-yolo11 See translation</description><pubDate>Tue, 01 Apr 2025 05:22:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Yehor/460907086720491</guid></item><item><title>AReal-Boba üî• a fully open RL Frameworks released by AntGroup, an affiliate company of Alibaba.</title><link>https://huggingface.co/posts/AdinaY/252351292657061</link><description>AReal-Boba üî• a fully open RL Frameworks released by AntGroup, an affiliate company of Alibaba. inclusionAI/areal-boba-67e9f3fa5aeb74b76dcf5f0a ‚ú® 7B/32B - Apache2.0 ‚ú® Outperform on math reasoning ‚ú® Replicating QwQ-32B with 200 data under $200 ‚ú® All-in-one: weights, datasets, code &amp; tech report See translation</description><pubDate>Tue, 01 Apr 2025 05:22:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/252351292657061</guid></item><item><title>‚ÄºÔ∏è huggingface_hub's v0.30.0 is out with our biggest update of the past two years!</title><link>https://huggingface.co/posts/Wauplin/747413191251683</link><description>‚ÄºÔ∏è huggingface_hub's v0.30.0 is out with our biggest update of the past two years! Full release notes: https://github.com/huggingface/huggingface_hub/releases/tag/v0.30.0 . üöÄ Ready. Xet. Go! Xet is a groundbreaking new protocol for storing large objects in Git repositories, designed to replace Git LFS. Unlike LFS, which deduplicates files, Xet operates at the chunk level‚Äîmaking it a game-changer for AI builders collaborating on massive models and datasets. Our Python integration is powered by [xet-core]( https://github.com/huggingface/xet-core ), a Rust-based package that handles all the low-level details. You can start using Xet today by installing the optional dependency: pip install -U huggingface_hub[hf_xet] With that, you can seamlessly download files from Xet-enabled repositories! And don‚Äôt worry‚Äîeverything remains fully backward-compatible if you‚Äôre not ready to upgrade yet. Blog post: https://huggingface.co/blog/xet-on-the-hub Docs:...</description><pubDate>Tue, 01 Apr 2025 05:22:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Wauplin/747413191251683</guid></item></channel></rss>