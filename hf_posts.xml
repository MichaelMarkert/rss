<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ovi054/LTX-2-19b-Squish-LoRA</title><link>https://huggingface.co/posts/ovi054/176731931119322</link><description>ovi054/LTX-2-19b-Squish-LoRA âš¡ I trained a Squish LoRA for LTX-2. Upload an image and give prompt "squish it" to get the squish video. Demo output videos are attached. ğŸ‘‰Try it now: ovi054/LTX-2-19b-Squish-LoRA ovi054/ltx-2-Audio-to-Video See translation</description><pubDate>Wed, 28 Jan 2026 09:39:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/176731931119322</guid></item><item><title>SecureCode: security-aware code models (3Bâ€“20B), trained for review + remediation</title><link>https://huggingface.co/posts/scthornton/483996346337767</link><description>SecureCode: security-aware code models (3Bâ€“20B), trained for review + remediation Iâ€™ve been frustrated by how often code assistants recommend patterns that pass tests but fail security review (e.g., string-built SQL, brittle auth logic, unsafe parsing, insecure defaults, etc.). So I built **SecureCode**: a collection of **8 code models (3B â†’ 20B)** trained to behave more like a security reviewer. What you should expect from SecureCode: - identify likely vuln patterns and explain *why* theyâ€™re risky - outline plausible abuse paths (defensive framing) - propose a secure rewrite (drop-in where possible) - include defense-in-depth guidance + regression tests/checks Links: - **Models:** https://huggingface.co/collections/scthornton/securecode - **Dataset:** scthornton/securecode-v2 - **Paper:** https://arxiv.org/html/2512.18542v1 SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models (2512.18542) **How to test it (copy/paste prompt):** &gt; You are a...</description><pubDate>Wed, 28 Jan 2026 09:39:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/scthornton/483996346337767</guid></item><item><title>Transformers v5 just landed! ğŸš€</title><link>https://huggingface.co/posts/IlyasMoutawwakil/848772925772411</link><description>Transformers v5 just landed! ğŸš€ It significantly unifies and reduces modeling code across architectures, while opening the door to a whole new class of performance optimizations. My favorite new feature? ğŸ¤” The new dynamic weight loader + converter. Hereâ€™s why ğŸ‘‡ Over the last few months, the core Transformers maintainers built an incredibly fast weight loader, capable of converting tensors on the fly while loading them in parallel threads. This means weâ€™re no longer constrained by how parameters are laid out inside the safetensors weight files. In practice, this unlocks two big things: - Much more modular modeling code. You can now clearly see how architectures build on top of each other (DeepSeek v2 â†’ v3, Qwen v2 â†’ v3 â†’ MoE, etc.). This makes shared bottlenecks obvious and lets us optimize the right building blocks once, for all model families. - Performance optimizations beyond what torch.compile can do alone. torch.compile operates on the computation graph, but it canâ€™t change...</description><pubDate>Wed, 28 Jan 2026 09:39:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/IlyasMoutawwakil/848772925772411</guid></item><item><title>I'm absolutely stunned by the aesthetics of HunyuanImage-3.0</title><link>https://huggingface.co/posts/kelsend/790764867019535</link><description>I'm absolutely stunned by the aesthetics of HunyuanImage-3.0 The visual effects of this model are simply beyond imagination itâ€™s every bit as good as NanoBanana, no compromise at all. I fine-tuned my micro-scene prompts by adding text overlays and background effects, and its adaptability is truly breathtaking. With just one prompt, you can generate scene posters for any movie or novel. Every detail, from scene design to text style and atmospheric effects, perfectly aligns with the tone of the original material. No forced elements, just seamless, film-grade visual effects that exactly match what I envisioned. ğŸ‘‰ Repo: https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&amp;modelId=Hunyuan-Image-3.0-Instruct See translation</description><pubDate>Wed, 28 Jan 2026 09:39:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kelsend/790764867019535</guid></item><item><title>Date idea: read the entire Transformers v5.0.0 release notes</title><link>https://huggingface.co/posts/sergiopaniego/425017162094241</link><description>Date idea: read the entire Transformers v5.0.0 release notes Officially stable now: https://github.com/huggingface/transformers/releases/tag/v5.0.0 See translation</description><pubDate>Wed, 28 Jan 2026 09:39:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/425017162094241</guid></item><item><title>ğŸš€ Wanna train your own AI Model or Tokenizer from scratch?</title><link>https://huggingface.co/posts/Parveshiiii/254137909606959</link><description>ğŸš€ Wanna train your own AI Model or Tokenizer from scratch? Building models isnâ€™t just for big labs anymore â€” with the right data, compute, and workflow, you can create **custom AI models** and **tokenizers** tailored to any domain. Whether itâ€™s NLP, domainâ€‘specific datasets, or experimental architectures, training from scratch gives you full control over vocabulary, embeddings, and performance. âœ¨ Why train your own? - Full control over vocabulary &amp; tokenization - Domainâ€‘specific optimization (medical, legal, technical, etc.) - Better performance on niche datasets - Freedom to experiment with architectures âš¡ The best part? - Tokenizer training (TikToken / BPE) can be done in **just 3 lines of code**. - Model training runs smoothly on **Google Colab notebooks** â€” no expensive hardware required. ğŸ“‚ Try out my work: - ğŸ”— https://github.com/OE-Void/Tokenizer-from_scratch - ğŸ”— https://github.com/OE-Void/GPT See translation</description><pubDate>Wed, 28 Jan 2026 09:39:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Parveshiiii/254137909606959</guid></item><item><title>Really thanks to the community for their help! LoongFlow now has its first Chinese hands-on video!</title><link>https://huggingface.co/posts/FreshmanD/838231777554030</link><description>Really thanks to the community for their help! LoongFlow now has its first Chinese hands-on video! Version 2.0 is under intensive development. The next version will support Skills, making it easier for users to create their own expert agents to solve various challenging and complex real-world problems. Meanwhile, we are also exploring whether we can automatically generate high-quality expert Skills after a task is completed, reducing the difficulty of writing Skills and letting the LoongFlow framework automatically output the best Skills for challenging scenarios! See translation</description><pubDate>Wed, 28 Jan 2026 09:39:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/FreshmanD/838231777554030</guid></item><item><title>ğŸ‘€Just published a first-look at Tencent HunyuanImage 3.0-Instruct!</title><link>https://huggingface.co/posts/imnotkitty/991984513173348</link><description>ğŸ‘€Just published a first-look at Tencent HunyuanImage 3.0-Instruct! Tested its multi-image fusion and single-reference consistency. The results on complex prompts are quite impressive. Whatâ€™s the most creative image task youâ€™d give it? ğŸ‘‰ Read the full analysis: https://huggingface.co/blog/imnotkitty/tencent-hy-image-v30-i2i See translation</description><pubDate>Wed, 28 Jan 2026 09:39:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/imnotkitty/991984513173348</guid></item><item><title>HunyuanImage 3.0-Instruct just dropped</title><link>https://huggingface.co/posts/wangbuer999/572194851679348</link><description>HunyuanImage 3.0-Instruct just dropped fresh -sourceImage 3.0model! Spent 20 mins testing it on a Messi + retro scrambler fusion case Ran on diffusers v0.26.3 + CUDA 12.1 | 8B MoE params (1.3B activated) | zero VRAM issues strength=0.9 Messi #10 kit/tattoo sharp, motoâ€™s rusted metal texture blurred (classic open-source pain) strength=0.7 Moto/cobblestone background crisp, Messiâ€™s jersey details faded completely strength=0.75 + prompt "Blend seamlessly, keep all original details": both subject &amp; background sharp No ControlNet, no manual masking the modelâ€™s chain-of-thought reasoning parses image+prompt first Already outperforms Qwen-Image-Edit 2511 (GSB eval +25.7% on single-image edits) | 100% open-source ğŸ‘‰ Repo: https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&amp;modelId=Hunyuan-Image-3.0-Instruct technical reportï¼šhttps://arxiv.org/abs/2509.23951 Anyone else struggled with strength tweaks for fusion? This fixed it for my Messi+moto case did it work as well for yours?...</description><pubDate>Wed, 28 Jan 2026 09:39:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wangbuer999/572194851679348</guid></item><item><title>ğŸ“¢ New release! World_events Dataset now available featuring global events spanning 2023 through 2025</title><link>https://huggingface.co/posts/Reubencf/777019745383250</link><description>ğŸ“¢ New release! World_events Dataset now available featuring global events spanning 2023 through 2025 ğŸŒ https://huggingface.co/collections/Reubencf/world-events ğŸš€ 2026 dataset dropping soon See translation</description><pubDate>Wed, 28 Jan 2026 09:39:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Reubencf/777019745383250</guid></item></channel></rss>