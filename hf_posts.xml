<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Kimi-K2 is now available on the hubğŸ”¥ğŸš€</title><link>https://huggingface.co/posts/AdinaY/423045666935241</link><description>Kimi-K2 is now available on the hubğŸ”¥ğŸš€ This is a trillion-parameter MoE model focused on long context, code, reasoning, and agentic behavior. moonshotai/kimi-k2-6871243b990f2af5ba60617d âœ¨ Base &amp; Instruct âœ¨ 1T total / 32B active - Modified MIT License âœ¨ 128K context length âœ¨ Muon optimizer for stable trillion-scale training See translation</description><pubDate>Sun, 13 Jul 2025 09:26:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/423045666935241</guid></item><item><title>âœ… New Article on Hugging Face: Teaching AI to Think Like a System â€” Not a Toolkit</title><link>https://huggingface.co/posts/kanaria007/210554569109150</link><description>âœ… New Article on Hugging Face: Teaching AI to Think Like a System â€” Not a Toolkit Title: ğŸ—ï¸ Understanding Structured Cognitive Architecture: A Unified Framework for AI Reasoning Systems ğŸ”— Read it here: https://huggingface.co/blog/kanaria007/understanding-structured-cognitive-architecture Summary: After exploring how AI can select reasoning modes or learn from failure, this new article zooms out: *How do all these capabilities form a single mind, not just a menu of functions?* The **Structured Cognitive Architecture** defines a unified framework where protocols interact coherently â€” forming a self-organizing, reflective, and ethically grounded reasoning system. This architecture enables agents to: â€¢ Integrate memory, ethics, reasoning, and identity across layers â€¢ Select and execute reasoning jumps with traceable structure â€¢ Coordinate failure recovery and adaptive learning â€¢ Maintain cross-session identity and self-editing capability Itâ€™s not modular stacking. Itâ€™s **structured...</description><pubDate>Sun, 13 Jul 2025 09:26:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/210554569109150</guid></item><item><title>Since when are H200s on ZeroGPU?</title><link>https://huggingface.co/posts/nroggendorff/580094805938772</link><description>Since when are H200s on ZeroGPU? See translation</description><pubDate>Sun, 13 Jul 2025 09:26:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/580094805938772</guid></item><item><title>LiquidAI</title><link>https://huggingface.co/posts/mlabonne/882001725108546</link><description>LiquidAI open-sources a new generation of edge LLMs! ğŸ¥³ Based on a new hybrid architecture, these 350M, 700M, and 1.2B models are both fast and performant, ideal for on-device deployment. I recommend fine-tuning them to power your next edge application. We already provide Colab notebooks to guide you. More to come soon! ğŸ“ Blog post: https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models ğŸ¤— Models: LiquidAI/lfm2-686d721927015b2ad73eaa38 See translation</description><pubDate>Sun, 13 Jul 2025 09:26:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/882001725108546</guid></item><item><title>ğŸš€ Announcing the Synthetic-to-Real Multi-Class Object Detection Challenge!</title><link>https://huggingface.co/posts/3LC/827651059369427</link><description>ğŸš€ Announcing the Synthetic-to-Real Multi-Class Object Detection Challenge! Weâ€™re excited to announce the launch of the Synthetic-to-Real Multi-Class Object Detection Challengeâ€”now live on Kaggle! This exciting competition is brought to you by 3LC in partnership with Duality AI, creators of the powerful FalconCloud tool for generating targeted synthetic data. Together, we're offering a unique opportunity to push the boundaries of object detection through high-fidelity, simulation-to-real workflows. ğŸ§ª What Makes This Challenge Special? ğŸ’» Create customized training data with Dualityâ€™s cloud-based scenario ğŸ§  Analyze data weaknesses and take precise, data-driven actions using 3LC's robust tooling âš™ï¸ Optimize data for peak model training ğŸ† Why Join? â€¢ Win cash prizes, certificates, and global recognition â€¢ Gain exposure to real-world simulation workflows used in top AI companies â€¢ Collaborate and compete with leading minds in computer vision, ML, and AI Whether you're a student,...</description><pubDate>Sun, 13 Jul 2025 09:26:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/3LC/827651059369427</guid></item><item><title>ğŸ“¢ The Kaggle Synthetic-to-Real Object Detection Challenge is back, and it's a multi-class challenge ğŸ˜ !</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/395314150122330</link><description>ğŸ“¢ The Kaggle Synthetic-to-Real Object Detection Challenge is back, and it's a multi-class challenge ğŸ˜ ! ğŸ’µThis time we made a harder, multi-class challenge and are offering a larger prize pool. Sign up here: https://www.kaggle.com/competitions/multi-class-object-detection-challenge ğŸ¤ Duality AI is partnering in this competition with 3LC.AI, a cutting-edge platform which enables users to capture per sample metrics and take meaningful action leading to better, faster, and smaller AI models. Competitors will be challenged to: âœ¨ Create customized training data with Dualityâ€™s cloud-based scenario âœ¨Analyze data weaknesses and make calculated changes using 3LCâ€™s robust software âœ¨Optimize data for peak model training Compete for prizes, certificates, and recognition from peer competitors around the world. Whether youâ€™re a student, researcher, or industry pro, this challenge offers hands-on experience customizing high-fidelity synthetic data for robust models. Ready to bridge the Sim2Real...</description><pubDate>Sun, 13 Jul 2025 09:26:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/395314150122330</guid></item><item><title>New Space: Generate Knowledge Graphs from input data using LLM's (OpenRouter). It's a trial project but seems to be working alright so far!</title><link>https://huggingface.co/posts/CultriX/969512018114620</link><description>New Space: Generate Knowledge Graphs from input data using LLM's (OpenRouter). It's a trial project but seems to be working alright so far! CultriX/Generate-Knowledge-Graphs Below is an example after feeding it the wikipedia page about Elon Musk: See translation</description><pubDate>Sun, 13 Jul 2025 09:26:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/CultriX/969512018114620</guid></item><item><title>I am happy to announce that Ark now supports the following robots:</title><link>https://huggingface.co/posts/hba123/547491071894207</link><description>I am happy to announce that Ark now supports the following robots: 1. Franka Panda 2. Kuka LWR 3. UFactory XArm 4. Husky Robot Everything is done in Python. You can even control your robot from a Jupiter notebook. Check out the tutorials: https://arkrobotics.notion.site/ARK-Home-22be053d9c6f8096bcdbefd6276aba61 Check out the code: https://github.com/orgs/Robotics-Ark/repositories Check out the documentation: https://robotics-ark.github.io/ark_robotics.github.io/docs/html/index.html Check out the paper: https://robotics-ark.github.io/ark_robotics.github.io/static/images/ark_framework_2025.pdf Hope you find it useful. Let us know if you want a specific feature! We would love to support you ğŸ˜„ See translation</description><pubDate>Sun, 13 Jul 2025 09:26:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/547491071894207</guid></item><item><title>ğŸ¯ Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science!</title><link>https://huggingface.co/posts/DawnC/760622875415705</link><description>ğŸ¯ Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science! This isn't just another computer vision project. VisionScout represents a fundamental shift from simple object detection to genuine scene understanding, where four specialized AI models work together to interpret what's actually happening in an image. ğŸ—ï¸ Part 1: Architecture Foundation How careful system design transforms independent models into collaborative intelligence through proper layering and coordination strategies. âš™ï¸ Part 2: Deep Technical Implementation The five core algorithms powering the system: dynamic weight adjustment, attention mechanisms, statistical methods, lighting analysis, and CLIP's zero-shot learning. ğŸŒ Part 3: Real-World Validation Concrete case studies from indoor spaces to cultural landmarks, demonstrating how integrated systems deliver insights no single model could achieve. What makes this valuable:...</description><pubDate>Sun, 13 Jul 2025 09:26:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/760622875415705</guid></item><item><title>in case you didnâ€™t know, Claude now has a developer training course with certificates,</title><link>https://huggingface.co/posts/hesamation/850768471232119</link><description>in case you didnâ€™t know, Claude now has a developer training course with certificates, this is better than anything you can find on Coursera. covers Claude Code, MCP and its advanced topics and even more: https://www.anthropic.com/learn/build-with-claude See translation</description><pubDate>Sun, 13 Jul 2025 09:26:35 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/850768471232119</guid></item></channel></rss>