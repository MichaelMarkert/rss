<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Poll: Will 2026 be the year of subquadratic attention?</title><link>https://huggingface.co/posts/marksverdhei/554043140037847</link><description>Poll: Will 2026 be the year of subquadratic attention? The transformer architecture is cursed by its computational complexity. It is why you run out of tokens and have to compact. But some would argue that this is a feature not a bug and that this is also why these models are so good. We've been doing a lot of research on trying to make equally good models that are computationally cheaper, But so far, none of the approaches have stood the test of time. Or so it seems. Please vote, don't be shy. Remember that the Dunning-Kruger effect is very real, so the person who knows less about transformers than you is going to vote. We want everyone's opinion, no matter confidence. üëç if you think at least one frontier model* will have no O(n^2) attention by the end of 2026 üî• If you disagree * Frontier models - models that match / outperform the flagship claude, gemini or chatgpt at the time on multiple popular benchmarks See translation</description><pubDate>Mon, 09 Feb 2026 17:52:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/marksverdhei/554043140037847</guid></item><item><title>I just pushed Claude Code Agent Swarm with 20 coding agents on my desktop GPU workstation.</title><link>https://huggingface.co/posts/mitkox/464155376106577</link><description>I just pushed Claude Code Agent Swarm with 20 coding agents on my desktop GPU workstation. With local AI, I don‚Äôt have /fast CC switch, but I have /absurdlyfast: - 100‚Äô499 tokens/second read, yeah 100k, not a typo | 811 tok/sec generation - KV cache: 707‚Äô200 tokens - Hardware: 5+ year old GPUs 4xA6K gen1; It‚Äôs not the car. It‚Äôs the driver. Qwen3 Coder Next AWQ with cache at BF16. Scores 82.1% in C# on 29-years-in-dev codebase vs Opus 4.5 at only 57.5%. When your codebase predates Stack Overflow, you don't need the biggest model; you need the one that actually remembers Windows 95. My current bottleneck is my 27" monitor. Can't fit all 20 Theos on screen without squinting. See translation</description><pubDate>Mon, 09 Feb 2026 17:52:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/464155376106577</guid></item><item><title>Tiny but mighty: LFM 1.2B - 11 Distill / Fine tunes : Exceeding all benchmarks at 300-700+ T/S on GPU, 60+ T/S CPU.</title><link>https://huggingface.co/posts/DavidAU/768773942094149</link><description>Tiny but mighty: LFM 1.2B - 11 Distill / Fine tunes : Exceeding all benchmarks at 300-700+ T/S on GPU, 60+ T/S CPU. Almost all exceed LFM 1.2B Benchmarks - which are already very impressive. All benchmarks posted. A specialized merge of multiple of these fine tunes by @ nightmedia FAR exceeds the benchmarks set by the already impressive LFM. (LFM2.5-1.2B-MEGABRAIN-Thinking-Polaris-ClaudeHOPUS-Deepseek-GLM) Included are GLM 4.7 Flash, DeepSeek, Claude, Kimi V2 and other distill fine tunes. Here is the collection ( Quants by MRadermarcher). https://huggingface.co/collections/DavidAU/lfm-12b-sota-400-700-t-s-enhanced-fine-tunes-distills See translation</description><pubDate>Mon, 09 Feb 2026 17:52:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DavidAU/768773942094149</guid></item><item><title>I made my own avatar banner maker</title><link>https://huggingface.co/posts/melvindave/248709425212035</link><description>I made my own avatar banner maker https://avatar.donvitocodes.com/ Using Claude Code and Opus 4.6 in a day I use it in my HF profile too See translation</description><pubDate>Mon, 09 Feb 2026 17:52:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/melvindave/248709425212035</guid></item><item><title>SecureCode v2.1: framework-specific secure coding patterns, now on HuggingFace</title><link>https://huggingface.co/posts/scthornton/416355719015343</link><description>SecureCode v2.1: framework-specific secure coding patterns, now on HuggingFace Quick update on the SecureCode dataset. After testing the v2.0 models against real codebases, one gap kept showing up: the models understood *what* was insecure but generated language-generic fixes. A developer using Express.js doesn't need "set security headers"they need helmet() middleware chains configured correctly. Spring Boot developers need @PreAuthorize annotations, not abstract RBAC pseudocode. What changed in v2.1: - 1,435 total examples (v2.0's 1,216 baseline + 219 new framework-specific additions) - 9 production frameworks: Express.js, Spring Boot, React, Next.js, FastAPI, GraphQL, SQLAlchemy, Flask, Vue.js - 475 unique CVEs (73 new, including framework-specific treatments of Log4Shell, Spring4Shell, and others) - 5-tier quality rubric: Every new example scores 90+/100 across correctness, new dataset average is nearly 97+, security hardening, real-world grounding, educational scaffolding, and...</description><pubDate>Mon, 09 Feb 2026 17:52:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/scthornton/416355719015343</guid></item><item><title>üö® Day 8/8: OpenMed Medical Reasoning Dataset Release - THE GRAND FINALE</title><link>https://huggingface.co/posts/MaziyarPanahi/787722961401438</link><description>üö® Day 8/8: OpenMed Medical Reasoning Dataset Release - THE GRAND FINALE Today I complete my 8-day release series with Medical-Reasoning-SFT-Mega. The largest open medical reasoning dataset, combining 7 state-of-the-art AI models with fair distribution deduplication. THE 7 SOURCE MODELS (Original Sample Counts): 1. Trinity-Mini: 810,284 samples 2. Qwen3-Next-80B: 604,249 samples 3. GPT-OSS-120B: 506,150 samples 4. Nemotron-Nano-30B: 444,544 samples 5. GLM-4.5-Air: 225,179 samples 6. MiniMax-M2.1: 204,773 samples 7. Baichuan-M3-235B: 124,520 samples TOTAL BEFORE DEDUPLICATION: 2,919,699 samples TOKEN COUNTS: - Content tokens: 2.22 Billion - Reasoning tokens: 1.56 Billion - Total tokens: 3.78 Billion - Samples with chain-of-thought: 100% Quick Start: from datasets import load_dataset ds = load_dataset ( "OpenMed/Medical-Reasoning-SFT-Mega" ) All datasets Apache 2.0 licensed. Free for research and commercial use. Thank you for following OpenMed's release series. I can't wait to see what...</description><pubDate>Mon, 09 Feb 2026 17:52:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MaziyarPanahi/787722961401438</guid></item><item><title>You don't need a massive research lab to build a privacy-preserving AI tool thanks to open datasets. With the right ingredients, anyone can.</title><link>https://huggingface.co/posts/MikeDoes/444764433252763</link><description>You don't need a massive research lab to build a privacy-preserving AI tool thanks to open datasets. With the right ingredients, anyone can. A fantastic new guide shows how the democratization of AI is helping to advance safety. It walks through how to use Google's new fine-tuning API to turn Gemini into a powerful tool for PII anonymization. This project was powered by two key components: An accessible platform from Google. High-quality, open-source training data. We are honored that the author chose the Ai4Privacy pii-masking-200k dataset to provide the crucial data foundation. Our dataset delivered the volume and structure needed to successfully teach a state-of-the-art model how to perform a critical privacy function. This is the future we're working towards: powerful platforms combined with open, safety-focused data to create tools that benefit everyone. Kudos to the author for showcasing what's possible! üîó Read the full step-by-step guide:...</description><pubDate>Mon, 09 Feb 2026 17:52:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/444764433252763</guid></item><item><title>Context Engineering for Code Agents: Why They Fail and How to Fix Them</title><link>https://huggingface.co/posts/aufklarer/357748681968688</link><description>Context Engineering for Code Agents: Why They Fail and How to Fix Them Code agents don't fail because they can't code ‚Äî they fail because their context turns into a junk drawer. I wrote a practical survey covering the emerging discipline of context engineering for agentic hybrid applications: the techniques, papers, and architectural patterns that keep long-running code agents on track as their token windows fill up with tool logs, stale diffs, and repeated file dumps. What's covered: Why long context windows alone don't save you (position bias, distractor sensitivity) Observation masking vs. LLM summarization ‚Äî and when simple beats clever Tool-output compression with approaches like LLMLingua-2 Trajectory reduction: pruning dead branches from agent history Memory hierarchies: session ‚Üí working set ‚Üí notes ‚Üí cross-session How MCP and standardized tool interfaces reduce context debt Dynamic context policies trained with RL (DeepMiner, MEM1) Meta-agent CI loops for measuring...</description><pubDate>Mon, 09 Feb 2026 17:52:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aufklarer/357748681968688</guid></item><item><title>The moment we've been waiting for ‚Äî ACE-Step dropped their new model: Ace-Step 1.5 üéâ</title><link>https://huggingface.co/posts/efecelik/345746434857283</link><description>The moment we've been waiting for ‚Äî ACE-Step dropped their new model: Ace-Step 1.5 üéâ üîó ACE-Step/Ace-Step1.5 And the best part? It's released under the MIT license. We've already started integrating it into our project. Let's go üöÄ See translation</description><pubDate>Mon, 09 Feb 2026 17:52:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/efecelik/345746434857283</guid></item><item><title>LoongFlow Big News!!!</title><link>https://huggingface.co/posts/FreshmanD/759883646504275</link><description>LoongFlow Big News!!! @ all We‚Äôve put AI Agents into a production GPU cluster to handle GPU failure prediction. Not as a demo. Not as AutoML. But as an evolving system that designs and improves its own models. On two GPU types: ‚Äì IT21HMDB01-B2: +30% prediction accuracy ‚Äì H800: +25% prediction accuracy The resulting models already meet production standards and are being wired into the ops pipeline. How it works: ‚Ä¢ An ML agent designs the full ML pipeline from scratch ‚Ä¢ A Math agent performs targeted evolutionary optimization ‚Ä¢ The agents explore, discard, and iterate toward better modelsHumans don‚Äôt hand-tune parameters. This is not offline analysis. GPU failure prediction means: ‚Ä¢ heavy assets ‚Ä¢ real incidents ‚Ä¢ real operational risk The agents now trigger maintenance before failures happen. This feels like an early signal: AI agents are starting to take responsibility for infrastructure-level engineering decisions in production systems. For ML Agent, you can check:...</description><pubDate>Mon, 09 Feb 2026 17:52:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/FreshmanD/759883646504275</guid></item></channel></rss>