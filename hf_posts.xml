<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>6 Free resources on Reinforcement Learning (RL)</title><link>https://huggingface.co/posts/Kseniase/484268922176188</link><description>6 Free resources on Reinforcement Learning (RL) RL now is where the real action is, it's the engine behind autonomous tech, robots, and the next wave of AI that thinks, moves and solves problems on its own. To stay up to date with what‚Äôs happening in RL, we offer some fresh materials on it: 1. "Reinforcement Learning from Human Feedback" by Nathan Lambert -&gt; https://rlhfbook.com/ It's a short introduction to RLHF, explaining instruction tuning, reward modeling, alignment methods, synthetic data, evaluation, and more 2. "A Course in Reinforcement Learning (2nd Edition)" by Dimitri P. Bertsekas -&gt; https://www.mit.edu/~dimitrib/RLbook.html Explains dynamic programming (DP) and RL, diving into rollout algorithms, neural networks, policy learning, etc. It‚Äôs packed with solved exercises and real-world examples 3. "Mathematical Foundations of Reinforcement Learning" video course by Shiyu Zhao -&gt; https://www.youtube.com/playlist?list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8 Offers a mathematical...</description><pubDate>Tue, 29 Apr 2025 05:22:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/484268922176188</guid></item><item><title>üöÄ Building Better Evaluations: 32K Image Annotations Now Available</title><link>https://huggingface.co/posts/jasoncorkill/653065305581211</link><description>üöÄ Building Better Evaluations: 32K Image Annotations Now Available Today, we're releasing an expanded version: 32K images annotated with 3.7M responses from over 300K individuals which was completed in under two weeks using the Rapidata Python API. Rapidata/text-2-image-Rich-Human-Feedback-32k A few months ago, we published one of our most liked dataset with 13K images based on the @ data-is-better-together 's dataset, following Google's research on "Rich Human Feedback for Text-to-Image Generation" ( https://arxiv.org/abs/2312.10240 ). It collected over 1.5M responses from 150K+ participants. Rapidata/text-2-image-Rich-Human-Feedback In the examples below, users highlighted words from prompts that were not correctly depicted in the generated images. Higher word scores indicate more frequent issues. If an image captured the prompt accurately, users could select [No_mistakes]. We're continuing to work on large-scale human feedback and model evaluation. If you're working on related...</description><pubDate>Tue, 29 Apr 2025 05:22:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/653065305581211</guid></item><item><title># ‚ú® Dream of IKEA: The Future of AI Interior Design ‚ú®</title><link>https://huggingface.co/posts/ginipick/433098376713304</link><description># ‚ú® Dream of IKEA: The Future of AI Interior Design ‚ú® Hello, AI interior design enthusiasts! üè† Today I'm thrilled to introduce you to **"Dream of IKEA"** - an amazing project that will completely transform your living spaces! ## üåü What Can It Do? **Dream of IKEA** is a magical tool that uses artificial intelligence to transform your ordinary spaces into the interior design of your dreams! ü™Ñ - üì∏ Simply upload a photo of your room - üí≠ Describe your desired style or concept - üé® The AI will redesign your space with stunning results! ## üèÜ Key Features - **Diverse Style Selection** - Over 20 design styles including Minimalist, Bohemian, Japanese, Scandinavian, and more - **User-Friendly Interface** - Beautiful, intuitive UI that anyone can use - **High-Quality Image Generation** - Amazing quality powered by ControlNet and Stable Diffusion - **Customizable Prompts** - Create completely personalized designs with your own prompts ## üõ†Ô∏è Technical Highlights This project utilizes cutting-edge...</description><pubDate>Tue, 29 Apr 2025 05:22:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/433098376713304</guid></item><item><title>Qwen 3 model family releasedüî•</title><link>https://huggingface.co/posts/merterbak/337137510653930</link><description>Qwen 3 model family releasedüî• It offers 2 MoE and 6 dense models with following parameter sizes: 0.6B, 1.7B, 4B, 8B, 14B, 30B(MoE), 32B, and 235B(MoE). Models: Qwen/qwen3-67dd247413f0e2e4f653967f Blog: https://qwenlm.github.io/blog/qwen3/ Demo: Qwen/Qwen3-Demo GitHub: https://github.com/QwenLM/Qwen3 ‚úÖ Pre-trained 119 languages(36 trillion tokens) and dialects with strong translation and instruction following abilities. (Qwen2.5 was pre-trained on 18 trillion tokens.) ‚úÖQwen3 dense models match the performance of larger Qwen2.5 models. For example, Qwen3-1.7B/4B/8B/14B/32B perform like Qwen2.5-3B/7B/14B/32B/72B. ‚úÖ Three stage done while pretraining: ‚Ä¢ Stage 1: General language learning and knowledge building. ‚Ä¢ Stage 2: Reasoning boost with STEM, coding, and logic skills. ‚Ä¢ Stage 3: Long context training ‚úÖ It supports MCP in the model ‚úÖ Strong agent skills ‚úÖ Supports seamless between thinking mode (for hard tasks like math and coding) and non-thinking mode (for fast chatting) inside...</description><pubDate>Tue, 29 Apr 2025 05:22:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/337137510653930</guid></item><item><title>Kimi-Audio üöÄüéß an OPEN audio foundation model released by Moonshot AI</title><link>https://huggingface.co/posts/AdinaY/658134534684379</link><description>Kimi-Audio üöÄüéß an OPEN audio foundation model released by Moonshot AI moonshotai/Kimi-Audio-7B-Instruct ‚ú® 7B ‚ú® 13M+ hours of pretraining data ‚ú® Novel hybrid input architecture ‚ú® Universal audio capabilities (ASR, AQA, AAC, SER, SEC/ASC, end-to-end conversation) See translation</description><pubDate>Tue, 29 Apr 2025 05:22:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/658134534684379</guid></item><item><title>Post of the Day</title><link>https://huggingface.co/posts/ProCreations/966463038154493</link><description>Post of the Day I‚Äôm fine-tuning Qwen 2.5-0.5B to be extremely good at math, using high-quality datasets and some smart training strategies. The logs are looking really promising so far! Expected release: Tomorrow morning? I‚Äôll post as soon as it‚Äôs ready ‚Äî stay tuned. If you want faster updates or just wanna chat about it, come join my Discord: https://discord.gg/EXsug2Ux29 (Heads up: we might ask a couple quick questions when you join ‚Äî just making sure we keep the server safe.) Also, check out one of the datasets we‚Äôre using: ProCreations/SimpleMath This project is also helping shape the future of IntellIte. The insights and techniques we‚Äôre developing here ‚Äî better dataset curation, fine-tuning tricks, and evaluation methods ‚Äî will directly contribute to making IntellIte even sharper, faster, and more reliable, especially for math and reasoning tasks. Big progress ahead. Can‚Äôt wait to share it with you all! See translation</description><pubDate>Tue, 29 Apr 2025 05:22:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ProCreations/966463038154493</guid></item><item><title>ComfyUI 1-click Installers updated for latest official Torch 2.7 with CUDA 12.8 (RTX 5000 series support + older GPUs) - Automatically installs xFormers, Flash Attention, Sage Attention, Triton, DeepSpeed, insightface, accelerate, onnxruntime-gpu for Windows</title><link>https://huggingface.co/posts/MonsterMMORPG/127802170120239</link><description>ComfyUI 1-click Installers updated for latest official Torch 2.7 with CUDA 12.8 (RTX 5000 series support + older GPUs) - Automatically installs xFormers, Flash Attention, Sage Attention, Triton, DeepSpeed, insightface, accelerate, onnxruntime-gpu for Windows 1-click Installers zip file is here : https://www.patreon.com/posts/105023709 xFormers compiled by me for Windows (Python 3.10, 3.11 and 3.12) and Linux (Python 3.10 only) Flash Attention compiled by me for Windows (Python 3.10, 3.11 and 3.12) and Linux (Python 3.10 only) Sage Attention compiled by me for Linux (Python 3.10 only) insightface compiled by me for Windows (Python 3.10, 3.11 and 3.12) Pre-compiled Triton + Sage Attention + DeepSpeed installed for Windows You must have pre-installed Python on your system manually Tutorial for Python + CUDA 12.8 installation : https://youtu.be/DrhUHnYfwC0 See translation</description><pubDate>Tue, 29 Apr 2025 05:22:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/127802170120239</guid></item><item><title>Finally my first solo preprint is here:) a love letter to the field. Nothing much lol, this is just me trying to finetune my understanding of research behind the recent breakthroughs in reasoning models. It‚Äôs a preprint targeting beginners in the field -  will eventually make necessary changes later. In the meantime have fun with it:)</title><link>https://huggingface.co/posts/Jaward/748880241075570</link><description>Finally my first solo preprint is here:) a love letter to the field. Nothing much lol, this is just me trying to finetune my understanding of research behind the recent breakthroughs in reasoning models. It‚Äôs a preprint targeting beginners in the field - will eventually make necessary changes later. In the meantime have fun with it:) Download: https://github.com/Jaykef/Jaykef/blob/main/papers/The-Dawn-of-Thinking-Machines.pdf See translation</description><pubDate>Tue, 29 Apr 2025 05:22:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/748880241075570</guid></item><item><title>If your Space stops working after restarting mainly for the last 5 days (</title><link>https://huggingface.co/posts/John6666/369491746519704</link><description>If your Space stops working after restarting mainly for the last 5 days ( https://discuss.huggingface.co/t/my-space-suddenly-went-offline-the-cpu-cannot-restart/151121/22 ), try some of following. 1. Add pydantic==2.10.6 to requirements.txt or upgrade Gradio to the latest version. 2. Upgrade PyTorch to 2.2.0 or later ( torch&gt;=2.2.0 for Zero GPU space). 3. Fix Transformers to 4.49.0 or earlier ( transformers&lt;=4.49.0 for spaces using Transformers or Diffusers). 4. Fix huggingface_hub to the old version ( huggingface_hub==0.25.2 for if an error like cached_download is not available occurs or inference does not work properly) 5. Specifying WORKDIR in Dockerfile may cause the application to fail to start with error 137. (Docker Spaces, https://discuss.huggingface.co/t/error-code-137-cache-error/152177 ) About pydantic==2.10.6 : https://discuss.huggingface.co/t/error-no-api-found/146226 https://discuss.huggingface.co/t/internal-server-error-bool-not-iterable/149494 See translation</description><pubDate>Tue, 29 Apr 2025 05:22:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/John6666/369491746519704</guid></item><item><title>Until recently,</title><link>https://huggingface.co/posts/eaddario/144557041313740</link><description>Until recently, watt-ai/watt-tool-70B was the best performing model in the Berkeley Function-Calling Leaderboard ( https://gorilla.cs.berkeley.edu/leaderboard.html ), which evaluates LLM's ability to call functions (tools) accurately. The top spot now belongs to Salesforce/Llama-xLAM-2-70b-fc-r and by a quite wide margin! Layer-wise quantized versions for both models are available at eaddario/Llama-xLAM-2-8b-fc-r-GGUF and eaddario/Watt-Tool-8B-GGUF See translation</description><pubDate>Tue, 29 Apr 2025 05:22:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/eaddario/144557041313740</guid></item></channel></rss>