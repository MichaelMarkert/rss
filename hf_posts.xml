<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Dropping the general-purpose reasoning dataset Poseidon-Reasoning-5M, which supports general thought processes, math, and science ‚Äî featuring a diverse mixture of domains üåä :</title><link>https://huggingface.co/posts/prithivMLmods/657825800018453</link><description>Dropping the general-purpose reasoning dataset Poseidon-Reasoning-5M, which supports general thought processes, math, and science ‚Äî featuring a diverse mixture of domains üåä : prithivMLmods/Poseidon-Reasoning-5M from datasets import load_dataset dataset = load_dataset( "prithivMLmods/Poseidon-Reasoning-5M" , split = "data" ) The compact version is as follows ‚Äî Poseidon-Reasoning-Mini-300K : prithivMLmods/Poseidon-Reasoning-Mini-300K from datasets import load_dataset dataset = load_dataset( "prithivMLmods/Poseidon-Reasoning-Mini-300K" , split = "train" ) Collection : prithivMLmods/poseidon-reasoning-6879ca98e118b307c781a9ba See translation</description><pubDate>Sun, 20 Jul 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/657825800018453</guid></item><item><title>ü§ñ Technology means power, and whoever owns the technology owns the power.</title><link>https://huggingface.co/posts/giadap/921319982169542</link><description>ü§ñ Technology means power, and whoever owns the technology owns the power. Thrilled to share insights from my recent interview with MIT Technology Review about the growing movement toward local LLMs and what it means for AI democratization. Read here: https://www.technologyreview.com/2025/07/17/1120391/how-to-run-an-llm-on-your-laptop/ ü§î Why this matters: When we use "free" online AI services, we're often the product. Our conversations become training data, our personal stories get "cooked into" models, and our privacy becomes a commodity. But there's an alternative path forward. üí° The power shift is real: Local LLMs aren't just about privacy; they're about redistributing AI power away from a handful of tech giants. When individuals, organizations, and even entire nations can run their own models, we're democratizing access to AI capabilities. ü§ó At Hugging Face, we're proud to be at the center of this transformation. Our platform hosts the world's largest library of freely...</description><pubDate>Sun, 20 Jul 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/921319982169542</guid></item><item><title>ZML just released a technical preview of their new Inference Engine: LLMD.</title><link>https://huggingface.co/posts/erikkaum/383373646875765</link><description>ZML just released a technical preview of their new Inference Engine: LLMD. - Just 2.4GB container, which means fast startup times and efficient autoscaling - Cross-Platform GPU Support: works on both NVIDIA and AMD GPUs. - written in Zig I just tried it out and deployed it on Hugging Face Inference Endpoints and wrote a quick guide üëá You can try it in like 5 minutes! https://huggingface.co/blog/erikkaum/test-driving-llmd-inference-engine See translation</description><pubDate>Sun, 20 Jul 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/erikkaum/383373646875765</guid></item><item><title>AudioRAG is becoming real! Just built a demo with ColQwen-Omni that does semantic search on raw audio, no transcription needed.</title><link>https://huggingface.co/posts/fdaudens/135737241770101</link><description>AudioRAG is becoming real! Just built a demo with ColQwen-Omni that does semantic search on raw audio, no transcription needed. Drop in a podcast, ask your question, and it finds the exact chunks where it happens. You can also get a written answer. What‚Äôs exciting: it skips transcription, making it faster and better at capturing emotion, ambient sound, and tone, surfacing results text search would miss. - Demo: fdaudens/colqwen-omni-demo - Blog post from ColQwen team: https://huggingface.co/blog/manu/colqwen-omni-omnimodal-retrieval See translation</description><pubDate>Sun, 20 Jul 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/135737241770101</guid></item><item><title>Seed-X üî• a suite of multilingual translation models released by ByteDance.</title><link>https://huggingface.co/posts/AdinaY/422814235949912</link><description>Seed-X üî• a suite of multilingual translation models released by ByteDance. ByteDance-Seed/seed-x-6878753f2858bc17afa78543 ‚ú® instruction/reinforcement learning/reward model ‚ú® Supports 28 languages, bidirectional translation ‚ú® Optimized for deployment &amp; inference: 7B with mistral architecture ‚ú® Excels across domains: science, law, finance, literature &amp; more See translation</description><pubDate>Sun, 20 Jul 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/422814235949912</guid></item><item><title>For anyone who wants to try the new Voxtral models, you can do this from here:</title><link>https://huggingface.co/posts/MohamedRashad/539521978505113</link><description>For anyone who wants to try the new Voxtral models, you can do this from here: MohamedRashad/Voxtral Also you can find the transformers version of them here: MohamedRashad/Voxtral-Mini-3B-2507-transformers MohamedRashad/Voxtral-Small-24B-2507-transformers See translation</description><pubDate>Sun, 20 Jul 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MohamedRashad/539521978505113</guid></item><item><title>Dropped a dataset on here for linking org data: half a billion records scraped from LinkedIn networks. Positive/negative matches, bipartite graphs, Markov clusters ‚Äì all the goods to train models that actually work on fuzzy company names.</title><link>https://huggingface.co/posts/cjerzak/447037381568580</link><description>Dropped a dataset on here for linking org data: half a billion records scraped from LinkedIn networks. Positive/negative matches, bipartite graphs, Markov clusters ‚Äì all the goods to train models that actually work on fuzzy company names. NegMatches, PosMatches, holdouts for eval. Check it out: cjerzak/LinkOrgs See translation</description><pubDate>Sun, 20 Jul 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/cjerzak/447037381568580</guid></item><item><title>üßë‚Äçüç≥ New Multimodal Fine-Tuning Recipe üßë‚Äçüç≥</title><link>https://huggingface.co/posts/sergiopaniego/300981041730206</link><description>üßë‚Äçüç≥ New Multimodal Fine-Tuning Recipe üßë‚Äçüç≥ ‚ö°Ô∏è In this new @ huggingface Cookbook recipe, I walk you though the process of fine tuning a Visual Language Model (VLM) for Object Detection with Visual Grounding, using TRL. üîç Object detection typically involves detecting categories in images (e.g., vase). By combining it with visual grounding, we add contextual understanding so instead of detecting just "vase", we can detect "middle vase" in an image. VLMs are super powerful! In this case, I use PaliGemma 2 which already supports object detection and extend it to also add visual grounding. ü§ó Check it out here: https://huggingface.co/learn/cookbook/fine_tuning_vlm_object_detection_grounding See translation</description><pubDate>Sun, 20 Jul 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/300981041730206</guid></item><item><title>In our recent push to make more models available on Azure, we recently added SmolLM v3 in the catalog! üöÄ</title><link>https://huggingface.co/posts/pagezyhf/602957516699349</link><description>In our recent push to make more models available on Azure, we recently added SmolLM v3 in the catalog! üöÄ @ juanjucm wrote a really detailed guide on how to deploy on Azure AI ü§ó https://huggingface.co/docs/microsoft-azure/azure-ai/examples/deploy-smollm3 If you want to see other models, please let us know See translation</description><pubDate>Sun, 20 Jul 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/pagezyhf/602957516699349</guid></item><item><title>M2-Reasoningüî• a unified multimodal model for general (math, logic) and spatial (motion, physics, orientation) reasoning, released by  AntGroup.</title><link>https://huggingface.co/posts/AdinaY/685652769208143</link><description>M2-Reasoningüî• a unified multimodal model for general (math, logic) and spatial (motion, physics, orientation) reasoning, released by AntGroup. Model: inclusionAI/M2-Reasoning Paper: M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning (2507.08306) ‚ú® 7B with MIT license ‚ú® 294K high quality samples via novel data pipeline ‚ú® Dynamic multi-task training to resolve task conflicts See translation</description><pubDate>Sun, 20 Jul 2025 13:33:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/685652769208143</guid></item></channel></rss>