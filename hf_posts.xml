<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Qwen3-Coder ğŸ’» agentic code model by Alibaba Qwen teamğŸš€</title><link>https://huggingface.co/posts/AdinaY/235205941898959</link><description>Qwen3-Coder ğŸ’» agentic code model by Alibaba Qwen teamğŸš€ Qwen/Qwen3-Coder-480B-A35B-Instruct âœ¨ 480B total, 35B activated MoE âœ¨ Agentic Coding + Browser Use â†’ Top code model performance âœ¨ 256K context (up to 1M via Yarn) for repo-scale understanding See translation</description><pubDate>Thu, 24 Jul 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/235205941898959</guid></item><item><title>longer context doesn't generate better responses. it can even hurt your llm/agent.  1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it.</title><link>https://huggingface.co/posts/hesamation/830297477341251</link><description>longer context doesn't generate better responses. it can even hurt your llm/agent. 1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it. here are 4 types of context failure and why each one happens: 1. context poisoning: if hallucination finds its way into your context, the agent will rely on that false information to make its future moves. for example if the agent hallucinates about the "task description", all of its planning to solve the task would also be corrupt. 2. context distraction: when the context becomes too bloated, the model focuses too much on it rather than come up with novel ideas or to follow what it has learned during training. as Gemini 2.5 Pro technical report points out, as context grows significantly from 100K tokens, "the agent showed a tendency toward favoring repeating actions from its vast history rather than synthesizing novel plans". 3. context confusion: everyone lost it when MCPs became popular, it...</description><pubDate>Thu, 24 Jul 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/830297477341251</guid></item><item><title>Many VLMs claim to process hours of video. But can they follow the story?ğŸ¤”</title><link>https://huggingface.co/posts/andito/542123544707457</link><description>Many VLMs claim to process hours of video. But can they follow the story?ğŸ¤” Today, we introduce TimeScope: The benchmark that separates true temporal understanding from marketing hype. Let's see how much VLMs really understand!â³ We test three skills that matter for real-world use: ğŸ” Localized Retrieval: Find a specific action. ğŸ§© Information Synthesis: Piece together scattered clues. ğŸƒ Fine-Grained Perception: Analyze detailed motion (e.g., count how many times a person swings an axe). The results are in, and they're revealing. Only Gemini 2.5 pro handles 1-hour-long videos. Performance drops sharply with duration, proving that long video understanding is still challenging. We've found the breaking pointsâ€”now the community can start fixing them.ğŸ“ˆ Want to learn more? TimeScope is 100% open-source. Benchmark your model and help us build the next generation of video AI. ğŸ“– Blog: https://huggingface.co/blog/timescope-video-lmm-benchmark ğŸ‘©â€ğŸ’» Leaderboard &amp; Demo: Apollo-LMMs/TimeScope ğŸ“Š...</description><pubDate>Thu, 24 Jul 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/andito/542123544707457</guid></item><item><title>KAT-V1 ğŸ”¥ a LLM that tackles overthinking by switching between reasoning and direct answers, by Kuaishou.</title><link>https://huggingface.co/posts/AdinaY/439685097705510</link><description>KAT-V1 ğŸ”¥ a LLM that tackles overthinking by switching between reasoning and direct answers, by Kuaishou. Kwaipilot/KAT-V1-40B âœ¨ 40B âœ¨ Step-SRPO: smarter reasoning control via RL âœ¨ MTP + Distillation: efficient training, lower cost See translation</description><pubDate>Thu, 24 Jul 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/439685097705510</guid></item><item><title>Kimi K2 tech report is full of gems as always. Here are my notes on it:</title><link>https://huggingface.co/posts/eliebak/684400126414992</link><description>Kimi K2 tech report is full of gems as always. Here are my notes on it: &gt; MuonClip: Pretty crazy how after 70k the training stabilizes and the QK-clip is basically inactive. There is also no loss in perf with QK-clip which is not trivial at all (at small scale but with aggressive threshold). Also a cool explanation of why muon makes the logit explode in appendix E (tl;dr is that muon makes the singular value of the update matrix higher) &gt; Sparsity scaling laws to justify their ratio, they have a very solid training infra that allows the model to be trained at this sparsity level, they could have increased even more but as sparsity increases the training becomes less efficient. &gt; They diminish the number of attention heads to make it more efficient for long context since attention heads are a big bottleneck for long context. They also remove 2 of the 3 "first dense" layers in the dsv3 arch. With the sparsity and attention heads (divided by 2) they achieve 83% increased flops compared...</description><pubDate>Thu, 24 Jul 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/eliebak/684400126414992</guid></item><item><title>Now it's possible to do RAG with any-to-any models ğŸ”¥</title><link>https://huggingface.co/posts/merve/633613682232574</link><description>Now it's possible to do RAG with any-to-any models ğŸ”¥ Learn how to search in a video dataset and generate using Tevatron/OmniEmbed-v0.1-multivent an all modality retriever, and Qwen/Qwen2.5-Omni-7B , any-to-any model in this notebook ğŸ¤ merve/smol-vision See translation</description><pubDate>Thu, 24 Jul 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/633613682232574</guid></item><item><title>olmOCR [Allen AI] just got an upgrade! ğŸ“ˆğŸ§‘â€ğŸ³</title><link>https://huggingface.co/posts/prithivMLmods/906521786731164</link><description>olmOCR [Allen AI] just got an upgrade! ğŸ“ˆğŸ§‘â€ğŸ³ The allenai/olmOCR-7B-0725 â€” fine-tuned with allenai/olmOCR-mix-0225 on top of Qwen/Qwen2.5-VL-7B-Instruct , pushing the boundaries of OCR technology. It takes a single document image as input, with the longest side resized to 1288 pixels. High-quality, openly available approach to parsing pdfs and other complex documents optical character recognition. Try the demo here: prithivMLmods/Multimodal-OCR âœ¨ Model: allenai/olmOCR-7B-0725 âœ¨ Model [fp8]: allenai/olmOCR-7B-0725-FP8 âœ¨ Multimodal Implementations Space Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Thu, 24 Jul 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/906521786731164</guid></item><item><title>It's Qwen3 week! ğŸ’œ We uploaded Dynamic 2-bit GGUFs for:</title><link>https://huggingface.co/posts/danielhanchen/754522453041743</link><description>It's Qwen3 week! ğŸ’œ We uploaded Dynamic 2-bit GGUFs for: Qwen3-Coder: unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF Qwen3-2507: unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF So you can run them both locally! Guides are in model cards. See translation</description><pubDate>Thu, 24 Jul 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/754522453041743</guid></item><item><title>How to achieve 100% Pass Rate on HumanEval ? ğŸ”¥</title><link>https://huggingface.co/posts/YerbaPage/468681185371266</link><description>How to achieve 100% Pass Rate on HumanEval ? ğŸ”¥ Meet MGDebugger if you are tired of LLMs failing on complex bugs ğŸ¤” Our MGDebugger, just hit 100% accuracy on HumanEval using the DeepSeek-R1 model. ğŸš€ âœ¨ Demo: learnmlf/MGDebugger ğŸ“ Paper: From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging (2410.01215) ğŸ’» Code: https://github.com/YerbaPage/MGDebugger HumanEval may be retired, we're ready for the next challenge In more complex scenarios! You may also take look at this repo for a collection of awesome repo-level coding tasks! ğŸ–¥ï¸ https://github.com/YerbaPage/Awesome-Repo-Level-Code-Generation See translation</description><pubDate>Thu, 24 Jul 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/468681185371266</guid></item><item><title>Ever wish you could have someone watching your Github repo 24/7?</title><link>https://huggingface.co/posts/dhruv3006/705437635458343</link><description>Ever wish you could have someone watching your Github repo 24/7? We built an agent that monitors your repo, finds who most recently starred it, and autonomously reaches out via email! Discord : https://discord.com/invite/ZYN7f7KPjS Get your API Key here : https://tally.so/r/nrYr4X See translation</description><pubDate>Thu, 24 Jul 2025 17:24:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/705437635458343</guid></item></channel></rss>