<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>new smol course</title><link>https://huggingface.co/posts/burtenshaw/724732252831042</link><description>new smol course If youâ€™re building with or learning about post training AI models right now, we have a new FREE and CERTIFIED course. ğŸ”— Follow the org to join in smol-course The course builds on smol course v1 which was the fastest way to learn to train your custom AI models. It now has: - A leaderboard for students to submit models to - Certification based on exams and leaderboards - Prizes based on Leaderboards - Up to date content on TRL and SmolLM3 - Deep integration with the Hubâ€™s compute for model training and evaluation We will release chapters every few weeks, so you can follow the org to stay updated. See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/724732252831042</guid></item><item><title>ModernBERT goes MULTILINGUAL! One of the most requested models I've seen, The Johns Hopkins University's CLSP has trained state-of-the-art massively multilingual encoders using the ModernBERT architecture: mmBERT.</title><link>https://huggingface.co/posts/tomaarsen/906557413568289</link><description>ModernBERT goes MULTILINGUAL! One of the most requested models I've seen, The Johns Hopkins University's CLSP has trained state-of-the-art massively multilingual encoders using the ModernBERT architecture: mmBERT. Model details: - 2 model sizes: - jhu-clsp/mmBERT-small - jhu-clsp/mmBERT-base - Uses the ModernBERT architecture, but with the Gemma2 multilingual tokenizer (so: flash attention, alternating global/local attention, unpadding/sequence packing, etc.) - Maximum sequence length of 8192 tokens, on the high end for encoders - Trained on 1833 languages using DCLM, FineWeb2, and many more sources - 3 training phases: 2.3T tokens pretraining on 60 languages, 600B tokens mid-training on 110 languages, and 100B tokens decay training on all 1833 languages. - Both models are MIT Licensed, and the full datasets and intermediary checkpoints are also publicly released Evaluation details: - Very competitive with ModernBERT at equivalent sizes on English (GLUE, MTEB v2 English after...</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/906557413568289</guid></item><item><title>ğŸš€ Ever dreamed of training your own Large Language Model from scratch? What if I told you it doesn't require a supercomputer or PhD in ML? ğŸ¤¯</title><link>https://huggingface.co/posts/Abhaykoul/356298508659280</link><description>ğŸš€ Ever dreamed of training your own Large Language Model from scratch? What if I told you it doesn't require a supercomputer or PhD in ML? ğŸ¤¯ Introducing LLM Trainer - the educational framework that makes LLM training accessible to EVERYONE! Whether you're on a CPU-only laptop or scaling to distributed GPUs, we've got you covered. ğŸ’»â¡ï¸ğŸ–¥ï¸ Why LLM Trainer? Because existing tools are either too simplistic (hiding the magic) or too complex (requiring expert knowledge). We bridge the gap with: ğŸ“ Educational transparency - every component built from scratch with clear code ğŸ’» CPU-first approach - start training immediately, no GPU needed ğŸ”§ Full customization - modify anything you want ğŸ“ˆ Seamless scaling - from laptop to cluster without code changes ğŸ¤ HuggingFace integration - works with existing models &amp; tokenizers Key highlights: âœ… Built-in tokenizers (BPE, WordPiece, HF wrappers) âœ… Complete Transformer implementation from scratch âœ… Optimized for CPU training âœ… Advanced features: mixed...</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/356298508659280</guid></item><item><title>Build something cool with Nano Banana aka Gemini 2.5 Flash Image AIO [All-in-One]. Draw and transform on canvas, edit images, and generate imagesâ€”all in one place!ğŸŒ</title><link>https://huggingface.co/posts/prithivMLmods/195813965636174</link><description>Build something cool with Nano Banana aka Gemini 2.5 Flash Image AIO [All-in-One]. Draw and transform on canvas, edit images, and generate imagesâ€”all in one place!ğŸŒ âœ¦ï¸ Constructed with the Gemini API (GCP). Try it here: https://nano-banana-aio-op72ohwdda-uw.a.run.app/ âš ï¸ Note: The serverâ€™s health status is currently stable, but this may change at any time. If you experience network issues, please refresh the current app tab or trigger the discussion below. See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/195813965636174</guid></item><item><title>Introducing the Nano Banana Node Editor! ğŸŒ</title><link>https://huggingface.co/posts/Reubencf/961832649483016</link><description>Introducing the Nano Banana Node Editor! ğŸŒ Now you can control and manipulate Nano Banana images with a powerful, intuitive node-based system. Explore the creative possibilities at: Reubencf/Nano_Banana_Editor This version is clearer, more inviting, and emphasizes the creative potential of your tool. See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Reubencf/961832649483016</guid></item><item><title>Hello everyone</title><link>https://huggingface.co/posts/andywu-kby/346535541829962</link><description>Hello everyone Good day! We have launched the product - Virtual Try On ğŸš€ Say goodbye to the uncertainty of online shopping with Miragicâ€™s Virtual Try-On solution! Our cutting-edge AI technology lets you try on clothes virtually, offering a seamless and interactive shopping experience. Whether you're exploring new outfits or simply trying before you buy, Miragic gives you a realistic view of how items will look on youâ€”without ever stepping into a store. Miragic-AI/Miragic-Virtual-Try-On ğŸŒŸ Key Features: - Realistic 3D Try-On: See how clothes fit and look on your virtual self in real-time. - Personalized Fit: Using advanced body-scanning tech, Miragic adjusts the fit based on your unique measurements. - Wide Fashion Selection: Browse through various brands and styles, all available for a virtual try-on. - Sustainable Shopping: Reduce the need for returns and make more eco-friendly choices with a virtual experience that helps you shop smarter. ğŸ‘š Why Virtual Try-On? - Save time and money...</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/andywu-kby/346535541829962</guid></item><item><title>Science is the vibe-killer</title><link>https://huggingface.co/posts/salma-remyx/619394412045148</link><description>Science is the vibe-killer Some critique on the state of the technology Presenting an alternative vision for scaling the scientific method in AI engineering https://remyxai.substack.com/p/vibes-dont-scale See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/619394412045148</guid></item><item><title>Hunyuan Image 2.1 by Tencent Full Tutorial and 1-Click to Install Ultra Advanced App to Use Locally :</title><link>https://huggingface.co/posts/MonsterMMORPG/330916531424120</link><description>Hunyuan Image 2.1 by Tencent Full Tutorial and 1-Click to Install Ultra Advanced App to Use Locally : https://youtu.be/dNeA5mJ36hA Tutorial video : https://youtu.be/dNeA5mJ36hA Check the below screenshots Hunyuan Image 2.1 just published by Tencent and I have been working on developing the very best app to let you use HunyuanImage-2.1 with easiest and most accurate way. In this tutorial video, I will show you how to literally 1-click to install this model and our app on Windows (locally), Massed Compute (cloud) and RunPod (cloud). The images are all raw 2560x1440 pixels with 8-steps Refiner of Hunyuan Image 2.1 model This model native resolution is 2048x2048 pixels See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/330916531424120</guid></item><item><title>Excited to share our Unified Multimodal Models new work Reconstruction Alignment (RecA)! Our BAGEL-RecA shows superior capability on image generation and editing.</title><link>https://huggingface.co/posts/sanaka87/107769277937246</link><description>Excited to share our Unified Multimodal Models new work Reconstruction Alignment (RecA)! Our BAGEL-RecA shows superior capability on image generation and editing. ğŸ“„ Paper: https://alphaxiv.org/abs/2509.07295 ğŸ’» Code: https://github.com/HorizonWind2004/reconstruction-alignment ğŸ¤— HF Models: sanaka87/reca-68ad2176380355a3dcedc068 âœï¸ DEMO: sanaka87/BAGEL-RecA ğŸŒ Project Page: https://reconstruction-alignment.github.io ğŸ”¥ X: https://x.com/XDWang101/status/1965908302581420204 ğŸ“° Zhihu: https://zhuanlan.zhihu.com/p/1947584568187159814 ğŸ¤— HF Daily Paper: Reconstruction Alignment Improves Unified Multimodal Models (2509.07295) âš¡ &lt;10k images &amp; 27 GPU hours (no-arch-changes) â†’ SOTA, surpassing much larger open-source &amp; private models: ğŸ“Š GenEval: 0.73 â†’ 0.90 | ğŸ“Š DPGBench: 80.93 â†’ 88.15 ğŸ–¼ï¸ ImgEdit: 3.38 â†’ 3.75 | ğŸ–Œï¸ GEdit: 6.94 â†’ 7.25 âœ… RecA trains UMMs to reconstruct images from their own visual understanding encoder embeddings â†’ big gains in image generation ğŸ¨ &amp; editing âœ‚ï¸. See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sanaka87/107769277937246</guid></item><item><title>Smol course has a distinctive approach to teaching post-training, so I'm posting about how itâ€™s different to other post-training courses, including the llm course thatâ€™s already available.</title><link>https://huggingface.co/posts/burtenshaw/833511511767176</link><description>Smol course has a distinctive approach to teaching post-training, so I'm posting about how itâ€™s different to other post-training courses, including the llm course thatâ€™s already available. In short, the smol course is just more direct that any of the other course, and intended for semi-pro post trainers. - Itâ€™s a minimal set of instructions on the core parts. - Itâ€™s intended to bootstrap real projects you're working on. - The material handsover to existing documentation for details - Likewise, it handsover to the LLM course for basics. - Assessment is based on a leaderboard, without reading all the material. To start the smol course, follow here: smol-course See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/833511511767176</guid></item></channel></rss>