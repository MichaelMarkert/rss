<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Poll: Will 2026 be the year of subquadratic attention?</title><link>https://huggingface.co/posts/marksverdhei/554043140037847</link><description>Poll: Will 2026 be the year of subquadratic attention? The transformer architecture is cursed by its computational complexity. It is why you run out of tokens and have to compact. But some would argue that this is a feature not a bug and that this is also why these models are so good. We've been doing a lot of research on trying to make equally good models that are computationally cheaper, But so far, none of the approaches have stood the test of time. Or so it seems. Please vote, don't be shy. Remember that the Dunning-Kruger effect is very real, so the person who knows less about transformers than you is going to vote. We want everyone's opinion, no matter confidence. üëç if you think at least one frontier model* will have no O(n^2) attention by the end of 2026 üî• If you disagree * Frontier models - models that match / outperform the flagship claude, gemini or chatgpt at the time on multiple popular benchmarks See translation</description><pubDate>Wed, 11 Feb 2026 09:58:02 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/marksverdhei/554043140037847</guid></item><item><title>We collaborated with Hugging Face to enable you to train MoE models 12√ó faster with 35% less VRAM via our new Triton kernels (no accuracy loss). ü§ó</title><link>https://huggingface.co/posts/danielhanchen/156941968722021</link><description>We collaborated with Hugging Face to enable you to train MoE models 12√ó faster with 35% less VRAM via our new Triton kernels (no accuracy loss). ü§ó Train gpt-oss locally on 12.8GB VRAM with our free notebooks: https://unsloth.ai/docs/new/faster-moe See translation</description><pubDate>Wed, 11 Feb 2026 09:58:02 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/156941968722021</guid></item><item><title>I just pushed Claude Code Agent Swarm with 20 coding agents on my desktop GPU workstation.</title><link>https://huggingface.co/posts/mitkox/464155376106577</link><description>I just pushed Claude Code Agent Swarm with 20 coding agents on my desktop GPU workstation. With local AI, I don‚Äôt have /fast CC switch, but I have /absurdlyfast: - 100‚Äô499 tokens/second read, yeah 100k, not a typo | 811 tok/sec generation - KV cache: 707‚Äô200 tokens - Hardware: 5+ year old GPUs 4xA6K gen1; It‚Äôs not the car. It‚Äôs the driver. Qwen3 Coder Next AWQ with cache at BF16. Scores 82.1% in C# on 29-years-in-dev codebase vs Opus 4.5 at only 57.5%. When your codebase predates Stack Overflow, you don't need the biggest model; you need the one that actually remembers Windows 95. My current bottleneck is my 27" monitor. Can't fit all 20 Theos on screen without squinting. See translation</description><pubDate>Wed, 11 Feb 2026 09:58:02 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/464155376106577</guid></item><item><title>Imagine a person with so much potential, stuck in a country where no one knows him and no one give a fuck at him and he probably will die soon, yes thats me, a coder/ai model maker that is waiting‚Ä¶ waiting and waiting for a chance to get an awesome job, because i know i have studies for years for a reason!!! i know im not just a random guy with linux saying omg im a hacker, i know what i am and im actually stuck with my thoughts of people saying that if die, no one will give a care, im sure no one will give a care, the only two persons that knows me and love me is my girlfriend and my mother, i have no business wanting me, i just want a awesome job, not any job, i need a good job, a job i deserve for years of helping opensource ai community, just to you know(for those who are reading this, im part of the mradermacher team and i make a lot of research and ai models and i feel that the clock is ticking and i will not finish my current projects at the actual time, i fear it</title><link>https://huggingface.co/posts/Guilherme34/911966214994611</link><description>Imagine a person with so much potential, stuck in a country where no one knows him and no one give a fuck at him and he probably will die soon, yes thats me, a coder/ai model maker that is waiting‚Ä¶ waiting and waiting for a chance to get an awesome job, because i know i have studies for years for a reason!!! i know im not just a random guy with linux saying omg im a hacker, i know what i am and im actually stuck with my thoughts of people saying that if die, no one will give a care, im sure no one will give a care, the only two persons that knows me and love me is my girlfriend and my mother, i have no business wanting me, i just want a awesome job, not any job, i need a good job, a job i deserve for years of helping opensource ai community, just to you know(for those who are reading this, im part of the mradermacher team and i make a lot of research and ai models and i feel that the clock is ticking and i will not finish my current projects at the actual time, i fear it See...</description><pubDate>Wed, 11 Feb 2026 09:58:02 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Guilherme34/911966214994611</guid></item><item><title>Can you teach a giant like Google's Gemini to protect user privacy? A new step-by-step guide shows that the answer is a resounding "yes."</title><link>https://huggingface.co/posts/MikeDoes/800845216560901</link><description>Can you teach a giant like Google's Gemini to protect user privacy? A new step-by-step guide shows that the answer is a resounding "yes." While powerful, large language models aren't specialized for privacy tasks. This tutorial by Analytics Vidhya walks through how to fine-tune Gemini into a dedicated tool for PII anonymization. To teach the model this critical skill, the author needed a robust dataset with thousands of clear 'before' and 'after' examples. We're thrilled they chose the Ai4Privacy pii-masking-200k dataset for this task. Our data provided the high-quality, paired examples of masked and unmasked text necessary to effectively train Gemini to identify and hide sensitive information accurately. This is a perfect example of how the community can use open-source data to add a crucial layer of safety to the world's most powerful models. Great work! üîó Check out the full tutorial here: https://www.analyticsvidhya.com/blog/2024/03/guide-to-fine-tuning-gemini-for-masking-pii-...</description><pubDate>Wed, 11 Feb 2026 09:58:02 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/800845216560901</guid></item><item><title>SeedVR2 and FlashVSR+ Studio Level Image and Video Upscaler Pro Released</title><link>https://huggingface.co/posts/MonsterMMORPG/552992030098244</link><description>SeedVR2 and FlashVSR+ Studio Level Image and Video Upscaler Pro Released Tutorial video : https://www.youtube.com/watch?v=bPWsg8DREiM üìÇ Resources &amp; Links: üíª SECourses Ultimate Video and Image Upscaler Pro Download Link : [ https://www.patreon.com/posts/Upscaler-Studio-Pro-150202809 ] üöÜ Requirements Tutorial : https://youtu.be/DrhUHnYfwC0 üõ†Ô∏è Requirements Written Post : [ https://www.patreon.com/posts/Windows-AI-Requirements-Setup-Guide-111553210 ] üëã SECourses Discord Channel for 7/24 Support: [ https://bit.ly/SECoursesDiscord ] It has been long waited to have a studio level video and image upscaler app. Today we have publishing the version 1.0 of SECourses Ultimate Video and Image Upscaler Pro. It is supporting SeedVR2, FlashVSR+, Gan based upscalers, RIFE frame interpolation, full queue system, full batch folder processing, scene / chunked based processing and many more. It is fully working on every cloud and consumer GPUs like RTX 2000, 3000, 4000, 5000 series and H100, H200, B200,...</description><pubDate>Wed, 11 Feb 2026 09:58:02 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/552992030098244</guid></item><item><title>test</title><link>https://huggingface.co/posts/paasthaamz/730113013208944</link><description>test</description><pubDate>Wed, 11 Feb 2026 09:58:02 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/paasthaamz/730113013208944</guid></item><item><title>LoongFlow Big News!!!</title><link>https://huggingface.co/posts/FreshmanD/759883646504275</link><description>LoongFlow Big News!!! @ all We‚Äôve put AI Agents into a production GPU cluster to handle GPU failure prediction. Not as a demo. Not as AutoML. But as an evolving system that designs and improves its own models. On two GPU types: ‚Äì IT21HMDB01-B2: +30% prediction accuracy ‚Äì H800: +25% prediction accuracy The resulting models already meet production standards and are being wired into the ops pipeline. How it works: ‚Ä¢ An ML agent designs the full ML pipeline from scratch ‚Ä¢ A Math agent performs targeted evolutionary optimization ‚Ä¢ The agents explore, discard, and iterate toward better modelsHumans don‚Äôt hand-tune parameters. This is not offline analysis. GPU failure prediction means: ‚Ä¢ heavy assets ‚Ä¢ real incidents ‚Ä¢ real operational risk The agents now trigger maintenance before failures happen. This feels like an early signal: AI agents are starting to take responsibility for infrastructure-level engineering decisions in production systems. For ML Agent, you can check:...</description><pubDate>Wed, 11 Feb 2026 09:58:02 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/FreshmanD/759883646504275</guid></item><item><title>You don't need a massive research lab to build a privacy-preserving AI tool thanks to open datasets. With the right ingredients, anyone can.</title><link>https://huggingface.co/posts/MikeDoes/444764433252763</link><description>You don't need a massive research lab to build a privacy-preserving AI tool thanks to open datasets. With the right ingredients, anyone can. A fantastic new guide shows how the democratization of AI is helping to advance safety. It walks through how to use Google's new fine-tuning API to turn Gemini into a powerful tool for PII anonymization. This project was powered by two key components: An accessible platform from Google. High-quality, open-source training data. We are honored that the author chose the Ai4Privacy pii-masking-200k dataset to provide the crucial data foundation. Our dataset delivered the volume and structure needed to successfully teach a state-of-the-art model how to perform a critical privacy function. This is the future we're working towards: powerful platforms combined with open, safety-focused data to create tools that benefit everyone. Kudos to the author for showcasing what's possible! üîó Read the full step-by-step guide:...</description><pubDate>Wed, 11 Feb 2026 09:58:02 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/444764433252763</guid></item><item><title>Made this with ByteDance's Seedance 2.0</title><link>https://huggingface.co/posts/imnotkitty/153097834236594</link><description>Made this with ByteDance's Seedance 2.0 It's crazyyyyyyüî•üî•üî• See translation</description><pubDate>Wed, 11 Feb 2026 09:58:02 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/imnotkitty/153097834236594</guid></item></channel></rss>