<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>My home lab for AI models - llmlaba v1</title><link>https://huggingface.co/posts/kostakoff/584269728210158</link><description>My home lab for AI models - llmlaba v1 After I began learning MLOps I realized that I needed some kind of home lab, there are a lot of GPUs that I need to learn how to set up and test. So I spent some time to do a researching which platform I could buy or build. My requirements ware: - Limited budget - Power supply 1 kW or higher - Few PCIe slots to be able to install more than one gpu - Zero maintenance cost, I don't want spend a lot of time or money to maintain lab hardware, except for the GPUs I chose the Intel Mac Pro 7.1: - Prices on eBay acceptable - Excelent cooling - 1.4 kW power supply - 7 PCIe slots - Zero maintenance: I don't need to do anything with the Mac Pro hardware; it just works - Classic UEFI boot loader It requires a bit of OS preparation: 1. Install Ubuntu 24.04 (it works with the general PC ISO image) 2. Set up T2 drivers sudo apt install -y dkms linux-headers-$( uname -r) applesmc-t2 apple-bce lm-sensors 3. Install t2fanrd to manually manage fans...</description><pubDate>Mon, 16 Feb 2026 14:06:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kostakoff/584269728210158</guid></item><item><title>You can now run MiniMax-2.5 locally! ðŸš€</title><link>https://huggingface.co/posts/danielhanchen/750340203924335</link><description>You can now run MiniMax-2.5 locally! ðŸš€ At 230B parameters, MiniMax-2.5 is the strongest LLM under 700B params, delivering SOTA agentic coding &amp; chat. Run Dynamic 3/4-bit on a 128GB Mac for 20 tokens/s. Guide: https://unsloth.ai/docs/models/minimax-2.5 GGUF: unsloth/MiniMax-M2.5-GGUF See translation</description><pubDate>Mon, 16 Feb 2026 14:06:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/750340203924335</guid></item><item><title>MiniMax M2.5 is now available on the hub ðŸš€</title><link>https://huggingface.co/posts/AdinaY/578564678362048</link><description>MiniMax M2.5 is now available on the hub ðŸš€ MiniMaxAI/MiniMax-M2.5 âœ¨ 229B - Modified MIT license âœ¨37% faster than M2.1 âœ¨ ~$1/hour at 100 TPS See translation</description><pubDate>Mon, 16 Feb 2026 14:06:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/578564678362048</guid></item><item><title>Here is one of the equations that make up the worlds first Artificial General Intelligence. Remember when building Artificial Intelligence or anything on a device it all starts out binary. Everything starts out with data flow physics and mathmatics</title><link>https://huggingface.co/posts/Janady07/852502523222465</link><description>Here is one of the equations that make up the worlds first Artificial General Intelligence. Remember when building Artificial Intelligence or anything on a device it all starts out binary. Everything starts out with data flow physics and mathmatics See translation</description><pubDate>Mon, 16 Feb 2026 14:06:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Janady07/852502523222465</guid></item><item><title>âš¡ Why is Kimi-K2.5 a Dark Horse? Tested it against ChatGPT, Gemini &amp; Claude on real tasks.</title><link>https://huggingface.co/posts/imnotkitty/936122341221611</link><description>âš¡ Why is Kimi-K2.5 a Dark Horse? Tested it against ChatGPT, Gemini &amp; Claude on real tasks. moonshotai/Kimi-K2.5 âœ… Multimodal capabilities: Precise programmatic approach âœ… Slide generation: Strong semantic understanding âœ… Web prototyping: Production-ready HTML/CSS output ðŸ‘‰ Read the full article:https://huggingface.co/blog/imnotkitty/kimi-k25 See translation</description><pubDate>Mon, 16 Feb 2026 14:06:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/imnotkitty/936122341221611</guid></item><item><title>In 2017, my RNNs were babbling. Today, they are hallucinating beautifully.</title><link>https://huggingface.co/posts/mrs83/555686632418762</link><description>In 2017, my RNNs were babbling. Today, they are hallucinating beautifully. 10 years ago, getting an LSTM to output coherent English was a struggle. 10 years later, after a "cure" based on FineWeb-EDU and a custom synthetic mix for causal conversation, the results are fascinating. We trained this on ~10B tokens on a single AMD GPU (ROCm). It is not a Transformer: Echo-DSRN (400M) is a novel recurrent architecture inspired by Hymba, RWKV, and xLSTM, designed to challenge the "Attention is All You Need" monopoly on the Edge. The ambitious goal is to build a small instruct model with RAG and tool usage capabilities ( ethicalabs/Kurtis-EON1 ) ðŸ“Š The Benchmarks (Size: 400M) For a model this size (trained on &lt;10B tokens), the specialized performance is surprising: *SciQ*: 73.8% ðŸ¦„ (This rivals billion-parameter models in pure fact retrieval). *PIQA*: 62.3% (Solid physical intuition for a sub-1B model). The Reality Check: HellaSwag (29.3%) and Winogrande (50.2%) show the limits of 400M...</description><pubDate>Mon, 16 Feb 2026 14:06:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mrs83/555686632418762</guid></item><item><title>While doing various projects I kept running into situations where I wanted to be able to have representative samples of some of the current large SOTA datasets that were smaller so I didn't need to worry about slicing or anything else at runtime.  So, I created sub datasets making sure to keep the same ratios of data sources.  Each dataset card provides info for what's in it.</title><link>https://huggingface.co/posts/krisbailey/322212397790634</link><description>While doing various projects I kept running into situations where I wanted to be able to have representative samples of some of the current large SOTA datasets that were smaller so I didn't need to worry about slicing or anything else at runtime. So, I created sub datasets making sure to keep the same ratios of data sources. Each dataset card provides info for what's in it. 100M token datasets: RedPajama v2 100M Falcon RefinedWeb 100M Cosmopedia 100M 1B token datasets: Fineweb-edu 1B RedPajama v1 1B RedPajama v2 1B (use this one) Cosmopedia 1B 10B token datasets: RedPajama v1 10B Cosmopedia 10B Collection here: https://huggingface.co/collections/krisbailey/bite-size-data See translation</description><pubDate>Mon, 16 Feb 2026 14:06:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/krisbailey/322212397790634</guid></item><item><title>Run open-source models with up to 120B parameters locally on your Mac!</title><link>https://huggingface.co/posts/EricFillion/822659161925864</link><description>Run open-source models with up to 120B parameters locally on your Mac! https://youtu.be/Ql4PDjoxNXQ?si=3yHpz51uinUjgyNh See translation</description><pubDate>Mon, 16 Feb 2026 14:06:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/EricFillion/822659161925864</guid></item><item><title>Dropping the Qwen3 VL Series of Unredacted MAX-VL models. These models have undergone multi-stage training to minimize refusal rates through continuous abliterated optimization. You can find the models in BF16, FP8-Dynamic, and GGUF formats at the links below.ðŸ”¥ðŸš€</title><link>https://huggingface.co/posts/prithivMLmods/560324615932995</link><description>Dropping the Qwen3 VL Series of Unredacted MAX-VL models. These models have undergone multi-stage training to minimize refusal rates through continuous abliterated optimization. You can find the models in BF16, FP8-Dynamic, and GGUF formats at the links below.ðŸ”¥ðŸš€ Unredacted MAX - VL: âžœ prithivMLmods/Qwen3-VL-4B-Instruct-Unredacted-MAX âžœ prithivMLmods/Qwen3-VL-4B-Thinking-Unredacted-MAX âžœ prithivMLmods/Qwen3-VL-8B-Instruct-Unredacted-MAX âžœ prithivMLmods/Qwen3-VL-8B-Thinking-Unredacted-MAX Unredacted MAX - VL [FP8] âžœ prithivMLmods/Qwen3-VL-4B-Instruct-Unredacted-MAX-FP8 âžœ prithivMLmods/Qwen3-VL-4B-Thinking-Unredacted-MAX-FP8 âžœ prithivMLmods/Qwen3-VL-8B-Instruct-Unredacted-MAX-FP8 âžœ prithivMLmods/Qwen3-VL-8B-Thinking-Unredacted-MAX-FP8 Unredacted MAX - VL [GGUF] âžœ prithivMLmods/Qwen3-VL-4B-Instruct-Unredacted-MAX-GGUF âžœ prithivMLmods/Qwen3-VL-4B-Thinking-Unredacted-MAX-GGUF âžœ prithivMLmods/Qwen3-VL-8B-Instruct-Unredacted-MAX-GGUF âžœ prithivMLmods/Qwen3-VL-8B-Thinking-Unredacted-MAX-GGUF...</description><pubDate>Mon, 16 Feb 2026 14:06:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/560324615932995</guid></item><item><title>ðŸŽ¯   WAVe-1B-Multimodal-NL: Word-Level Speech Quality Assessment for Dutch</title><link>https://huggingface.co/posts/yuriyvnv/654373931030541</link><description>ðŸŽ¯ WAVe-1B-Multimodal-NL: Word-Level Speech Quality Assessment for Dutch Following the release of the Portuguese model, we're releasing the Dutch variant of WAVe â€” a 1B multimodal embedding model that assesses synthetic speech quality at the word level, thereby improving the quality of synthetically augmented datasets for training ASR models. Trained on CommonVoice 16.1 Dutch with 5 corruption strategies, this model catches mispronunciations, timing errors, and prosody issues in synthetic data that sentence-level embeddings miss entirely. Resources - Dutch model: yuriyvnv/WAVe-1B-Multimodal-NL - Portuguese model: yuriyvnv/WAVe-1B-Multimodal-PT - Code: https://github.com/yuriyvnv/WAVe This model builds on CommonVoice Dutch data â€” thanks to @ mozilla and the CommonVoice community for making multilingual speech data accessible. Would be great to hear from the Dutch NLP community â€” @ BramVanroy @ GroNLP â€” especially if you're working on Dutch ASR or TTS pipelines where quality filtering...</description><pubDate>Mon, 16 Feb 2026 14:06:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yuriyvnv/654373931030541</guid></item></channel></rss>