<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Today, we're unveiling two new open-source AI robots! HopeJR for $3,000 &amp; Reachy Mini for $300 🤖🤖🤖</title><link>https://huggingface.co/posts/clem/522668354429256</link><description>Today, we're unveiling two new open-source AI robots! HopeJR for $3,000 &amp; Reachy Mini for $300 🤖🤖🤖 Let's go open-source AI robotics! See translation</description><pubDate>Fri, 30 May 2025 13:33:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/522668354429256</guid></item><item><title>deepseek-ai/DeepSeek-R1-0528</title><link>https://huggingface.co/posts/AtAndDev/639250895656011</link><description>deepseek-ai/DeepSeek-R1-0528 This is the end See translation</description><pubDate>Fri, 30 May 2025 13:33:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AtAndDev/639250895656011</guid></item><item><title>VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration</title><link>https://huggingface.co/posts/DawnC/538322807718464</link><description>VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration I'm excited to share significant improvements to VisionScout that substantially enhance accuracy and analytical capabilities. ⭐️ Key Enhancements - CLIP Zero-Shot Landmark Detection: The system now identifies famous landmarks and architectural features without requiring specific training data, expanding scene understanding beyond generic object detection. - Places365 Environmental Classification: Integration of MIT's Places365 model provides robust scene baseline classification across 365 categories, significantly improving lighting analysis accuracy and overall scene identification precision. - Enhanced Multi-Modal Fusion: Advanced algorithms now dynamically combine insights from YOLOv8, CLIP, and Places365 to optimize accuracy across diverse scenarios. - Refined LLM Narratives: Llama 3.2 integration continues to transform analytical data into fluent, contextually rich descriptions while maintaining...</description><pubDate>Fri, 30 May 2025 13:33:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/538322807718464</guid></item><item><title>VEO 3 FLOW Full Tutorial - How To Use VEO3 in FLOW Guide :</title><link>https://huggingface.co/posts/MonsterMMORPG/279762403721253</link><description>VEO 3 FLOW Full Tutorial - How To Use VEO3 in FLOW Guide : https://youtu.be/AoEmQPU2gtg Tutorial link : https://youtu.be/AoEmQPU2gtg VEO 3 AI is rocking generative AI field right now. FLOW is the platform that lets you use VEO 3 with so many cool features. This is an official tutorial and guide made by Google team. I edited it slightly. I hope this be helpful. FLOW : https://labs.google/flow/about Veo 3 is Google DeepMind’s most advanced video generation model to date. It allows users to create high-quality, cinematic video clips from simple text prompts, making it one of the most powerful AI tools for video creation. What sets Veo 3 apart is its ability to generate videos with native audio. This means that along with stunning visuals, Veo 3 can produce synchronized dialogue, ambient sounds, and background music—all from a single prompt. For filmmakers, this is a significant leap forward, as it eliminates the need for separate audio generation or complex syncing processes. Veo 3...</description><pubDate>Fri, 30 May 2025 13:33:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/279762403721253</guid></item><item><title>🤗👨🏻‍🎓</title><link>https://huggingface.co/posts/darkc0de/635443238112929</link><description>🤗👨🏻‍🎓</description><pubDate>Fri, 30 May 2025 13:33:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/darkc0de/635443238112929</guid></item><item><title>introducing: VLM vibe eval 🪭</title><link>https://huggingface.co/posts/merve/470654136703534</link><description>introducing: VLM vibe eval 🪭 visionLMsftw/VLMVibeEval vision LMs are saturated over benchmarks, so we built vibe eval 💬 &gt; compare different models with refreshed in-the-wild examples in different categories 🤠 &gt; submit your favorite model for eval no numbers -- just vibes! See translation</description><pubDate>Fri, 30 May 2025 13:33:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/470654136703534</guid></item><item><title>🎵 Dream come true for content creators! TIGER AI can extract voice, effects &amp; music from ANY audio file 🤯</title><link>https://huggingface.co/posts/fdaudens/323840314242853</link><description>🎵 Dream come true for content creators! TIGER AI can extract voice, effects &amp; music from ANY audio file 🤯 This lightweight model uses frequency band-split technology to separate speech like magic. Kudos to @ fffiloni for the amazing demo! fffiloni/TIGER-audio-extraction See translation</description><pubDate>Fri, 30 May 2025 13:33:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/323840314242853</guid></item><item><title>I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book.</title><link>https://huggingface.co/posts/hesamation/260011784391977</link><description>I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book. It gives an overview, then goes into detail for each stage, even providing best practices. It’s 115 pages on arxiv, definitely worth a read. Check it out: https://arxiv.org/abs/2408.13296 See translation</description><pubDate>Fri, 30 May 2025 13:33:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/260011784391977</guid></item><item><title>👏 Congrats</title><link>https://huggingface.co/posts/jeffboudier/548880163054097</link><description>👏 Congrats @ jinanz adding TimesFM times series forecasting to Transformers! Learn how to use TimesFM in this blog post by the Nutanix team: https://huggingface.co/blog/Nutanix/introducing-timesfm-for-time-series-forcasting See translation</description><pubDate>Fri, 30 May 2025 13:33:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jeffboudier/548880163054097</guid></item><item><title>🔥 New benchmark &amp; dataset for Subject-to-Video generation</title><link>https://huggingface.co/posts/AdinaY/228294422537840</link><description>🔥 New benchmark &amp; dataset for Subject-to-Video generation OPENS2V-NEXUS by Pekin University ✨ Fine-grained evaluation for subject consistency BestWishYsh/OpenS2V-Eval ✨ 5M-scale dataset: BestWishYsh/OpenS2V-5M ✨ New metrics – automatic scores for identity, realism, and text match See translation</description><pubDate>Fri, 30 May 2025 13:33:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/228294422537840</guid></item></channel></rss>