<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ðŸš€ I built a Multimodal Vision-Language Model from using Gemma-270M + CLIP!</title><link>https://huggingface.co/posts/sagar007/387080486731099</link><description>ðŸš€ I built a Multimodal Vision-Language Model from using Gemma-270M + CLIP! Just finished training my multimodal model on the full LLaVA-Instruct-150K dataset (157K samples) and wanted to share the results! ðŸ”§ What I Built: A vision-language model that can understand images and answer questions about them, combining: - Google Gemma-3-270M (language) - OpenAI CLIP ViT-Large/14 (vision) - LoRA fine-tuning for efficiency ðŸ“Š Training Stats: - 157,712 training samples (full LLaVA dataset) - 3 epochs on A100 40GB - ~9 hours training time - Final loss: 1.333 training / 1.430 validation - Only 18.6M trainable params (3.4% of 539M total) ðŸ“ˆ sagar007/multigemma Benchmark Results: - VQA Accuracy: 53.8% - Works great for: animal detection, room identification, scene understanding ðŸ”— **Try it yourself:** - ðŸ¤— Model: sagar007/multigemma - ðŸŽ® Demo: https://huggingface.co/spaces/sagar007/Multimodal-Gemma - ðŸ’» GitHub: https://github.com/sagar431/multimodal-gemma-270m Built with PyTorch Lightning + MLflow...</description><pubDate>Tue, 20 Jan 2026 05:33:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sagar007/387080486731099</guid></item><item><title>Compared Quality and Speed Difference (with CUDA 13 &amp; Sage Attention) of BF16 vs GGUF Q8 vs FP8 Scaled vs NVFP4 for Z Image Turbo, FLUX Dev, FLUX SRPO, FLUX Kontext, FLUX 2 - Full 4K step by step tutorial also published</title><link>https://huggingface.co/posts/MonsterMMORPG/425042812655046</link><description>Compared Quality and Speed Difference (with CUDA 13 &amp; Sage Attention) of BF16 vs GGUF Q8 vs FP8 Scaled vs NVFP4 for Z Image Turbo, FLUX Dev, FLUX SRPO, FLUX Kontext, FLUX 2 - Full 4K step by step tutorial also published Full 4K tutorial : https://youtu.be/XDzspWgnzxI Check above full 4K tutorial to learn more and see uncompressed original quality and size images It was always wondered how much quality and speed difference exists between BF16, GGUF, FP8 Scaled and NVFP4 precisions. In this tutorial I have compared all these precision and quantization variants for both speed and quality. The results are pretty surprising. Moreover, we have developed and published NVFP4 model quant generator app and FP8 Scaled quant generator apps. The links of the apps are below if you want to use them. Furthermore, upgrading ComfyUI to CUDA 13 with properly compiled libraries is now very much recommended. We have observed some noticeable performance gains with CUDA 13. So for both SwarmUI and ComfyUI...</description><pubDate>Tue, 20 Jan 2026 05:33:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/425042812655046</guid></item><item><title>VividFlow: Complete AI Image Transformation Platform ðŸŽ¬ðŸŽ¨âœ¨</title><link>https://huggingface.co/posts/DawnC/976121143006478</link><description>VividFlow: Complete AI Image Transformation Platform ðŸŽ¬ðŸŽ¨âœ¨ Three powerful creative tools in one streamlined workspace. VividFlow combines professional video generation, intelligent background replacement, and artistic style transfer to transform your images with precision and creativity. ðŸŽ­ Triple Creative Powers - Cinematic Video Generation transforms static images into smooth motion sequences from 0.5 to 5 seconds. Eight curated motion categories cover portraits, products, landscapes, and artistic content with precision-tuned templates. - Intelligent Background Replacement generates photorealistic scenes from 24 professionally crafted presets spanning studios, natural environments, urban settings, and seasonal atmospheres. Advanced edge refinement handles complex subjects, while the built-in Touch Up tool eliminates artifacts through AI-powered inpainting for flawless results. - Artistic Style Transfer converts photographs into stunning interpretations across six distinct styles...</description><pubDate>Tue, 20 Jan 2026 05:33:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/976121143006478</guid></item><item><title>ðŸ˜Ž My new personal website is live! Check out</title><link>https://huggingface.co/posts/ZennyKenny/848353801795401</link><description>ðŸ˜Ž My new personal website is live! Check out https://kennethhamilton.me to chat with an LLM about my professional skills and personal projects. ðŸ™ˆ Think of it like a really, really vain version of ChatGPT. See translation</description><pubDate>Tue, 20 Jan 2026 05:33:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/848353801795401</guid></item><item><title>So, Koreans are also doing great progress behind Chinese,</title><link>https://huggingface.co/posts/Ujjwal-Tyagi/869541950904709</link><description>So, Koreans are also doing great progress behind Chinese, Their two open source ai models that are actually good in coding. upstage/Solar-Open-100B skt/A.X-K1 See translation</description><pubDate>Tue, 20 Jan 2026 05:33:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ujjwal-Tyagi/869541950904709</guid></item><item><title>NEW MODEL:</title><link>https://huggingface.co/posts/unmodeled-tyler/270028211989897</link><description>NEW MODEL: vanta-research/mox-tiny-1 Excited to share the first model from the new Mox-1 model family from VANTA Research! Mox-Tiny-1 is finetuned from Llama's 3.1 8B architecture specifically for thinking partnership and high-fidelity AI interaction. Training domains were carefully selected with this goal in mind, and all datasets used were synthetically generated for the Mox-1 models specifically. Domains include but not limited to: - Direct opinions (clear stances on various topics) - Constructive disagreement (Pushing back on flawed premises) - Epistemic confidence (calibrated certainty and uncertainty) - Warmth &amp; presence (Empathetic, human responses) - Wonder &amp; puzzlement (Intellectual curiosity) Interaction Examples: Direct opinions: User: What 's the best programming language? Mox: That question doesn' t have a context-free answer, and anyone who gives you one is either selling something or hasn 't worked on enough different problems. But I' ll give you my actual take across...</description><pubDate>Tue, 20 Jan 2026 05:33:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/270028211989897</guid></item><item><title>Are you familiar with reverse residual connections or looping in language models?</title><link>https://huggingface.co/posts/Sunny111/812628018751645</link><description>Are you familiar with reverse residual connections or looping in language models? Excited to share my Looped-GPT blog post and codebase ðŸš€ https://github.com/sanyalsunny111/Looped-GPT TL;DR: looping during pre-training improves generalization. Plot shows GPT2 LMs pre-trained with 15.73B OWT tokens P.S. This is my first post here â€” I have ~4 followers and zero expectations for reach ðŸ˜„ See translation</description><pubDate>Tue, 20 Jan 2026 05:33:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Sunny111/812628018751645</guid></item><item><title>Z.ai just released a powerful lightweight option of GLM 4.7</title><link>https://huggingface.co/posts/AdinaY/893023705771972</link><description>Z.ai just released a powerful lightweight option of GLM 4.7 âœ¨ 30B total/3B active - MoE zai-org/GLM-4.7-Flash See translation</description><pubDate>Tue, 20 Jan 2026 05:33:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/893023705771972</guid></item><item><title>How do you prove your new, specialized AI model is a better solution? You test it against the best.</title><link>https://huggingface.co/posts/MikeDoes/893570587602568</link><description>How do you prove your new, specialized AI model is a better solution? You test it against the best. That's why we were excited to see the new AdminBERT paper from researchers at Nantes UniversitÃ© and others. To show the strength of their new model for French administrative texts, they compared it to the state-of-the-art generalist model, NERmemBERT. The direct connection to our work is clear: NERmemBERT was trained on a combination of datasets, including the Pii-masking-200k dataset by Ai4Privacy. This is a perfect win-win for the open-source community. Our foundational dataset helps create a strong, general-purpose benchmark, which in turn helps researchers prove the value of their specialized work. This is how we all get better. ðŸ”— Great work by Thomas Sebbag, Solen Quiniou, Nicolas Stucky, and Emmanuel Morin on tackling a challenging domain! Check out their paper: https://aclanthology.org/2025.coling-main.27.pdf ðŸš€ Stay updated on the latest in privacy-preserving AIâ€”follow us on...</description><pubDate>Tue, 20 Jan 2026 05:33:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/893570587602568</guid></item><item><title>Ethos: In our team at UT Austin, we train students to become full-stack researchersâ€”and increasingly, designers of the systems that do research. Our students learn to carry projects end-to-end: from idea generation and theory to data creation, analysis, and iterative refinement across diverse subfields. Using modern AI (including agentic workflows) and scalable computation, students build reproducible pipelines that can ingest and update planetary-scale dataâ€”like satellite imagery and other high-dimensional sources. But the goal isnâ€™t tool use for its own sake: students learn to set the objectives, constraints, and evaluation standards that guide these systems through large spaces of hypotheses, while grounding results in causal inference and careful measurement. The outcome is scholarship that can rigorously test policy counterfactuals and translate evidence into durable, responsible improvements in societal well-being.</title><link>https://huggingface.co/posts/cjerzak/570029529525543</link><description>Ethos: In our team at UT Austin, we train students to become full-stack researchersâ€”and increasingly, designers of the systems that do research. Our students learn to carry projects end-to-end: from idea generation and theory to data creation, analysis, and iterative refinement across diverse subfields. Using modern AI (including agentic workflows) and scalable computation, students build reproducible pipelines that can ingest and update planetary-scale dataâ€”like satellite imagery and other high-dimensional sources. But the goal isnâ€™t tool use for its own sake: students learn to set the objectives, constraints, and evaluation standards that guide these systems through large spaces of hypotheses, while grounding results in causal inference and careful measurement. The outcome is scholarship that can rigorously test policy counterfactuals and translate evidence into durable, responsible improvements in societal well-being. We welcome students at every stage to engage with...</description><pubDate>Tue, 20 Jan 2026 05:33:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/cjerzak/570029529525543</guid></item></channel></rss>