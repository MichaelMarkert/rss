<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>🧠 ThinkFlow: The Revolutionary Platform That Gives LLMs the Power to Think 🚀</title><link>https://huggingface.co/posts/openfree/953705052271600</link><description>🧠 ThinkFlow: The Revolutionary Platform That Gives LLMs the Power to Think 🚀 Hello AI community! We're excited to introduce you to ThinkFlow, an innovative service that transforms how language models solve problems. 🎉 VIDraft/ThinkFlow-llama ✨ What is ThinkFlow? ThinkFlow is a groundbreaking platform that automatically applies step-by-step reasoning capabilities to existing LLM models without any modifications. It makes complex problem-solving transparent, allowing you to witness the model's thought process in real-time. 🔍 Key Features Reasoning Without Model Modifications: Add step-by-step reasoning while utilizing existing LLMs as they are ⚙️ Visualized Thinking Process: See exactly how the model analyzes and solves problems 👁️ Before &amp; After Comparison: Compare standard responses with reasoning-enhanced outputs in real-time 📊 Improved Accuracy: Deliver more accurate solutions for complex math and logic problems 📈 Educational Value: Teach students systematic approaches to problem-...</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/953705052271600</guid></item><item><title>Dropping an entire collection of Style Intermixing Adapters on StrangerZone HF — including Realism, Anime, Sketch, Texture-Rich 3D Experimentals, Automotive Concept Images, and LoRA models based on Flux.1, SD 3.5 Turbo/Large, Stable Diffusion XL 🎨</title><link>https://huggingface.co/posts/prithivMLmods/567717355691306</link><description>Dropping an entire collection of Style Intermixing Adapters on StrangerZone HF — including Realism, Anime, Sketch, Texture-Rich 3D Experimentals, Automotive Concept Images, and LoRA models based on Flux.1, SD 3.5 Turbo/Large, Stable Diffusion XL 🎨 ╰┈➤Collection : ➜ sketch : strangerzonehf/sketch-fav-675ba869c7ceaec7e652ee1c ➜ sketch2 : strangerzonehf/q-series-sketch-678e3503bf3a661758429717 ➜ automotive : strangerzonehf/automotive-3d-675bb31a491d8c264d45d843 ➜ texture 3d : strangerzonehf/flux-3dxl-engine-674833c14a001d5b1fdb5139 ➜ super 3d : strangerzonehf/super-3d-engine-6743231d69f496df97addd2b ➜ style mix : strangerzonehf/mixer-engine-673582c9c5939d8aa5bf9533 ➜ realism : strangerzonehf/realism-engine-67343495b6daf0fbdb904cc1 ╰┈➤The Entire Collection : ➜ flux.1 : prithivMLmods/flux-lora-collections-66dd5908be2206cfaa8519be ➜ flux-ultimate-lora-collection : strangerzonehf/Flux-Ultimate-LoRA-Collection ➜ sd 3.5 large / turbo : prithivMLmods/sd-35-large-lora-671b39d7bc2e7f71a446b163...</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/567717355691306</guid></item><item><title>FramePack Full Tutorial: 1-Click to Install on Windows - Up to 120 Second Image-to-Videos with 6GB &gt;</title><link>https://huggingface.co/posts/MonsterMMORPG/401294184812659</link><description>FramePack Full Tutorial: 1-Click to Install on Windows - Up to 120 Second Image-to-Videos with 6GB &gt; https://youtu.be/HwMngohRmHg Tutorial video : https://youtu.be/HwMngohRmHg FramePack from legendary lllyasviel full Windows local tutorial with a very advanced Gradio app to generate consistent videos from images with as long as 120 seconds and as low as 6 GB GPUs. This tutorial will show you step by step how to install and use FramePack locall with a very advanced Graido app. Moreover, I have published installers for cloud services such as RunPod and Massed Compute for those GPU poor and who wants to scale. 🔗 Full Instructions, Installers and Links Shared Post (the one used in the tutorial) ⤵️ ▶️ https://www.patreon.com/posts/click-to-open-post-used-in-tutorial-126855226 🔗 SECourses Official Discord 10500+ Members ⤵️ ▶️ https://discord.com/servers/software-engineering-courses-secourses-772774097734074388 🔗 Stable Diffusion, FLUX, Generative AI Tutorials and Resources GitHub ⤵️ ▶️...</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/401294184812659</guid></item><item><title>OpenAI just released a 34-page practical guide to building agents,</title><link>https://huggingface.co/posts/hesamation/750913380201236</link><description>OpenAI just released a 34-page practical guide to building agents, Here's 10 things it teaches us: 1➜ agents are different from workflows: they are complete autonomous systems that perform tasks on your behalf. many applications use LLMs for workflows, but this is not an agent. 2➜ use them for tricky stuff: complex decision making, dynamic rules, unstructured data 3➜ core recipe: each agent has three main components: Model (the brain), Tools, Instructions on how to behave 4➜ choose the right brain: set up evals to get a baseline performance, use a smart model to see what's possible, gradually downgrade the model for cost and speed 5➜ tools are key: choose well-defined and tested tools. an agent needs tools to retrieve data and context, and take actions. 6➜ instruction matters A LOT: be super clear telling the agent its goals, steps, and rules. Vague instructions = unpredictable agent. Be explicit. 7➜ start simple, then scale: often a single agent with several tools is ok. don't jump...</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/750913380201236</guid></item><item><title>New king of open VLMs: InternVL3 takes Qwen 2.5's crown! 👑</title><link>https://huggingface.co/posts/m-ric/531366391123392</link><description>New king of open VLMs: InternVL3 takes Qwen 2.5's crown! 👑 InternVL have been a wildly successful series of model : and the latest iteration has just taken back their crown thanks to their superior, natively multimodal vision training pipeline. ➡️ Most of the vision language models (VLMs) these days are built like Frankenstein : take a good text-only Large Language Model (LLM) backbone, stitch a specific vision transformer (ViT) on top of it. Then the training is sequential 🔢 : 1. Freeze the LLM weights while you train the ViT only to work with the LLM part, then 2. Unfreeze all weights to train all weights in order to work together. 💫 The Shanghai Lab decided to challenge this paradigm and chose this approach that they call "native". For each of their model sizes, they still start from a good LLM (mostly Qwen-2.5 series, did I tell you I'm a huge fan of Qwen? ❤️), and stitch the ViT, but they don't freeze anything : they train all weights together with interleaved text and image...</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/531366391123392</guid></item><item><title>Gemini 2.5 Flash is here! We excited launch our first hybrid reasoning Gemini model. In Flash 2.5 developer can turn thinking off.</title><link>https://huggingface.co/posts/philschmid/318540305385241</link><description>Gemini 2.5 Flash is here! We excited launch our first hybrid reasoning Gemini model. In Flash 2.5 developer can turn thinking off. **TL;DR:** - 🧠 Controllable "Thinking" with thinking budget with up to 24k token - 🌌 1 Million multimodal input context for text, image, video, audio, and pdf - 🛠️ Function calling, structured output, google search &amp; code execution. - 🏦 $0.15 1M input tokens; $0.6 or $3.5 (thinking on) per million output tokens (thinking tokens are billed as output tokens) - 💡 Knowledge cut of January 2025 - 🚀 Rate limits - Free 10 RPM 500 req/day - 🏅Outperforms 2.0 Flash on every benchmark Try it ⬇️ https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-preview-04-17 See translation</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/philschmid/318540305385241</guid></item><item><title>Wan2.1-FLF2V🎥  a 14B start-end frame video generation model just released by Alibaba_Wan🔥</title><link>https://huggingface.co/posts/AdinaY/926684469376880</link><description>Wan2.1-FLF2V🎥 a 14B start-end frame video generation model just released by Alibaba_Wan🔥 Wan-AI/Wan2.1-FLF2V-14B-720P ✨ Give it two images (start &amp; end), it generates a smooth, high-quality video in between. ✨ Apache 2.0 licensed ✨ Built on DiT + Flow Matching See translation</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/926684469376880</guid></item><item><title>anyone have all their spaces stuck in building now?</title><link>https://huggingface.co/posts/educrpg/528156241277720</link><description>anyone have all their spaces stuck in building now? See translation</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/educrpg/528156241277720</guid></item><item><title>🧑‍🏫 I wrote a brief blogpost to give An Introduction to AI Model Optimization Techniques!</title><link>https://huggingface.co/posts/davidberenstein1957/707396604835513</link><description>🧑‍🏫 I wrote a brief blogpost to give An Introduction to AI Model Optimization Techniques! URL: https://huggingface.co/blog/PrunaAI/introduction-to-ai-model-optimization-techniques See translation</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/davidberenstein1957/707396604835513</guid></item><item><title>GPT-4.1 dropped this week - and it puts OpenAI back in the race for coding &amp; agentic leadership.</title><link>https://huggingface.co/posts/eugenesiow/891114817044097</link><description>GPT-4.1 dropped this week - and it puts OpenAI back in the race for coding &amp; agentic leadership. ⚙️ API only - no ChatGPT toggle for this. 💻 Coding performance is back on par with Claude 3.7 Sonnet &amp; Gemini 2.5 Pro (though Gemini still leads). 💸 Pricing: • Full: $3.50 / 1M tokens • Mini: $0.70 / 1M • Nano: $0.17 / 1M 👉 Gemini 2.5 Pro = best price/perf ($3.44 / 1M) 😵 Claude 3.5 Sonnet = $6 / 1M (!) 🧠 Not a "thinking" model. 📊 Mini shines on general reasoning tasks (e.g. GPQA), but only the full model holds up in SWE-bench-verified (GitHub issue solving). See translation</description><pubDate>Sat, 19 Apr 2025 13:27:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/eugenesiow/891114817044097</guid></item></channel></rss>