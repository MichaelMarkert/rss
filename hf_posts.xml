<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Has anyone ever backed up a model to a sequential tape drive, or I'm the world first? :D</title><link>https://huggingface.co/posts/csabakecskemeti/762115035937109</link><description>Has anyone ever backed up a model to a sequential tape drive, or I'm the world first? :D Just played around with my retro PC that has got a tape driveâ€”did it just because I can. See translation</description><pubDate>Wed, 11 Jun 2025 17:21:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/csabakecskemeti/762115035937109</guid></item><item><title>ğŸš€ Videoxity is live on Hugging Face! ğŸï¸</title><link>https://huggingface.co/posts/zamal/148346638153657</link><description>ğŸš€ Videoxity is live on Hugging Face! ğŸï¸ A powerful, modular toolkit for intelligent video manipulation and scene editing. With Videoxity, you can: ğŸ–¼ï¸ Auto-caption keyframes with BLIP ğŸ§  Filter scenes using natural language (e.g. â€œremove dog scenesâ€) âœ‚ï¸ Seamlessly trim videos with FFmpeg ğŸ“Š Generate frame-based summaries Powered by Groq LLM + LangChain, OpenCV, BLIP, and SentenceTransformers, Videoxity bridges vision and language to give developers full control over video content. ğŸ”§ Built for developers. Feedback welcome! ğŸ‘‰ Try it out here fau/videoxity See translation</description><pubDate>Wed, 11 Jun 2025 17:21:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/zamal/148346638153657</guid></item><item><title>ğŸš€ ZeroGPU now supports PyTorch native quantization via</title><link>https://huggingface.co/posts/cbensimon/565026286160860</link><description>ğŸš€ ZeroGPU now supports PyTorch native quantization via torchao While it hasnâ€™t been battle-tested yet, Int8WeightOnlyConfig is already working flawlessly in our tests. Let us know if you run into any issues â€” and weâ€™re excited to see what the community will build! import spaces from diffusers import FluxPipeline from torchao.quantization.quant_api import Int8WeightOnlyConfig, quantize_ pipeline = FluxPipeline.from_pretrained(...).to( 'cuda' ) quantize_(pipeline.transformer, Int8WeightOnlyConfig()) # Or any other component(s) @spaces.GPU def generate ( prompt: str ): return pipeline(prompt).images[ 0 ] See translation</description><pubDate>Wed, 11 Jun 2025 17:21:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/cbensimon/565026286160860</guid></item><item><title>12 Foundational AI Model Types</title><link>https://huggingface.co/posts/Kseniase/795992300839975</link><description>12 Foundational AI Model Types Letâ€™s refresh some fundamentals today to stay fluent in the what we all work with. Here are some of the most popular model types that shape the vast world of AI (with examples in the brackets): 1. LLM - Large Language Model (GPT, LLaMA) -&gt; Large Language Models: A Survey (2402.06196) + history of LLMs: https://www.turingpost.com/t/The%20History%20of%20LLMs It's trained on massive text datasets to understand and generate human language. They are mostly build on Transformer architecture, predicting the next token. LLMs scale by increasing overall parameter count across all components (layers, attention heads, MLPs, etc.) 2. SLM - Small Language Model (TinyLLaMA, Phi models, SmolLM) A Survey of Small Language Models (2410.20011) Lightweight LM optimized for efficiency, low memory use, fast inference, and edge use. SLMs work using the same principles as LLMs 3. VLM - Vision-Language Model (CLIP, Flamingo) -&gt; An Introduction to Vision-Language Modeling...</description><pubDate>Wed, 11 Jun 2025 17:21:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/795992300839975</guid></item><item><title>Having an insanely good medical LLM is pointless if it wonâ€™t answer your questions!</title><link>https://huggingface.co/posts/drwlf/878228510592624</link><description>Having an insanely good medical LLM is pointless if it wonâ€™t answer your questions! So weâ€™ve made 2 notebook for abliterating any model in order to achieve a good model that will actually help you! The notebooks are made using @ mlabonne â€˜s abliteration logic and datasets! Feel free to use them and happy training ğŸ˜Š https://github.com/dralexlup/LLM-Abliteration See translation</description><pubDate>Wed, 11 Jun 2025 17:21:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/drwlf/878228510592624</guid></item><item><title>Imagine you could have an Image Arena score equivalent at each checkpoint during training. We released the first version of just that:</title><link>https://huggingface.co/posts/jasoncorkill/871941197791232</link><description>Imagine you could have an Image Arena score equivalent at each checkpoint during training. We released the first version of just that: Crowd-Eval Add one line of code to your training loop and you will have a new real human loss curve in your W&amp;B dashboard. Thousands of real humans from around the world rating your model in real time at the cost of a few dollars per checkpoint is a game changer. Check it out here: https://github.com/RapidataAI/crowd-eval First 5 people to put it in their loop get 100'000 human responses for free! (ping me) See translation</description><pubDate>Wed, 11 Jun 2025 17:21:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/871941197791232</guid></item><item><title>Super excited to launch Hugging Face Sheets: Spreadsheets meet AI and unstructured data.</title><link>https://huggingface.co/posts/dvilasuero/324662497616161</link><description>Super excited to launch Hugging Face Sheets: Spreadsheets meet AI and unstructured data. A few months ago, we started imagining new ways to build and transform datasets with the latest open-source models. Today, I'm thrilled to introduce our first step in this direction. In a nutshell: ğŸ“ Effortlessly run prompts and models over your data. ğŸŒ Agentic search for accuracy and real-time information. ğŸ–¼ï¸ Familiar, minimalistic interface for interacting with data. ğŸ¯ Human feedback 2.0: Your input directly improves generated data. ğŸ’¯ Access hundreds of open models and leading inference providers. Go to this space to try it out! aisheets/sheets Leave your questions below, we're just getting started! See translation</description><pubDate>Wed, 11 Jun 2025 17:21:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dvilasuero/324662497616161</guid></item><item><title>As part of Duality AIâ€™s recent Kaggle competition, weâ€™ve released a free, fully customizable cloud scenario designed to help you create targeted datasets with YOLO-compatible labels.</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/946731855039439</link><description>As part of Duality AIâ€™s recent Kaggle competition, weâ€™ve released a free, fully customizable cloud scenario designed to help you create targeted datasets with YOLO-compatible labels. The cloud simulation lets you customize the: ğŸ“¸ camera distance ğŸï¸ film grain variation ğŸ–¼ï¸background objects, â• and more! Create the dataset that you need by following this link: https://falcon.duality.ai/secure/scenarios/edit/cca0bc47-265a-4f67-843f-a434b63271b3?utm_source=huggingface&amp;utm_medium=social&amp;utm_campaign=general Iâ€™ve attached an instructional video we used for the competition, but this feature is free for anyone who has an account. https://vimeo.com/1091271731?share=copy See translation</description><pubDate>Wed, 11 Jun 2025 17:21:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/946731855039439</guid></item><item><title>RoboBrain 2.0ğŸ”¥ OPEN embedded brain model by BAAIBeijing</title><link>https://huggingface.co/posts/AdinaY/444223242188874</link><description>RoboBrain 2.0ğŸ”¥ OPEN embedded brain model by BAAIBeijing BAAI/RoboBrain2.0-7B âœ¨ 7B - Apache 2.0 / 32B coming soon âœ¨ Supports multiple images, long videos, and high-resolution visuals âœ¨ Spatial + temporal reasoning âœ¨ Real-time memory &amp; scene graphs See translation</description><pubDate>Wed, 11 Jun 2025 17:21:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/444223242188874</guid></item><item><title>MCP just hit a tipping point:</title><link>https://huggingface.co/posts/fdaudens/888164728339934</link><description>MCP just hit a tipping point: - @ hf .co made it dead simple: just type "hf.co/mcp" in your chat. No JSON wrestling, no config files. - Meanwhile, OpenAI, Google, and Microsoft all adopted it as their standard. https://huggingface.co/blog/fdaudens/mcp-ai-industry-standard See translation</description><pubDate>Wed, 11 Jun 2025 17:21:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/888164728339934</guid></item></channel></rss>