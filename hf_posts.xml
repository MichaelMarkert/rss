<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>I recently worked on a LoRA that improves tool use in LLM. Thought the approach might interest folks here.</title><link>https://huggingface.co/posts/codelion/510406818109359</link><description>I recently worked on a LoRA that improves tool use in LLM. Thought the approach might interest folks here. The issue I have had when trying to use some of the local LLMs with coding agents is this: Me: "Find all API endpoints with authentication in this codebase" LLM: "You should look for @ app .route decorators and check if they have auth middleware..." But I often want it to search the files and show me but the LLM doesn't trigger a tool use call. To fine-tune it for tool use I combined two data sources: 1. Magpie scenarios - 5000+ diverse tasks (bug hunting, refactoring, security audits) 2. Real execution - Ran these on actual repos (FastAPI, Django, React) to get authentic tool responses This ensures the model learns both breadth (many scenarios) and depth (real tool behavior). Tools We Taught: - read_file - Actually read file contents - search_files - Regex/pattern search across codebases - find_definition - Locate classes/functions - analyze_imports - Dependency tracking -...</description><pubDate>Mon, 01 Sep 2025 17:18:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/510406818109359</guid></item><item><title>It’s absolutely mind blowing - the work Dynamics Lab is doing!!</title><link>https://huggingface.co/posts/Jaward/864148450814843</link><description>It’s absolutely mind blowing - the work Dynamics Lab is doing!! With just a single input image and in a few seconds, their new world engine model (Mirage 2) can generate a whole new interactive world that’s physics informed and fully explorable in real-time🤯 Try it yourself: https://demo.dynamicslab.ai/chaos See translation</description><pubDate>Mon, 01 Sep 2025 17:18:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/864148450814843</guid></item><item><title>🌲🍄 LLM Forest Orchestra: Turning Hidden States into Music</title><link>https://huggingface.co/posts/Locutusque/640139873710354</link><description>🌲🍄 LLM Forest Orchestra: Turning Hidden States into Music Hello everyone! I'm excited to introduce a new Space I've been developing called LLM Forest Orchestra. This project converts the hidden states and attention patterns of transformer models into layered MIDI compositions. The concept draws inspiration from mushrooms and mycelial networks in forests. Fungi create underground connections linking plants and trees, establishing what some call a "wood-wide web" where signals and nutrients travel. Researchers have discovered that these exchanges form patterns resembling rhythms and pulses. When translated appropriately, these patterns can become music. Transformers operate through remarkably similar principles: tokens share signals via hidden states and attention heads. This Space transforms those invisible information flows into notes, chords, and rhythms, treating the model as a digital forest orchestra. 🎛 Features * Two compute modes: - Full model operates on a Hugging Face model...</description><pubDate>Mon, 01 Sep 2025 17:18:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Locutusque/640139873710354</guid></item><item><title>Introducing</title><link>https://huggingface.co/posts/prithivMLmods/604588784783928</link><description>Introducing prithivMLmods/DeepCaption-VLA-7B , a multimodal VLM designed for reasoning with long-shot captions (Captioning and Vision-Language Attribution). It focuses on defining visual properties, object attributes, and scene details across a wide spectrum of images and aspect ratios, generating attribute-rich image captions. The model supports creative, artistic, and technical applications that require detailed descriptions. 🤗🔥 ✦︎ Models: prithivMLmods/DeepCaption-VLA-7B , also includes prithivMLmods/DeepAttriCap-VLA-3B , an experimental model for vision-language attribution. ✦︎ Try the demo here: prithivMLmods/VisionScope-R2 ✦︎ Try it now on Google Colab, with support for T4 GPUs in 4-bit quant_type: https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-Notebooks/blob/main/DeepCaption-VLA-7B%5B4bit%20-%20notebook%20demo%5D/DeepCaption-VLA-7B.ipynb ✦︎ Collection: prithivMLmods/deepcaption-attr-68b041172ebcb867e45c556a . . . To know more about it, visit the model card of the...</description><pubDate>Mon, 01 Sep 2025 17:18:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/604588784783928</guid></item><item><title>PawMatchAI — Now with SBERT-Powered Recommendations! 🐶✨</title><link>https://huggingface.co/posts/DawnC/381537695345047</link><description>PawMatchAI — Now with SBERT-Powered Recommendations! 🐶✨ ⭐️ NEW: Description-based recommendations are here! Just type in your lifestyle or preferences (e.g. “I live in an apartment and want a quiet dog”), and PawMatchAI uses SBERT semantic embeddings to understand your needs and suggest compatible breeds. What can PawMatchAI do today? 📸 Upload a photo to identify your dog from 124 breeds with detailed info. ⚖️ Compare two breeds side-by-side, from grooming needs to health insights. 📊 Visualize breed traits with radar and comparison charts. 🎨 Try Style Transfer to turn your dog’s photo into anime, watercolor, cyberpunk, and more. What’s next? 🎯 More fine-tuned recommendations. 📱 Mobile-friendly deployment. 🐾 Expansion to additional species. My goal: To make breed discovery not only accurate but also interactive and fun — combining computer vision, semantic understanding, and creativity to help people find their perfect companion. 👉 Try it here: DawnC/PawMatchAI If you enjoy...</description><pubDate>Mon, 01 Sep 2025 17:18:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/381537695345047</guid></item><item><title>Huge updates made for SECourses Musubi Tuner - 1-Click to Install App for LoRA Training and Full Fine Tuning Qwen Image, Qwen Image Edit, Wan 2.1 and Wan 2.2 Models with Musubi Tuner with Ready Presets</title><link>https://huggingface.co/posts/MonsterMMORPG/262579091589206</link><description>Huge updates made for SECourses Musubi Tuner - 1-Click to Install App for LoRA Training and Full Fine Tuning Qwen Image, Qwen Image Edit, Wan 2.1 and Wan 2.2 Models with Musubi Tuner with Ready Presets 1-Click to install app link : https://www.patreon.com/posts/137551634 Check all the screenshots 30 August 2025 Update V7 Dataset TOML file generate error fixed Qwen2.5-VL image captioning turns out working perfect on Windows It turns out my model file was corrupted even though it was same size Therefore I have updated the model downloader and now it will check and verify SHA 256 of files therefore it will be 100% accurate Prompt file selection folder icon issue fixed Downloader file will use generated venv of installation Make sure to run it after installation completed Fixed skip existing captions functionality in Image Captioning with Qwen2.5-VL Previously skipping was happening after caption generation which was destroying the skip logic Now properly checks for existing captions...</description><pubDate>Mon, 01 Sep 2025 17:18:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/262579091589206</guid></item><item><title>🍌 Nano Banana: Google AI Completely Free!</title><link>https://huggingface.co/posts/openfree/636576339128278</link><description>🍌 Nano Banana: Google AI Completely Free! 🎉 Finally, Google's Nano Banana AI is available for everyone - absolutely FREE! 🎯 Choose Your Perfect Version! 🌟 Free Nano Banana - For Everyone Transform images with AI - It's that simple! 🚀 Start in 3 Seconds 1️⃣ Click Here 2️⃣ Upload Image 3️⃣ Enter Style → Done! ✨ No Sign-up ❌ | No Payment ❌ | No Ads ❌ | Just Free ⭕ 📸 Simple drag &amp; drop upload ✏️ Describe styles in any language ⚡ Results in under 30 seconds 🎨 Perfect for SNS, blogs, presentations 👉 Start Now: openfree/Free-Nano-Banana 🔍 Nano Banana Upscale - For Designers Professional high-resolution output when you need it! 🖼️ 4x resolution upscaling (Real-ESRGAN) 🎯 Optimized for print &amp; large displays 💎 Premium quality with preserved details 📐 Professional quality without Photoshop 👉 Create in HD: openfree/Nano-Banana-Upscale 💻 Nano Banana API - For Developers Power your app with AI! 🔧 Instant RESTful API integration 📦 Python, JS, Java code examples included ⚙️ Batch processing &amp;...</description><pubDate>Mon, 01 Sep 2025 17:18:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/636576339128278</guid></item><item><title>large AI labs have dropped so many open models last week 🔥 don't miss out on them</title><link>https://huggingface.co/posts/merve/771481819901416</link><description>large AI labs have dropped so many open models last week 🔥 don't miss out on them → Apple released on-device vision LMs apple/fastvlm-68ac97b9cd5cacefdd04872e &amp; apple/mobileclip2-68ac947dcb035c54bcd20c47 → OpenGVLab released InternVL3.5, 32 new vision LMs with one based on gpt-oss! (OS) OpenGVLab/internvl35-68ac87bd52ebe953485927fb → MSFT released a killer small TTS model (OS) microsoft/VibeVoice-1.5B find more herehttps://huggingface.co/collections/merve/august-29-releases-68b5a3754cfb8abf59e2b486 See translation</description><pubDate>Mon, 01 Sep 2025 17:18:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/771481819901416</guid></item><item><title>I've noticed something. While we're careful about what we post on social media, we're sharing our deepest and most intimate thoughts with AI chatbots -- health concerns, financial worries, relationship issues, business ideas...</title><link>https://huggingface.co/posts/giadap/942293221747024</link><description>I've noticed something. While we're careful about what we post on social media, we're sharing our deepest and most intimate thoughts with AI chatbots -- health concerns, financial worries, relationship issues, business ideas... With OpenAI hinting at ChatGPT advertising, this matters more than ever. Unlike banner ads, AI advertising happens within the conversation itself. Sponsors could subtly influence that relationship advice or financial guidance. The good news? We have options. 🤝 Open source AI models let us keep conversations private, avoid surveillance-based business models, and build systems that actually serve users first. Read more about it in our latest blog post, co-written with @ frimelle https://huggingface.co/blog/giadap/privacy-conversational-ai See translation</description><pubDate>Mon, 01 Sep 2025 17:18:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/942293221747024</guid></item><item><title>The combination of Gemini Nano (Google's AI model) and the Tensor G5 chip (Google's AI processor), built into the Pixel 10 (Google's Smartphone), provides Google with a strong foundation to continue pushing the limits of edge AI → 🔮Magic Cue.</title><link>https://huggingface.co/posts/jjokah/677752694420450</link><description>The combination of Gemini Nano (Google's AI model) and the Tensor G5 chip (Google's AI processor), built into the Pixel 10 (Google's Smartphone), provides Google with a strong foundation to continue pushing the limits of edge AI → 🔮Magic Cue. Magic Cue digs through your device (Gmail, Calendar, Messages, Photos, screenshots, notes, and more) to surface what’s useful at that moment. Ref (Magic Cue): https://store.google.com/intl/en/ideas/articles/magic-cue/ See translation</description><pubDate>Mon, 01 Sep 2025 17:18:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jjokah/677752694420450</guid></item></channel></rss>