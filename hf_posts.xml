<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Should we assemble affordable open-source robots at Hugging Face for the community. Would you buy them? At what price?</title><link>https://huggingface.co/posts/clem/968928866217294</link><description>Should we assemble affordable open-source robots at Hugging Face for the community. Would you buy them? At what price? See translation</description><pubDate>Sat, 22 Mar 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/968928866217294</guid></item><item><title>Folks, let's get ready.ü•≥ We will be busy soon.  üòÖü§óhttps://github.com/huggingface/transformers/pull/36878</title><link>https://huggingface.co/posts/onekq/812629409559433</link><description>Folks, let's get ready.ü•≥ We will be busy soon. üòÖü§óhttps://github.com/huggingface/transformers/pull/36878 See translation</description><pubDate>Sat, 22 Mar 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/812629409559433</guid></item><item><title>The Hugging Face Agents Course now includes three major agent frameworks!</title><link>https://huggingface.co/posts/burtenshaw/105046709529701</link><description>The Hugging Face Agents Course now includes three major agent frameworks! üîó https://huggingface.co/agents-course This includes LlamaIndex, LangChain, and our very own smolagents. We've worked to integrate the three frameworks in distinctive ways so that learners can reflect on when and where to use each. This also means that you can follow the course if you're already familiar with one of these frameworks, and soak up some of the fundamental knowledge in earlier units. Hopefully, this makes the agents course as open to as many people as possible. See translation</description><pubDate>Sat, 22 Mar 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/105046709529701</guid></item><item><title>Play with Orpheus TTS, a Llama-based Speech-LLM designed for high-quality, empathetic text-to-speech generation. This model has been fine-tuned to deliver human-level speech synthesis üî•üó£Ô∏è</title><link>https://huggingface.co/posts/prithivMLmods/894580491700125</link><description>Play with Orpheus TTS, a Llama-based Speech-LLM designed for high-quality, empathetic text-to-speech generation. This model has been fine-tuned to deliver human-level speech synthesis üî•üó£Ô∏è üëâDemo: prithivMLmods/Orpheus-Edge Demo supporting both text-to-speech and text-to-llm responses in speech. &gt; voice: tara, dan, emma, josh &gt; emotion: &lt;laugh&gt;, &lt;chuckle&gt;, &lt;sigh&gt;, &lt;cough&gt;, &lt;sniffle&gt;, &lt;groan&gt;, &lt;yawn&gt;, &lt;gasp&gt;. ü•†Orpheus-3b-0.1-ft Model Page: canopylabs/orpheus-3b-0.1-ft ü•†Orpheus-3b-0.1-ft Colab Inference Notebook: https://colab.research.google.com/drive/1KhXT56UePPUHhqitJNUxq63k-pQomz3N?usp=sharing ü•†Finetune [ orpheus-3b-0.1-pretrained ] Resource: https://github.com/canopyai/Orpheus-TTS/tree/main/finetune ü•†Model-releases: https://canopylabs.ai/model-releases See translation</description><pubDate>Sat, 22 Mar 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/894580491700125</guid></item><item><title>JFK declassified documents datasets</title><link>https://huggingface.co/posts/zlatinb/969200508438848</link><description>JFK declassified documents datasets Hello, I've prepared two datasets (raw and cleaned) of the recently declassified documents related to the assassination of President John F. Kennedy. Raw zlatinb/jfk-2025-raw Cleaned zlatinb/jfk-2025-cleaned The 2182 documents cover a vast range of topics, so it may be interesting to train on them to generate insights. See translation</description><pubDate>Sat, 22 Mar 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/zlatinb/969200508438848</guid></item><item><title>Finally, the ground truth / AlexNet‚Äôs original source code is available to all.</title><link>https://huggingface.co/posts/Jaward/142231502952502</link><description>Finally, the ground truth / AlexNet‚Äôs original source code is available to all. Context: AlexNet had a historic win in the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC), reducing error rate from 26% (previous best) to 15.3%. It‚Äôs a deep CNN with 8 layers (5 convolutional + 3 fully connected), pioneering the use of ReLU activations for faster training, dropout for regularization, and GPU acceleration for large-scale learning. This moment marked the beginning of the deep learning revolution, inspiring architectures like VGG, ResNet, and modern transformers. Code: https://github.com/computerhistory/AlexNet-Source-Code See translation</description><pubDate>Sat, 22 Mar 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/142231502952502</guid></item><item><title>FlexWorld üî• an open framework that generates 3D scenes from a single image!</title><link>https://huggingface.co/posts/AdinaY/812841775665112</link><description>FlexWorld üî• an open framework that generates 3D scenes from a single image! Model: GSAI-ML/FlexWorld Paper: FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis (2503.13265) ‚ú® 360¬∞ rotation &amp; zooming ‚ú® High quality novel views powered by video-to-video diffusion model ‚ú® Progressive 3D expansion See translation</description><pubDate>Sat, 22 Mar 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/812841775665112</guid></item><item><title>Hey üëã Want to build your own personal timeline algorithm?</title><link>https://huggingface.co/posts/stefan-french/995663487518303</link><description>Hey üëã Want to build your own personal timeline algorithm? ‚≠êÔ∏è -&gt; https://github.com/mozilla-ai/byota üî• Try the live demo mozilla-ai/byota üßê Read more about it https://huggingface.co/blog/mozilla-ai/build-your-own-timeline-algorithm See translation</description><pubDate>Sat, 22 Mar 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/stefan-french/995663487518303</guid></item><item><title>We introduce FAT5 (Flash Attention T5) ‚ö°</title><link>https://huggingface.co/posts/lbourdois/751427866406946</link><description>We introduce FAT5 (Flash Attention T5) ‚ö° An implementation of T5 in PyTorch with UL2 objective optimized for GPGPU for both training and inference thanks to 13 different optimizations. The main one is that we have designed a CUDA kernel to expand the Flash Attention by @ tridao with RPE biases and supports other PE such as RoPE, ALiBi or FIRE. The result kernel is 2 times faster than a SPDA implementation. We also use Triton kernels to optimize certain parts of the architecture, such as the cross-entropy and RMSNorm layer. The various kernels have been carefully built to be compatible with BF16 and torch.compile to go even faster and achieve efficient pretraining. All other optimizations are described in a üìù subsequent blog post available on @ huggingface ü§ó: CATIE-AQ/FAT5-report . This methodology enabled us to efficiently pretrain as a proof of concept a FAT5 with 147M parameters in French in a reasonable time (1,461H for 419B tokens), with limited resources (1 A100 i.e. a...</description><pubDate>Sat, 22 Mar 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lbourdois/751427866406946</guid></item><item><title>We open-sourced the</title><link>https://huggingface.co/posts/sharpenb/393986344403251</link><description>We open-sourced the pruna package that can be easily installed with pip install pruna :) It allows to easily ccompress and evaluate AI models including transformers and diffusers. - Github repo: https://github.com/PrunaAI/pruna - Documentation: https://docs.pruna.ai/en/stable/index.html With open-sourcing, people can now inspect and contribute to the open code. Beyond the code, we provide detailed readme, tutorials, benchmarks, and documentation to make transparent compression, evaluation, and saving/loading/serving of AI models. Happy to share it with you and always interested in collecting your feedback :) See translation</description><pubDate>Sat, 22 Mar 2025 17:17:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sharpenb/393986344403251</guid></item></channel></rss>