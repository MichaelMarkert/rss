<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Inference for generative ai models looks like a mine field, but thereâ€™s a simple protocol for picking the best inference:</title><link>https://huggingface.co/posts/burtenshaw/697123415535373</link><description>Inference for generative ai models looks like a mine field, but thereâ€™s a simple protocol for picking the best inference: ğŸŒ 95% of users &gt;&gt; If youâ€™re using open (large) models and need fast online inference, then use Inference providers on auto mode, and let it choose the best provider for the model. https://huggingface.co/docs/inference-providers/index ğŸ‘· fine-tuners/ bespoke &gt;&gt; If youâ€™ve got custom setups, use Inference Endpoints to define a configuration from AWS, Azure, GCP. https://endpoints.huggingface.co/ ğŸ¦« Locals &gt;&gt; If youâ€™re trying to stretch everything you can out of a server or local machine, use Llama.cpp, Jan, LMStudio or vLLM. https://huggingface.co/settings/local-apps#local-apps ğŸªŸ Browsers &gt;&gt; If you need open models running right here in the browser, use transformers.js. https://github.com/huggingface/transformers.js Let me know what youâ€™re using, and if you think itâ€™s more complex than this. See translation</description><pubDate>Wed, 02 Jul 2025 17:22:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/697123415535373</guid></item><item><title>ğŸ‰ Dhanishtha 2.0 Preview is Now Open Source!</title><link>https://huggingface.co/posts/Abhaykoul/404767027882987</link><description>ğŸ‰ Dhanishtha 2.0 Preview is Now Open Source! The world's first Intermediate Thinking Model is now available to everyone! Dhanishtha 2.0 Preview brings revolutionary intermediate thinking capabilities to the open-source community. Unlike traditional reasoning models that think once, Dhanishtha can think, answer, rethink, answer again, and continue rethinking as needed using multiple blocks between responses. ğŸš€ Key Features - Intermediate thinking: Think â†’ Answer â†’ Rethink â†’ Answer â†’ Rethink if needed... - Token efficient: Uses up to 79% fewer tokens than DeepSeek R1 on similar queries - Transparent thinking: See the model's reasoning process in real-time - Open source: Freely available for research and development HelpingAI/Dhanishtha-2.0-preview https://helpingai.co/chat See translation</description><pubDate>Wed, 02 Jul 2025 17:22:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/404767027882987</guid></item><item><title>ğŸ° DNA CASINO: Hit the Genetic Jackpot! ğŸ§¬</title><link>https://huggingface.co/posts/openfree/646247384264829</link><description>ğŸ° DNA CASINO: Hit the Genetic Jackpot! ğŸ§¬ ğŸ² When Biotech Meets Vegas = ?? Hey there! Today I'm thrilled to introduce something truly extraordinary. We've transformed DNA-Diffusion into a full-blown casino slot machine - welcome to DNA CASINO! ğŸŠ ğŸ¯ What's This All About? ğŸ§¬ Generate 200bp DNA Regulatory Sequences: AI-powered generation of cell-type specific synthetic biology sequences ğŸ° Slot Machine UI: Watch each nucleotide (A,T,C,G) spin like real casino reels! ğŸ”¬ Real-time Protein Analysis: Instantly translate generated DNA to protein and get AI-powered structure/function analysis ğŸ’« Key Features 1ï¸âƒ£ Choose Your Cell Type (Like Casino Chips!) ğŸŸ¢ K562 - Leukemia cell line ğŸ”µ GM12878 - Lymphoblastoid cell line ğŸŸ¡ HepG2 - Liver cancer cell line 2ï¸âƒ£ Pull the Lever to Begin! Just like a real slot machine - pull the lever or hit SPIN and watch 200 nucleotides whirl in spectacular fashion! ğŸª 3ï¸âƒ£ AI-Powered Protein Analysis DNA â†’ Protein translation Structure/function prediction via...</description><pubDate>Wed, 02 Jul 2025 17:22:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/646247384264829</guid></item><item><title>A few months ago, I shared that I was building with</title><link>https://huggingface.co/posts/blaise-tk/599826348587266</link><description>A few months ago, I shared that I was building with @ deeivihh something like "the Steam for open source apps"... ğŸš€ Today, Iâ€™m excited to announce that Dione is now open source and live in public beta! Our mission is simple: make it easier to discover, use, and contribute to open source applications. ğŸ”— GitHub: https://github.com/dioneapp/dioneapp ğŸ’¬ Join the community: https://discord.gg/JDFJp33vrM Want to give it a try? Iâ€™d love your feedback! ğŸ‘€ See translation</description><pubDate>Wed, 02 Jul 2025 17:22:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/blaise-tk/599826348587266</guid></item><item><title>ğŸ”¥ HuggingFace Heatmap Leaderboard</title><link>https://huggingface.co/posts/aiqtech/840192921912390</link><description>ğŸ”¥ HuggingFace Heatmap Leaderboard Visualizing AI ecosystem activity at a glance aiqtech/Heatmap-Leaderboard ğŸ¯ Introduction A leaderboard that visualizes the vibrant HuggingFace community activity through heatmaps. âœ¨ Key Features ğŸ“Š Real-time Tracking - Model/dataset/app releases from AI labs and developers ğŸ† Auto Ranking - Rankings based on activity over the past year ğŸ¨ Responsive UI - Unique colors per organization, mobile optimized âš¡ Auto Updates - Hourly data refresh for latest information ğŸŒ Major Participants Big Tech: OpenAI, Google, Meta, Microsoft, Apple, NVIDIA AI Startups: Anthropic, Mistral, Stability AI, Cohere, DeepSeek Chinese Companies: Tencent, Baidu, ByteDance, Qwen HuggingFace Official: HuggingFaceH4, HuggingFaceM4, lerobot, etc. Active Developers: prithivMLmods, lllyasviel, multimodalart and many more ğŸš€ Value Trend Analysis ğŸ“ˆ Real-time open source contribution insights Inspiration ğŸ’ª Learn from other developers' activity patterns Ecosystem Growth ğŸŒ± Visualize AI...</description><pubDate>Wed, 02 Jul 2025 17:22:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aiqtech/840192921912390</guid></item><item><title>Gemma 3n finetuning is now 1.5x faster and uses 50% less VRAM in Unsloth!</title><link>https://huggingface.co/posts/danielhanchen/374907101016508</link><description>Gemma 3n finetuning is now 1.5x faster and uses 50% less VRAM in Unsloth! Click "Use this model" and click "Google Colab"! unsloth/gemma-3n-E4B-it unsloth/gemma-3n-E2B-it https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb See translation</description><pubDate>Wed, 02 Jul 2025 17:22:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/374907101016508</guid></item><item><title>â€¼ï¸Sentence Transformers v5.0 is out! The biggest update yet introduces Sparse Embedding models, encode methods improvements, Router module for asymmetric models &amp; much more. Sparse + Dense = ğŸ”¥ hybrid search performance! Details:</title><link>https://huggingface.co/posts/tomaarsen/190568030432786</link><description>â€¼ï¸Sentence Transformers v5.0 is out! The biggest update yet introduces Sparse Embedding models, encode methods improvements, Router module for asymmetric models &amp; much more. Sparse + Dense = ğŸ”¥ hybrid search performance! Details: 1ï¸âƒ£ Sparse Encoder Models Brand new support for sparse embedding models that generate high-dimensional embeddings (30,000+ dims) where &lt;1% are non-zero: - Full SPLADE, Inference-free SPLADE, and CSR architecture support - 4 new modules, 12 new losses, 9 new evaluators - Integration with @ elastic-co , @ opensearch-project , @ NAVER LABS Europe, @ qdrant , @ IBM , etc. - Decode interpretable embeddings to understand token importance - Hybrid search integration to get the best of both worlds 2ï¸âƒ£ Enhanced Encode Methods &amp; Multi-Processing - Introduce encode_query &amp; encode_document automatically use predefined prompts - No more manual pool management - just pass device list directly to encode() - Much cleaner and easier to use than the old multi-process approach...</description><pubDate>Wed, 02 Jul 2025 17:22:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/190568030432786</guid></item><item><title>I played around with the new RXTX paper (XX^T) and was able to train nanogpt with 4x4 RXTX matmuls in both attention layer and optimizerğŸ¤•</title><link>https://huggingface.co/posts/Jaward/639375924369190</link><description>I played around with the new RXTX paper (XX^T) and was able to train nanogpt with 4x4 RXTX matmuls in both attention layer and optimizerğŸ¤• It just works (well I had to add some guardrails) but still saves 5% of memory usage: The Patch: - Computes attention scores with a 4x4 blockwise RXTX matmuls (no pytorch dot prod) - Handles arbitrary sequence lengths by padding to the nearest multiple of 4. - An RXTX variant of shampoo with params reshaped into 4x4 blocks during each optimizer step. - Uses 5% less ops Code: https://github.com/Jaykef/ai-algorithms/blob/main/nanogpt-rxtx.ipynb Paper: https://arxiv.org/pdf/2505.09814 See translation</description><pubDate>Wed, 02 Jul 2025 17:22:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/639375924369190</guid></item><item><title>The full Celestia 3 science-reasoning dataset is here!</title><link>https://huggingface.co/posts/sequelbox/523631078445392</link><description>The full Celestia 3 science-reasoning dataset is here! - 91k high-quality synthetic science prompts answered by DeepSeek-R1-0528 - subjects include physics, biology, chemistry, computer science, Earth science, astronomy, and information theory - one of the reasoning datasets powering the upcoming Shining Valiant 3 :) coming soon! GET IT NOW, FOR EVERYONE: sequelbox/Celestia3-DeepSeek-R1-0528 SUPPORT OUR RELEASES: sequelbox/SupportOpenSource with love, allegra See translation</description><pubDate>Wed, 02 Jul 2025 17:22:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sequelbox/523631078445392</guid></item><item><title>Check out new symbolic music AI front end and CLI training app</title><link>https://huggingface.co/posts/asigalov61/301808424415801</link><description>Check out new symbolic music AI front end and CLI training app https://webchatappai.github.io/midi-gen/ https://github.com/WebChatAppAi/Orpheus-Midi-Model-Maker @ Timzoid @ Csplk @ not-lain @ victor @ bartowski @ John6666 See translation</description><pubDate>Wed, 02 Jul 2025 17:22:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/asigalov61/301808424415801</guid></item></channel></rss>