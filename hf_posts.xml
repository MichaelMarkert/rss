<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The Hugging Face agents course is finally out!</title><link>https://huggingface.co/posts/burtenshaw/457613029588941</link><description>The Hugging Face agents course is finally out! ğŸ‘‰ https://huggingface.co/agents-course This first unit of the course sets you up with all the fundamentals to become a pro in agents. - What's an AI Agent? - What are LLMs? - Messages and Special Tokens - Understanding AI Agents through the Thought-Action-Observation Cycle - Thought, Internal Reasoning and the Re-Act Approach - Actions, Enabling the Agent to Engage with Its Environment - Observe, Integrating Feedback to Reflect and Adapt See translation</description><pubDate>Thu, 13 Feb 2025 05:19:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/457613029588941</guid></item><item><title>ğŸ˜Š Panorama X3 Image</title><link>https://huggingface.co/posts/fantos/903293233196599</link><description>ğŸ˜Š Panorama X3 Image an innovative system that leverages a Stable Diffusion XL-based tiling pipeline to generate unique and vibrant panoramic images by applying different prompts to the left, center, and right sections of a single image. Key Features &amp; Strengths Multi-Area Prompt Support Input distinct descriptions for the left, center, and right regions (e.g., "dense forest" for the left, "calm lake" for the center, and "majestic mountains" for the right). This allows the system to seamlessly blend multiple scenes into one stunning panoramic image. ğŸŒ„ Automatic Korean-to-English Translation If your prompt contains Korean text, it will be automatically translated into English before image generation. (For example, "ì•ˆê°œ ë‚€ ì‚°" becomes "Misty mountain") ğŸ”„ This feature ensures that you can effortlessly use both English and Korean prompts. Advanced Tiling Technology The project uses a sophisticated tiling approach that manages overlapping regions to produce natural transitions and high-...</description><pubDate>Thu, 13 Feb 2025 05:19:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fantos/903293233196599</guid></item><item><title>Time Stream â³ğŸš€</title><link>https://huggingface.co/posts/ginipick/776720011919298</link><description>Time Stream â³ğŸš€ Time Stream is a groundbreaking AI tool that transforms your text into a mesmerizing video journey from the past to the future. With this innovative technology, your ideas evolve over time, visualized through a dynamic image strip and a fluid video narrative. Imagine typing a simple prompt and watching as your words transform into vivid scenes that capture every moment of changeâ€”like a time machine for creativity! ğŸ¥âœ¨ Key Features: â€¢ Text-to-Video Transformation: Enter any text, and Time Stream converts it into a compelling video that travels through time, turning your ideas into a visual story. ğŸ“½ï¸ â€¢ Dynamic Image Strip: Alongside the video, a vibrant image strip is created, showcasing each stage of the transformation so you can see every detail of the evolution. ğŸ“¸ â€¢ Customizable Settings: Adjust parameters such as strength, guidance scale, and more to fine-tune your videoâ€™s appearance and ensure it perfectly matches your creative vision. âš™ï¸ â€¢ User-Friendly Interface:...</description><pubDate>Thu, 13 Feb 2025 05:19:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/776720011919298</guid></item><item><title>Runway Gen-3 Alpha: The Style and Coherence Champion</title><link>https://huggingface.co/posts/jasoncorkill/476446672223675</link><description>Runway Gen-3 Alpha: The Style and Coherence Champion Runway's latest video generation model, Gen-3 Alpha, is something special. It ranks #3 overall on our text-to-video human preference benchmark, but in terms of style and coherence, it outperforms even OpenAI Sora. However, it struggles with alignment, making it less predictable for controlled outputs. We've released a new dataset with human evaluations of Runway Gen-3 Alpha: Rapidata's text-2-video human preferences dataset. If you're working on video generation and want to see how your model compares to the biggest players, we can benchmark it for you. ğŸš€ DM us if youâ€™re interested! Dataset: Rapidata/text-2-video-human-preferences-runway-alpha See translation</description><pubDate>Thu, 13 Feb 2025 05:19:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/476446672223675</guid></item><item><title>I've completed the first unit of the just-launched Hugging Face Agents Course. I would highly recommend it, even for experienced builders, because it is a great walkthrough of the smolagents library and toolkit.</title><link>https://huggingface.co/posts/ZennyKenny/584467772865203</link><description>I've completed the first unit of the just-launched Hugging Face Agents Course. I would highly recommend it, even for experienced builders, because it is a great walkthrough of the smolagents library and toolkit. See translation</description><pubDate>Thu, 13 Feb 2025 05:19:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/584467772865203</guid></item><item><title>Hugging Face just launched the AI Agents Course â€“ a free journey from beginner to expert in AI agents!</title><link>https://huggingface.co/posts/ImranzamanML/161215925152068</link><description>Hugging Face just launched the AI Agents Course â€“ a free journey from beginner to expert in AI agents! - Learn AI Agent fundamentals, use cases and frameworks - Use top libraries like LangChain &amp; LlamaIndex - Compete in challenges &amp; earn a certificate - Hands-on projects &amp; real-world applications https://huggingface.co/learn/agents-course/unit0/introduction You can join for a live Q&amp;A on Feb 12 at 5PM CET to learn more about the course here https://www.youtube.com/live/PopqUt3MGyQ See translation</description><pubDate>Thu, 13 Feb 2025 05:19:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ImranzamanML/161215925152068</guid></item><item><title>"ğŸ®ğŸ¬ğŸ®ğŸ± ğ˜„ğ—¶ğ—¹ğ—¹ ğ—¯ğ—² ğ˜ğ—µğ—² ğ˜†ğ—²ğ—®ğ—¿ ğ—¼ğ—³ ğ—”ğ—œ ğ—®ğ—´ğ—²ğ—»ğ˜ğ˜€": this statement has often been made, here are numbers to support it.</title><link>https://huggingface.co/posts/m-ric/116861695030454</link><description>"ğŸ®ğŸ¬ğŸ®ğŸ± ğ˜„ğ—¶ğ—¹ğ—¹ ğ—¯ğ—² ğ˜ğ—µğ—² ğ˜†ğ—²ğ—®ğ—¿ ğ—¼ğ—³ ğ—”ğ—œ ğ—®ğ—´ğ—²ğ—»ğ˜ğ˜€": this statement has often been made, here are numbers to support it. I've plotted the progress of AI agents on GAIA test set, and it seems they're headed to catch up with the human baseline in early 2026. And that progress is still driven mostly by the improvement of base LLMs: progress would be even faster with fine-tuned agentic models. See translation</description><pubDate>Thu, 13 Feb 2025 05:19:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/116861695030454</guid></item><item><title>Mixture-of-Diffusers pipeline tiling for SDXL</title><link>https://huggingface.co/posts/elismasilva/251775641926329</link><description>Mixture-of-Diffusers pipeline tiling for SDXL This strives to provide a better tool for image composition by using several diffusion processes in parallel, each configured with a specific prompt and settings, and focused on a particular region of the image. The mixture of diffusion processes is done in a way that harmonizes the generation process, preventing "seam" effects in the generated image. Using several diffusion processes in parallel has also practical advantages when generating very large images, as the GPU memory requirements are similar to that of generating an image of the size of a single tile. elismasilva/mixture-of-diffusers-sdxl-tiling See translation</description><pubDate>Thu, 13 Feb 2025 05:19:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/elismasilva/251775641926329</guid></item><item><title>Introducing OpenR1-Math-220k!</title><link>https://huggingface.co/posts/lewtun/162100455547462</link><description>Introducing OpenR1-Math-220k! open-r1/OpenR1-Math-220k The community has been busy distilling DeepSeek-R1 from inference providers, but we decided to have a go at doing it ourselves from scratch ğŸ’ª Whatâ€™s new compared to existing reasoning datasets? â™¾ Based on AI-MO/NuminaMath-1.5 : we focus on math reasoning traces and generate answers for problems in NuminaMath 1.5, an improved version of the popular NuminaMath-CoT dataset. ğŸ³ 800k R1 reasoning traces: We generate two answers for 400k problems using DeepSeek R1. The filtered dataset contains 220k problems with correct reasoning traces. ğŸ“€ 512 H100s running locally: Instead of relying on an API, we leverage vLLM and SGLang to run generations locally on our science cluster, generating 180k reasoning traces per day. â³ Automated filtering: We apply Math Verify to only retain problems with at least one correct answer. We also leverage Llama3.3-70B-Instruct as a judge to retrieve more correct examples (e.g for cases with malformed answers...</description><pubDate>Thu, 13 Feb 2025 05:19:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lewtun/162100455547462</guid></item><item><title>ğŸ“¢ New Research Alert: Making Language Models Smaller &amp; Smarter!</title><link>https://huggingface.co/posts/schuler/395413718646507</link><description>ğŸ“¢ New Research Alert: Making Language Models Smaller &amp; Smarter! Thrilled to share the latest technical report demonstrating how to reduce language model parameters by 77% while maintaining performance. The secret? Grouped pointwise convolutions. Yes. We brought a method from computer vision to the transformers arena. ğŸ”‘ Key Findings: â€¢ 77% parameter reduction. â€¢ Maintained model capabilities. â€¢ Improved generalization. Paper: https://www.researchgate.net/publication/388835829_SAVING_77_OF_THE_PARAMETERS_IN_LARGE_LANGUAGE_MODELS_TECHNICAL_REPORT Code: https://github.com/joaopauloschuler/less-parameters-llm See translation</description><pubDate>Thu, 13 Feb 2025 05:19:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/schuler/395413718646507</guid></item></channel></rss>