<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>A real-time object detector much faster and accurate than YOLO with Apache 2.0 license just landed to Hugging Face transformers üî•</title><link>https://huggingface.co/posts/merve/183549115190705</link><description>A real-time object detector much faster and accurate than YOLO with Apache 2.0 license just landed to Hugging Face transformers üî• D-FINE is the sota real-time object detector that runs on T4 (free Colab) ü§© &gt; Collection with all checkpoints and demo ustc-community/d-fine-68109b427cbe6ee36b4e7352 Notebooks: &gt; Tracking https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/DFine_tracking.ipynb &gt; Inference https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/DFine_inference.ipynb &gt; Fine-tuning https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/DFine_finetune_on_a_custom_dataset.ipynb h/t @ vladislavbro @ qubvel-hf @ ariG23498 and the authors of the paper üé© Regular object detectors attempt to predict bounding boxes in (x, y, w, h) pixel perfect coordinates, which is very rigid and hard to solve ü•≤‚òπÔ∏è D-FINE formulates object detection as a distribution for bounding box coordinates, refines them iteratively, and it's more accurate ü§© Another...</description><pubDate>Thu, 08 May 2025 13:32:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/183549115190705</guid></item><item><title>üî• We're thrilled to share some exciting news about ICEdit! Currently, ICEdit app (</title><link>https://huggingface.co/posts/RiverZ/221754259422855</link><description>üî• We're thrilled to share some exciting news about ICEdit! Currently, ICEdit app ( RiverZ/ICEdit ) has soared to the second place on the weekly trend list of Hugging Face Space, just trailing behind Qwen3. What's more, it also holds the second position on the overall space trend list. This achievement wouldn't have been possible without your incredible support and love. A huge thank you to each and every one of you‚ù§! üéâ The ICEdit community has been incredibly active, and we've seen a plethora of amazing ComfyUI workflows being shared. For instance, with the help of ComfyUI - nunchaku, you can run ICEdit locally with just 4GB of VRAM. This makes it much more accessible for those with limited hardware resources. üéá If you're interested in the detailed information, please head over to our repository. We highly encourage you to give these workflows a try and explore the creative possibilities that ICEdit offers. Github Repo: https://github.com/River-Zhang/ICEdit Hugging Face Space:...</description><pubDate>Thu, 08 May 2025 13:32:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RiverZ/221754259422855</guid></item><item><title>A ton of impactful models and datasets in open AI past week, let's summarize the best ü§©</title><link>https://huggingface.co/posts/merve/742287367367358</link><description>A ton of impactful models and datasets in open AI past week, let's summarize the best ü§© merve/releases-apr-21-and-may-2-6819dcc84da4190620f448a3 üí¨ Qwen made it rain! They released Qwen3: new dense and MoE models ranging from 0.6B to 235B ü§Ø as well as Qwen2.5-Omni, any-to-any model in 3B and 7B! &gt; Microsoft AI released Phi4 reasoning models (that also come in mini and plus sizes) &gt; NVIDIA released new CoT reasoning datasets üñºÔ∏è &gt; ByteDance released UI-TARS-1.5, native multimodal UI parsing agentic model &gt; Meta released EdgeTAM, an on-device object tracking model (SAM2 variant) üó£Ô∏è NVIDIA released parakeet-tdt-0.6b-v2, a smol 600M automatic speech recognition model &gt; Nari released Dia, a 1.6B text-to-speech model &gt; Moonshot AI released Kimi Audio, a new audio understanding, generation, conversation model üë©üèª‚Äçüíª JetBrains released Melium models in base and SFT for coding &gt; Tesslate released UIGEN-T2-7B, a new text-to-frontend-code model ü§© See translation</description><pubDate>Thu, 08 May 2025 13:32:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/742287367367358</guid></item><item><title>Agents vs. Workflows</title><link>https://huggingface.co/posts/VirtualOasis/885212606719735</link><description>Agents vs. Workflows Agents are systems where LLMs dynamically direct their processes and tool usage, maintaining control over how they accomplish tasks. Workflows are through predefined code paths, ensuring that each step is executed in a deterministic manner. Agents are like smart assistants that can think on their own. They understand situations, make decisions, and act, whatever the task is new or unpredictable. Think of the Agent as a chef who can make a meal based on what they have. Workflows are like a recipe with fixed steps. They‚Äôre a series of tasks done in order, like following a checklist for approving a loan. They‚Äôre great for tasks that don‚Äôt change much. See translation</description><pubDate>Thu, 08 May 2025 13:32:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/VirtualOasis/885212606719735</guid></item><item><title>ACE-Step üéµ a music generation foundation model released by</title><link>https://huggingface.co/posts/AdinaY/892703768040278</link><description>ACE-Step üéµ a music generation foundation model released by StepFun &amp; ACEStudio Model: ACE-Step/ACE-Step-v1-3.5B Demo: ACE-Step/ACE-Step ‚ú® 3.5B, Apache2.0 licensed ‚ú® 115√ó faster than LLMs (4-min music in 20s on A100) ‚ú® Diffusion + DCAE + linear transformer = speed + coherence ‚ú® Supports voice cloning, remixing, lyric editing &amp; more See translation</description><pubDate>Thu, 08 May 2025 13:32:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/892703768040278</guid></item><item><title>Finally finished my extensive **Qwen 3 evaluations** across a range of formats and quantisations, focusing on **MMLU-Pro** (Computer Science).</title><link>https://huggingface.co/posts/wolfram/819510719695955</link><description>Finally finished my extensive **Qwen 3 evaluations** across a range of formats and quantisations, focusing on **MMLU-Pro** (Computer Science). A few take-aways stood out - especially for those interested in local deployment and performance trade-offs: 1Ô∏è‚É£ **Qwen3-235B-A22B** (via Fireworks API) tops the table at **83.66%** with ~55 tok/s. 2Ô∏è‚É£ But the **30B-A3B Unsloth** quant delivered **82.20%** while running locally at ~45 tok/s and with zero API spend. 3Ô∏è‚É£ The same Unsloth build is ~5x faster than Qwen's **Qwen3-32B**, which scores **82.20%** as well yet crawls at &lt;10 tok/s. 4Ô∏è‚É£ On Apple silicon, the **30B MLX** port hits **79.51%** while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups. 5Ô∏è‚É£ The **0.6B** micro-model races above 180 tok/s but tops out at **37.56%** - that's why it's not even on the graph (50 % performance cut-off). All local runs were done with LM Studio on an M4 MacBook Pro, using Qwen's official recommended settings....</description><pubDate>Thu, 08 May 2025 13:32:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wolfram/819510719695955</guid></item><item><title>What are you using to evaluate models or AI systems? So far we're building lighteval &amp; leaderboards on the hub but still feels early &amp; a lot more to build. What would be useful to you?</title><link>https://huggingface.co/posts/clem/191562047619282</link><description>What are you using to evaluate models or AI systems? So far we're building lighteval &amp; leaderboards on the hub but still feels early &amp; a lot more to build. What would be useful to you? See translation</description><pubDate>Thu, 08 May 2025 13:32:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/191562047619282</guid></item><item><title>How to learn about efficient AI? - Happy to announce the Awesome AI Efficiency repo that gathers a curated list of 100+ materials to understand the challenges and solutions in making AI faster, smaller, cheaper, greener.</title><link>https://huggingface.co/posts/sharpenb/996977777058725</link><description>How to learn about efficient AI? - Happy to announce the Awesome AI Efficiency repo that gathers a curated list of 100+ materials to understand the challenges and solutions in making AI faster, smaller, cheaper, greener. üöÄ It is designed for a **large audience** including beginners, decision-makers, engineers, and researchers. üìö It contains **diverse materials** with newspaper articles, blogs, tools, tech reports, research papers, books, and lectures. This is an ongoing project. Do not hesitate to share your feedback/suggestions and star the repo! üåü https://github.com/PrunaAI/awesome-ai-efficiency See translation</description><pubDate>Thu, 08 May 2025 13:32:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sharpenb/996977777058725</guid></item><item><title>Ever notice how some AI assistants feel like tools while others feel like companions? Turns out, it's not always about fancy tech upgrades, because sometimes it's just clever design.</title><link>https://huggingface.co/posts/giadap/315154856088110</link><description>Ever notice how some AI assistants feel like tools while others feel like companions? Turns out, it's not always about fancy tech upgrades, because sometimes it's just clever design. Our latest blog post at Hugging Face dives into how minimal design choices can completely transform how users experience AI. We've seen our community turn the same base models into everything from swimming coaches to interview prep specialists with surprisingly small tweaks. The most fascinating part? When we tested identical models with different "personalities" in our Inference Playground, the results were mind-blowing. Want to experiment yourself? Our Inference Playground lets anyone (yes, even non-coders!) test these differences in real-time. You can: - Compare multiple models side-by-side - Customize system prompts - Adjust parameters like temperature - Test multi-turn conversations It's fascinating how a few lines of instruction text can transform the same AI from strictly professional to...</description><pubDate>Thu, 08 May 2025 13:32:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/315154856088110</guid></item><item><title>Well, here‚Äôs the updated version with the 20,000+ entry sampled dataset for Watermark Filter Content Moderation models incl. [Food25, Weather, Watermark, Marathi/Hindi Sign Language Detection], post-trained from the base models: sigLip2 patch16 224 ‚Äî now with mixed aspect ratios for better performance and reduced misclassification. üî•</title><link>https://huggingface.co/posts/prithivMLmods/146991496058553</link><description>Well, here‚Äôs the updated version with the 20,000+ entry sampled dataset for Watermark Filter Content Moderation models incl. [Food25, Weather, Watermark, Marathi/Hindi Sign Language Detection], post-trained from the base models: sigLip2 patch16 224 ‚Äî now with mixed aspect ratios for better performance and reduced misclassification. üî• Models : ‚ûÆ Watermark-Detection : prithivMLmods/Watermark-Detection-SigLIP2 ‚å®Ô∏é Watermark Detection &amp; Batch Image Processing Experimentals, Colab Notebook : https://colab.research.google.com/drive/1mlQrSsSjkGimUt0VyRi3SoWMv8OMyvw3?usp=drive_link ‚ûÆ Weather-Image-Classification : prithivMLmods/Weather-Image-Classification ‚ûÆ TurkishFoods-25 : prithivMLmods/TurkishFoods-25 ‚ûÆ Marathi-Sign-Language-Detection : prithivMLmods/Marathi-Sign-Language-Detection ‚ûÆ Hindi-Sign-Language-Detection : prithivMLmods/Hindi-Sign-Language-Detection Datasets : Watermark : qwertyforce/scenery_watermarks Weather : prithivMLmods/WeatherNet-05-18039 Turkish Foods 25 :...</description><pubDate>Thu, 08 May 2025 13:32:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/146991496058553</guid></item></channel></rss>