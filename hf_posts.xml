<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Eyyyy 50 followers 🤯</title><link>https://huggingface.co/posts/ProCreations/321100188234240</link><description>Eyyyy 50 followers 🤯 See translation</description><pubDate>Thu, 29 May 2025 17:21:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ProCreations/321100188234240</guid></item><item><title>I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book.</title><link>https://huggingface.co/posts/hesamation/260011784391977</link><description>I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book. It gives an overview, then goes into detail for each stage, even providing best practices. It’s 115 pages on arxiv, definitely worth a read. Check it out: https://arxiv.org/abs/2408.13296 See translation</description><pubDate>Thu, 29 May 2025 17:21:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/260011784391977</guid></item><item><title>I am so happy  to share to all that I’ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I’m doing my best to keep up. Excited for what’s ahead!</title><link>https://huggingface.co/posts/lukmanaj/495766537273785</link><description>I am so happy to share to all that I’ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I’m doing my best to keep up. Excited for what’s ahead! See translation</description><pubDate>Thu, 29 May 2025 17:21:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lukmanaj/495766537273785</guid></item><item><title>Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity!</title><link>https://huggingface.co/posts/fdaudens/719212082746895</link><description>Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity! The final project uses the GAIA dataset - your agent has to solve tasks like analyzing Excel files, processing audio recordings, answering questions about YouTube videos, and diving into research papers. This isn't toy examples, it's the messy, multimodal stuff agents need to handle in practice. Whether you’re just getting started with agents or want to go deeper with tools like LangChain, LlamaIndex, and SmolAgents, this course has tons of useful stuff. A few key insights: - Code agents are incredibly versatile once you get the architecture right - The sweet spot is finding the right balance of guidance vs autonomy for each use case - Once the logic clicks, the possibilities really are endless - it's like letting LLMs break free from the chatbox The course is free and the certification deadline is July 1st, 2025. The Hugging Face...</description><pubDate>Thu, 29 May 2025 17:21:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/719212082746895</guid></item><item><title>VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration</title><link>https://huggingface.co/posts/DawnC/538322807718464</link><description>VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration I'm excited to share significant improvements to VisionScout that substantially enhance accuracy and analytical capabilities. ⭐️ Key Enhancements - CLIP Zero-Shot Landmark Detection: The system now identifies famous landmarks and architectural features without requiring specific training data, expanding scene understanding beyond generic object detection. - Places365 Environmental Classification: Integration of MIT's Places365 model provides robust scene baseline classification across 365 categories, significantly improving lighting analysis accuracy and overall scene identification precision. - Enhanced Multi-Modal Fusion: Advanced algorithms now dynamically combine insights from YOLOv8, CLIP, and Places365 to optimize accuracy across diverse scenarios. - Refined LLM Narratives: Llama 3.2 integration continues to transform analytical data into fluent, contextually rich descriptions while maintaining...</description><pubDate>Thu, 29 May 2025 17:21:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/538322807718464</guid></item><item><title>Cua : Docker for computer-use agents</title><link>https://huggingface.co/posts/dhruv3006/675063918098240</link><description>Cua : Docker for computer-use agents Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers. Github : https://github.com/trycua See translation</description><pubDate>Thu, 29 May 2025 17:21:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/675063918098240</guid></item><item><title>Introducing our new work: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation​​ 🚀</title><link>https://huggingface.co/posts/BestWishYsh/693532821570217</link><description>Introducing our new work: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation​​ 🚀 We tackle the core challenges of ​​Subject-to-Video Generation (S2V)​​ by systematically building the first complete infrastructure—featuring an evaluation benchmark and a million-scale dataset! ✨ 🧠 Introducing ​​OpenS2V-Eval​​—the first ​​fine-grained S2V benchmark​​, with ​​180 multi-domain prompts + real/synthetic test pairs​​. We propose ​​NexusScore​​, ​​NaturalScore​​, and ​​GmeScore​​ to precisely quantify model performance across ​​subject consistency, naturalness, and text alignment​​ ✔ 📊 Using this framework, we conduct a ​​comprehensive evaluation of 16 leading S2V models​​, revealing their strengths/weaknesses in complex scenarios! 🔥 ​​OpenS2V-5M dataset​​ now available! A ​​5.4M 720P HD​​ collection of ​​subject-text-video triplets​​, enabled by ​​cross-video association segmentation + multi-view synthesis​​ for ​​diverse subjects &amp; high-quality...</description><pubDate>Thu, 29 May 2025 17:21:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/BestWishYsh/693532821570217</guid></item><item><title>🎵 Dream come true for content creators! TIGER AI can extract voice, effects &amp; music from ANY audio file 🤯</title><link>https://huggingface.co/posts/fdaudens/323840314242853</link><description>🎵 Dream come true for content creators! TIGER AI can extract voice, effects &amp; music from ANY audio file 🤯 This lightweight model uses frequency band-split technology to separate speech like magic. Kudos to @ fffiloni for the amazing demo! fffiloni/TIGER-audio-extraction See translation</description><pubDate>Thu, 29 May 2025 17:21:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/323840314242853</guid></item><item><title>Introducing AutoThink: Adaptive reasoning for LLMs that improves performance by 43% on reasoning benchmarks!</title><link>https://huggingface.co/posts/codelion/194217585449623</link><description>Introducing AutoThink: Adaptive reasoning for LLMs that improves performance by 43% on reasoning benchmarks! Instead of using fixed thinking budgets, AutoThink: - Classifies query complexity (HIGH/LOW) using adaptive classification - Dynamically allocates thinking tokens based on complexity - Uses steering vectors derived from Pivotal Token Search to guide reasoning patterns Results on DeepSeek-R1-Distill-Qwen-1.5B: - GPQA-Diamond: 31.06% vs 21.72% baseline (+9.34 points) - MMLU-Pro: 26.38% vs 25.58% baseline (+0.8 points) - Uses fewer tokens than baseline approaches Works with any local reasoning model - DeepSeek, Qwen, Llama, custom models. The technique combines our research on Pivotal Token Search (PTS) implementation and adaptive classification frameworks. Paper: AutoThink: efficient inference for reasoning LLMs https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327 Code and examples: https://github.com/codelion/optillm/tree/main/optillm/autothink PTS implementation and...</description><pubDate>Thu, 29 May 2025 17:21:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/194217585449623</guid></item><item><title>Real Steel Became a Reality — Full AI Robots Boxing Tournament — With English Subtitles — So How These Robots Working Actually in Full Details</title><link>https://huggingface.co/posts/MonsterMMORPG/399804017135457</link><description>Real Steel Became a Reality — Full AI Robots Boxing Tournament — With English Subtitles — So How These Robots Working Actually in Full Details Video link : https://youtu.be/fw2yezpn_bo Article (free and public) : https://www.patreon.com/posts/130031621 Summary of article 🎮 Control Philosophy AI Dominance ✅ Complex physical movement execution ✅ Low-level motor control and coordination ✅ Sensor data interpretation ✅ Safety protocol management Human Dominance 🎯 Overall fight strategy 🎯 Attack timing and selection 🎯 Action intensity modulation 🎯 Real-time tactical decisions See translation</description><pubDate>Thu, 29 May 2025 17:21:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/399804017135457</guid></item></channel></rss>