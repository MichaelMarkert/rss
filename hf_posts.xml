<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Whisper-WebUI Premium - Ultra Fast and High Accuracy Speech to Text Transcripton App for All Languages - Windows, RunPod, Massed Compute 1-Click Installers - Supporting RTX 1000 to 5000 series</title><link>https://huggingface.co/posts/MonsterMMORPG/232609167066612</link><description>Whisper-WebUI Premium - Ultra Fast and High Accuracy Speech to Text Transcripton App for All Languages - Windows, RunPod, Massed Compute 1-Click Installers - Supporting RTX 1000 to 5000 series Latest installer zip file : https://www.patreon.com/posts/145395299 New Features Password protected version, password is just 1 : WhisperWeb_UI_v1_password_is_1.zip It has better interface, more features, default settings set for maximum accuracy It will show transcription realtime both on Gradio interface and also on CMD It will show better status and output at the cmd like starting time, starting file, etc It will save every generated transcription properly with same name as input file name with proper name sanitization After deep scan of the entire pipeline, default parameters are set for maximum accuracy and quality 1-Click installers for Windows local PC, RunPod (Linux-Cloud) and Massed Compute (Linux-Cloud) The app the installers are made for RTX 1000 series to RTX 5000 series with pre-...</description><pubDate>Wed, 10 Dec 2025 17:27:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/232609167066612</guid></item><item><title>Two new releases today!</title><link>https://huggingface.co/posts/sequelbox/418257138244612</link><description>Two new releases today! Firstly, our new Raiden-Mini dataset, powered by DeepSeek's newest deepseek-ai/DeepSeek-V3.2-Speciale model! - A V3.2-Speciale reasoning showcase: the Raiden prompts test the model's creative, analytic, and general reasoning skills! - HEAD TO HEAD: a comparison subset pits V3.2-Speciale against V3.2 with the same prompts, providing a direct look at each model's advantages! Get the new Raiden-Mini dataset: sequelbox/Raiden-Mini-DeepSeek-V3.2-Speciale On the model side, we've also brought Shining Valiant 3 to Ministral 3! - Science-reasoning: sequelbox/Celestia3-DeepSeek-R1-0528 for physics, biology, chemistry, compsci, astronomy, Earth science, and information theory. - AI to build AI: the sequelbox/Mitakihara-DeepSeek-R1-0528 dataset for high-quality reasoning performance on AI, MLOps, math and CUDA, complex adaptive and agentic systems, cognition, logic, linguistics, simulation, knowledge management, and more! - Creative reasoning and general chat...</description><pubDate>Wed, 10 Dec 2025 17:27:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sequelbox/418257138244612</guid></item><item><title>15 Outstanding Research Papers from NeurIPS 2025</title><link>https://huggingface.co/posts/Kseniase/673116238988658</link><description>15 Outstanding Research Papers from NeurIPS 2025 NeurIPS 2025, as a premier annual event in machine learning and computational neuroscience, tackles major topics like the future of AI, current research, and the most difficult challenges. While we‚Äôre not attending this year, we‚Äôre closely following the updates and today we pull together a quick, easy-to-digest roundup of a few standout papers so you can jump in without getting overwhelmed. Here is a list of 15 papers from NeurIPS 2025, including 8 top research papers that received awards, along with 7 others that caught our attention: 1. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks ‚Üí https://neurips.cc/virtual/2025/loc/san-diego/test-of-time/128328 Test of Time Award winner. Introduces the RPN, a small convnet that predicts objectness and boxes on shared features, enabling Faster R-CNN to share computation and run around 5 fps on a GPU 2. Artificial Hivemind: The Open-Ended Homogeneity of LMs (and...</description><pubDate>Wed, 10 Dec 2025 17:27:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/673116238988658</guid></item><item><title>Currently having a blast learning the transformers library.</title><link>https://huggingface.co/posts/melvindave/358781763788308</link><description>Currently having a blast learning the transformers library. I noticed that model cards usually have Transformers code as usage examples. So I tried to figure out how to load a model just using the transformers library without using ollama, lmstudio, or llamacpp. Learned how to install dependencies required to make it work like pytorch and CUDA. I also used Conda for python environment dependencies. Once I got the model loaded and sample inference working, I made an API to serve it. I know it's very basic stuff for machine learning experts here in HF but I'm completely new to this so I'm happy to get it working! Model used: Qwen/Qwen3-VL-8B-Instruct GPU: NVIDIA GeForce RTX 3090 Here's the result of my experimentation See translation</description><pubDate>Wed, 10 Dec 2025 17:27:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/melvindave/358781763788308</guid></item><item><title>Got to 1199.8 tokens/sec with Devstral Small -2 on my desktop GPU workstation. vLLM nightly.</title><link>https://huggingface.co/posts/mitkox/706030667212965</link><description>Got to 1199.8 tokens/sec with Devstral Small -2 on my desktop GPU workstation. vLLM nightly. Works out of the box with Mistral Vibe. Next is time to test the big one. See translation</description><pubDate>Wed, 10 Dec 2025 17:27:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/706030667212965</guid></item><item><title>I built something crazy you never saw before.</title><link>https://huggingface.co/posts/RakshitAralimatti/606573836468382</link><description>I built something crazy you never saw before. Please check - https://huggingface.co/blog/RakshitAralimatti/streaming-data-rag A real-time Streaming Data to RAG system that listens to live radio, transcribes it on-the-fly, and lets you query across TIME. Not just "what was discussed" ‚Äì but "what happened in the last 10 minutes on channel 0?" or "at 9 AM, what was the breaking news?" This is RAG that understands temporal context. See translation</description><pubDate>Wed, 10 Dec 2025 17:27:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/606573836468382</guid></item><item><title>NEW:</title><link>https://huggingface.co/posts/sergiopaniego/936636401476551</link><description>NEW: @ EssentialAI just released Rnj-1, their first 8B model. You can easily fine-tune it with GRPO using TRL to add reasoning capabilities to a compact mode Free Colab link: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_rnj_1_instruct.ipynb More free TRL notebooks: https://huggingface.co/docs/trl/main/en/example_overview#notebooks See translation</description><pubDate>Wed, 10 Dec 2025 17:27:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/936636401476551</guid></item><item><title>Introducing the D.Markdown Experimental Models, Proxima and Epsilon OCR models, built on top of Qwen3-VL and Qwen2.5-VL respectively. Proxima is optimized for Markdown generation and is capable of embedding inline programming code snippets and generating rich nodes such as HTML, XML, JSON, and YAML. Epsilon is optimized for reconstructing complex layouts including tables, forms, and mathematical content. üåå‚ú®</title><link>https://huggingface.co/posts/prithivMLmods/177760462404074</link><description>Introducing the D.Markdown Experimental Models, Proxima and Epsilon OCR models, built on top of Qwen3-VL and Qwen2.5-VL respectively. Proxima is optimized for Markdown generation and is capable of embedding inline programming code snippets and generating rich nodes such as HTML, XML, JSON, and YAML. Epsilon is optimized for reconstructing complex layouts including tables, forms, and mathematical content. üåå‚ú® ‚óè proxima-ocr-d.markdown-post3.0.l: prithivMLmods/proxima-ocr-d.markdown-post3.0.l ‚óè epsilon-ocr-d.markdown-post3.0.m: prithivMLmods/epsilon-ocr-d.markdown-post3.0.m ‚óè proxima-ocr-d.markdown-post3.0.l-gguf: prithivMLmods/proxima-ocr-d.markdown-post3.0.l-GGUF ‚óè epsilon-ocr-d.markdown-post3.0.m-gguf: prithivMLmods/epsilon-ocr-d.markdown-post3.0.m-GGUF ‚óè Collection: https://huggingface.co/collections/prithivMLmods/dynamic-markdowns ‚óè Multimodal Apps: https://huggingface.co/collections/prithivMLmods/multimodal-implementations üëâ These models are stage progression models, and currently...</description><pubDate>Wed, 10 Dec 2025 17:27:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/177760462404074</guid></item><item><title>installama.sh at the TigerBeetle 1000x World Tour !</title><link>https://huggingface.co/posts/angt/186034800220690</link><description>installama.sh at the TigerBeetle 1000x World Tour ! Last week I had the chance to give a short talk during the TigerBeetle 1000x World Tour (organized by @ jedisct1 üëè ) a fantastic event celebrating high-performance engineering and the people who love pushing systems to their limits! In the talk, I focused on the CPU and Linux side of things, with a simple goal in mind: making the installation of llama.cpp instant, automatic, and optimal, no matter your OS or hardware setup. For the curious, here are the links worth checking out: Event page: https://tigerbeetle.com/event/1000x GitHub repo: https://github.com/angt/installama.sh Talk: https://youtu.be/pg5NOeJZf0o?si=9Dkcfi2TqjnT_30e More improvements are coming soon. Stay tuned! See translation</description><pubDate>Wed, 10 Dec 2025 17:27:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/angt/186034800220690</guid></item><item><title>Hey all  üëã</title><link>https://huggingface.co/posts/StJohnDeakins/147365886941567</link><description>Hey all üëã A Quick one for any founders building with Small Language Models in mobile apps: We‚Äôre opening 10 Innovation Partner spots this month for our Device Native AI (DNA) platform. What you get: - Device Native AI SDK ‚Å†(AI processes data on-device, not cloud üì≤) - 99% off for 3 months, then 90% off for the rest of the year (no lock-in) - Direct engineering access + feature releases - It's an Innovation community, so at least some participation is required Perfect if you're building consumer apps and want: ‚úì Hyper-personalization without privacy risks ‚úì Zero cloud AI token costs ‚úì Early access to next-gen mobile AI Limited spots, and on a first-come basis, so DM me "DNA" for more info and an access code. Cheers Singe üêµ See translation</description><pubDate>Wed, 10 Dec 2025 17:27:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/StJohnDeakins/147365886941567</guid></item></channel></rss>