<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Your weekly recap of open AI is here, and it's packed with models!</title><link>https://huggingface.co/posts/merve/171433424068357</link><description>Your weekly recap of open AI is here, and it's packed with models! merve/feb-14-releases-67af876b404cc27c6d837767 ğŸ‘€ Multimodal &gt; OpenGVLab released InternVideo 2.5 Chat models, new video LMs with long context &gt; AIDC released Ovis2 model family along with Ovis dataset, new vision LMs in different sizes (1B, 2B, 4B, 8B, 16B, 34B), with video and OCR support &gt; ColQwenStella-2b is a multilingual visual retrieval model that is sota in it's size &gt; Hoags-2B-Exp is a new multilingual vision LM with contextual reasoning, long context video understanding ğŸ’¬ LLMs A lot of math models! &gt; Open-R1 team released OpenR1-Math-220k large scale math reasoning dataset, along with Qwen2.5-220K-Math fine-tuned on the dataset, OpenR1-Qwen-7B &gt; Nomic AI released new Nomic Embed multilingual retrieval model, a MoE with 500 params with 305M active params, outperforming other models &gt; DeepScaleR-1.5B-Preview is a new DeepSeek-R1-Distill fine-tune using distributed RL on math &gt; LIMO is a new fine-tune of...</description><pubDate>Sun, 16 Feb 2025 05:19:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/171433424068357</guid></item><item><title>Hey, Iâ€™m Ben and I work at Hugging Face.</title><link>https://huggingface.co/posts/burtenshaw/555593095351748</link><description>Hey, Iâ€™m Ben and I work at Hugging Face. Right now, Iâ€™m focusing on educational stuff and getting loads of new people to build open AI models using free and open source tools. Iâ€™ve made a collection of some of the tools Iâ€™m building and using for teaching. Stuff like quizzes, code challenges, and certificates. burtenshaw/tools-for-learning-ai-6797453caae193052d3638e2 See translation</description><pubDate>Sun, 16 Feb 2025 05:19:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/555593095351748</guid></item><item><title>How do you make 1M+ Hugging Face models &amp; datasets more discoverable?</title><link>https://huggingface.co/posts/davanstrien/966932095656116</link><description>How do you make 1M+ Hugging Face models &amp; datasets more discoverable? davanstrien/Smol-Hub-tldr ! I fine-tuned HuggingFaceTB/SmolLM2-360M to generate one-line summaries from a model or dataset README. Its own self-description? "A model for generating concise summaries of model &amp; dataset cards from the Hugging Face Hub" The goal? Make it easier to find the right models and datasets for your specific needs. It's already powering a semantic search for datasets Space. It's still a WIP but thanks to @ loubnabnl , @ anton-l , @ eliebak et al, for cooking such a nice base model for fine-tuning small, efficient models for specific domains and tasks. ğŸ™ See translation</description><pubDate>Sun, 16 Feb 2025 05:19:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/davanstrien/966932095656116</guid></item><item><title>For those who haven't come across it yet, here's a handy trick to discuss an entire GitHub repo with an LLM:</title><link>https://huggingface.co/posts/m-ric/573811868569071</link><description>For those who haven't come across it yet, here's a handy trick to discuss an entire GitHub repo with an LLM: =&gt; Just replace "github" with "gitingest" in the url, and you get the whole repo as a single string that you can then paste in your LLMs See translation</description><pubDate>Sun, 16 Feb 2025 05:19:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/573811868569071</guid></item><item><title>hello, dev mode explorers!</title><link>https://huggingface.co/posts/nroggendorff/464265972064174</link><description>hello, dev mode explorers! See translation</description><pubDate>Sun, 16 Feb 2025 05:19:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/464265972064174</guid></item><item><title>I was thinking i need to step up my game on training Flux LoRas models, time to have some fun ! â˜€ï¸</title><link>https://huggingface.co/posts/fffiloni/806803691807876</link><description>I was thinking i need to step up my game on training Flux LoRas models, time to have some fun ! â˜€ï¸ Expect a new drop per week on aesthetics that catched my attention, here are 3 of them that worked really well ! fffiloni/cute-comic-800 fffiloni/carbo-800 fffiloni/oniric-750 See translation</description><pubDate>Sun, 16 Feb 2025 05:19:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fffiloni/806803691807876</guid></item><item><title>ğŸ”Š Meet Kokoro Web - Free, ML speech synthesis on your computer, that'll make you ditch paid services!</title><link>https://huggingface.co/posts/fdaudens/730182210725139</link><description>ğŸ”Š Meet Kokoro Web - Free, ML speech synthesis on your computer, that'll make you ditch paid services! 28 natural voices, unlimited generations, and WebGPU acceleration. Perfect for journalists and content creators. Test it with full articlesâ€”sounds amazingly human! ğŸ¯ğŸ™ï¸ Xenova/kokoro-web See translation</description><pubDate>Sun, 16 Feb 2025 05:19:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/730182210725139</guid></item><item><title>Nice paper comparing the fp8 inference efficiency of Nvidia H100 and Intel Gaudi2:</title><link>https://huggingface.co/posts/regisss/901655788646796</link><description>Nice paper comparing the fp8 inference efficiency of Nvidia H100 and Intel Gaudi2: An Investigation of FP8 Across Accelerators for LLM Inference (2502.01070) The conclusion is interesting: "Our findings highlight that the Gaudi 2, by leveraging FP8, achieves higher throughput-to-power efficiency during LLM inference" One aspect of AI hardware accelerators that is often overlooked is how they consume less energy than GPUs. It's nice to see researchers starting carrying out experiments to measure this! Gaudi3 results soon... See translation</description><pubDate>Sun, 16 Feb 2025 05:19:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/regisss/901655788646796</guid></item><item><title>ğ—šğ—¿ğ—²ğ—®ğ˜ ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—®ğ—¹ğ—²ğ—¿ğ˜: you can now share agents to the Hub! ğŸ¥³ğŸ¥³</title><link>https://huggingface.co/posts/m-ric/668305263865285</link><description>ğ—šğ—¿ğ—²ğ—®ğ˜ ğ—³ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—®ğ—¹ğ—²ğ—¿ğ˜: you can now share agents to the Hub! ğŸ¥³ğŸ¥³ And any agent pushed to Hub get a cool Space interface to directly chat with it. This was a real technical challenge: for instance, serializing tools to export them meant that you needed to get all the source code for a tool, verify that it was standalone (not relying on external variables), and gathering all the packages required to make it run. Go try it out! ğŸ‘‰ https://github.com/huggingface/smolagents See translation</description><pubDate>Sun, 16 Feb 2025 05:19:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/668305263865285</guid></item><item><title>Some things are simple</title><link>https://huggingface.co/posts/etemiz/440192103698875</link><description>Some things are simple See translation</description><pubDate>Sun, 16 Feb 2025 05:19:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/440192103698875</guid></item></channel></rss>