<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Weâ€™ve reached a point where on device AI coding that is free, offline, and capable isnâ€™t just a theoretical possibility; itâ€™s sitting on my lap, barely warming my thighs.</title><link>https://huggingface.co/posts/mitkox/598805408500117</link><description>Weâ€™ve reached a point where on device AI coding that is free, offline, and capable isnâ€™t just a theoretical possibility; itâ€™s sitting on my lap, barely warming my thighs. My local MacBook Air setup includes a Qwen3 Coder Flash with a 1M context, Cline in a VSCode IDE. No internet, no cloud, no ID verification- this is the forbidden tech. Current stats: All agentic tools work great local, sandboxed, and MCP OK model output precision 17 tokens/sec. Not great, not terrible 65K tokens context, the model can do 1M, but letâ€™s be real, my MacBook Air would probably achieve fusion before hitting that smoothly Standard backend and cache off for the test All inference and function calling happen locally, offline, untethered. The cloud didnâ€™t even get a memo. See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/598805408500117</guid></item><item><title>Latest work on SWE-Bench ğŸ›</title><link>https://huggingface.co/posts/YerbaPage/437092763804097</link><description>Latest work on SWE-Bench ğŸ› Our two new papers from the SJTU &amp; Huawei: Powered by DeepSeek-V3, we've achieved a new SOTA on the SWE-Bench benchmark! We introduce two innovative approaches: âš”ï¸ SWE-Debate: AI agents compete and "debate" to generate the best code fix. ğŸ§  SWE-Exp: An AI agent learns from past repair "experience" to solve new issues more efficiently. ğŸ‘‡ Explore the future of software development: SWE-Debate ğŸ“„ Paper: https://arxiv.org/abs/2507.23348 ğŸ’» Code: https://github.com/YerbaPage/SWE-Debate SWE-Exp ğŸ“„ Paper: https://arxiv.org/abs/2507.23361 ğŸ’» Code: https://github.com/YerbaPage/SWE-Exp See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/437092763804097</guid></item><item><title>ğŸš€ Dhanishtha-2.0-preview-0825 Is Here</title><link>https://huggingface.co/posts/Abhaykoul/625756342268823</link><description>ğŸš€ Dhanishtha-2.0-preview-0825 Is Here The Intermediate Thinking Model just leveled up again. With sharper reasoning, better tool use, and expanded capabilities, Dhanishtha-2.0-preview-0825 is now live and ready to impress. ğŸ§  What Makes Dhanishtha Special? Unlike typical CoT models that only thinks one time, Dhanishtha thinks iteratively: &gt; Think â†’ Answer â†’ Rethink â†’ Improve â†’ Rethink again if needed. ğŸ”— Try it now: HelpingAI/Dhanishtha-2.0-preview-0825 ğŸ” Dhanishtha NSFW Preview For those exploring more expressive and immersive roleplay scenarios, weâ€™re also releasing: HelpingAI/Dhanishtha-nsfw A specialized version tuned for adult-themed interactions and character-driven roleplay. ğŸ”— Explore it here: HelpingAI/Dhanishtha-nsfw ğŸ’¬ You can also try all of these live at chat.helpingai.co See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/625756342268823</guid></item><item><title>Weâ€™re excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100.</title><link>https://huggingface.co/posts/chintankp/680096865746882</link><description>Weâ€™re excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100. Super v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. Besides leading accuracy, it also delivers high throughput. Key features: - Leading accuracy on multi-step reasoning, math, coding, and function-calling - Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples - Fully transparent training data on HF (Nemotron-Post-Training-Dataset-v1) Try Super v1.5 on build.nvidia.com or download from Hugging Face See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/chintankp/680096865746882</guid></item><item><title>Cohere just dropped</title><link>https://huggingface.co/posts/merve/770620321222949</link><description>Cohere just dropped CohereLabs/command-a-vision-07-2025 , a 112B (dense!) vision LM &gt; based on SigLIP2 &amp; Command-A &gt; built for enterprise use cases ğŸ”¥ &gt; use with Inference Providers or transformers ğŸ¤— read their blog https://huggingface.co/blog/CohereLabs/introducing-command-a-vision-07-2025 See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/770620321222949</guid></item><item><title>Qwen team did it again!!</title><link>https://huggingface.co/posts/AdinaY/564352975503737</link><description>Qwen team did it again!! They just released Qwen3-Coder-30B-A3B-Instruct on the hubğŸ”¥ Qwen/Qwen3-Coder-30B-A3B-Instruct âœ¨ Apache 2.0 âœ¨30B total / 3.3B active (128 experts, 8 top-k) âœ¨ Native 256K context, extendable to 1M via Yarn âœ¨ Built for Agentic Coding See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/564352975503737</guid></item><item><title>Introducing Camel-Doc-OCR-080125(v2), a document content-structure retrieval VLM designed for content extraction and summarization. This is the second model in the Camel Doc OCR VLM series, following Camel-Doc-OCR-062825(v1). The new version fixes formal table reconstruction issues in both en and zh language, achieving optimal performance for long-context inferences.ğŸ¤—ğŸª</title><link>https://huggingface.co/posts/prithivMLmods/210600524016945</link><description>Introducing Camel-Doc-OCR-080125(v2), a document content-structure retrieval VLM designed for content extraction and summarization. This is the second model in the Camel Doc OCR VLM series, following Camel-Doc-OCR-062825(v1). The new version fixes formal table reconstruction issues in both en and zh language, achieving optimal performance for long-context inferences.ğŸ¤—ğŸª â¤· Camel-Doc-OCR(v2) : prithivMLmods/Camel-Doc-OCR-080125 â¤· Camel-Doc-OCR(v1) : prithivMLmods/Camel-Doc-OCR-062825 â¤· Demo : prithivMLmods/core-OCR Multimodal Model Collections and Spaces: â Camel-Doc-OCR : prithivMLmods/camel-doc-ocr-080125-688c0c61c5dba648756f31f8 â Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a â Multimodal Spaces : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 â Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 . . . To know more about it, visit the model card of the respective model. !! See...</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/210600524016945</guid></item><item><title>ğŸš€ Launch Alert: Dev-Stack-Agents</title><link>https://huggingface.co/posts/Parveshiiii/913365249348370</link><description>ğŸš€ Launch Alert: Dev-Stack-Agents Meet your 50-agent senior AI team â€” principal-level experts in engineering, AI, DevOps, security, product, and more â€” all bundled into one modular repo. + Code. Optimize. Scale. Secure. - Full-stack execution, Claude-powered. No human bottlenecks. ğŸ”§ Built for Claude Code Seamlessly plug into Claudeâ€™s dev environment: * ğŸ§  Each .md file = a fully defined expert persona * âš™ï¸ Claude indexes them as agents with roles, skills &amp; strategy * ğŸ¤– You chat â†’ Claude auto-routes to the right agent(s) * âœï¸ Want precision? Just call @agent-name directly * ğŸ‘¥ Complex task? Mention multiple agents for team execution Examples: "@security-auditor please review auth flow for risks" "@cloud-architect + @devops-troubleshooter â†’ design a resilient multi-region setup" "@ai-engineer + @legal-advisor â†’ build a privacy-safe RAG pipeline" ğŸ”— https://github.com/Parveshiiii/Dev-Stack-Agents MIT License | Claude-Ready | PRs Welcome See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Parveshiiii/913365249348370</guid></item><item><title>Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, weâ€™ve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources.</title><link>https://huggingface.co/posts/prithivMLmods/923940739727688</link><description>Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, weâ€™ve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources. âœ¨ Multimodal-VLM-Thinking : prithivMLmods/Multimodal-VLM-Thinking âœ¨ Multimodal-VLM-OCR : prithivMLmods/Multimodal-VLM-OCR âœ¦ Models used in these spaces: âœ¨ Lumian-VLR-7B-Thinking : prithivMLmods/Lumian-VLR-7B-Thinking âœ¨ Enesidaon-VLR-7B-no-Thinking : prithivMLmods/Enesidaon-VLR-7B-no-Thinking âœ¨ GLM-4.1V-9B-Thinking : zai-org/GLM-4.1V-9B-Thinking âœ¨ DREX-062225-exp : prithivMLmods/DREX-062225-exp &amp; more ... âœ¦ Multimodal Model Collections and Spaces: âœ¨ Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a âœ¨ Multimodal Spaces : prithivMLmods/multimodal-...</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/923940739727688</guid></item><item><title>We've crossed 1 million repositories backed by Xet storage on Hugging Face! ğŸš€ğŸš€ğŸš€</title><link>https://huggingface.co/posts/jsulz/651298897017923</link><description>We've crossed 1 million repositories backed by Xet storage on Hugging Face! ğŸš€ğŸš€ğŸš€ You can follow along our progress converting the Hub from Git LFS to Xet at jsulz/ready-xet-go We have a lot of repos left to migrate, which means I have plenty of time to add more animations ğŸ¤ª See translation</description><pubDate>Sun, 03 Aug 2025 09:25:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/651298897017923</guid></item></channel></rss>