<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>10 Open-source Deep Research assistants</title><link>https://huggingface.co/posts/Kseniase/947704683052150</link><description>10 Open-source Deep Research assistants Deep Research agents are quickly becoming our daily co-workers â€” built for complex investigations, not just chat. With modular architecture, advanced tool use and real web access, they go far beyond typical AI. While big-name agents get the spotlight, we want to highlight some powerful recent open-source alternatives: 1. DeerFlow -&gt; https://github.com/bytedance/deer-flow A modular multi-agent system combining LMs and tools for automated research and code analysis. It links a coordinator, planner, team of specialized agent, and reporter, and converts reports to speech via Text-to-Speech (TTS) 2. Alita -&gt; https://github.com/CharlesQ9/Alita Uses a single problem-solving module for scalable reasoning through simplicity. It self-evolves by generating and reusing Model Context Protocols (MCPs) from open-source tools to build external capabilities for diverse tasks 3. WebThinker -&gt; https://github.com/RUC-NLPIR/WebThinker Lets reasoning models...</description><pubDate>Mon, 30 Jun 2025 17:21:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/947704683052150</guid></item><item><title>ğŸ§  SOMA: The Core Architecture for AGI Level 1 ğŸš€</title><link>https://huggingface.co/posts/openfree/674788441765421</link><description>ğŸ§  SOMA: The Core Architecture for AGI Level 1 ğŸš€ VIDraft/SOMA-AGI ğŸ¯ The First Step Toward AGI SOMA (Self-Orchestrating Modular Architect) is a revolutionary architecture that fulfills the essential requirements for AGI (Artificial General Intelligence) Level 1. It perfectly implements the common AGI prerequisites emphasized by Yann LeCun (Meta), OpenAI, and Google DeepMind within a single LLM. ğŸ“‹ AGI Level 1 Core Requirements = SOMA's Perfect Implementation âœ… ğŸ¯ Planning Capability â†’ Supervisor AI autonomously designs and executes comprehensive analysis roadmaps ğŸ§© Role Differentiation &amp; Modularity â†’ A single LLM instantly differentiates into 5 expert AIs for collaboration ğŸ”„ Self-reflection &amp; Feedback Loops â†’ Evaluator AI continuously validates and directs improvements ğŸ› ï¸ Tool-use &amp; Autonomy â†’ Full automation from web search to report generation ğŸ® Long-term Agency Structure â†’ Completes complex 11-stage collaborative processes end-to-end ğŸ”· SOMA's Three Core Structures ğŸ§­ Self-...</description><pubDate>Mon, 30 Jun 2025 17:21:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/674788441765421</guid></item><item><title>I find it really annoying that you can use a Zero GPU model that will throw an error but itâ€™ll still take the processing time from your quota. So youre just left with less time and nothing to show for it.</title><link>https://huggingface.co/posts/wewittc/347088512345422</link><description>I find it really annoying that you can use a Zero GPU model that will throw an error but itâ€™ll still take the processing time from your quota. So youre just left with less time and nothing to show for it. See translation</description><pubDate>Mon, 30 Jun 2025 17:21:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wewittc/347088512345422</guid></item><item><title>Layer-wise and Pruned versions of Qwen/Qwen3-30B-A3B</title><link>https://huggingface.co/posts/eaddario/678649623001881</link><description>Layer-wise and Pruned versions of Qwen/Qwen3-30B-A3B * Tesor-wise: eaddario/Qwen3-30B-A3B-GGUF * Pruned: eaddario/Qwen3-30B-A3B-pruned-GGUF Even though the Perplexity scores of the pruned version are 3 times higher, the ARC, HellaSwag, MMLU, Truthful QA and WinoGrande scores are holding remarkably well, considering two layers were removed (5 and 39). This seems to support Xin Men et al conclusions in ShortGPT: Layers in Large Language Models are More Redundant Than You Expect (2403.03853) Results summary in the model's card and test results in the ./scores directory. Questions/feedback is always welcomed. See translation</description><pubDate>Mon, 30 Jun 2025 17:21:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/eaddario/678649623001881</guid></item><item><title>ğŸ¬ How to Use Seedance, the #1 Video Generation Model, for Free</title><link>https://huggingface.co/posts/fantos/713394258829998</link><description>ğŸ¬ How to Use Seedance, the #1 Video Generation Model, for Free ğŸ“Œ A Hidden Gem I Stumbled Upon While browsing Hugging Face, I discovered an amazing project. I found ByteDance's Seedance video generation service - which knocked Google's VEO3 down to 2nd place on the video generation leaderboard - available for free on Hugging Face! ginigen/Seedance-Free Leaderboard standings: ğŸ¥‡ 1st: ByteDance Seedance ğŸ¥ˆ 2nd: Google VEO3 It's called "Bytedance Seedance Video Free" and is provided by Ginigen. ğŸ’¡ My Experience Using It Key Features Natural Physics Engine -Realistic object movements -Sophisticated light and shadow rendering Fast Generation Speed -Average 30 seconds to 1 minute completion -No waiting - instant access ğŸ› ï¸ Available Features Text to Video -Generate 5-second videos from text descriptions -Multiple aspect ratio support (16:9, 9:16, 1:1, etc.) Image to Video -Convert static images to videos -Supports URL input or direct upload AI Prompt Enhancement -AI-based prompt optimization...</description><pubDate>Mon, 30 Jun 2025 17:21:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fantos/713394258829998</guid></item><item><title>ğŸ¨ Flux-Kontext FaceLORA - AI Portrait Style Transfer</title><link>https://huggingface.co/posts/ginipick/250323904227045</link><description>ğŸ¨ Flux-Kontext FaceLORA - AI Portrait Style Transfer ğŸŒŸ Introduction Transform your photos into masterpieces! Flux-Kontext FaceLORA is an innovative AI-powered tool that converts portrait photos into various artistic styles using cutting-edge technology. ginigen/Flux-Kontext-FaceLORA âœ¨ Key Features ğŸ“¸ Easy to Use: Upload photo â†’ Select style â†’ Click Generate! ğŸ¨ 7 Art Styles: Famous painter styles including Van Gogh, Monet, Renoir ğŸ¤– Face Preservation: AI maintains your facial features while transforming the style âš¡ Fast Generation: Get results in seconds with ZeroGPU support ğŸ¯ Custom LoRA: Use any LoRA model from HuggingFace ğŸ–¼ï¸ Available Styles ğŸ¯ Studio Ghibli - Whimsical anime art style ğŸŒŠ Winslow Homer - American realist watercolor ğŸŒ» Van Gogh - Post-impressionist with swirling brushstrokes ğŸ Paul CÃ©zanne - Geometric post-impressionist structure ğŸŒ¸ Renoir - Impressionist with soft luminous light ğŸª· Claude Monet - Impressionist light and color âš”ï¸ Fantasy Art - Epic magical character...</description><pubDate>Mon, 30 Jun 2025 17:21:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/250323904227045</guid></item><item><title>Curated list of **Next Gen Code Generation** papers &amp; benchmarks! ğŸ”¥ with 100+ â­ï¸ now!</title><link>https://huggingface.co/posts/YerbaPage/653320741659718</link><description>Curated list of **Next Gen Code Generation** papers &amp; benchmarks! ğŸ”¥ with 100+ â­ï¸ now! Stay ahead with the latest in: âœ… Repo-level Issue Resolution (SWE-bench, Agents) âœ… Repo-level Code Completion (Repo understanding) âœ… Repo-level Code QA/Translation âœ… Datasets &amp; Benchmarks ğŸ‘‰ Check it out: https://github.com/YerbaPage/Awesome-Repo-Level-Code-Generation ğŸ”¥ ğŸ’¡PRs are welcomed! See translation</description><pubDate>Mon, 30 Jun 2025 17:21:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/653320741659718</guid></item><item><title>The demo for Camel-Doc-OCR-062825 (exp) is optimized for document retrieval and direct Markdown (.md) generation from images and PDFs. Additional demos include OCRFlux-3B (document OCR), VilaSR (spatial reasoning with visual drawing), and ShotVL (cinematic language understanding). ğŸª</title><link>https://huggingface.co/posts/prithivMLmods/106480718165942</link><description>The demo for Camel-Doc-OCR-062825 (exp) is optimized for document retrieval and direct Markdown (.md) generation from images and PDFs. Additional demos include OCRFlux-3B (document OCR), VilaSR (spatial reasoning with visual drawing), and ShotVL (cinematic language understanding). ğŸª âœ¦ Space : prithivMLmods/Doc-VLMs-v2-Localization Models : â¤· camel-doc-ocr-062825 : prithivMLmods/Camel-Doc-OCR-062825 â¤· ocrflux-3b : ChatDOC/OCRFlux-3B â¤· vilasr : AntResearchNLP/ViLaSR â¤· shotvl : Vchitect/ShotVL-7B â¤· Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 The community GPU grant was given by Hugging Face â€” special thanks to them. This space supports the following tasks: (image inference, video inference) with result markdown canvas and object detection/localization. ğŸ¤—ğŸš€ . . . To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Mon, 30 Jun 2025 17:21:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/106480718165942</guid></item><item><title>A few months ago, I shared that I was building with</title><link>https://huggingface.co/posts/blaise-tk/599826348587266</link><description>A few months ago, I shared that I was building with @ deeivihh something like "the Steam for open source apps"... ğŸš€ Today, Iâ€™m excited to announce that Dione is now open source and live in public beta! Our mission is simple: make it easier to discover, use, and contribute to open source applications. ğŸ”— GitHub: https://github.com/dioneapp/dioneapp ğŸ’¬ Join the community: https://discord.gg/JDFJp33vrM Want to give it a try? Iâ€™d love your feedback! ğŸ‘€ See translation</description><pubDate>Mon, 30 Jun 2025 17:21:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/blaise-tk/599826348587266</guid></item><item><title>Model pruning for the masses!</title><link>https://huggingface.co/posts/eaddario/163350286978506</link><description>Model pruning for the masses! As of version [5740]( https://github.com/ggml-org/llama.cpp/releases/tag/b5740 ), llama-quantize now supports layer pruning via the --prune-layers flag! Findings so far are that removing one or two layers has a relatively moderate impact on quality. PPL and KLD suffer quite a lot, as expected considering that pruning changes the logits distribution, but the drop in inference quality, as reflected by tests' scores, is less pronounced. For example, using the Q4_K_M variants as a benchmark, the average drop between eaddario/gemma-3-12b-it-pruned-GGUF and eaddario/gemma-3-12b-it-GGUF is &lt; 3% (60.03 vs 61.65). Similar behaviour for eaddario/Qwen3-30B-A3B-pruned-GGUF and eaddario/Qwen3-30B-A3B-GGUF , albeit with a bit higher impact at ~5.5% (54.19 vs 57.36). These results seem to confirm Xin Men's et al ShortGPT: Layers in Large Language Models are More Redundant Than You Expect (2403.03853) Another interesting side-effect, at least with Qwen3-30B-A3B, is...</description><pubDate>Mon, 30 Jun 2025 17:21:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/eaddario/163350286978506</guid></item></channel></rss>