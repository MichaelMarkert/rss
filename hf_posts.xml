<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Let's talk about one of the hidden gems in the ReasonScape evaluation results, lucky #13:</title><link>https://huggingface.co/posts/mike-ravkine/324105560308241</link><description>Let's talk about one of the hidden gems in the ReasonScape evaluation results, lucky #13: aquif-ai/aquif-3.5-8B-Think Built on top of the solid Qwen3-8B foundation, aquif-3.5-8B-Think successfully preserves the high performance of the original model while consuming 30-50% less reasoning tokens. The most notable regression vs the base model here is in arithmetic - if your workload is math heavy this model demonstrates an unfortunate collapse with performance under growing complexity. The interesting combination of awesome overall performance on SVG simple shapes identification coupled with a total inability to recognize more complex shapes like 'House' or 'Arrow' is a behavior directly inherited from the base model (but with a ~20% improvement in token utilization). If you like your reasoning models token-efficient, Aquif-3.5-8B-Think is well worth a spin. Higher resolution, more detailed, interactive plots are available at the m12X explorer: https://reasonscape.com/m12x/explorer/...</description><pubDate>Fri, 17 Oct 2025 13:31:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mike-ravkine/324105560308241</guid></item><item><title>ğŸ‰ I am excited to share news of a project my brother, Umar Butler, and I have been working on for what feels like an eternity now.</title><link>https://huggingface.co/posts/abdurrahmanbutler/994710514786612</link><description>ğŸ‰ I am excited to share news of a project my brother, Umar Butler, and I have been working on for what feels like an eternity now. ğˆğ§ğ­ğ«ğ¨ğğ®ğœğ¢ğ§ğ  ğŒğ‹ğ„ğ â€” ğ­ğ¡ğ ğŒğšğ¬ğ¬ğ¢ğ¯ğ ğ‹ğğ ğšğ¥ ğ„ğ¦ğ›ğğğğ¢ğ§ğ  ğğğ§ğœğ¡ğ¦ğšğ«ğ¤. A suite of 10 high-quality English legal IR datasets, designed by legal experts to set a new standard for comparing embedding models. Whether youâ€™re exploring legal RAG on your home computer, or running enterprise-scale retrieval, apples-to-apples evaluation is crucial. Thatâ€™s why weâ€™ve open-sourced everything - including our 7 brand-new, hand-crafted retrieval datasets. All of these datasets are now live on Hugging Face. Any guesses which embedding model leads on legal retrieval? ğ‡ğ¢ğ§ğ­: itâ€™s not OpenAI or Google - they place 7th and 9th on our leaderboard. To do well on MLEB, embedding models must demonstrate both extensive legal domain knowledge and strong legal reasoning skills. https://huggingface.co/blog/isaacus/introducing-mleb See translation</description><pubDate>Fri, 17 Oct 2025 13:31:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/abdurrahmanbutler/994710514786612</guid></item><item><title>MLEB  is the largest, most diverse, and most comprehensive benchmark for legal text embedding models.</title><link>https://huggingface.co/posts/adlumal/955872232459431</link><description>MLEB is the largest, most diverse, and most comprehensive benchmark for legal text embedding models. https://huggingface.co/blog/isaacus/introducing-mleb See translation</description><pubDate>Fri, 17 Oct 2025 13:31:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/adlumal/955872232459431</guid></item><item><title>AutoRound keeps evolving its LLM quantization algorithm! ğŸš€</title><link>https://huggingface.co/posts/wenhuach/917073841450527</link><description>AutoRound keeps evolving its LLM quantization algorithm! ğŸš€ After enhancing W2A16 quantization, we now offer a fast algorithm to generate mixed bits/data-type schemes (~2mins for 8B models), great for MXFP4 and W2A16. Learn more: https://github.com/intel/auto-round/blob/main/docs/step_by_step.md#autoscheme See translation</description><pubDate>Fri, 17 Oct 2025 13:31:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wenhuach/917073841450527</guid></item><item><title>Did Hugging Face just ban hammer a bunch of bot accounts or am I just so uninteresting that 30% of my subs dropped me overnight?</title><link>https://huggingface.co/posts/ZennyKenny/876142925777221</link><description>Did Hugging Face just ban hammer a bunch of bot accounts or am I just so uninteresting that 30% of my subs dropped me overnight? ğŸ˜¬ Wait, don't answer that. See translation</description><pubDate>Fri, 17 Oct 2025 13:31:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/876142925777221</guid></item><item><title>@Qwen</title><link>https://huggingface.co/posts/sergiopaniego/524517454338742</link><description>@ Qwen released their new small and dense VLMs (Qwen3-VL). They're incredibly capable and one of my all-time favourite VLMs. ğŸ¤— Weâ€™ve prepared some resources to help you get started. &gt; Fine-tune Qwen3-VL-4B with SFT or GRPO (free Colab notebooks): &gt; SFT: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_qwen_vl.ipynb &gt; GRPO: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_qwen3_vl.ipynb &gt; Compare object detection vs. Moondream3: sergiopaniego/vlm_object_understanding &gt; Fine-tune from the CLI using TRL: https://github.com/kashif/Qwen3-VL/blob/trl-sft/qwen-vl-finetune/README.md#trl-based-training-single-gpu See translation</description><pubDate>Fri, 17 Oct 2025 13:31:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/524517454338742</guid></item><item><title>Now you can try all the latest state-of-the-art multimodal vision-language models from the Qwen3-VL series demo on Hugging Face Spaces â€” including 4B, 8B, and 30B (Instruct, 4B-Thinking) variants. Iâ€™ve also uploaded the weights for the Abliterated variants of these models, up to 30B parameters. Check out the Spaces and model links below! ğŸ¤—ğŸ”¥</title><link>https://huggingface.co/posts/prithivMLmods/967861422994938</link><description>Now you can try all the latest state-of-the-art multimodal vision-language models from the Qwen3-VL series demo on Hugging Face Spaces â€” including 4B, 8B, and 30B (Instruct, 4B-Thinking) variants. Iâ€™ve also uploaded the weights for the Abliterated variants of these models, up to 30B parameters. Check out the Spaces and model links below! ğŸ¤—ğŸ”¥ âœ¨ Qwen3-VL[4B,8B]: prithivMLmods/Qwen3-VL-Outpost âœ¨ Qwen3-VL-30B-A3B-Demo: prithivMLmods/Qwen3-VL-HF-Demo âœ¨ Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Qwen3-VL Abliterated Model Collection [ Version 1.0 ] âœ¨ Qwen3-VL-8B-Instruct-abliterated: prithivMLmods/Qwen3-VL-8B-Instruct-abliterated âœ¨ Qwen3-VL-4B-Instruct-abliterated: prithivMLmods/Qwen3-VL-4B-Instruct-abliterated âœ¨ Qwen3-VL-8B-Thinking-abliterated: prithivMLmods/Qwen3-VL-8B-Thinking-abliterated âœ¨ Qwen3-VL-4B-Thinking-abliterated: prithivMLmods/Qwen3-VL-4B-Thinking-abliterated âœ¨ Qwen3-VL-30B-A3B-Instruct-abliterated: prithivMLmods/Qwen3-VL-30B-A3B-Instruct-...</description><pubDate>Fri, 17 Oct 2025 13:31:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/967861422994938</guid></item><item><title>ğŸ‘‹ Hey i have Just uploaded 2 new datasets for code and scientific reasoning models:</title><link>https://huggingface.co/posts/nick007x/296214873413452</link><description>ğŸ‘‹ Hey i have Just uploaded 2 new datasets for code and scientific reasoning models: 1. ArXiv Papers (4.6TB) A massive scientific corpus with papers and metadata across all domains.Perfect for training models on academic reasoning, literature review, and scientific knowledge mining. ğŸ”—Link: nick007x/arxiv-papers 2. GitHub Code 2025 (1 TB)a comprehensive code dataset for code generation and analysis tasks. mostly contains GitHub's high quality top 1 million repos above 2 stars ğŸ”—Link: nick007x/github-code-2025 See translation</description><pubDate>Fri, 17 Oct 2025 13:31:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nick007x/296214873413452</guid></item><item><title>Building AI Agents from First Principles at GoDaddy</title><link>https://huggingface.co/posts/TravisMuhlestein/661240541677725</link><description>Building AI Agents from First Principles at GoDaddy Everyoneâ€™s talking about AI agents lately, and for good reason. But at GoDaddy, weâ€™re going deeper: starting from first principles to explore what makes an agent truly robust and usable in real-world scenarios. Instead of asking â€œWhat can we build fast?â€ weâ€™re asking â€œWhat design choices make agents flexible, testable, and reliable long term?â€ Core Concepts â€¢ Tool-centric design: everything an agent does is a tool call, with precise APIs and granularity. â€¢ Decision vs. delivery: agents decide what to do; tools handle how to do itâ€”keeping systems modular. â€¢ Structured outputs &amp; reflection: LLMs output both the tool call and the reason behind it, making debugging and iteration easier. â€¢ Universal tools: even user interactions (inform, confirm, request) are abstracted as tools, clarifying boundaries between logic and interface. Real-world use cases â†’ Not just theory âœ…Routing and responding to support messages âœ…Surfacing emerging...</description><pubDate>Fri, 17 Oct 2025 13:31:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/TravisMuhlestein/661240541677725</guid></item><item><title>Released an AWQ quantized version of BosonAIâ€™s Higgs-Llama-3-70B model! ğŸ‰</title><link>https://huggingface.co/posts/ronantakizawa/301388923540512</link><description>Released an AWQ quantized version of BosonAIâ€™s Higgs-Llama-3-70B model! ğŸ‰ The Higgs-Llama-3-70B is an LLM specialized in role-playing, useful for game characters. Using an NVIDIA B200 GPU, I was able to compress the huge 140GB model into 37GB while keeping minimal perplexity ğŸ‘ ronantakizawa/higgs-llama-3-70b-awq See translation</description><pubDate>Fri, 17 Oct 2025 13:31:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/301388923540512</guid></item></channel></rss>