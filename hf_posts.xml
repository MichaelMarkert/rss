<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>🚀 FLUX Workflow Canvas</title><link>https://huggingface.co/posts/ginipick/141662077994282</link><description>🚀 FLUX Workflow Canvas Welcome to Workflow Canvas, your ultimate AI-driven platform for crafting stunning design concepts and intricate workflow diagrams that empower your business! 🤖✨ ginigen/Workflow-Canvas Features Product Design 🛠️ Transform your ideas into reality with sleek, industrial product designs that blend modern aesthetics with advanced technology. Mindmap 🧠 Generate vibrant, educational mind maps that outline your strategies and processes in a clear, visually engaging layout. Mockup 📱 Quickly prototype intuitive app interfaces and web designs using clean, hand-drawn wireframes that capture your vision. Infographic 📊 Build polished, data-rich infographics that communicate complex corporate metrics and trends with style and clarity. Diagram 📈 Illustrate comprehensive, end-to-end business workflows—from market analysis to implementation—with detailed and organized diagrams. Flowchart 🔄 Design easy-to-follow, hand-drawn style flowcharts that map out your operational...</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/141662077994282</guid></item><item><title>🎯 Perplexity drops their FIRST open-weight model on Hugging Face: A decensored DeepSeek-R1 with full reasoning capabilities. Tested on 1000+ examples for unbiased responses.</title><link>https://huggingface.co/posts/fdaudens/121352437859372</link><description>🎯 Perplexity drops their FIRST open-weight model on Hugging Face: A decensored DeepSeek-R1 with full reasoning capabilities. Tested on 1000+ examples for unbiased responses. Check it out: perplexity-ai/r1-1776 Blog post: https://perplexity.ai/hub/blog/open-sourcing-r1-1776 See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/121352437859372</guid></item><item><title>Dino: The Minimalist Multipurpose Chat System 🌠</title><link>https://huggingface.co/posts/prithivMLmods/874083632338295</link><description>Dino: The Minimalist Multipurpose Chat System 🌠 Agent-Dino : prithivMLmods/Agent-Dino By default, it performs the following tasks: {Text-to-Text Generation}, {Image-Text-Text Generation} @image : Generates an image using Stable Diffusion xL. @3d : Generates a 3D mesh. @web : Web search agents. @rAgent : Initiates a reasoning chain using Llama mode for coding explanations. @tts1-♀ , @tts2-♂ : Voice generation (Female and Male voices). See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/874083632338295</guid></item><item><title>We crossed 1B+ tokens routed to inference providers partners on HF, that we released just a few days ago.</title><link>https://huggingface.co/posts/clem/679572962523651</link><description>We crossed 1B+ tokens routed to inference providers partners on HF, that we released just a few days ago. Just getting started of course but early users seem to like it &amp; always happy to be able to partner with cool startups in the ecosystem. Have you been using any integration and how can we make it better? https://huggingface.co/blog/inference-providers See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/679572962523651</guid></item><item><title>🚀 StepFun阶跃星辰 is making BIG open moves!</title><link>https://huggingface.co/posts/AdinaY/709023807759284</link><description>🚀 StepFun阶跃星辰 is making BIG open moves! Last year, their GOT-OCR 2.0 took the community by storm 🔥but many didn’t know they were also building some amazing models. Now, they’ve just dropped something huge on the hub! 📺 Step-Video-T2V: a 30B bilingual open video model that generates 204 frames (8-10s) at 540P resolution with high information density &amp; consistency. stepfun-ai/stepvideo-t2v 🔊 Step-Audio-TTS-3B : a TTS trained with the LLM-Chat paradigm on a large synthetic dataset, capable of generating RAP &amp; Humming stepfun-ai/step-audio-67b33accf45735bb21131b0b See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/709023807759284</guid></item><item><title>Finally here it is: a faster, custom, scalable GRPO trainer for smaller models with &lt; 500M params, can train on 8gb ram cpu,  also supports gpu for sanity sake (includes support for vllm + flash attention). Using smolLM2-135M/360M-instructs as ref &amp; base models. Experience your own “aha” moment 🐳 on 8gb ram.</title><link>https://huggingface.co/posts/Jaward/905904518817417</link><description>Finally here it is: a faster, custom, scalable GRPO trainer for smaller models with &lt; 500M params, can train on 8gb ram cpu, also supports gpu for sanity sake (includes support for vllm + flash attention). Using smolLM2-135M/360M-instructs as ref &amp; base models. Experience your own “aha” moment 🐳 on 8gb ram. Code: https://github.com/Jaykef/ai-algorithms/blob/main/smollm2_360M_135M_grpo_gsm8k.ipynb See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/905904518817417</guid></item><item><title>This dataset was collected in roughly 4 hours using the Rapidata Python API, showcasing how quickly large-scale annotations can be performed with the right tooling!</title><link>https://huggingface.co/posts/jasoncorkill/578148904408624</link><description>This dataset was collected in roughly 4 hours using the Rapidata Python API, showcasing how quickly large-scale annotations can be performed with the right tooling! All that at less than the cost of a single hour of a typical ML engineer in Zurich! The new dataset of ~22,000 human annotations evaluating AI-generated videos based on different dimensions, such as Prompt-Video Alignment, Word for Word Prompt Alignment, Style, Speed of Time flow and Quality of Physics. Rapidata/text-2-video-Rich-Human-Feedback See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/578148904408624</guid></item><item><title>AGENTS + FINETUNING! This week Hugging Face learn has a whole pathway on finetuning for agentic applications. You can follow these two courses to get knowledge on levelling up your agent game beyond prompts:</title><link>https://huggingface.co/posts/burtenshaw/189514834246661</link><description>AGENTS + FINETUNING! This week Hugging Face learn has a whole pathway on finetuning for agentic applications. You can follow these two courses to get knowledge on levelling up your agent game beyond prompts: 1️⃣ New Supervised Fine-tuning unit in the NLP Course https://huggingface.co/learn/nlp-course/en/chapter11/1 2️⃣New Finetuning for agents bonus module in the Agents Course https://huggingface.co/learn/agents-course/bonus-unit1/introduction Fine-tuning will squeeze everything out of your model for how you’re using it, more than any prompt. See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/189514834246661</guid></item><item><title>Inference-time scaling meets Flux.1-Dev (and others) 🔥</title><link>https://huggingface.co/posts/sayakpaul/418493639663017</link><description>Inference-time scaling meets Flux.1-Dev (and others) 🔥 Presenting a simple re-implementation of "Inference-time scaling diffusion models beyond denoising steps" by Ma et al. I did the simplest random search strategy, but results can potentially be improved with better-guided search methods. Supports Gemini 2 Flash &amp; Qwen2.5 as verifiers for "LLMGrading" 🤗 The steps are simple: For each round: 1&gt; Starting by sampling 2 starting noises with different seeds. 2&gt; Score the generations w.r.t a metric. 3&gt; Obtain the best generation from the current round. If you have more compute budget, go to the next search round. Scale the noise pool ( 2 ** search_round ) and repeat 1 - 3. This constitutes the random search method as done in the paper by Google DeepMind. Code, more results, and a bunch of other stuff are in the repository. Check it out here: https://github.com/sayakpaul/tt-scale-flux/ 🤗 See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sayakpaul/418493639663017</guid></item><item><title>📢 For those who start to work with LLM streaming in web, here is a minimalistic example in JS for accessing server hosted by FastAPI via REST:</title><link>https://huggingface.co/posts/nicolay-r/892116616544214</link><description>📢 For those who start to work with LLM streaming in web, here is a minimalistic example in JS for accessing server hosted by FastAPI via REST: https://gist.github.com/nicolay-r/840425749cf6d3e397da3d329e894d59 The code above is a revised verison for accessing Replicate API posted earlier https://huggingface.co/posts/nicolay-r/390307941200307 The key difference from Replicate API: - using only POST for passing a body with parameters and fetching the reader. See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nicolay-r/892116616544214</guid></item></channel></rss>