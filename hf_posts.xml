<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>new smol course</title><link>https://huggingface.co/posts/burtenshaw/724732252831042</link><description>new smol course If you’re building with or learning about post training AI models right now, we have a new FREE and CERTIFIED course. 🔗 Follow the org to join in smol-course The course builds on smol course v1 which was the fastest way to learn to train your custom AI models. It now has: - A leaderboard for students to submit models to - Certification based on exams and leaderboards - Prizes based on Leaderboards - Up to date content on TRL and SmolLM3 - Deep integration with the Hub’s compute for model training and evaluation We will release chapters every few weeks, so you can follow the org to stay updated. See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/724732252831042</guid></item><item><title>ModernBERT goes MULTILINGUAL! One of the most requested models I've seen, The Johns Hopkins University's CLSP has trained state-of-the-art massively multilingual encoders using the ModernBERT architecture: mmBERT.</title><link>https://huggingface.co/posts/tomaarsen/906557413568289</link><description>ModernBERT goes MULTILINGUAL! One of the most requested models I've seen, The Johns Hopkins University's CLSP has trained state-of-the-art massively multilingual encoders using the ModernBERT architecture: mmBERT. Model details: - 2 model sizes: - jhu-clsp/mmBERT-small - jhu-clsp/mmBERT-base - Uses the ModernBERT architecture, but with the Gemma2 multilingual tokenizer (so: flash attention, alternating global/local attention, unpadding/sequence packing, etc.) - Maximum sequence length of 8192 tokens, on the high end for encoders - Trained on 1833 languages using DCLM, FineWeb2, and many more sources - 3 training phases: 2.3T tokens pretraining on 60 languages, 600B tokens mid-training on 110 languages, and 100B tokens decay training on all 1833 languages. - Both models are MIT Licensed, and the full datasets and intermediary checkpoints are also publicly released Evaluation details: - Very competitive with ModernBERT at equivalent sizes on English (GLUE, MTEB v2 English after...</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/906557413568289</guid></item><item><title>🚀 Ever dreamed of training your own Large Language Model from scratch? What if I told you it doesn't require a supercomputer or PhD in ML? 🤯</title><link>https://huggingface.co/posts/Abhaykoul/356298508659280</link><description>🚀 Ever dreamed of training your own Large Language Model from scratch? What if I told you it doesn't require a supercomputer or PhD in ML? 🤯 Introducing LLM Trainer - the educational framework that makes LLM training accessible to EVERYONE! Whether you're on a CPU-only laptop or scaling to distributed GPUs, we've got you covered. 💻➡️🖥️ Why LLM Trainer? Because existing tools are either too simplistic (hiding the magic) or too complex (requiring expert knowledge). We bridge the gap with: 🎓 Educational transparency - every component built from scratch with clear code 💻 CPU-first approach - start training immediately, no GPU needed 🔧 Full customization - modify anything you want 📈 Seamless scaling - from laptop to cluster without code changes 🤝 HuggingFace integration - works with existing models &amp; tokenizers Key highlights: ✅ Built-in tokenizers (BPE, WordPiece, HF wrappers) ✅ Complete Transformer implementation from scratch ✅ Optimized for CPU training ✅ Advanced features: mixed...</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/356298508659280</guid></item><item><title>Build something cool with Nano Banana aka Gemini 2.5 Flash Image AIO [All-in-One]. Draw and transform on canvas, edit images, and generate images—all in one place!🍌</title><link>https://huggingface.co/posts/prithivMLmods/195813965636174</link><description>Build something cool with Nano Banana aka Gemini 2.5 Flash Image AIO [All-in-One]. Draw and transform on canvas, edit images, and generate images—all in one place!🍌 ✦︎ Constructed with the Gemini API (GCP). Try it here: https://nano-banana-aio-op72ohwdda-uw.a.run.app/ ⚠️ Note: The server’s health status is currently stable, but this may change at any time. If you experience network issues, please refresh the current app tab or trigger the discussion below. See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/195813965636174</guid></item><item><title>Introducing the Nano Banana Node Editor! 🍌</title><link>https://huggingface.co/posts/Reubencf/961832649483016</link><description>Introducing the Nano Banana Node Editor! 🍌 Now you can control and manipulate Nano Banana images with a powerful, intuitive node-based system. Explore the creative possibilities at: Reubencf/Nano_Banana_Editor This version is clearer, more inviting, and emphasizes the creative potential of your tool. See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Reubencf/961832649483016</guid></item><item><title>Hello everyone</title><link>https://huggingface.co/posts/andywu-kby/346535541829962</link><description>Hello everyone Good day! We have launched the product - Virtual Try On 🚀 Say goodbye to the uncertainty of online shopping with Miragic’s Virtual Try-On solution! Our cutting-edge AI technology lets you try on clothes virtually, offering a seamless and interactive shopping experience. Whether you're exploring new outfits or simply trying before you buy, Miragic gives you a realistic view of how items will look on you—without ever stepping into a store. Miragic-AI/Miragic-Virtual-Try-On 🌟 Key Features: - Realistic 3D Try-On: See how clothes fit and look on your virtual self in real-time. - Personalized Fit: Using advanced body-scanning tech, Miragic adjusts the fit based on your unique measurements. - Wide Fashion Selection: Browse through various brands and styles, all available for a virtual try-on. - Sustainable Shopping: Reduce the need for returns and make more eco-friendly choices with a virtual experience that helps you shop smarter. 👚 Why Virtual Try-On? - Save time and money...</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/andywu-kby/346535541829962</guid></item><item><title>Science is the vibe-killer</title><link>https://huggingface.co/posts/salma-remyx/619394412045148</link><description>Science is the vibe-killer Some critique on the state of the technology Presenting an alternative vision for scaling the scientific method in AI engineering https://remyxai.substack.com/p/vibes-dont-scale See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/619394412045148</guid></item><item><title>Hunyuan Image 2.1 by Tencent Full Tutorial and 1-Click to Install Ultra Advanced App to Use Locally :</title><link>https://huggingface.co/posts/MonsterMMORPG/330916531424120</link><description>Hunyuan Image 2.1 by Tencent Full Tutorial and 1-Click to Install Ultra Advanced App to Use Locally : https://youtu.be/dNeA5mJ36hA Tutorial video : https://youtu.be/dNeA5mJ36hA Check the below screenshots Hunyuan Image 2.1 just published by Tencent and I have been working on developing the very best app to let you use HunyuanImage-2.1 with easiest and most accurate way. In this tutorial video, I will show you how to literally 1-click to install this model and our app on Windows (locally), Massed Compute (cloud) and RunPod (cloud). The images are all raw 2560x1440 pixels with 8-steps Refiner of Hunyuan Image 2.1 model This model native resolution is 2048x2048 pixels See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/330916531424120</guid></item><item><title>Excited to share our Unified Multimodal Models new work Reconstruction Alignment (RecA)! Our BAGEL-RecA shows superior capability on image generation and editing.</title><link>https://huggingface.co/posts/sanaka87/107769277937246</link><description>Excited to share our Unified Multimodal Models new work Reconstruction Alignment (RecA)! Our BAGEL-RecA shows superior capability on image generation and editing. 📄 Paper: https://alphaxiv.org/abs/2509.07295 💻 Code: https://github.com/HorizonWind2004/reconstruction-alignment 🤗 HF Models: sanaka87/reca-68ad2176380355a3dcedc068 ✍️ DEMO: sanaka87/BAGEL-RecA 🌐 Project Page: https://reconstruction-alignment.github.io 🔥 X: https://x.com/XDWang101/status/1965908302581420204 📰 Zhihu: https://zhuanlan.zhihu.com/p/1947584568187159814 🤗 HF Daily Paper: Reconstruction Alignment Improves Unified Multimodal Models (2509.07295) ⚡ &lt;10k images &amp; 27 GPU hours (no-arch-changes) → SOTA, surpassing much larger open-source &amp; private models: 📊 GenEval: 0.73 → 0.90 | 📊 DPGBench: 80.93 → 88.15 🖼️ ImgEdit: 3.38 → 3.75 | 🖌️ GEdit: 6.94 → 7.25 ✅ RecA trains UMMs to reconstruct images from their own visual understanding encoder embeddings → big gains in image generation 🎨 &amp; editing ✂️. See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sanaka87/107769277937246</guid></item><item><title>Smol course has a distinctive approach to teaching post-training, so I'm posting about how it’s different to other post-training courses, including the llm course that’s already available.</title><link>https://huggingface.co/posts/burtenshaw/833511511767176</link><description>Smol course has a distinctive approach to teaching post-training, so I'm posting about how it’s different to other post-training courses, including the llm course that’s already available. In short, the smol course is just more direct that any of the other course, and intended for semi-pro post trainers. - It’s a minimal set of instructions on the core parts. - It’s intended to bootstrap real projects you're working on. - The material handsover to existing documentation for details - Likewise, it handsover to the LLM course for basics. - Assessment is based on a leaderboard, without reading all the material. To start the smol course, follow here: smol-course See translation</description><pubDate>Thu, 11 Sep 2025 05:21:42 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/833511511767176</guid></item></channel></rss>