<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>13 New types of LoRA</title><link>https://huggingface.co/posts/Kseniase/384482543815919</link><description>13 New types of LoRA LoRA (Low-Rank Adaptation) is a popular lightweight method for fine-tuning AI models. It doesn't update the full model, it adds small trainable components, low-rank matrices, while keeping the original weights frozen. Only these adapters are trained. Recently, many interesting new LoRA variations came out, so it‚Äôs a great time to take a look at these 13 clever approaches: 1. T-LoRA ‚Üí T-LoRA: Single Image Diffusion Model Customization Without Overfitting (2507.05964) A timestep-dependent LoRA method for adapting diffusion models with a single image. It dynamically adjusts updates and uses orthogonal initialization to reduce overlap, achieving better fidelity‚Äìalignment balance than standard LoRA 2. SingLoRA ‚Üí SingLoRA: Low Rank Adaptation Using a Single Matrix (2507.05566) Simplifies LoRA by using only one small matrix instead of usual two, and multiplying it by its own transpose (like A √ó A·µÄ). It uses half the parameters of LoRA and avoids scale mismatch between...</description><pubDate>Tue, 15 Jul 2025 05:28:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/384482543815919</guid></item><item><title>üôãüèª‚Äç‚ôÇÔ∏è Normalize adding compute &amp; runtime traces to your model cards</title><link>https://huggingface.co/posts/Tonic/414083244384754</link><description>üôãüèª‚Äç‚ôÇÔ∏è Normalize adding compute &amp; runtime traces to your model cards See translation</description><pubDate>Tue, 15 Jul 2025 05:28:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Tonic/414083244384754</guid></item><item><title>Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. ü§óüìÑ</title><link>https://huggingface.co/posts/prithivMLmods/700925755780035</link><description>Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. ü§óüìÑ Spaces &amp; Models : &gt; Doc-VLMs-OCR : prithivMLmods/Doc-VLMs-OCR &gt; core-OCR : prithivMLmods/core-OCR &gt; Megalodon-OCR (3B) : prithivMLmods/Megalodon-OCR-Sync-0713 &gt; Perseus-Doc-vl (7B): prithivMLmods/Perseus-Doc-vl-0712 Datasets Caption Mix : &gt; Corvus-OCR-Caption-Mix : prithivMLmods/Corvus-OCR-Caption-Mix &gt; Corvus-OCR-Caption-Mini-Mix : prithivMLmods/Corvus-OCR-Caption-Mini-Mix Collections : &gt; Corvus OCR Caption Mix: prithivMLmods/corvus-ocr-caption-mix-687349bfaceffbd10976f0cc &gt; Captioning / OCR / DocTable : prithivMLmods/captioning-ocr-doctable-687382e1da822008bb5c06f2 GitHub : &gt; OCR-ReportLab :...</description><pubDate>Tue, 15 Jul 2025 05:28:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/700925755780035</guid></item><item><title>in case you didn‚Äôt know, Claude now has a developer training course with certificates,</title><link>https://huggingface.co/posts/hesamation/850768471232119</link><description>in case you didn‚Äôt know, Claude now has a developer training course with certificates, this is better than anything you can find on Coursera. covers Claude Code, MCP and its advanced topics and even more: https://www.anthropic.com/learn/build-with-claude See translation</description><pubDate>Tue, 15 Jul 2025 05:28:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/850768471232119</guid></item><item><title>Kimi-K2 is now available on the hubüî•üöÄ</title><link>https://huggingface.co/posts/AdinaY/423045666935241</link><description>Kimi-K2 is now available on the hubüî•üöÄ This is a trillion-parameter MoE model focused on long context, code, reasoning, and agentic behavior. moonshotai/kimi-k2-6871243b990f2af5ba60617d ‚ú® Base &amp; Instruct ‚ú® 1T total / 32B active - Modified MIT License ‚ú® 128K context length ‚ú® Muon optimizer for stable trillion-scale training See translation</description><pubDate>Tue, 15 Jul 2025 05:28:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/423045666935241</guid></item><item><title>üì¢ Generate your own data in simulation using two new free and customizable data-generating Scenarios on Duality's FalconCloud service.</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/377698831818015</link><description>üì¢ Generate your own data in simulation using two new free and customizable data-generating Scenarios on Duality's FalconCloud service. üôå These multi-class Scenarios are designed to target model weaknesses for our recent Kaggle competition, but they are free to anyone for non-commercial use! üì∏ Control object and camera posing üëâ Select random variable ranges üñºÔ∏è Set post-processing effects ‚ûï and more to create a robust dataset for strong model training. Access the 2 Scenarios here: üí† https://falcon.duality.ai/secure/scenarios/edit/9e90e036-8af9-41e4-8af0-1343b8e8f467?utm_source=Kaggle&amp;utm_medium=post&amp;utm_campaign=competition_4 üí† https://falcon.duality.ai/secure/scenarios/edit/e3294c19-49d4-4f64-9ca8-8373876c2c94?utm_source=Kaggle&amp;utm_medium=post&amp;utm_campaign=competition_4 See translation</description><pubDate>Tue, 15 Jul 2025 05:28:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/377698831818015</guid></item><item><title>In our latest paper, Bourbaki (7b), we show how one can achieve state-of-the-art 7B theorem provers on PutnamBench by applying MCTS to what we call self-generated and goal-conditioned MDPs. I started a series of Blogs on this!</title><link>https://huggingface.co/posts/hba123/150628355666585</link><description>In our latest paper, Bourbaki (7b), we show how one can achieve state-of-the-art 7B theorem provers on PutnamBench by applying MCTS to what we call self-generated and goal-conditioned MDPs. I started a series of Blogs on this! Why a series of Blogs üòù? I want to try to make everyone understand what Bourbaki (7b) is and what it does. I don't want to just give you a ChatGPT summary with some result hype. I think there are many things to improve, and I am hoping with more exposure to this, beyond experiments and codes, some people would be interested and help us improve it! In this first blog, we will be talking basics: 1) MCTS and why it should be applied to LLMs so that the whole world is not just fine-tuning a 100000000000000000000000 b model on 10 data points (not that i have not done it before ü§™ü§™), 2) the basics of MDPs, and 3) the Vanilla MCTS algorithm. Check it out: https://huggingface.co/blog/hba123/bourbaki7b If you find it useful, consider upvoting and sharing this post and...</description><pubDate>Tue, 15 Jul 2025 05:28:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/150628355666585</guid></item><item><title>üéØ Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science!</title><link>https://huggingface.co/posts/DawnC/760622875415705</link><description>üéØ Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science! This isn't just another computer vision project. VisionScout represents a fundamental shift from simple object detection to genuine scene understanding, where four specialized AI models work together to interpret what's actually happening in an image. üèóÔ∏è Part 1: Architecture Foundation How careful system design transforms independent models into collaborative intelligence through proper layering and coordination strategies. ‚öôÔ∏è Part 2: Deep Technical Implementation The five core algorithms powering the system: dynamic weight adjustment, attention mechanisms, statistical methods, lighting analysis, and CLIP's zero-shot learning. üåç Part 3: Real-World Validation Concrete case studies from indoor spaces to cultural landmarks, demonstrating how integrated systems deliver insights no single model could achieve. What makes this valuable:...</description><pubDate>Tue, 15 Jul 2025 05:28:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/760622875415705</guid></item><item><title>MultiTalk Levelled Up - Way Better Animation Compared to Before with New Workflows - Image to Video &gt;</title><link>https://huggingface.co/posts/MonsterMMORPG/139363304280237</link><description>MultiTalk Levelled Up - Way Better Animation Compared to Before with New Workflows - Image to Video &gt; https://youtu.be/wgCtUeog41g MultiTalk is greatly upgraded. After doing more than 1 day more research with MultiTalk by using 8x A6000 48 GB GPUs, I have significantly improved the MultiTalk workflows and now I am sharing 4 different category workflows with you. VRAM usages and speeds are same but just better quality and animation. Moreover I am introducing a new app which is image and video comparison sliders. Ultra fast and lightweight. Runs as a html app and no GPU is required. https://youtu.be/wgCtUeog41g MultiTalk Full Tutorial With 1-Click Installer - Make Talking and Singing Videos From Static Images &gt; https://youtu.be/8cMIwS9qo4M By using MeiGen MultiTalk you can generate amazing fully animated real-like videos from given audio input. Not only talking but also animating the body movements is possible. In this video I will show you how to install ComfyUI on Windows and...</description><pubDate>Tue, 15 Jul 2025 05:28:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/139363304280237</guid></item><item><title>Some new releases:</title><link>https://huggingface.co/posts/sequelbox/729247272054764</link><description>Some new releases: - brought the new Shining Valiant 3 series (science-reasoning, AI-reasoning, general chat) to Qwen 3 4B: ValiantLabs/Qwen3-4B-ShiningValiant3 - merged models for Shining Valiant 3 and Esper 3, combining their technical expertise and reasoning skills: 4b: sequelbox/Qwen3-4B-PlumEsper 8b: sequelbox/Qwen3-8B-PlumEsper coming up we'll have some experimental reasoning releases - datasets and models will be out soon! also will be bringing SV3 and Esper 3 to more models. lets keep working for open source :) love, allegra See translation</description><pubDate>Tue, 15 Jul 2025 05:28:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sequelbox/729247272054764</guid></item></channel></rss>