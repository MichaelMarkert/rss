<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>üî• AgenticAI: The Ultimate Multimodal AI with 16 MBTI Girlfriend Personas! üî•</title><link>https://huggingface.co/posts/seawolf2357/796388354612946</link><description>üî• AgenticAI: The Ultimate Multimodal AI with 16 MBTI Girlfriend Personas! üî• Hello AI community! Today, our team is thrilled to introduce AgenticAI, an innovative open-source AI assistant that combines deep technical capabilities with uniquely personalized interaction. üíò üõ†Ô∏è MBTI 16 Types SPACES Collections link seawolf2357/heartsync-mbti-67f793d752ef1fa542e16560 ‚ú® 16 MBTI Girlfriend Personas Complete MBTI Implementation: All 16 MBTI female personas modeled after iconic characters (Dana Scully, Lara Croft, etc.) Persona Depth: Customize age groups and thinking patterns for hyper-personalized AI interactions Personality Consistency: Each MBTI type demonstrates consistent problem-solving approaches, conversation patterns, and emotional expressions üöÄ Cutting-Edge Multimodal Capabilities Integrated File Analysis: Deep analysis and cross-referencing of images, videos, CSV, PDF, and TXT files Advanced Image Understanding: Interprets complex diagrams, mathematical equations, charts, and...</description><pubDate>Fri, 11 Apr 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/seawolf2357/796388354612946</guid></item><item><title>Google published a 69-page whitepaper on Prompt Engineering and its best practices, a must-read if you are using LLMs in production:</title><link>https://huggingface.co/posts/hesamation/789492772324435</link><description>Google published a 69-page whitepaper on Prompt Engineering and its best practices, a must-read if you are using LLMs in production: &gt; zero-shot, one-shot, few-shot &gt; system prompting &gt; chain-of-thought (CoT) &gt; ReAct LINK: https://www.kaggle.com/whitepaper-prompt-engineering &gt; code prompting &gt; best practices See translation</description><pubDate>Fri, 11 Apr 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/789492772324435</guid></item><item><title>Qwen 3 can launch very soon. üëÄ</title><link>https://huggingface.co/posts/merterbak/235850739835485</link><description>Qwen 3 can launch very soon. üëÄ https://github.com/ggml-org/llama.cpp/pull/12828 See translation</description><pubDate>Fri, 11 Apr 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/235850739835485</guid></item><item><title>üé® Designers, meet OmniSVG! This new model helps you create professional vector graphics from text/images, generate editable SVGs from icons to detailed characters, convert rasters to vectors, maintain style consistency with references, and integrate into your workflow.</title><link>https://huggingface.co/posts/fdaudens/513864434208106</link><description>üé® Designers, meet OmniSVG! This new model helps you create professional vector graphics from text/images, generate editable SVGs from icons to detailed characters, convert rasters to vectors, maintain style consistency with references, and integrate into your workflow. @ OmniSVG See translation</description><pubDate>Fri, 11 Apr 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/513864434208106</guid></item><item><title>Hi All,  I recently released two Audio datasets  which are generated using my earlier released dataset:</title><link>https://huggingface.co/posts/ajibawa-2023/282296415348325</link><description>Hi All, I recently released two Audio datasets which are generated using my earlier released dataset: ajibawa-2023/Children-Stories-Collection First Audio Dataset:https://huggingface.co/datasets/ajibawa-2023/Audio-Children-Stories-Collection-Large has 5600++ stories in .mp3 format. Second Audio Dataset:https://huggingface.co/datasets/ajibawa-2023/Audio-Children-Stories-Collection has 600 stories in .mp3 format. See translation</description><pubDate>Fri, 11 Apr 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ajibawa-2023/282296415348325</guid></item><item><title>I got rejected from llama4.</title><link>https://huggingface.co/posts/Steven10429/887336506731659</link><description>I got rejected from llama4. So that means I can use quantinized model without following their TOS. Interesting. See translation</description><pubDate>Fri, 11 Apr 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Steven10429/887336506731659</guid></item><item><title>üî• Yesterday was a fire day!</title><link>https://huggingface.co/posts/jasoncorkill/726469711226418</link><description>üî• Yesterday was a fire day! We dropped two brand-new datasets capturing Human Preferences for text-to-video and text-to-image generations powered by our own crowdsourcing tool! Whether you're working on model evaluation, alignment, or fine-tuning, this is for you. 1. Text-to-Video Dataset (Pika 2.2 model): Rapidata/text-2-video-human-preferences-pika2.2 2. Text-to-Image Dataset (Reve-AI Halfmoon): Rapidata/Reve-AI-Halfmoon_t2i_human_preference Let‚Äôs train AI on AI-generated content with humans in the loop. Let‚Äôs make generative models that actually get us. See translation</description><pubDate>Fri, 11 Apr 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/726469711226418</guid></item><item><title>You can now run Llama 4 on your own local device! ü¶ô</title><link>https://huggingface.co/posts/danielhanchen/859959880164586</link><description>You can now run Llama 4 on your own local device! ü¶ô Run our Dynamic 1.78-bit and 2.71-bit Llama 4 GGUFs: unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF You can run them on llama.cpp and other inference engines. See our guide here: https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4 See translation</description><pubDate>Fri, 11 Apr 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/859959880164586</guid></item><item><title>We desperately need GPU for model inference. CPU can't replace GPU.</title><link>https://huggingface.co/posts/onekq/215048320445842</link><description>We desperately need GPU for model inference. CPU can't replace GPU. I will start with the basics. GPU is designed to serve predictable workloads with many parallel units (pixels, tensors, tokens). So a GPU allocates as much transistor budget as possible to build thousands of compute units (Cuda cores in NVidia or execution units in Apple Silicon), each capable of running a thread. But CPU is designed to handle all kinds of workloads. CPU cores are much larger (hence a lot fewer) with branch prediction and other complex things. In addition, more and more transistors are allocated to build larger cache (~50% now) to house the unpredictable, devouring the compute budget. Generalists can't beat specialists. See translation</description><pubDate>Fri, 11 Apr 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/215048320445842</guid></item><item><title>Moonshot AI Êúà‰πãÊöóÈù¢ üåõ @Kimi_Moonshotis just dropped an MoE VLM  and an MoE Reasoning VLM on the hub!!</title><link>https://huggingface.co/posts/AdinaY/423063846745216</link><description>Moonshot AI Êúà‰πãÊöóÈù¢ üåõ @Kimi_Moonshotis just dropped an MoE VLM and an MoE Reasoning VLM on the hub!! Model:https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85 ‚ú®3B with MIT license ‚ú®Long context windows up to 128K ‚ú®Strong multimodal reasoning (36.8% on MathVision, on par with 10x larger models) and agent skills (34.5% on ScreenSpot-Pro) See translation</description><pubDate>Fri, 11 Apr 2025 05:23:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/423063846745216</guid></item></channel></rss>