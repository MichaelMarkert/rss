<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸš€ GPT-OSS 120B &amp; 20B - Use Both Models in One Space!</title><link>https://huggingface.co/posts/openfree/275314685023370</link><description>ğŸš€ GPT-OSS 120B &amp; 20B - Use Both Models in One Space! openfree/OpenAI-gpt-oss ğŸ¯ Two Models, One Space! GPT-OSS hit #1 on HF just 2 hours after release! ğŸ† Now you can use both models conveniently in a single space. ğŸ“‹ Model Selection Made Easy! Just pick from the dropdown âœ… â”œâ”€â”€ GPT-OSS-120B (Complex tasks) â””â”€â”€ GPT-OSS-20B (Quick chats) ğŸ’« How to Use (Takes 30 seconds!) Sign in â†’ With your HF account ğŸ” Select model â†’ Choose what you need ğŸ“Œ Apply â†’ Click! âš¡ Start chatting â†’ That's it! ğŸ’¬ ğŸŒˆ Perfect For: 120B â†’ Deep analysis, professional work ğŸ§  20B â†’ Fast responses, casual conversations âš¡ No installation needed - just use it in your browser! ğŸŒ âœ¨ Special Features ğŸ¨ Beautiful gradient UI ğŸŒ™ Dark mode support ğŸ”„ Real-time model switching ğŸ†“ Completely free! ğŸ‘‰ Try it now! It's really that simple! #GPT-OSS #HuggingFace #FreeAI #EasyToUse See translation</description><pubDate>Wed, 06 Aug 2025 09:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/275314685023370</guid></item><item><title>Run OpenAI's new gpt-oss models locally with Unsloth GGUFs! ğŸ”¥ğŸ¦¥</title><link>https://huggingface.co/posts/danielhanchen/446160279272944</link><description>Run OpenAI's new gpt-oss models locally with Unsloth GGUFs! ğŸ”¥ğŸ¦¥ 20b GGUF: unsloth/gpt-oss-20b-GGUF 120b GGUF: unsloth/gpt-oss-120b-GGUF Model will run on 14GB RAM for 20b and 66GB for 120b. See translation</description><pubDate>Wed, 06 Aug 2025 09:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/446160279272944</guid></item><item><title>I run Claude Code with Qwen3 Coder Flash locally on my MacBook Air. It works offline, zero cloud, zero internet, zero EU AI Act anxiety. No limit with all tokens on the house.</title><link>https://huggingface.co/posts/mitkox/378542221866585</link><description>I run Claude Code with Qwen3 Coder Flash locally on my MacBook Air. It works offline, zero cloud, zero internet, zero EU AI Act anxiety. No limit with all tokens on the house. Itâ€™s not great, not terrible- adequate performance for an on device AI agent chewing through code on a 1.24 kg laptop. I wrote an interpreter to broker peace between Claude Code and my local AI runtime. Make sure you own your AI. AI in the cloud is not aligned with you; itâ€™s aligned with the company that owns it. See translation</description><pubDate>Wed, 06 Aug 2025 09:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/378542221866585</guid></item><item><title>Qwen Image â€“ The Latest Image Generation ModelğŸ”¥</title><link>https://huggingface.co/posts/prithivMLmods/372876915549424</link><description>Qwen Image â€“ The Latest Image Generation ModelğŸ”¥ Below are some samples generated using the Qwen Image Diffusion Model. Qwen-Image, a 20B MMDiT model for next-generation text-to-image generation, preserves typographic details, layout coherence, and contextual harmony with stunning accuracy. It is especially strong at creating stunning graphic posters with native text. The model is now open-source. [ ğš€ğš ğšğš—-ğ™¸ğš–ğšŠğšğš : Qwen/Qwen-Image ] â¤· Try the Qwen Image demo here: prithivMLmods/Qwen-Image-Diffusion , Qwen/Qwen-Image &amp; more ... â¤· Qwen-Image Technical Report : Qwen-Image Technical Report (2508.02324) â¤· Qwen Image [GitHub] : https://github.com/QwenLM/Qwen-Image Even more impressively, it demonstrates a strong ability to understand images. The model supports a wide range of vision-related tasks such as object detection, semantic segmentation, depth and edge (Canny) estimation, novel view synthesis, and image super-resolution. While each task is technically distinct, they can all be viewed...</description><pubDate>Wed, 06 Aug 2025 09:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/372876915549424</guid></item><item><title>Finaly OpenAI is open to share open-source models after GPT2-2019.</title><link>https://huggingface.co/posts/ImranzamanML/667361724381561</link><description>Finaly OpenAI is open to share open-source models after GPT2-2019. gpt-oss-120b gpt-oss-20b openai/gpt-oss-120b #AI #GPT #LLM #Openai See translation</description><pubDate>Wed, 06 Aug 2025 09:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ImranzamanML/667361724381561</guid></item><item><title>Trainable selective sampling and sparse attention kernels are indispensable in the era of context engineering. We hope our work will be helpful to everyone! ğŸ¤—</title><link>https://huggingface.co/posts/JingzeShi/527939367728783</link><description>Trainable selective sampling and sparse attention kernels are indispensable in the era of context engineering. We hope our work will be helpful to everyone! ğŸ¤— Trainable Dynamic Mask Sparse Attention (2508.02124) See translation</description><pubDate>Wed, 06 Aug 2025 09:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/JingzeShi/527939367728783</guid></item><item><title>Just included example scripts for aligning models using GSPO (including VLM example) ğŸ™†â€â™‚ï¸ğŸ™†â€â™‚ï¸</title><link>https://huggingface.co/posts/sergiopaniego/720514750677796</link><description>Just included example scripts for aligning models using GSPO (including VLM example) ğŸ™†â€â™‚ï¸ğŸ™†â€â™‚ï¸ GSPO is the latest RL alignment algo by @Alibaba_Qwen and it's already supported in the latest TRL v0.20 release. Super-easy-to-get-started example scripts below, GO run them!ğŸ‘©â€ğŸ’»ğŸ‘©â€ğŸ’» ğŸ§‘â€ğŸ¨ Script: https://github.com/huggingface/trl/blob/main/examples/scripts/gspo.py ğŸ¦„ VLM script: https://github.com/huggingface/trl/blob/main/examples/scripts/gspo_vlm.py ğŸ§© More TRL examples: https://huggingface.co/docs/trl/main/en/example_overview ğŸ§™â€â™‚ï¸ GSPO paper: Group Sequence Policy Optimization (2507.18071) See translation</description><pubDate>Wed, 06 Aug 2025 09:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/720514750677796</guid></item><item><title>I'm building a mmo-ish RPG with LLM agents that can (hopefully) complete player tasks, as an experiment. I've started documenting my progress here:</title><link>https://huggingface.co/posts/neph1/202844482773668</link><description>I'm building a mmo-ish RPG with LLM agents that can (hopefully) complete player tasks, as an experiment. I've started documenting my progress here: https://huggingface.co/blog/neph1/rpg-llm-agents Let me know if you want to see more of it. See translation</description><pubDate>Wed, 06 Aug 2025 09:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/neph1/202844482773668</guid></item><item><title>Want to learn how to align a Vision Language Model (VLM) for reasoning using GRPO and TRL? ğŸŒ‹</title><link>https://huggingface.co/posts/sergiopaniego/207447705634334</link><description>Want to learn how to align a Vision Language Model (VLM) for reasoning using GRPO and TRL? ğŸŒ‹ ğŸ§‘â€ğŸ³ We've got you covered!! NEW multimodal post training recipe to align a VLM using TRL in @ HuggingFace 's Cookbook. Go to the recipe ğŸ‘‰https://huggingface.co/learn/cookbook/fine_tuning_vlm_grpo_trl Powered by the latest TRL v0.20 release, this recipe shows how to teach Qwen2.5-VL-3B-Instruct to reason over images ğŸŒ‹ See translation</description><pubDate>Wed, 06 Aug 2025 09:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/207447705634334</guid></item><item><title>XBai o4 claims to beat Claude Opus 4 and o3-mini, and they provide verifiable proof. My skepticism circuits overloaded, but my local AI FOMO module screamed louder.</title><link>https://huggingface.co/posts/mitkox/604222926195865</link><description>XBai o4 claims to beat Claude Opus 4 and o3-mini, and they provide verifiable proof. My skepticism circuits overloaded, but my local AI FOMO module screamed louder. I've thrown this 33B monoblock LLM onto a single GPU and used Roo Code for someâ€¦ letâ€™s call it â€œvibe testingâ€. Itâ€™s terrifyingly competent. As an architect, itâ€™s the best open-weight model Iâ€™ve touched this side of 2025. See translation</description><pubDate>Wed, 06 Aug 2025 09:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/604222926195865</guid></item></channel></rss>