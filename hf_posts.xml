<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>A few months ago, I shared that I was building with</title><link>https://huggingface.co/posts/blaise-tk/599826348587266</link><description>A few months ago, I shared that I was building with @ deeivihh something like "the Steam for open source apps"... ğŸš€ Today, Iâ€™m excited to announce that Dione is now open source and live in public beta! Our mission is simple: make it easier to discover, use, and contribute to open source applications. ğŸ”— GitHub: https://github.com/dioneapp/dioneapp ğŸ’¬ Join the community: https://discord.gg/JDFJp33vrM Want to give it a try? Iâ€™d love your feedback! ğŸ‘€ See translation</description><pubDate>Tue, 01 Jul 2025 17:21:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/blaise-tk/599826348587266</guid></item><item><title>10 Open-source Deep Research assistants</title><link>https://huggingface.co/posts/Kseniase/947704683052150</link><description>10 Open-source Deep Research assistants Deep Research agents are quickly becoming our daily co-workers â€” built for complex investigations, not just chat. With modular architecture, advanced tool use and real web access, they go far beyond typical AI. While big-name agents get the spotlight, we want to highlight some powerful recent open-source alternatives: 1. DeerFlow -&gt; https://github.com/bytedance/deer-flow A modular multi-agent system combining LMs and tools for automated research and code analysis. It links a coordinator, planner, team of specialized agent, and reporter, and converts reports to speech via Text-to-Speech (TTS) 2. Alita -&gt; https://github.com/CharlesQ9/Alita Uses a single problem-solving module for scalable reasoning through simplicity. It self-evolves by generating and reusing Model Context Protocols (MCPs) from open-source tools to build external capabilities for diverse tasks 3. WebThinker -&gt; https://github.com/RUC-NLPIR/WebThinker Lets reasoning models...</description><pubDate>Tue, 01 Jul 2025 17:21:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/947704683052150</guid></item><item><title>ğŸ§  SOMA: The Core Architecture for AGI Level 1 ğŸš€</title><link>https://huggingface.co/posts/openfree/674788441765421</link><description>ğŸ§  SOMA: The Core Architecture for AGI Level 1 ğŸš€ VIDraft/SOMA-AGI ğŸ¯ The First Step Toward AGI SOMA (Self-Orchestrating Modular Architect) is a revolutionary architecture that fulfills the essential requirements for AGI (Artificial General Intelligence) Level 1. It perfectly implements the common AGI prerequisites emphasized by Yann LeCun (Meta), OpenAI, and Google DeepMind within a single LLM. ğŸ“‹ AGI Level 1 Core Requirements = SOMA's Perfect Implementation âœ… ğŸ¯ Planning Capability â†’ Supervisor AI autonomously designs and executes comprehensive analysis roadmaps ğŸ§© Role Differentiation &amp; Modularity â†’ A single LLM instantly differentiates into 5 expert AIs for collaboration ğŸ”„ Self-reflection &amp; Feedback Loops â†’ Evaluator AI continuously validates and directs improvements ğŸ› ï¸ Tool-use &amp; Autonomy â†’ Full automation from web search to report generation ğŸ® Long-term Agency Structure â†’ Completes complex 11-stage collaborative processes end-to-end ğŸ”· SOMA's Three Core Structures ğŸ§­ Self-...</description><pubDate>Tue, 01 Jul 2025 17:21:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/674788441765421</guid></item><item><title>ğŸ‰ Dhanishtha 2.0 Preview is Now Open Source!</title><link>https://huggingface.co/posts/Abhaykoul/404767027882987</link><description>ğŸ‰ Dhanishtha 2.0 Preview is Now Open Source! The world's first Intermediate Thinking Model is now available to everyone! Dhanishtha 2.0 Preview brings revolutionary intermediate thinking capabilities to the open-source community. Unlike traditional reasoning models that think once, Dhanishtha can think, answer, rethink, answer again, and continue rethinking as needed using multiple blocks between responses. ğŸš€ Key Features - Intermediate thinking: Think â†’ Answer â†’ Rethink â†’ Answer â†’ Rethink if needed... - Token efficient: Uses up to 79% fewer tokens than DeepSeek R1 on similar queries - Transparent thinking: See the model's reasoning process in real-time - Open source: Freely available for research and development HelpingAI/Dhanishtha-2.0-preview https://helpingai.co/chat See translation</description><pubDate>Tue, 01 Jul 2025 17:21:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/404767027882987</guid></item><item><title>so many multimodal releases these days ğŸ¤ </title><link>https://huggingface.co/posts/merve/587280854326828</link><description>so many multimodal releases these days ğŸ¤  &gt; ERNIE-4.5-VL: new vision language MoE models by Baidu https://huggingface.co/models?search=ernie-4.5-vl &gt; new visual document retrievers by NVIDIA (sota on ViDoRe!) nvidia/llama-nemoretriever-colembed-3b-v1 nvidia/llama-nemoretriever-colembed-1b-v1 &gt; Ovis-3b: new image-text in image-text out models by Alibaba â¤µï¸ https://huggingface.co/spaces/AIDC-AI/Ovis-U1- See translation</description><pubDate>Tue, 01 Jul 2025 17:21:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/587280854326828</guid></item><item><title>I find it really annoying that you can use a Zero GPU model that will throw an error but itâ€™ll still take the processing time from your quota. So youre just left with less time and nothing to show for it.</title><link>https://huggingface.co/posts/wewittc/347088512345422</link><description>I find it really annoying that you can use a Zero GPU model that will throw an error but itâ€™ll still take the processing time from your quota. So youre just left with less time and nothing to show for it. See translation</description><pubDate>Tue, 01 Jul 2025 17:21:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wewittc/347088512345422</guid></item><item><title>Inference for generative ai models looks like a mine field, but thereâ€™s a simple protocol for picking the best inference:</title><link>https://huggingface.co/posts/burtenshaw/697123415535373</link><description>Inference for generative ai models looks like a mine field, but thereâ€™s a simple protocol for picking the best inference: ğŸŒ 95% of users &gt;&gt; If youâ€™re using open (large) models and need fast online inference, then use Inference providers on auto mode, and let it choose the best provider for the model. https://huggingface.co/docs/inference-providers/index ğŸ‘· fine-tuners/ bespoke &gt;&gt; If youâ€™ve got custom setups, use Inference Endpoints to define a configuration from AWS, Azure, GCP. https://endpoints.huggingface.co/ ğŸ¦« Locals &gt;&gt; If youâ€™re trying to stretch everything you can out of a server or local machine, use Llama.cpp, Jan, LMStudio or vLLM. https://huggingface.co/settings/local-apps#local-apps ğŸªŸ Browsers &gt;&gt; If you need open models running right here in the browser, use transformers.js. https://github.com/huggingface/transformers.js Let me know what youâ€™re using, and if you think itâ€™s more complex than this. See translation</description><pubDate>Tue, 01 Jul 2025 17:21:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/697123415535373</guid></item><item><title>The full Celestia 3 science-reasoning dataset is here!</title><link>https://huggingface.co/posts/sequelbox/523631078445392</link><description>The full Celestia 3 science-reasoning dataset is here! - 91k high-quality synthetic science prompts answered by DeepSeek-R1-0528 - subjects include physics, biology, chemistry, computer science, Earth science, astronomy, and information theory - one of the reasoning datasets powering the upcoming Shining Valiant 3 :) coming soon! GET IT NOW, FOR EVERYONE: sequelbox/Celestia3-DeepSeek-R1-0528 SUPPORT OUR RELEASES: sequelbox/SupportOpenSource with love, allegra See translation</description><pubDate>Tue, 01 Jul 2025 17:21:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sequelbox/523631078445392</guid></item><item><title>Check out new symbolic music AI front end and CLI training app</title><link>https://huggingface.co/posts/asigalov61/301808424415801</link><description>Check out new symbolic music AI front end and CLI training app https://webchatappai.github.io/midi-gen/ https://github.com/WebChatAppAi/Orpheus-Midi-Model-Maker @ Timzoid @ Csplk @ not-lain @ victor @ bartowski @ John6666 See translation</description><pubDate>Tue, 01 Jul 2025 17:21:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/asigalov61/301808424415801</guid></item><item><title>ğŸ¬ How to Use Seedance, the #1 Video Generation Model, for Free</title><link>https://huggingface.co/posts/fantos/713394258829998</link><description>ğŸ¬ How to Use Seedance, the #1 Video Generation Model, for Free ğŸ“Œ A Hidden Gem I Stumbled Upon While browsing Hugging Face, I discovered an amazing project. I found ByteDance's Seedance video generation service - which knocked Google's VEO3 down to 2nd place on the video generation leaderboard - available for free on Hugging Face! ginigen/Seedance-Free Leaderboard standings: ğŸ¥‡ 1st: ByteDance Seedance ğŸ¥ˆ 2nd: Google VEO3 It's called "Bytedance Seedance Video Free" and is provided by Ginigen. ğŸ’¡ My Experience Using It Key Features Natural Physics Engine -Realistic object movements -Sophisticated light and shadow rendering Fast Generation Speed -Average 30 seconds to 1 minute completion -No waiting - instant access ğŸ› ï¸ Available Features Text to Video -Generate 5-second videos from text descriptions -Multiple aspect ratio support (16:9, 9:16, 1:1, etc.) Image to Video -Convert static images to videos -Supports URL input or direct upload AI Prompt Enhancement -AI-based prompt optimization...</description><pubDate>Tue, 01 Jul 2025 17:21:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fantos/713394258829998</guid></item></channel></rss>