<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Qwen3-Next can now be Run locally! (30GB RAM)</title><link>https://huggingface.co/posts/danielhanchen/212249714773740</link><description>Qwen3-Next can now be Run locally! (30GB RAM) Instruct GGUF: unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF The models come in Thinking and Instruct versions and utilize a new architecture, allowing it to have ~10x faster inference than Qwen32B. ğŸ’œ Step-by-step Guide: https://docs.unsloth.ai/models/qwen3-next Thinking GGUF: unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF See translation</description><pubDate>Sun, 30 Nov 2025 09:23:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/212249714773740</guid></item><item><title>Introducing the  Super-OCRs Demo, a comparison of state-of-the-art multimodal OCR VLMs, including HunyuanOCR, DeepSeekOCR, Dots, and Nanonets in one space for performing OCR, rendering LaTeX and Markdown, and visual grounding (layout). Find the related Spaces and models below.ğŸ¤—ğŸ”¥</title><link>https://huggingface.co/posts/prithivMLmods/107899220924462</link><description>Introducing the Super-OCRs Demo, a comparison of state-of-the-art multimodal OCR VLMs, including HunyuanOCR, DeepSeekOCR, Dots, and Nanonets in one space for performing OCR, rendering LaTeX and Markdown, and visual grounding (layout). Find the related Spaces and models below.ğŸ¤—ğŸ”¥ âœ¨Super-OCRs[Demo]: prithivMLmods/Super-OCRs-Demo âœ¨Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations âœ¨GitHub: https://github.com/PRITHIVSAKTHIUR/Super-OCRs-Demo â­ Models Used: âœ¦ HunyuanOCR: tencent/HunyuanOCR âœ¦ DeepSeek-OCR: (-) deepseek-ai/DeepSeek-OCR (+) prithivMLmods/DeepSeek-OCR-Latest-BF16.I64 âœ¦ Dots.OCR: (-) rednote-hilab/dots.ocr (+) prithivMLmods/Dots.OCR-Latest-BF16 âœ¦ Nanonets-OCR2-3B: nanonets/Nanonets-OCR2-3B â­ Some Other Relevant Apps: âœ¦ Qwen3-VL-HF-Demo: prithivMLmods/Qwen3-VL-HF-Demo âœ¦ Qwen3-VL-Outpost: prithivMLmods/Qwen3-VL-Outpost âœ¦ Multimodal-OCR: prithivMLmods/Multimodal-OCR âœ¦ Multimodal-OCR2: prithivMLmods/Multimodal-OCR2 âœ¦ Multimodal-OCR3:...</description><pubDate>Sun, 30 Nov 2025 09:23:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/107899220924462</guid></item><item><title>4æœˆï¼Ÿã”ã‚ã«å‚åŠ ã—ãŸCerebrasã®ãƒãƒƒã‚«ã‚½ãƒ³ã‹ã‚‰ä½•æ•…ã‹Huggingfaceã®proãƒ—ãƒ©ãƒ³ãŒç¶šã„ã¦ã‚‹ã‚“ã§ã™ã‚ˆã­...</title><link>https://huggingface.co/posts/Holy-fox/916850799845292</link><description>4æœˆï¼Ÿã”ã‚ã«å‚åŠ ã—ãŸCerebrasã®ãƒãƒƒã‚«ã‚½ãƒ³ã‹ã‚‰ä½•æ•…ã‹Huggingfaceã®proãƒ—ãƒ©ãƒ³ãŒç¶šã„ã¦ã‚‹ã‚“ã§ã™ã‚ˆã­... å¤šåˆ†ãƒãƒƒã‚«ã‚½ãƒ³æœŸé–“ã ã‘ã®ã¯ãšãªã‚“ã ã‘ã©ã€å¤–ã‚Œãªã„ã®ã‚ˆã­ã€‚ ã¾ã‚ã€ã‚¯ãƒ¬ã‚«ã¨ã‹ã¯ç™»éŒ²ã—ã¦ãªã„ã‹ã‚‰å¤§ä¸ˆå¤«ã ã¨ã¯æ€ã†ã‘ã© See translation</description><pubDate>Sun, 30 Nov 2025 09:23:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Holy-fox/916850799845292</guid></item><item><title>nanochat is now in transformers!</title><link>https://huggingface.co/posts/sergiopaniego/367599205240435</link><description>nanochat is now in transformers! The LLM by @ karpathy is officially in the library, and we wrote a blog covering: how did we port the model, differences from the original, and how to run or train it. go read it ğŸ¤“ nanochat-students/transformers See translation</description><pubDate>Sun, 30 Nov 2025 09:23:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/367599205240435</guid></item><item><title>Why I think local, open-source models will eventually win.</title><link>https://huggingface.co/posts/abidlabs/941146046599374</link><description>Why I think local, open-source models will eventually win. The most useful AI applications are moving toward multi-turn agentic behavior: systems that take hundreds or even thousands of iterative steps to complete a task, e.g. Claude Code, computer-control agents that click, type, and test repeatedly. In these cases, the power of the model is not how smart it is per token, but in how quickly it can interact with its environment and tools across many steps. In that regime, model quality becomes secondary to latency. An open-source model that can call tools quickly, check that the right thing was clicked, or verify that a code change actually passes tests can easily outperform a slightly â€œsmarterâ€ closed model that has to make remote API calls for every move. Eventually, the balance tips: it becomes impractical for an agent to rely on remote inference for every micro-action. Just as no one would tolerate a keyboard that required a network request per keystroke, users wonâ€™t accept...</description><pubDate>Sun, 30 Nov 2025 09:23:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/abidlabs/941146046599374</guid></item><item><title>hello, who can help me setup a local LLM and RAG for my job i can pay</title><link>https://huggingface.co/posts/aiconta/768879073281998</link><description>hello, who can help me setup a local LLM and RAG for my job i can pay See translation</description><pubDate>Sun, 30 Nov 2025 09:23:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aiconta/768879073281998</guid></item><item><title>import asyncio</title><link>https://huggingface.co/posts/KCAVEMAN/887147755592135</link><description>import asyncio import edge_tts # CONFIGURATION # Voices: "en-US-ChristopherNeural" (Male, Deep), "en-US-AriaNeural" (Female, Sharp) VOICE = "en-US-ChristopherNeural" OUTPUT_FILE = "assets/full_audio.mp3" SCRIPT = """ You believe you are living one life. You are not. You are living two lives simultaneously. Life runs on two: Darkness, light. Earth, water. Good, bad. Man, woman. Death, life. Sand, rock. Forward, back. You are obsessed with these two. But the real pattern isn't the dualityâ€”it's the Loop. The constant cycle of belief that traps you. The forward ones only win if you stop believing in the loops. Women aren't evilâ€”they're just told they are, so they become it. The system tells you the only choice is Ascend, descend. Itâ€™s a lie. The true power is found in the third pattern: Freedom. The life you want runs parallel to the life you have. Break the loop. Choose the forward pattern. Stop operating on someone else's rhythm. Left, right. Up, down. That is the slave pattern. Hit...</description><pubDate>Sun, 30 Nov 2025 09:23:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/KCAVEMAN/887147755592135</guid></item><item><title>Many updates. Cantor route experiments, GeoViT-david-beans 70% test standalone cifar100 geofractal 30m encoder. MultiHeaded Cantor Attention heavily optimized. The migration is primarily complete between geofractal and geovocab2.</title><link>https://huggingface.co/posts/AbstractPhil/724250342491821</link><description>Many updates. Cantor route experiments, GeoViT-david-beans 70% test standalone cifar100 geofractal 30m encoder. MultiHeaded Cantor Attention heavily optimized. The migration is primarily complete between geofractal and geovocab2. https://github.com/AbstractEyes/geofractal/blob/main/src/geofractal/model/david_beans/model.py Cantor route staircase and wormhole excavation findings posted. A full article will be posted to represent the findings of cantor routing and the potentials for self-learning fractals through loss. https://github.com/AbstractEyes/lattice_vocabulary/blob/master/src/geovocab2/proofs/cantor_steps_experiments.md The steps experiments show profoundly important implications for cross-contamination problems with fractal and linear spaces, with some currently assessed as useful utilities as of today. Today the classification experiment will continue by using mini-experts applied to patches within a miniature david-beans. The mini-experts were an accident that showed...</description><pubDate>Sun, 30 Nov 2025 09:23:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AbstractPhil/724250342491821</guid></item><item><title>Hello guys, iâ€™m struggling like hell to use the TRELLIS model via API, any one did use zeroGPU spaces with api from n8n or other automation tool ?</title><link>https://huggingface.co/posts/walidchouchane/902402126652271</link><description>Hello guys, iâ€™m struggling like hell to use the TRELLIS model via API, any one did use zeroGPU spaces with api from n8n or other automation tool ? Iâ€™ve been trying for day without success. trellis-community/TRELLIS See translation</description><pubDate>Sun, 30 Nov 2025 09:23:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/walidchouchane/902402126652271</guid></item><item><title>ğŸš€</title><link>https://huggingface.co/posts/angt/417840783643705</link><description>ğŸš€ installama.sh update: Vulkan &amp; FreeBSD support added! The fastest way to install and run llama.cpp has just been updated! We are expanding hardware and OS support to make local AI even more accessible. This includes: ğŸŒ‹ Vulkan support for Linux on x86_64 and aarch64 . ğŸ˜ˆ FreeBSD support (CPU backend) on x86_64 and aarch64 too. âœ¨ Lots of small optimizations and improvements under the hood. Give it a try right now: curl angt.github.io /installama.sh | MODEL=unsloth/ Qwen3- 4 B-GGUF:Q4_0 sh See translation</description><pubDate>Sun, 30 Nov 2025 09:23:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/angt/417840783643705</guid></item></channel></rss>