<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Starts erasing! ğŸ‰ ğŸ‰ ğŸ‰</title><link>https://huggingface.co/posts/piercus/167394123498038</link><description>Starts erasing! ğŸ‰ ğŸ‰ ğŸ‰ This is made with a one-step SD1.5 LBM [1] eraser ! Data is open. Data pipeline is open. Training code is open. On our LBM fork : https://github.com/finegrain-ai/LBM [1] LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/piercus/167394123498038</guid></item><item><title>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥</title><link>https://huggingface.co/posts/nouamanetazi/972464132222376</link><description>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥ Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious ğğ‚ğ‚ğ‹ ğğ«ğ«ğ¨ğ«ğ¬, or when your expensive GPU cluster is running at ğŸ”ğŸ% ğğŸğŸğ¢ğœğ¢ğğ§ğœğ², the problem isn't your model. It's most probably a ğ¦ğ¢ğ¬ğ®ğ¬ğ ğ¨ğŸ ğ­ğ¡ğ ğ¡ğšğ«ğğ°ğšğ«ğ. ğŸ› ï¸ Questions that seemed simple but had no clear answers: Why is ğŒğ¨ğ„ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğ¬ğ¥ğ¨ğ°ğğ« ğ­ğ¡ğšğ§ ğğğ§ğ¬ğ ğ¦ğ¨ğğğ¥ğ¬? Which ğğ‚ğ‚ğ‹ ğŸğ¥ğšğ ğ¬ should we actually set? How often should we checkpoint without killing throughput? That's why we built ğ“ğ¡ğ ğ’ğ¦ğ¨ğ¥ ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğğ¥ğšğ²ğ›ğ¨ğ¨ğ¤ ğŸ“–: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¥ğšğ²ğğ« that most teams get wrong. We validated real vs...</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nouamanetazi/972464132222376</guid></item><item><title>ğŸ¤– Did you know your voice might be cloned without your consent from just *one sentence* of audio?</title><link>https://huggingface.co/posts/meg/795374277994612</link><description>ğŸ¤– Did you know your voice might be cloned without your consent from just *one sentence* of audio? That's not great. So with @ frimelle , we brainstormed a new idea for developers who want to curb malicious use: âœ¨The Voice Consent Gate.âœ¨ Details, code, here: https://huggingface.co/blog/voice-consent-gate See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/meg/795374277994612</guid></item><item><title>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the</title><link>https://huggingface.co/posts/sergiopaniego/207791817757812</link><description>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the @ TogetherAgent x @ huggingface workshop we hosted in our Paris office ğŸƒ! Link: https://github.com/sergiopaniego/talks/blob/main/fine_tuning_with_trl/Fine%20tuning%20with%20TRL%20(Oct%2025).pdf See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/207791817757812</guid></item><item><title>AI Speedpainting of a Tranquil Mountain Temple!</title><link>https://huggingface.co/posts/wang12390/946400201713761</link><description>AI Speedpainting of a Tranquil Mountain Temple! Just upload one image then it will generate hand-drawn video. Please watch till the end, if you like the result, please upvote. See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wang12390/946400201713761</guid></item><item><title>ğŸš€ğŸ’¡ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸª„ğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/213442382070723</link><description>ğŸš€ğŸ’¡ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸª„ğŸš€ ğŸ“„ Title: LoftUp: Learning a Coordinate-based Feature Upsampler for Vision Foundation Models ğŸ” ğŸ“ Description: LoftUp is a coordinate-based transformer that upscales the low-resolution features of VFMs (e.g. DINOv2 and CLIP) using cross-attention and self-distilled pseudo-ground truth (pseudo-GT) from SAM. ğŸ‘¥ Authors: Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models (2504.14032) ğŸŒ Github Page: https://andrehuang.github.io/loftup-site ğŸ“ Repository: https://github.com/andrehuang/loftup ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the Foundation Models and Representation Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/foundation-models-and-representation-learning.md ğŸ“š...</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/213442382070723</guid></item><item><title>ğŸš€ğŸ‘ŒğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ¤ŒğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/744756733617336</link><description>ğŸš€ğŸ‘ŒğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ¤ŒğŸš€ ğŸ“„ Title: Understanding Co-speech Gestures in-the-wild ğŸ” ğŸ“ Description: JEGAL is a tri-modal model that learns from gestures, speech and text simultaneously, enabling devices to interpret co-speech gestures in the wild. ğŸ‘¥ Authors: @ sindhuhegde , K R Prajwal, Taein Kwon, and Andrew Zisserman ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: Understanding Co-speech Gestures in-the-wild (2503.22668) ğŸŒ Web Page: https://www.robots.ox.ac.uk/~vgg/research/jegal ğŸ“ Repository: https://github.com/Sindhu-Hegde/jegal ğŸ“º Video: https://www.youtube.com/watch?v=TYFOLKfM-rM ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the Human Modeling Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/human-modeling.md ğŸ“š More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers curated by @ DmitryRyumin ğŸ” Keywords:...</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/744756733617336</guid></item><item><title>ğŸš€ Big news for AI builders!</title><link>https://huggingface.co/posts/pagezyhf/128586778684407</link><description>ğŸš€ Big news for AI builders! Weâ€™re thrilled to announce that the Qwen3-VL family of vision-language models is now available on Azure AI Foundry, thanks to our collaboration with Microsoft. We bring open-source innovation to enterprise-grade AI infrastructure, making it easier than ever for enterprise to deploy and scale the latest and greatest from models from hugging Face securely within Azure. ğŸ” Highlights: - Deploy Qwen3-VL instantly via managed endpoints - Built-in governance, telemetry, and lifecycle management - True multimodal reasoning â€” vision, language, and code understanding - State-of-the-art performance, outperforming closed-source models like Gemini 2.5 Pro and GPT-5 - Available in both *Instruct* and *Thinking* modes, across 24 model sizes ğŸ‘‰ Get started today: search for Qwen3-VL in the Hugging Face Collection on Azure AI Foundry. See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/pagezyhf/128586778684407</guid></item><item><title>Kimi K2 is a bit disappointing by my expectations. It is on a par with Codex mini.</title><link>https://huggingface.co/posts/onekq/456763679689481</link><description>Kimi K2 is a bit disappointing by my expectations. It is on a par with Codex mini. onekq-ai/WebApp1K-models-leaderboard See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/456763679689481</guid></item><item><title>A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on â†—ï¸</title><link>https://huggingface.co/posts/prithivMLmods/710644146568512</link><description>A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on â†—ï¸ https://huggingface.co/blog/prithivMLmods/multimodal-ocr-vlms on behalf of strangervisionhf It discusses the latest trends in OCR models, the multilingual support offered by modern OCR systems, their unique capabilities, OCR benchmark model comparisons, transformer-based implementations, and strategies for streamlining transformers compatibility. See translation</description><pubDate>Sat, 01 Nov 2025 05:21:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/710644146568512</guid></item></channel></rss>