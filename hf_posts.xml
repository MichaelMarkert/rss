<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Mistral's new SOTA coding models Devstral 2 can now be Run locally! (25GB RAM) üê±</title><link>https://huggingface.co/posts/danielhanchen/563432838268940</link><description>Mistral's new SOTA coding models Devstral 2 can now be Run locally! (25GB RAM) üê± We fixed the chat template, so performance should be much better now! 24B: unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF 123B: unsloth/Devstral-2-123B-Instruct-2512-GGUF üß°Step-by-step Guide: https://docs.unsloth.ai/models/devstral-2 See translation</description><pubDate>Sat, 13 Dec 2025 13:30:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/563432838268940</guid></item><item><title>üê¶‚Äçüî• I've just published Sentence Transformers v5.2.0! It introduces multi-processing for CrossEncoder (rerankers), multilingual NanoBEIR evaluators, similarity score outputs in mine_hard_negatives, Transformers v5 support and more. Details:</title><link>https://huggingface.co/posts/tomaarsen/853653818134091</link><description>üê¶‚Äçüî• I've just published Sentence Transformers v5.2.0! It introduces multi-processing for CrossEncoder (rerankers), multilingual NanoBEIR evaluators, similarity score outputs in mine_hard_negatives, Transformers v5 support and more. Details: - CrossEncoder multi-processing: Similar to SentenceTransformer and SparseEncoder, you can now use multi-processing with CrossEncoder rerankers. Useful for multi-GPU and CPU settings, and simple to configure: just device=["cuda:0", "cuda:1"] or device=["cpu"]*4 on the model.predict or model.rank calls. - Multilingual NanoBEIR Support: You can now use community translations of the tiny NanoBEIR retrieval benchmark instead of only the English one, by passing dataset_id , e.g. dataset_id="lightonai/NanoBEIR-de" for the German benchmark. - Similarity scores in Hard Negatives Mining: When mining for hard negatives to create a strong training dataset, you can now pass output_scores=True to get similarity scores returned. This can be useful for some...</description><pubDate>Sat, 13 Dec 2025 13:30:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/853653818134091</guid></item><item><title>Muon vs MuonClip vs Muon+Adamw</title><link>https://huggingface.co/posts/KingNish/784885155042758</link><description>Muon vs MuonClip vs Muon+Adamw Muon has gone from an experiment to a mainstream optimizer, but does it hold up for fine‚Äëtuning? We ran head‚Äëto‚Äëhead tests on Qwen3‚Äë4B (10k+ high‚Äëquality instruction rows) to find out. Short story: Pure Muon converged fastest at the start, but its gradient‚Äënorm spikes made training unstable. MuonClip (Kimi K2‚Äôs clipping) stabilizes long pretraining runs, yet in our small‚Äëscale fine‚Äëtune it underperformed, lower token accuracy and slower convergence. The winner was the hybrid: Muon for 2D layers + AdamW for 1D layers. It delivered the best balance of stability and final performance and even beat vanilla AdamW. Takeaway: for small-scale fine-tuning, hybrid = practical and reliable. Next Step: scale to larger models/datasets to see if Muon‚Äôs spikes become catastrophic or if clipping wins out. Full Blog Link: https://huggingface.co/blog/KingNish/optimizer-part1 See translation</description><pubDate>Sat, 13 Dec 2025 13:30:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/KingNish/784885155042758</guid></item><item><title>I wasted days on a GPU node on a bug that shouldn't exist</title><link>https://huggingface.co/posts/martinsu/305383997158992</link><description>I wasted days on a GPU node on a bug that shouldn't exist So I was fine-tuning TildeOPEN-30B and the outputs were... weird. Token ID 179 (&lt;0x00&gt;) kept appearing between almost every token pair. Took me a bit to figure out what was going on. Turns out I used the fast tokenizer for training, but the model was trained on the slow one. Silent failure. Well... long story short‚ÄîTGI uses (forces) the fast tokenizer, no questions asked. And you'll have agile's kryptonite: silent failure. If the model was trained on slow, it's a silent disaster. I got curious and wrote a quick script to check how common this is. Ran it on 6,014 LLM HF models overnight. Roughly 10% of HF model downloads have mismatched tokenizers. Not all mismatches are catastrophic, but some are brutal ‚Äî like chat template markers inflating from 1 token to 3, silently wrecking context windows and causing model act weird. This wasn't rigorous research, but the drift is real. And the worst part? 968 models(out of 500+...</description><pubDate>Sat, 13 Dec 2025 13:30:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/martinsu/305383997158992</guid></item><item><title>üèÜ BEHAVIOR Challenge 1st Place ‚Äì Solution Summary</title><link>https://huggingface.co/posts/IliaLarchenko/908879505364910</link><description>üèÜ BEHAVIOR Challenge 1st Place ‚Äì Solution Summary My team recently won 1st place in the BEHAVIOR Challenge at NeurIPS. The competition focused on training a single policy to complete 50 long-horizon household tasks in simulation. We built an end-to-end policy based on Pi0.5 with a bunch of custom modifications. Everything is open-sourced, and it should be useful for anyone exploring VLAs or adapting them to specific tasks. Key Architecture Changes: - Replaced language model with 50 trainable task embeddings (no text at all) - Correlated noise for Flow Matching: œµ ‚àº N(0, 0.5I + 0.5Œ£) using dataset action covariance - Learnable mixed-layer attention: each action expert layer attends to a trainable mix of all VLM layers - System 2 stage tracking: model predicts task stage, we smooth it with voting and feed it back as context Training: - Multi-sample Flow Matching: 15 FM samples per VLM pass to reduce gradient variance - Delta action space + per-timestamp normalization - FAST auxiliary...</description><pubDate>Sat, 13 Dec 2025 13:30:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/IliaLarchenko/908879505364910</guid></item><item><title>TRL now includes agent training support for GRPO‚ÄºÔ∏è</title><link>https://huggingface.co/posts/sergiopaniego/777084007649682</link><description>TRL now includes agent training support for GRPO‚ÄºÔ∏è Train üïµÔ∏è agents with üîß tools, enabling interaction with external functions and APIs. And of course, a new notebook and scripts to get you up to speed üìò notebook tutorial: https://github.com/huggingface/trl/blob/main/examples/notebooks/grpo_agent.ipynb üìÇ script examples: https://github.com/huggingface/trl/blob/main/examples/scripts/grpo_agent.py üì¶ TRL v0.26.0 release: https://github.com/huggingface/trl/releases/tag/v0.26.0 See translation</description><pubDate>Sat, 13 Dec 2025 13:30:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/777084007649682</guid></item><item><title>Z-Image Turbo + LoRA ‚ö°</title><link>https://huggingface.co/posts/ovi054/902660269971069</link><description>Z-Image Turbo + LoRA ‚ö° ovi054/Z-Image-LORA Z-Image Turbo is the No. 1 trending Text-to-Image model right now. You can add a custom LoRA and generate images with this Space. üëâ Try it now: ovi054/Z-Image-LORA See translation</description><pubDate>Sat, 13 Dec 2025 13:30:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/902660269971069</guid></item><item><title>ICYMI, you can fine-tune open LLMs using Claude Code</title><link>https://huggingface.co/posts/sergiopaniego/384415208092213</link><description>ICYMI, you can fine-tune open LLMs using Claude Code just tell it: ‚ÄúFine-tune Qwen3-0.6B on open-r1/codeforces-cots‚Äù and Claude submits a real training job on HF GPUs using TRL. it handles everything: &gt; dataset validation &gt; GPU selection &gt; training + Trackio monitoring &gt; job submission + cost estimation when it‚Äôs done, your model is on the Hub, ready to use read more about the process: https://huggingface.co/blog/hf-skills-training See translation</description><pubDate>Sat, 13 Dec 2025 13:30:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/384415208092213</guid></item><item><title>I just released LayaCodec, a highly efficient neural audio tokenizer/codec for TTS models, far better than most previous audio tokenizers.</title><link>https://huggingface.co/posts/YatharthS/731858391215619</link><description>I just released LayaCodec, a highly efficient neural audio tokenizer/codec for TTS models, far better than most previous audio tokenizers. ü§Ø Next-gen TTS models that use this could achieve several 100s of times real-time speed while producing clearer audio!! ü§Ø GitHub repo: https://github.com/ysharma3501/LayaCodec Model: YatharthS/LayaCodec See translation</description><pubDate>Sat, 13 Dec 2025 13:30:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YatharthS/731858391215619</guid></item><item><title>Language Dexterity Benchmark</title><link>https://huggingface.co/posts/prabhatkr/593169299364014</link><description>Language Dexterity Benchmark I am working on a new benchmark to establish human language dexterity. My hypothesis is that certain language allow for more accurate dexterous behaviour - Pointed, unambigous, and confusion-free references of parts of speech in small and large contexts. There are certain languages with high degree of accurate grammar like Sanskrit, Esperanto, and Turkish. I am native Sanskrit speaker. I have plans to establish this benchmark and test this hypothesis across 100 langauges. I have created 25 task prompts for text, image, video and robotics manipulation. We can test langauges across multiple popular models. Here is the github link: https://github.com/ParamTatva-org/Linguistic-Dexterity-Benchmark See translation</description><pubDate>Sat, 13 Dec 2025 13:30:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prabhatkr/593169299364014</guid></item></channel></rss>