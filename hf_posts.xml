<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Why I think local, open-source models will eventually win.</title><link>https://huggingface.co/posts/abidlabs/941146046599374</link><description>Why I think local, open-source models will eventually win. The most useful AI applications are moving toward multi-turn agentic behavior: systems that take hundreds or even thousands of iterative steps to complete a task, e.g. Claude Code, computer-control agents that click, type, and test repeatedly. In these cases, the power of the model is not how smart it is per token, but in how quickly it can interact with its environment and tools across many steps. In that regime, model quality becomes secondary to latency. An open-source model that can call tools quickly, check that the right thing was clicked, or verify that a code change actually passes tests can easily outperform a slightly â€œsmarterâ€ closed model that has to make remote API calls for every move. Eventually, the balance tips: it becomes impractical for an agent to rely on remote inference for every micro-action. Just as no one would tolerate a keyboard that required a network request per keystroke, users wonâ€™t accept...</description><pubDate>Wed, 05 Nov 2025 09:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/abidlabs/941146046599374</guid></item><item><title>fine-tuning a 14B model with TRL + SFT on a free Colab (T4 GPU)?</title><link>https://huggingface.co/posts/sergiopaniego/990279445625588</link><description>fine-tuning a 14B model with TRL + SFT on a free Colab (T4 GPU)? thanks to the latest TRL optimizations, you actually can! sharing a new notebook showing how to do it ğŸ˜ colab: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_trl_lora_qlora.ipynb notebooks in TRL: https://github.com/huggingface/trl/tree/main/examples/notebooks See translation</description><pubDate>Wed, 05 Nov 2025 09:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/990279445625588</guid></item><item><title>Experience the future of fashion with our AI-powered virtual try-on technology. See how clothes look on anyone instantly, create realistic outfit visualizations, and mix-and-match styles with unprecedented accuracy.</title><link>https://huggingface.co/posts/wang12390/323744389614625</link><description>Experience the future of fashion with our AI-powered virtual try-on technology. See how clothes look on anyone instantly, create realistic outfit visualizations, and mix-and-match styles with unprecedented accuracy. https://miragic.ai/products/virtual-try-on See translation</description><pubDate>Wed, 05 Nov 2025 09:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wang12390/323744389614625</guid></item><item><title>11 Fascinating new Policy Optimization techniques</title><link>https://huggingface.co/posts/Kseniase/468043722468280</link><description>11 Fascinating new Policy Optimization techniques Policy optimization (PO) algorithms are central to training AI models with preference-based feedback. In recent weeks, numerous new PO methods have emerged that build on or replace the popular PPO and GRPO, solving their issues. Here are 11 of them: 1. BAlanced Policy Optimization (BAPO) â†’ BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping (2510.18927) Dynamically adjusting the clipping bounds in PPO-style updates to balance positive and negative gradients and prevent entropy collapse 2. Training-Free GRPO â†’ Training-Free Group Relative Policy Optimization (2510.08191) Instead of using numeric rewards, it compares rollouts semantically to distill useful knowledge as a token prior, which is then applied during inference to guide the modelâ€™s behavior 3. Asymmetric Importance Sampling Policy Optimization (ASPO) â†’ ASPO: Asymmetric Importance Sampling Policy Optimization...</description><pubDate>Wed, 05 Nov 2025 09:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/468043722468280</guid></item><item><title>Some weeks ago, i've just decide its time to leave LinkedIn for me.</title><link>https://huggingface.co/posts/flozi00/890663421107803</link><description>Some weeks ago, i've just decide its time to leave LinkedIn for me. It got silent around my open source activities the last year, so i thought something has to change. That's why my focus will move to share experiences and insights about hardware, drivers, kernels and linux. I won't post about how to use models, built agents or do prompting. I want to share about some deeper layers the actual hypes are built on. I will start posting summarizations of my articles here on the hub. English version: https://flozi.net/en German translated version: https://flozi.net/de Feel free to reach me if you want to read something specific. See translation</description><pubDate>Wed, 05 Nov 2025 09:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/flozi00/890663421107803</guid></item><item><title>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥</title><link>https://huggingface.co/posts/nouamanetazi/972464132222376</link><description>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥ Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious ğğ‚ğ‚ğ‹ ğğ«ğ«ğ¨ğ«ğ¬, or when your expensive GPU cluster is running at ğŸ”ğŸ% ğğŸğŸğ¢ğœğ¢ğğ§ğœğ², the problem isn't your model. It's most probably a ğ¦ğ¢ğ¬ğ®ğ¬ğ ğ¨ğŸ ğ­ğ¡ğ ğ¡ğšğ«ğğ°ğšğ«ğ. ğŸ› ï¸ Questions that seemed simple but had no clear answers: Why is ğŒğ¨ğ„ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğ¬ğ¥ğ¨ğ°ğğ« ğ­ğ¡ğšğ§ ğğğ§ğ¬ğ ğ¦ğ¨ğğğ¥ğ¬? Which ğğ‚ğ‚ğ‹ ğŸğ¥ğšğ ğ¬ should we actually set? How often should we checkpoint without killing throughput? That's why we built ğ“ğ¡ğ ğ’ğ¦ğ¨ğ¥ ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğğ¥ğšğ²ğ›ğ¨ğ¨ğ¤ ğŸ“–: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¥ğšğ²ğğ« that most teams get wrong. We validated real vs...</description><pubDate>Wed, 05 Nov 2025 09:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nouamanetazi/972464132222376</guid></item><item><title>On this day in 2019, OpenAI released the final GPT-2 model as part of their staged release. I still remember that November well - so much was happening, but GPT-2's release felt like a watershed moment for the field. It showed us what was possible with carefully trained language models.</title><link>https://huggingface.co/posts/codelion/382097318111878</link><description>On this day in 2019, OpenAI released the final GPT-2 model as part of their staged release. I still remember that November well - so much was happening, but GPT-2's release felt like a watershed moment for the field. It showed us what was possible with carefully trained language models. To recreate some of that GPT-2 magic, I recently tackled an interesting challenge: can you pretrain a language model with just 1 billion tokens - roughly 1/10th of what GPT-2 used - and still get comparable performance? After 50+ systematic experiments testing different dataset mixtures, the answer is yes. The result is **codelion/gpt-2-70m** ( codelion/gpt-2-70m ), which achieves over 90% of GPT-2's benchmark performance despite being trained on 10x less data. The key was finding the optimal dataset composition: 50% high-quality textbook PDFs, 30% filtered web content, and 20% educational resources. It even beats GPT-2 on TruthfulQA (47.31% vs 40.69%). If you're interested in the full story of how...</description><pubDate>Wed, 05 Nov 2025 09:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/382097318111878</guid></item><item><title>ğŸš€ğŸ‘ï¸ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ‘ï¸ğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/716491468051168</link><description>ğŸš€ğŸ‘ï¸ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ‘ï¸ğŸš€ ğŸ“„ Title: Diving into the Fusion of Monocular Priors for Generalized Stereo Matching ğŸ” ğŸ“ Description: The proposed method enhances stereo matching by efficiently combining unbiased monocular priors from vision foundation models. This method addresses misalignment and local optima issues using a binary local ordering map and pixel-wise linear regression. ğŸ‘¥ Authors: Chengtang Yao, Lidong Yu, Zhidan Liu, Jiaxi Zeng, Yuwei Wu, and Yunde Jia ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: Diving into the Fusion of Monocular Priors for Generalized Stereo Matching (2505.14414) ğŸ“ Repository: https://github.com/YaoChengTang/Diving-into-the-Fusion-of-Monocular-Priors-for-Generalized-Stereo-Matching ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the 3D Pose Understanding Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/3d-pose-...</description><pubDate>Wed, 05 Nov 2025 09:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/716491468051168</guid></item><item><title>Hey builders ğŸ‘·â€â™€ï¸</title><link>https://huggingface.co/posts/Ihor/565863655331711</link><description>Hey builders ğŸ‘·â€â™€ï¸ Weâ€™re Knowledgator, the team behind open-source NLP models like GLiNER, GLiClass, and many other used for zero-shot text classification and information extraction. If youâ€™ve explored them on Hugging Face or used our frameworks from GitHub, weâ€™d love your input: ğŸ§© Which of our models, like GLiNER or zero-shot classifiers, do you find helpful in your practical workflows? ğŸ§© Howâ€™s the setup, performance, and accuracy been for you? ğŸ§© Anything confusing, buggy, or missing that would make your workflow smoother? Your feedback helps us improve speed, clarity, and stability for everyone in the open-source community. ğŸ’¬ Comment directly here or join the discussion. We read every one ğŸ˜‰: GitHub: https://github.com/Knowledgator Discord: https://discord.gg/GXRcAVJQ HuggingFace: knowledgator ğŸ“ Want to shape our next release? Click here to complete this 2-min survey: https://docs.google.com/forms/d/e/1FAIpQLSdyz2UMHrMDX8S9stpBk0wyfngtKSYzwk-02mN1VNYDdTw8OQ/viewform See translation</description><pubDate>Wed, 05 Nov 2025 09:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ihor/565863655331711</guid></item><item><title>A week ago, I shared a post about the latest transformers test implementation of DeepSeek-OCR Compatibility (</title><link>https://huggingface.co/posts/prithivMLmods/374605520852651</link><description>A week ago, I shared a post about the latest transformers test implementation of DeepSeek-OCR Compatibility ( https://tinyurl.com/ykc4mm66 ). Now, Iâ€™m dropping the most compatible version of it to support the model with the latest transformers. ğŸ¤—ğŸ”¥ â  DeepSeek-OCR-Latest-BF16.I64: prithivMLmods/DeepSeek-OCR-Latest-BF16.I64 â  DeepSeek OCR [exp] : prithivMLmods/DeepSeek-OCR-experimental âœ…Supports the latest transformers v4.57.1 âœ…torch: 2.6.0+cu124 (or) the latest version (i.e., torch 2.9.0) âœ…cuda version: 12.4 âœ…users can also opt out of specific attention implementations if desired. âœ¨Previous version: strangervisionhf/deepseek-ocr-latest-transformers â†—ï¸Related Blog: https://huggingface.co/blog/prithivMLmods/multimodal-ocr-vlms âœ¨Community Page: strangervisionhf âœ¨Original Model Page: deepseek-ai/DeepSeek-OCR To know more about it, visit the app page or the respective model page! See translation</description><pubDate>Wed, 05 Nov 2025 09:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/374605520852651</guid></item></channel></rss>