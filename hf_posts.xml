<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The new Mistral 3 models are here !</title><link>https://huggingface.co/posts/Jofthomas/993866418471203</link><description>The new Mistral 3 models are here ! Today, we announce Mistral 3, the next generation of Mistral models. Mistral 3 includes three state-of-the-art small, dense models (14B, 8B, and 3B) and Mistral Large 3 ‚Äì our most capable model to date ‚Äì a sparse mixture-of-experts trained with 41B active and 675B total parameters. All models are released under the Apache 2.0 license. Ministrals : https://huggingface.co/collections/mistralai/ministral-3 Mistral Large 3: https://huggingface.co/collections/mistralai/mistral-large-3 See translation</description><pubDate>Thu, 04 Dec 2025 09:29:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jofthomas/993866418471203</guid></item><item><title>this is big... 50 AI researchers from Bytedance, Alibaba, Tencent, and other labs/universities just published a 300-page paper with surprising lessons about coding models and agents (data, pre and post-training, etc).</title><link>https://huggingface.co/posts/hesamation/869653062191419</link><description>this is big... 50 AI researchers from Bytedance, Alibaba, Tencent, and other labs/universities just published a 300-page paper with surprising lessons about coding models and agents (data, pre and post-training, etc). key highlights: &gt; small LLMs can beat proprietary giants RL (RLVR specifically) gives small open-source models an edge over big models in reasoning. a 14B model trained with RLVR on high-quality verified problems can match the performance of OpenAI's o3. &gt; models have a hard time learning Python. mixing language models during pre-training is good, but Python behaves different from statically typed languages. languages with similar syntax (Java and C#, or JavaScript and TypeScript) creates high positive synergy. mixing Python heavily into the training of statically typed languages can actually hurt because of Python's dynamic typing. &gt; not all languages are equal (coding scaling laws) the amount of data required to specialize a model on a language drastically depends on...</description><pubDate>Thu, 04 Dec 2025 09:29:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/869653062191419</guid></item><item><title>ICYMI, transformers v5 is out!</title><link>https://huggingface.co/posts/sergiopaniego/920736659206393</link><description>ICYMI, transformers v5 is out! Grab a coffee ‚òï and go read the announcement blog https://huggingface.co/blog/transformers-v5 See translation</description><pubDate>Thu, 04 Dec 2025 09:29:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/920736659206393</guid></item><item><title>Mistral's new Ministral 3 models can now be Run &amp; Fine-tuned locally! (16GB RAM)</title><link>https://huggingface.co/posts/danielhanchen/849127033892624</link><description>Mistral's new Ministral 3 models can now be Run &amp; Fine-tuned locally! (16GB RAM) Ministral 3 have vision support and the best-in-class performance for their sizes. 14B Instruct GGUF: unsloth/Ministral-3-14B-Instruct-2512-GGUF 14B Reasoning GGUF: unsloth/Ministral-3-14B-Reasoning-2512-GGUF üê± Step-by-step Guide: https://docs.unsloth.ai/new/ministral-3 All GGUFs, BnB, FP8 etc. variants uploads: https://huggingface.co/collections/unsloth/ministral-3 See translation</description><pubDate>Thu, 04 Dec 2025 09:29:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/849127033892624</guid></item><item><title>ScalingOpt | Welcome to join and co-build the Optimization Community!</title><link>https://huggingface.co/posts/Juanxi/977874890039450</link><description>ScalingOpt | Welcome to join and co-build the Optimization Community! ScalingOpt is a professional platform focusing on optimization for large-scale deep learning, aiming to advocate for "Optimization at Scale," which means verifiable and scalable optimization algorithms. This community platform is dedicated to gathering, discovering, comparing, and contributing various cutting-edge optimizers and optimization algorithms. It's not just a simple Awesome List, it also includes: Visualizations: Covers visualization scripts for the Rosenbrock Function and the Rastrigin Function for users to freely explore. Benchmark: We recommend Algoperf as the primary source, along with other verifiable benchmarks and analysis articles, for users to reference the best optimizer. Papers &amp; Blogs Recommendation: The platform summarizes high-quality papers and blogs from recent years, and continuously adds the latest papers based on daily arXiv updates, currently totaling nearly a hundred articles....</description><pubDate>Thu, 04 Dec 2025 09:29:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Juanxi/977874890039450</guid></item><item><title>We recently discussed how Tensor Parallelism slices matrices to reduce latency within a single node. But what happens when you need to scale beyond that, where the bandwidth drops?</title><link>https://huggingface.co/posts/flozi00/166942879954778</link><description>We recently discussed how Tensor Parallelism slices matrices to reduce latency within a single node. But what happens when you need to scale beyond that, where the bandwidth drops? That is where Pipeline Parallelism (PP) takes over. Instead of slicing the operation, PP slices the model depth. It turns your GPU cluster into an assembly line: GPU 0 handles layers 1-12, GPU 1 handles 13-24, and so on. The hardware challenge here isn't the interconnect speed‚Äîit is the "Pipeline Bubble." In a naive setup, expensive H100s sit idle for most of the cycle waiting for data to flow through the chain. My latest guide breaks down the scheduling strategies used to minimize this idle silicon time. In this deep dive, we cover: The Hardware Mechanics: Vertical Slicing Unlike TP which requires "chatty" All-Reduce operations, PP relies on lightweight Point-to-Point (Send/Recv) communication. This makes it the only viable strategy for crossing node boundaries over Ethernet or InfiniBand. Fighting the...</description><pubDate>Thu, 04 Dec 2025 09:29:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/flozi00/166942879954778</guid></item><item><title>NEW RELEASE: Esper 3.1 for Ministral 3 14b, 8b, and 3b!</title><link>https://huggingface.co/posts/sequelbox/532546402634546</link><description>NEW RELEASE: Esper 3.1 for Ministral 3 14b, 8b, and 3b! - Esper is our full-stack, full-cycle coding, DevOps, and architecture specialist! - Our newest, best DeepSeek technical datasets emphasize more challenging queries and tough real-world coding tasks across a variety of programming languages and development paradigms: - Titanium 3 for coding and reasoning in DevOps and architecture: sequelbox/Titanium3-DeepSeek-V3.1-Terminus - Tachibana 3 for high-difficulty code production in a variety of topics and programming languages: - sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus - sequelbox/Tachibana3-Part2-DeepSeek-V3.2 - Mitakihara for MLOps, AI building, use, expertise, and research: sequelbox/Mitakihara-DeepSeek-R1-0528 Get Esper 3.1 now in all 3 Ministral 3 sizes! (We recommend 14b for general use.) 14b: ValiantLabs/Ministral-3-14B-Reasoning-2512-Esper3.1 8b: ValiantLabs/Ministral-3-8B-Reasoning-2512-Esper3.1 3b: ValiantLabs/Ministral-3-3B-Reasoning-2512-Esper3.1 We'll be...</description><pubDate>Thu, 04 Dec 2025 09:29:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sequelbox/532546402634546</guid></item><item><title>We've published a comprehensive evaluation of the Feetech STS3250 servo actuator.</title><link>https://huggingface.co/posts/branikita/887548171409943</link><description>We've published a comprehensive evaluation of the Feetech STS3250 servo actuator. Key Findings: - Speed: 77.6 RPM (exceeds spec by 3.2%) - Backlash: 0.43¬∞ (within 0.5¬∞ limit) - Repeatability: ¬±0.02mm at 95mm radius - Peak torque: 48 kg¬∑cm - Sustained torque: ~25 kg¬∑cm after thermal protection Full review: https://robonine.com/feetech-sts3250-smart-actuator-evaluation-of-accuracy-torque-and-backlash/ See translation</description><pubDate>Thu, 04 Dec 2025 09:29:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/887548171409943</guid></item><item><title>September(2025) LLM Safety &amp; Reliability Benchmarks Report By AI Parivartan Research Lab (AIPRL-LIR)</title><link>https://huggingface.co/posts/rajkumarrawal/715420178191599</link><description>September(2025) LLM Safety &amp; Reliability Benchmarks Report By AI Parivartan Research Lab (AIPRL-LIR) Monthly LLM's Intelligence Reports for AI Decision Makers : Our "aiprl-llm-intelligence-report" repo to establishes (AIPRL-LIR) framework for Large Language Model overall evaluation and analysis through systematic monthly intelligence reports. Unlike typical AI research papers or commercial reports. It provides structured insights into AI model performance, benchmarking methodologies, Multi-hosting provider analysis, industry trends ... ( all in one monthly report ) Leading Models &amp; Companies, 23 Benchmarks in 6 Categories, Global Hosting Providers, &amp; Research Highlights Here‚Äôs what you‚Äôll find inside this month‚Äôs intelligence report:- Leading Models &amp; Companies : 23 Benchmarks in 6 Categories : With a special focus on Safety &amp; Reliability performance across diverse tasks. Global Hosting Providers : Research Highlights : Comparative insights, evaluation methodologies, and industry...</description><pubDate>Thu, 04 Dec 2025 09:29:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/rajkumarrawal/715420178191599</guid></item><item><title>Hello everyone,</title><link>https://huggingface.co/posts/prithivMLmods/649242345044742</link><description>Hello everyone, The strangerzonehf [HF] Community / Organization Page, which is maintained by me, has reached the Top 10 Developer Pages ranking at 6th place, contributing 3.4% in the calendar cycle from August 2024 to August 2025. It is also the only South Asia / Indian page in the list. I could not be more proud to be doing things for the community. ‚ù§Ô∏èü§ó Source: https://www.dataprovenance.org/economies-of-open-intelligence.pdf It is a pleasure to be a part of it. Thank you! @ prithivMLmods See translation</description><pubDate>Thu, 04 Dec 2025 09:29:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/649242345044742</guid></item></channel></rss>