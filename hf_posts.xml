<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>11 Types of JEPA</title><link>https://huggingface.co/posts/Kseniase/659118746872319</link><description>11 Types of JEPA Since Meta released the newest V-JEPA 2 this week, we thought it's a good time to revisit a few other interesting JEPA variants. JEPA, or Joint Embedding Predictive Architecture, a self-supervised learning framework that predicts the latent representation of a missing part of the input. Here are 11 JEPA types that you should know about: 1. V-JEPA 2 -&gt; V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning (2506.09985) Trained on 1M+ hours of internet videos and a little bit of robot interaction data, V-JEPA 2 can watch, understand, answer questions, and help robots plan and act in physical world 2. Time-Series-JEPA (TS-JEPA) -&gt; Time-Series JEPA for Predictive Remote Control under Capacity-Limited Networks (2406.04853) It's a time-series predictive model that learns compact, meaningful representations. A self-supervised semantic actor then uses them to generate control commands without raw data 3. Denoising JEPA (D-JEPA) -&gt; Denoising...</description><pubDate>Tue, 17 Jun 2025 09:26:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/659118746872319</guid></item><item><title>stop writing CUDA kernels yourself</title><link>https://huggingface.co/posts/merve/593764353844896</link><description>stop writing CUDA kernels yourself we have launched Kernel Hub: easy optimized kernels for all models on Hugging Face ğŸ”¥ use them right away! it's where the community populates optimized kernels ğŸ¤ this release comes in three parts &gt; Kernel Hub: contains (as of now) 14 kernels &gt; kernels: Python library to load kernels from Kernel Hub &gt; kernel-builder: Nix package to build kernels for PyTorch (made using PyTorch C++ frontend) when building models, your regular workflow should be pulling kernels from Hub and building your model with them ğŸ¤— here's a practical example with RMSNorm: 1. pull the kernel from Hub with get_kernel 2. decorate with use_kernel_forward_from_hub 3. inject it to your model we'd love to hear your feedback! ğŸ™ğŸ» we also welcome kernel contributions by community ğŸ¥¹ğŸ’— - request kernels here: kernels-community/README#1 - check out this org: kernels-community - read the blog: https://huggingface.co/blog/hello-hf-kernels See translation</description><pubDate>Tue, 17 Jun 2025 09:26:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/593764353844896</guid></item><item><title>ğŸ¬ VEO3 Directors - All-in-One AI Video Creation Suite</title><link>https://huggingface.co/posts/ginipick/718905723783644</link><description>ğŸ¬ VEO3 Directors - All-in-One AI Video Creation Suite ğŸš€ What is VEO3 Directors? VEO3 Directors is a revolutionary end-to-end AI video creation platform that transforms your ideas into cinematic reality. From story conception to final video with synchronized audio - all in one seamless workflow! ğŸ”— Try It Now ginigen/VEO3-Directors ginigen/VEO3-Free ginigen/VEO3-Free-mirror âœ¨ Key Features ğŸ“ Story Seed Generator ğŸ² Instantly generate creative story ideas across multiple genres ğŸŒ Bilingual support (English/Korean) ğŸ­ Rich categories: Genre, Setting, Characters, and more ğŸ¥ AI Script &amp; Prompt Crafting ğŸ’¬ Powered by Friendli API for Hollywood-quality prompts ğŸ¤– AI Director writes detailed cinematography instructions ğŸ¬ Professional elements: camera movements, lighting, VFX ğŸ¬ Video + Audio Generation ğŸ¨ Wan2.1-T2V-14B for stunning visual quality âš¡ NAG 4-step inference - 10x faster generation ğŸµ MMAudio auto-generates matching soundscapes ğŸ›ï¸ Full control over resolution, duration, and style...</description><pubDate>Tue, 17 Jun 2025 09:26:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/718905723783644</guid></item><item><title>Kimi-Dev ğŸ’» New coding model by Moonshot AI</title><link>https://huggingface.co/posts/AdinaY/465653536234983</link><description>Kimi-Dev ğŸ’» New coding model by Moonshot AI moonshotai/Kimi-Dev-72B âœ¨ 72B - MIT license âœ¨ 60.4% on SWE-bench Verified âœ¨ RL-trained to patch real repos in Docker âœ¨ Only rewarded if full test suite passes See translation</description><pubDate>Tue, 17 Jun 2025 09:26:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/465653536234983</guid></item><item><title>Beginnerâ€™s Guide â€” Generate Videos With SwarmUI</title><link>https://huggingface.co/posts/MonsterMMORPG/453570470195139</link><description>Beginnerâ€™s Guide â€” Generate Videos With SwarmUI Full article here please check out : https://huggingface.co/blog/MonsterMMORPG/beginners-guide-generate-videos-with-swarmui Proper ComfyUI backend then SwarmUI installation tutorial : https://youtu.be/fTzlQ0tjxj0 Proper ComfyUI backend then SwarmUI installation tutorial on RunPod : https://youtu.be/R02kPf9Y3_w See translation</description><pubDate>Tue, 17 Jun 2025 09:26:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/453570470195139</guid></item><item><title>I realised a small documentation on how to make your own LM architecture called [LM-From-Scratch](</title><link>https://huggingface.co/posts/FlameF0X/196232708808828</link><description>I realised a small documentation on how to make your own LM architecture called [LM-From-Scratch]( https://github.com/FlameF0X/LM-From-Scratch ) See translation</description><pubDate>Tue, 17 Jun 2025 09:26:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/FlameF0X/196232708808828</guid></item><item><title>Started</title><link>https://huggingface.co/posts/MikeDoes/954423414183854</link><description>Started aistatuscodes as a new project to create codes to understand AI performance better. Going to be posting daily here and on instagram until we get to 100m downloads :) https://www.instagram.com/MikeDoesDo/ Follow along the journey! See translation</description><pubDate>Tue, 17 Jun 2025 09:26:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/954423414183854</guid></item><item><title>You can now edit operations with a discrete flow model, supercoolğŸ‘! It's amazing to see the progress on DFM within one year since its introduction - literally my litmus test for how fast the field is progressing:</title><link>https://huggingface.co/posts/Jaward/740369227920658</link><description>You can now edit operations with a discrete flow model, supercoolğŸ‘! It's amazing to see the progress on DFM within one year since its introduction - literally my litmus test for how fast the field is progressing: 1st Introduced (2024): https://arxiv.org/abs/2402.04997 Discrete Flow Matching (2024): https://arxiv.org/abs/2407.15595 Edit Discrete Flow (2025): https://arxiv.org/pdf/2506.09018 Looking forward to a SaaS level reach like that of dLLMs e.g Mercury by inception labs ğŸš€ See translation</description><pubDate>Tue, 17 Jun 2025 09:26:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/740369227920658</guid></item><item><title>âš¡ FusionX Enhanced Wan 2.1 I2V (14B) ğŸ¬</title><link>https://huggingface.co/posts/seawolf2357/480409853177984</link><description>âš¡ FusionX Enhanced Wan 2.1 I2V (14B) ğŸ¬ ğŸš€ Revolutionary Image-to-Video Generation Model Generate cinematic-quality videos in just 8 steps! Heartsync/WAN2-1-fast-T2V-FusioniX âœ¨ Key Features ğŸ¯ Ultra-Fast Generation: Premium quality in just 8-10 steps ğŸ¬ Cinematic Quality: Smooth motion with detailed textures ğŸ”¥ FusionX Technology: Enhanced with CausVid + MPS Rewards LoRA ğŸ“ Optimized Resolution: 576Ã—1024 default settings âš¡ 50% Speed Boost: Faster rendering compared to base models ğŸ› ï¸ Technical Stack Base Model: Wan2.1 I2V 14B Enhancement Technologies: ğŸ”— CausVid LoRA (1.0 strength) - Motion modeling ğŸ”— MPS Rewards LoRA (0.7 strength) - Detail optimization Scheduler: UniPC Multistep (flow_shift=8.0) Auto Prompt Enhancement: Automatic cinematic keyword injection ğŸ¨ How to Use Upload Image - Select your starting image Enter Prompt - Describe desired motion and style Adjust Settings - 8 steps, 2-5 seconds recommended Generate - Complete in just minutes! ğŸ’¡ Optimization Tips âœ… Recommended Settings:...</description><pubDate>Tue, 17 Jun 2025 09:26:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/seawolf2357/480409853177984</guid></item><item><title>#CVPR2025 Paper Picks #1</title><link>https://huggingface.co/posts/merve/840226106342254</link><description>#CVPR2025 Paper Picks #1 VisionZip is a compression technique that reduces number of visual tokens to improve performance AND prefill time for vision language models demo: Senqiao/VisionZip paper: VisionZip: Longer is Better but Not Necessary in Vision Language Models (2412.04467) most of the image tokens are redundant for the LLM, so the authors ask "are all visual tokens necessary?" the method is simple: find which tokens have the highest attention score, merge rest of the tokens based on similarity, then merge both their method is both training-free and for fine-tuning the authors report 5 point improvement on average of vision language tasks + 8x improvement in prefilling time for Llava-Next 7B and 13B ğŸ¤¯ removing redundant tokens improve image token quality too ğŸ¥¹ See translation</description><pubDate>Tue, 17 Jun 2025 09:26:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/840226106342254</guid></item></channel></rss>