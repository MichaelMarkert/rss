<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Next level Realism with Qwen Image is now possible after new realism LoRA workflow - Top images are new realism workflow - Bottom ones are older default - Full tutorial published - 4+4 Steps only</title><link>https://huggingface.co/posts/MonsterMMORPG/695072229497754</link><description>Next level Realism with Qwen Image is now possible after new realism LoRA workflow - Top images are new realism workflow - Bottom ones are older default - Full tutorial published - 4+4 Steps only Tutorial of realism : https://youtu.be/XWzZ2wnzNuQ Tutorial of training : https://youtu.be/DPX3eBTuO_Y This is a full comprehensive step-by-step tutorial for how to train Qwen Image models. This tutorial covers how to do LoRA training and full Fine-Tuning / DreamBooth training on Qwen Image models. It covers both the Qwen Image base model and the Qwen Image Edit Plus 2509 model. This tutorial is the product of 21 days of full R&amp;D, costing over $800 in cloud services to find the best configurations for training. Furthermore, we have developed an amazing, ultra-easy-to-use Gradio app to use the legendary Kohya Musubi Tuner trainer with ease. You will be able to train locally on your Windows computer with GPUs with as little as 6 GB of VRAM for both LoRA and Fine-Tuning. See translation</description><pubDate>Mon, 17 Nov 2025 13:35:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/695072229497754</guid></item><item><title>I built a demo on how to implement Cache-Augmented Generation (CAG) in an LLM and compare its performance gains to RAG (111 stars, 20 forks).</title><link>https://huggingface.co/posts/ronantakizawa/762624768908636</link><description>I built a demo on how to implement Cache-Augmented Generation (CAG) in an LLM and compare its performance gains to RAG (111 stars, 20 forks). https://github.com/ronantakizawa/cacheaugmentedgeneration CAG preloads document content into an LLM‚Äôs context as a precomputed key-value (KV) cache. This caching eliminates the need for real-time retrieval during inference, reducing token usage by up to 76% while maintaining answer quality. CAG is particularly effective for constrained knowledge bases like internal documentation, FAQs, and customer support systems, where all relevant information can fit within the model's extended context window. #rag #retrievalaugmentedgeneration See translation</description><pubDate>Mon, 17 Nov 2025 13:35:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/762624768908636</guid></item><item><title>Hindi Speech to Text just crossed 20 million downloads. Grateful for everyone using it.</title><link>https://huggingface.co/posts/theainerd/926652286906905</link><description>Hindi Speech to Text just crossed 20 million downloads. Grateful for everyone using it. theainerd/Wav2Vec2-large-xlsr-hindi See translation</description><pubDate>Mon, 17 Nov 2025 13:35:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/theainerd/926652286906905</guid></item><item><title>12 Types of JEPA</title><link>https://huggingface.co/posts/Kseniase/762937246285628</link><description>12 Types of JEPA Since Yann LeCun together with Randall Balestriero released a new paper on JEPA (Joint-Embedding Predictive Architecture), laying out its theory and introducing an efficient practical version called LeJEPA, we figured you might need even more JEPA. Here are 7 recent JEPA variants plus 5 iconic ones: 1. LeJEPA ‚Üí LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics (2511.08544) Explains a full theory for JEPAs, defining the ‚Äúideal‚Äù JEPA embedding as an isotropic Gaussian, and proposes the SIGReg objective to push JEPA toward this ideal, resulting in practical LeJEPA 2. JEPA-T ‚Üí JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation (2510.00974) A text-to-image model that tokenizes images and captions with a joint predictive Transformer, enhances fusion with cross-attention and text embeddings before training loss, and generates images by iteratively denoising visual tokens conditioned on text 3. Text-JEPA ‚Üí...</description><pubDate>Mon, 17 Nov 2025 13:35:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/762937246285628</guid></item><item><title>Made a small write up and experimental finetuning guide for MetaCLIP2 for Image Classification on Downstream Tasks. The blog titled</title><link>https://huggingface.co/posts/prithivMLmods/462277719397337</link><description>Made a small write up and experimental finetuning guide for MetaCLIP2 for Image Classification on Downstream Tasks. The blog titled Fine Tuning MetaCLIP 2 for Image Classification on Downstream Tasks demonstrates the step by step finetuning using CIFAR10 and is also flexible for adapting to other datasets. For more details, check out the linked blog below. ü§ó‚ÜóÔ∏è ‚Æû Blog Article: https://huggingface.co/blog/prithivMLmods/metaclip2-downstream-finetune ‚Æû Demo Space[Zero-Shot Classification]: prithivMLmods/metaclip-2-demo Some other models ‚ï∞‚Ä∫ MetaCLIP-2-Cifar10: prithivMLmods/MetaCLIP-2-Cifar10 ‚ï∞‚Ä∫ MetaCLIP-2-Age-Range-Estimator: prithivMLmods/MetaCLIP-2-Age-Range-Estimator ‚ï∞‚Ä∫ MetaCLIP-2-Gender-Identifier: prithivMLmods/MetaCLIP-2-Gender-Identifier ‚ï∞‚Ä∫ MetaCLIP-2-Open-Scene: prithivMLmods/MetaCLIP-2-Open-Scene ‚Æû Collection: https://huggingface.co/collections/prithivMLmods/metaclip2-image-classification-experiments To know more about it, visit the app page or the respective model page! See...</description><pubDate>Mon, 17 Nov 2025 13:35:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/462277719397337</guid></item><item><title>Developing with ZeroGPU without a PRO account is painful. They give you so many requests at once, but then have like a 24 hour cooldown. I vote less requests in a batch, but then a shorter cooldown.</title><link>https://huggingface.co/posts/nroggendorff/877752190149689</link><description>Developing with ZeroGPU without a PRO account is painful. They give you so many requests at once, but then have like a 24 hour cooldown. I vote less requests in a batch, but then a shorter cooldown. or just less of a cooldown, but i understand if that is not allowed See translation</description><pubDate>Mon, 17 Nov 2025 13:35:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/877752190149689</guid></item><item><title>Fine-Tuning Qwen3 Embeddings for product category classification on the Large-Scale Product Corpus</title><link>https://huggingface.co/posts/aufklarer/281006786647431</link><description>Fine-Tuning Qwen3 Embeddings for product category classification on the Large-Scale Product Corpus Language-models such as GPT, Llama, DeepSeek, Qwen trained with a filtered slice of Common Crawl. For e-commerce work, though, we can start with the Web Data Commons (WDC), the project by the University of Mannheim. It extracts web pages that carry some metadata and publishes the result as the Large-Scale Product Corpus (LSPC). Search engines like Google reward pages that include detailed product markup, so merchants already populate their sites with SEO-friendly fields such as title, brand, GTIN, price ‚Äî and, crucially, category labels. Thanks to these built-in annotations, the WDC Large-Scale Product Corpus arrives almost fully self-labelled. I used those labels to fine-tune Qwen3 Embedding with Low-Rank Adaptation (LoRA), code is available on github. The resulting 615 million-parameter checkpoint fits comfortably in limited GPU memory yet updates the model‚Äôs representation space,...</description><pubDate>Mon, 17 Nov 2025 13:35:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aufklarer/281006786647431</guid></item><item><title>Hugging Face MCP Server v0.2.46</title><link>https://huggingface.co/posts/evalstate/966306150906160</link><description>Hugging Face MCP Server v0.2.46 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - Add "discover" to Dynamic Space tool. Recommend deselecting "space_search" if using dynamic spaces. See translation</description><pubDate>Mon, 17 Nov 2025 13:35:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/evalstate/966306150906160</guid></item><item><title>Implemented a proof of concept sampler in pure PyTorch and transformers.</title><link>https://huggingface.co/posts/grimjim/343972883837810</link><description>Implemented a proof of concept sampler in pure PyTorch and transformers. Max P consists of a dynamic token filter which applies Winsorization to cap the probabilties of top tokens. Specifically, a base probability in the range of [0,1] is used to cap individual token probability; the sampler then redistributes excess proportionally. https://github.com/jim-plus/maxp-sampler-poc Combined with Temperature and Min P, this could represent a more intuitive way of reducing repetition in text generation. See translation</description><pubDate>Mon, 17 Nov 2025 13:35:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/grimjim/343972883837810</guid></item><item><title>OCR has absolutely blown up in 2025, and honestly, my perspective on document processing has completely changed.</title><link>https://huggingface.co/posts/RakshitAralimatti/154491437278082</link><description>OCR has absolutely blown up in 2025, and honestly, my perspective on document processing has completely changed. This year has been wild. Vision Language Models like Nanonets OCR2-3B hit the scene and suddenly we're getting accuracy on complex forms (vs for traditional OCR). We're talking handwritten checkboxes, watermarked documents, multi-column layouts, even LaTeX equations all handled in a single pass.‚Äã The market numbers say it all: OCR accuracy passed 98% for printed text, AI integration is everywhere, and real-time processing is now standard. The entire OCR market is hitting $25.13 billion in 2025 because this tech actually works now. I wrote a detailed Medium article walking through: 1. Why vision LMs changed the game 2. NVIDIA NeMo Retriever architecture 3. Complete code breakdown 4. Real government/healthcare use cases 5. Production deployment guide Article: https://medium.com/@rakshitaralimatti2001/nvidia-nemo-retriever-ocr-building-document-intelligence-systems-for-...</description><pubDate>Mon, 17 Nov 2025 13:35:17 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/154491437278082</guid></item></channel></rss>