<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>We‚Äôve reached a point where on device AI coding that is free, offline, and capable isn‚Äôt just a theoretical possibility; it‚Äôs sitting on my lap, barely warming my thighs.</title><link>https://huggingface.co/posts/mitkox/598805408500117</link><description>We‚Äôve reached a point where on device AI coding that is free, offline, and capable isn‚Äôt just a theoretical possibility; it‚Äôs sitting on my lap, barely warming my thighs. My local MacBook Air setup includes a Qwen3 Coder Flash with a 1M context, Cline in a VSCode IDE. No internet, no cloud, no ID verification- this is the forbidden tech. Current stats: All agentic tools work great local, sandboxed, and MCP OK model output precision 17 tokens/sec. Not great, not terrible 65K tokens context, the model can do 1M, but let‚Äôs be real, my MacBook Air would probably achieve fusion before hitting that smoothly Standard backend and cache off for the test All inference and function calling happen locally, offline, untethered. The cloud didn‚Äôt even get a memo. See translation</description><pubDate>Sun, 03 Aug 2025 13:35:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/598805408500117</guid></item><item><title>Latest work on SWE-Bench üêõ</title><link>https://huggingface.co/posts/YerbaPage/437092763804097</link><description>Latest work on SWE-Bench üêõ Our two new papers from the SJTU &amp; Huawei: Powered by DeepSeek-V3, we've achieved a new SOTA on the SWE-Bench benchmark! We introduce two innovative approaches: ‚öîÔ∏è SWE-Debate: AI agents compete and "debate" to generate the best code fix. üß† SWE-Exp: An AI agent learns from past repair "experience" to solve new issues more efficiently. üëá Explore the future of software development: SWE-Debate üìÑ Paper: https://arxiv.org/abs/2507.23348 üíª Code: https://github.com/YerbaPage/SWE-Debate SWE-Exp üìÑ Paper: https://arxiv.org/abs/2507.23361 üíª Code: https://github.com/YerbaPage/SWE-Exp See translation</description><pubDate>Sun, 03 Aug 2025 13:35:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/437092763804097</guid></item><item><title>üöÄ Dhanishtha-2.0-preview-0825 Is Here</title><link>https://huggingface.co/posts/Abhaykoul/625756342268823</link><description>üöÄ Dhanishtha-2.0-preview-0825 Is Here The Intermediate Thinking Model just leveled up again. With sharper reasoning, better tool use, and expanded capabilities, Dhanishtha-2.0-preview-0825 is now live and ready to impress. üß† What Makes Dhanishtha Special? Unlike typical CoT models that only thinks one time, Dhanishtha thinks iteratively: &gt; Think ‚Üí Answer ‚Üí Rethink ‚Üí Improve ‚Üí Rethink again if needed. üîó Try it now: HelpingAI/Dhanishtha-2.0-preview-0825 üîû Dhanishtha NSFW Preview For those exploring more expressive and immersive roleplay scenarios, we‚Äôre also releasing: HelpingAI/Dhanishtha-nsfw A specialized version tuned for adult-themed interactions and character-driven roleplay. üîó Explore it here: HelpingAI/Dhanishtha-nsfw üí¨ You can also try all of these live at chat.helpingai.co See translation</description><pubDate>Sun, 03 Aug 2025 13:35:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/625756342268823</guid></item><item><title>We‚Äôre excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100.</title><link>https://huggingface.co/posts/chintankp/680096865746882</link><description>We‚Äôre excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100. Super v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. Besides leading accuracy, it also delivers high throughput. Key features: - Leading accuracy on multi-step reasoning, math, coding, and function-calling - Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples - Fully transparent training data on HF (Nemotron-Post-Training-Dataset-v1) Try Super v1.5 on build.nvidia.com or download from Hugging Face See translation</description><pubDate>Sun, 03 Aug 2025 13:35:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/chintankp/680096865746882</guid></item><item><title>Introducing Camel-Doc-OCR-080125(v2), a document content-structure retrieval VLM designed for content extraction and summarization. This is the second model in the Camel Doc OCR VLM series, following Camel-Doc-OCR-062825(v1). The new version fixes formal table reconstruction issues in both en and zh language, achieving optimal performance for long-context inferences.ü§óüê™</title><link>https://huggingface.co/posts/prithivMLmods/210600524016945</link><description>Introducing Camel-Doc-OCR-080125(v2), a document content-structure retrieval VLM designed for content extraction and summarization. This is the second model in the Camel Doc OCR VLM series, following Camel-Doc-OCR-062825(v1). The new version fixes formal table reconstruction issues in both en and zh language, achieving optimal performance for long-context inferences.ü§óüê™ ‚§∑ Camel-Doc-OCR(v2) : prithivMLmods/Camel-Doc-OCR-080125 ‚§∑ Camel-Doc-OCR(v1) : prithivMLmods/Camel-Doc-OCR-062825 ‚§∑ Demo : prithivMLmods/core-OCR Multimodal Model Collections and Spaces: ‚ûù Camel-Doc-OCR : prithivMLmods/camel-doc-ocr-080125-688c0c61c5dba648756f31f8 ‚ûù Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a ‚ûù Multimodal Spaces : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 ‚ûù Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 . . . To know more about it, visit the model card of the respective model. !! See...</description><pubDate>Sun, 03 Aug 2025 13:35:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/210600524016945</guid></item><item><title>Cohere just dropped</title><link>https://huggingface.co/posts/merve/770620321222949</link><description>Cohere just dropped CohereLabs/command-a-vision-07-2025 , a 112B (dense!) vision LM &gt; based on SigLIP2 &amp; Command-A &gt; built for enterprise use cases üî• &gt; use with Inference Providers or transformers ü§ó read their blog https://huggingface.co/blog/CohereLabs/introducing-command-a-vision-07-2025 See translation</description><pubDate>Sun, 03 Aug 2025 13:35:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/770620321222949</guid></item><item><title>Qwen team did it again!!</title><link>https://huggingface.co/posts/AdinaY/564352975503737</link><description>Qwen team did it again!! They just released Qwen3-Coder-30B-A3B-Instruct on the hubüî• Qwen/Qwen3-Coder-30B-A3B-Instruct ‚ú® Apache 2.0 ‚ú®30B total / 3.3B active (128 experts, 8 top-k) ‚ú® Native 256K context, extendable to 1M via Yarn ‚ú® Built for Agentic Coding See translation</description><pubDate>Sun, 03 Aug 2025 13:35:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/564352975503737</guid></item><item><title>üöÄ Launch Alert: Dev-Stack-Agents</title><link>https://huggingface.co/posts/Parveshiiii/913365249348370</link><description>üöÄ Launch Alert: Dev-Stack-Agents Meet your 50-agent senior AI team ‚Äî principal-level experts in engineering, AI, DevOps, security, product, and more ‚Äî all bundled into one modular repo. + Code. Optimize. Scale. Secure. - Full-stack execution, Claude-powered. No human bottlenecks. üîß Built for Claude Code Seamlessly plug into Claude‚Äôs dev environment: * üß† Each .md file = a fully defined expert persona * ‚öôÔ∏è Claude indexes them as agents with roles, skills &amp; strategy * ü§ñ You chat ‚Üí Claude auto-routes to the right agent(s) * ‚úçÔ∏è Want precision? Just call @agent-name directly * üë• Complex task? Mention multiple agents for team execution Examples: "@security-auditor please review auth flow for risks" "@cloud-architect + @devops-troubleshooter ‚Üí design a resilient multi-region setup" "@ai-engineer + @legal-advisor ‚Üí build a privacy-safe RAG pipeline" üîó https://github.com/Parveshiiii/Dev-Stack-Agents MIT License | Claude-Ready | PRs Welcome See translation</description><pubDate>Sun, 03 Aug 2025 13:35:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Parveshiiii/913365249348370</guid></item><item><title>Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, we‚Äôve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources.</title><link>https://huggingface.co/posts/prithivMLmods/923940739727688</link><description>Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, we‚Äôve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources. ‚ú® Multimodal-VLM-Thinking : prithivMLmods/Multimodal-VLM-Thinking ‚ú® Multimodal-VLM-OCR : prithivMLmods/Multimodal-VLM-OCR ‚ú¶ Models used in these spaces: ‚ú® Lumian-VLR-7B-Thinking : prithivMLmods/Lumian-VLR-7B-Thinking ‚ú® Enesidaon-VLR-7B-no-Thinking : prithivMLmods/Enesidaon-VLR-7B-no-Thinking ‚ú® GLM-4.1V-9B-Thinking : zai-org/GLM-4.1V-9B-Thinking ‚ú® DREX-062225-exp : prithivMLmods/DREX-062225-exp &amp; more ... ‚ú¶ Multimodal Model Collections and Spaces: ‚ú® Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a ‚ú® Multimodal Spaces : prithivMLmods/multimodal-...</description><pubDate>Sun, 03 Aug 2025 13:35:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/923940739727688</guid></item><item><title>Wan 2.2 &amp; FLUX Krea Full Tutorial - Automated Install - Ready Perfect Presets - SwarmUI with ComfyUI - Install Wan 2.2 and FLUX Krea with literally 1-click and use our pre-made most amazing quality presets :</title><link>https://huggingface.co/posts/MonsterMMORPG/425722660691765</link><description>Wan 2.2 &amp; FLUX Krea Full Tutorial - Automated Install - Ready Perfect Presets - SwarmUI with ComfyUI - Install Wan 2.2 and FLUX Krea with literally 1-click and use our pre-made most amazing quality presets : https://youtu.be/8MvvuX4YPeo https://youtu.be/8MvvuX4YPeo Video Chapters 0:00 Introduction: The Ultimate Wan 2.2 Tutorial with Optimized Presets 1:03 Free Prompt Generation Tool &amp; Introducing the New FLUX Krea Dev Model 2:01 How SwarmUI &amp; ComfyUI Enable Video Generation on Low-End Hardware 2:46 Quick Start Guide: Downloading the Latest SwarmUI &amp; ComfyUI Installers 3:10 Step-by-Step: How to Update or Perform a Fresh Installation of ComfyUI 3:51 Step-by-Step: How to Update or Perform a Fresh Installation of SwarmUI 4:18 Essential Setup: Configuring the SwarmUI Backend for ComfyUI 4:53 One-Click Setup: Downloading All Required Wan 2.2 Models Automatically 5:46 Importing the Ultimate SwarmUI Presets Pack for Best Results 6:22 Wan 2.2 Image-to-Video Generation: A Complete Step-by-...</description><pubDate>Sun, 03 Aug 2025 13:35:33 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/425722660691765</guid></item></channel></rss>