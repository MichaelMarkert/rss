<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>13 New types of LoRA</title><link>https://huggingface.co/posts/Kseniase/384482543815919</link><description>13 New types of LoRA LoRA (Low-Rank Adaptation) is a popular lightweight method for fine-tuning AI models. It doesn't update the full model, it adds small trainable components, low-rank matrices, while keeping the original weights frozen. Only these adapters are trained. Recently, many interesting new LoRA variations came out, so it‚Äôs a great time to take a look at these 13 clever approaches: 1. T-LoRA ‚Üí T-LoRA: Single Image Diffusion Model Customization Without Overfitting (2507.05964) A timestep-dependent LoRA method for adapting diffusion models with a single image. It dynamically adjusts updates and uses orthogonal initialization to reduce overlap, achieving better fidelity‚Äìalignment balance than standard LoRA 2. SingLoRA ‚Üí SingLoRA: Low Rank Adaptation Using a Single Matrix (2507.05566) Simplifies LoRA by using only one small matrix instead of usual two, and multiplying it by its own transpose (like A √ó A·µÄ). It uses half the parameters of LoRA and avoids scale mismatch between...</description><pubDate>Tue, 15 Jul 2025 13:39:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/384482543815919</guid></item><item><title>Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. ü§óüìÑ</title><link>https://huggingface.co/posts/prithivMLmods/700925755780035</link><description>Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. ü§óüìÑ Spaces &amp; Models : &gt; Doc-VLMs-OCR : prithivMLmods/Doc-VLMs-OCR &gt; core-OCR : prithivMLmods/core-OCR &gt; Megalodon-OCR (3B) : prithivMLmods/Megalodon-OCR-Sync-0713 &gt; Perseus-Doc-vl (7B): prithivMLmods/Perseus-Doc-vl-0712 Datasets Caption Mix : &gt; Corvus-OCR-Caption-Mix : prithivMLmods/Corvus-OCR-Caption-Mix &gt; Corvus-OCR-Caption-Mini-Mix : prithivMLmods/Corvus-OCR-Caption-Mini-Mix Collections : &gt; Corvus OCR Caption Mix: prithivMLmods/corvus-ocr-caption-mix-687349bfaceffbd10976f0cc &gt; Captioning / OCR / DocTable : prithivMLmods/captioning-ocr-doctable-687382e1da822008bb5c06f2 GitHub : &gt; OCR-ReportLab :...</description><pubDate>Tue, 15 Jul 2025 13:39:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/700925755780035</guid></item><item><title>üôãüèª‚Äç‚ôÇÔ∏è Normalize adding compute &amp; runtime traces to your model cards</title><link>https://huggingface.co/posts/Tonic/414083244384754</link><description>üôãüèª‚Äç‚ôÇÔ∏è Normalize adding compute &amp; runtime traces to your model cards See translation</description><pubDate>Tue, 15 Jul 2025 13:39:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Tonic/414083244384754</guid></item><item><title>Made some 245GB (80% size reduction) 1.8bit quants for Kimi K2!</title><link>https://huggingface.co/posts/danielhanchen/383539577783045</link><description>Made some 245GB (80% size reduction) 1.8bit quants for Kimi K2! unsloth/Kimi-K2-Instruct-GGUF See translation</description><pubDate>Tue, 15 Jul 2025 13:39:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/383539577783045</guid></item><item><title>"Why did the bee get married?"</title><link>https://huggingface.co/posts/jasoncorkill/847126227827487</link><description>"Why did the bee get married?" "Because he found his honey!" This was the "funniest" joke out of 10'000 jokes we generated with LLMs. With 68% of respondents rating it as "funny". Original jokes are particularly hard for LLMs, as jokes are very nuanced and a lot of context is needed to understand if something is "funny". Something that can only reliably be measured using humans. LLMs are not equally good at generating jokes in every language. Generated English jokes turned out to be way funnier than the Japanese ones. 46% of English-speaking voters on average found the generated joke funny. The same statistic for other languages: Vietnamese: 44% Portuguese: 40% Arabic: 37% Japanese: 28% There is not much variance in generation quality among models for any fixed language. But still Claude Sonnet 4 slightly outperforms others in Vietnamese, Arabic and Japanese and Gemini 2.5 Flash in Portuguese and English We have release the 1 Million (!) native speaker ratings and the 10'000 jokes...</description><pubDate>Tue, 15 Jul 2025 13:39:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/847126227827487</guid></item><item><title>MultiTalk Levelled Up - Way Better Animation Compared to Before with New Workflows - Image to Video &gt;</title><link>https://huggingface.co/posts/MonsterMMORPG/139363304280237</link><description>MultiTalk Levelled Up - Way Better Animation Compared to Before with New Workflows - Image to Video &gt; https://youtu.be/wgCtUeog41g MultiTalk is greatly upgraded. After doing more than 1 day more research with MultiTalk by using 8x A6000 48 GB GPUs, I have significantly improved the MultiTalk workflows and now I am sharing 4 different category workflows with you. VRAM usages and speeds are same but just better quality and animation. Moreover I am introducing a new app which is image and video comparison sliders. Ultra fast and lightweight. Runs as a html app and no GPU is required. https://youtu.be/wgCtUeog41g MultiTalk Full Tutorial With 1-Click Installer - Make Talking and Singing Videos From Static Images &gt; https://youtu.be/8cMIwS9qo4M By using MeiGen MultiTalk you can generate amazing fully animated real-like videos from given audio input. Not only talking but also animating the body movements is possible. In this video I will show you how to install ComfyUI on Windows and...</description><pubDate>Tue, 15 Jul 2025 13:39:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/139363304280237</guid></item><item><title>üì¢ Generate your own data in simulation using two new free and customizable data-generating Scenarios on Duality's FalconCloud service.</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/377698831818015</link><description>üì¢ Generate your own data in simulation using two new free and customizable data-generating Scenarios on Duality's FalconCloud service. üôå These multi-class Scenarios are designed to target model weaknesses for our recent Kaggle competition, but they are free to anyone for non-commercial use! üì∏ Control object and camera posing üëâ Select random variable ranges üñºÔ∏è Set post-processing effects ‚ûï and more to create a robust dataset for strong model training. Access the 2 Scenarios here: üí† https://falcon.duality.ai/secure/scenarios/edit/9e90e036-8af9-41e4-8af0-1343b8e8f467?utm_source=Kaggle&amp;utm_medium=post&amp;utm_campaign=competition_4 üí† https://falcon.duality.ai/secure/scenarios/edit/e3294c19-49d4-4f64-9ca8-8373876c2c94?utm_source=Kaggle&amp;utm_medium=post&amp;utm_campaign=competition_4 See translation</description><pubDate>Tue, 15 Jul 2025 13:39:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/377698831818015</guid></item><item><title>Kimi-K2 is now available on the hubüî•üöÄ</title><link>https://huggingface.co/posts/AdinaY/423045666935241</link><description>Kimi-K2 is now available on the hubüî•üöÄ This is a trillion-parameter MoE model focused on long context, code, reasoning, and agentic behavior. moonshotai/kimi-k2-6871243b990f2af5ba60617d ‚ú® Base &amp; Instruct ‚ú® 1T total / 32B active - Modified MIT License ‚ú® 128K context length ‚ú® Muon optimizer for stable trillion-scale training See translation</description><pubDate>Tue, 15 Jul 2025 13:39:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/423045666935241</guid></item><item><title>past week had huuuge releases üíó</title><link>https://huggingface.co/posts/merve/294233425076045</link><description>past week had huuuge releases üíó here's our picks üî• find more models, datasets, demos here merve/releases-july-11-68750452c358c98b0fa663f7 &gt; moonshotai/Kimi-K2-Instruct is the new sota LLM with 1T total 32B active parameters ü§Ø &gt; HuggingFaceTB/SmolLM3-3B is the new best LM for it's size, offers thinking mode üí≠ as well as the dataset HuggingFaceTB/smoltalk2 &gt; Alibaba-NLP/WebSailor-3B is the new agentic LLM for complex browsing &gt; Google DeepMind released medical vision LMs with an agentic doctor-patient app google/medgemma-release-680aade845f90bec6a3f60c4 &gt; fal released a LoRA to improve details on face images fal/Realism-Detailer-Kontext-Dev-LoRA See translation</description><pubDate>Tue, 15 Jul 2025 13:39:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/294233425076045</guid></item><item><title>üéØ Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science!</title><link>https://huggingface.co/posts/DawnC/760622875415705</link><description>üéØ Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science! This isn't just another computer vision project. VisionScout represents a fundamental shift from simple object detection to genuine scene understanding, where four specialized AI models work together to interpret what's actually happening in an image. üèóÔ∏è Part 1: Architecture Foundation How careful system design transforms independent models into collaborative intelligence through proper layering and coordination strategies. ‚öôÔ∏è Part 2: Deep Technical Implementation The five core algorithms powering the system: dynamic weight adjustment, attention mechanisms, statistical methods, lighting analysis, and CLIP's zero-shot learning. üåç Part 3: Real-World Validation Concrete case studies from indoor spaces to cultural landmarks, demonstrating how integrated systems deliver insights no single model could achieve. What makes this valuable:...</description><pubDate>Tue, 15 Jul 2025 13:39:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/760622875415705</guid></item></channel></rss>