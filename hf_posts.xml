<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>I have concluded first 8 traininings of Qwen Image LoRA - we are not at the level of FLUX yet and next 8 trainings starting hopefully - 2656x2656px image generated with 8 steps Fast Qwen LoRA + myself trained LoRA :</title><link>https://huggingface.co/posts/MonsterMMORPG/683040638338113</link><description>I have concluded first 8 traininings of Qwen Image LoRA - we are not at the level of FLUX yet and next 8 trainings starting hopefully - 2656x2656px image generated with 8 steps Fast Qwen LoRA + myself trained LoRA : Grid test results shared here along with App installer : https://www.patreon.com/posts/137551634 See translation</description><pubDate>Wed, 03 Sep 2025 05:20:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/683040638338113</guid></item><item><title>It’s absolutely mind blowing - the work Dynamics Lab is doing!!</title><link>https://huggingface.co/posts/Jaward/864148450814843</link><description>It’s absolutely mind blowing - the work Dynamics Lab is doing!! With just a single input image and in a few seconds, their new world engine model (Mirage 2) can generate a whole new interactive world that’s physics informed and fully explorable in real-time🤯 Try it yourself: https://demo.dynamicslab.ai/chaos See translation</description><pubDate>Wed, 03 Sep 2025 05:20:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/864148450814843</guid></item><item><title>large AI labs have dropped so many open models last week 🔥 don't miss out on them</title><link>https://huggingface.co/posts/merve/771481819901416</link><description>large AI labs have dropped so many open models last week 🔥 don't miss out on them → Apple released on-device vision LMs apple/fastvlm-68ac97b9cd5cacefdd04872e &amp; apple/mobileclip2-68ac947dcb035c54bcd20c47 → OpenGVLab released InternVL3.5, 32 new vision LMs with one based on gpt-oss! (OS) OpenGVLab/internvl35-68ac87bd52ebe953485927fb → MSFT released a killer small TTS model (OS) microsoft/VibeVoice-1.5B find more herehttps://huggingface.co/collections/merve/august-29-releases-68b5a3754cfb8abf59e2b486 See translation</description><pubDate>Wed, 03 Sep 2025 05:20:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/771481819901416</guid></item><item><title>Supercharge Apple’s Shortcuts using Cloudflare Workers and Gemini within minutes (and for free, up to 1,500 requests per day) ☁️✨</title><link>https://huggingface.co/posts/louisbrulenaudet/591445663705551</link><description>Supercharge Apple’s Shortcuts using Cloudflare Workers and Gemini within minutes (and for free, up to 1,500 requests per day) ☁️✨ Hello everyone, last week, while experimenting for fun, I created an API that allows you to easily access AI models (in this case, Google's) from the Shortcut app in order to analyze data from my apps and make the most of it thanks to the generative capabilities of advanced models. It costs me nothing, and I think it might be good to share it so that others can build on it. In README.md, you will find everything you need to get started and put your own microservice into production, which you can call from the app’s HTTP request features. You will simply be asked to have a free Cloudflare account and an API key obtained from Google's AI Studio. Feel free to take a look and get back to me if you encounter any problems during deployment. Here is the GitHub repo where you can find all the source code and run it on your own:...</description><pubDate>Wed, 03 Sep 2025 05:20:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/louisbrulenaudet/591445663705551</guid></item><item><title>PawMatchAI — Now with SBERT-Powered Recommendations! 🐶✨</title><link>https://huggingface.co/posts/DawnC/381537695345047</link><description>PawMatchAI — Now with SBERT-Powered Recommendations! 🐶✨ ⭐️ NEW: Description-based recommendations are here! Just type in your lifestyle or preferences (e.g. “I live in an apartment and want a quiet dog”), and PawMatchAI uses SBERT semantic embeddings to understand your needs and suggest compatible breeds. What can PawMatchAI do today? 📸 Upload a photo to identify your dog from 124 breeds with detailed info. ⚖️ Compare two breeds side-by-side, from grooming needs to health insights. 📊 Visualize breed traits with radar and comparison charts. 🎨 Try Style Transfer to turn your dog’s photo into anime, watercolor, cyberpunk, and more. What’s next? 🎯 More fine-tuned recommendations. 📱 Mobile-friendly deployment. 🐾 Expansion to additional species. My goal: To make breed discovery not only accurate but also interactive and fun — combining computer vision, semantic understanding, and creativity to help people find their perfect companion. 👉 Try it here: DawnC/PawMatchAI If you enjoy...</description><pubDate>Wed, 03 Sep 2025 05:20:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/381537695345047</guid></item><item><title>🌲🍄 LLM Forest Orchestra: Turning Hidden States into Music</title><link>https://huggingface.co/posts/Locutusque/640139873710354</link><description>🌲🍄 LLM Forest Orchestra: Turning Hidden States into Music Hello everyone! I'm excited to introduce a new Space I've been developing called LLM Forest Orchestra. This project converts the hidden states and attention patterns of transformer models into layered MIDI compositions. The concept draws inspiration from mushrooms and mycelial networks in forests. Fungi create underground connections linking plants and trees, establishing what some call a "wood-wide web" where signals and nutrients travel. Researchers have discovered that these exchanges form patterns resembling rhythms and pulses. When translated appropriately, these patterns can become music. Transformers operate through remarkably similar principles: tokens share signals via hidden states and attention heads. This Space transforms those invisible information flows into notes, chords, and rhythms, treating the model as a digital forest orchestra. 🎛 Features * Two compute modes: - Full model operates on a Hugging Face model...</description><pubDate>Wed, 03 Sep 2025 05:20:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Locutusque/640139873710354</guid></item><item><title>Okay this is insane... WebGPU-accelerated semantic video tracking, powered by DINOv3 and Transformers.js! 🤯</title><link>https://huggingface.co/posts/Xenova/448209562329557</link><description>Okay this is insane... WebGPU-accelerated semantic video tracking, powered by DINOv3 and Transformers.js! 🤯 Demo (+ source code): webml-community/DINOv3-video-tracking This will revolutionize AI-powered video editors... which can now run 100% locally in your browser, no server inference required (costs $0)! 😍 How does it work? 🤔 1️⃣ Generate and cache image features for each frame 2️⃣ Create a list of embeddings for selected patch(es) 3️⃣ Compute cosine similarity between each patch and the selected patch(es) 4️⃣ Highlight those whose score is above some threshold ... et voilà! 🥳 You can also make selections across frames to improve temporal consistency! This is super useful if the object changes its appearance slightly throughout the video. Excited to see what the community builds with it! See translation</description><pubDate>Wed, 03 Sep 2025 05:20:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Xenova/448209562329557</guid></item><item><title>FastVLMs by Apple are the talk of the week for edge device VLMs and also for consumer-grade VLMs on the Hub. They have some impressive demos available on the Hub for live captioning and inference tasks. Meanwhile, I’m still exploring one of the coolest edge-device multimodal releases—Liquid AI’s LFM2-VL (450M and 1.6B). I’ve also made a live camera video inference demo, which is capable of running on Colab’s free-tier T4 GPU.</title><link>https://huggingface.co/posts/prithivMLmods/632863448558657</link><description>FastVLMs by Apple are the talk of the week for edge device VLMs and also for consumer-grade VLMs on the Hub. They have some impressive demos available on the Hub for live captioning and inference tasks. Meanwhile, I’m still exploring one of the coolest edge-device multimodal releases—Liquid AI’s LFM2-VL (450M and 1.6B). I’ve also made a live camera video inference demo, which is capable of running on Colab’s free-tier T4 GPU. 🤗Live Captioning Notebooks: ➠ LiquidAI LFM2 VL 1.6B Live Cam: https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-Notebooks/blob/main/LiquidAI-LFM2-VL-Live-Cam/LiquidAI_LFM2_VL_1_6B_Live_Cam.ipynb ➠ LiquidAI LFM2 VL 450M Live Cam: https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-Notebooks/blob/main/LiquidAI-LFM2-VL-Live-Cam/LiquidAI_LFM2_VL_450M_Live_Cam.ipynb ✨I also made a demo for the FastVLM Live Captioning Notebook. ➠ FastVLM 0.5B Live Cam: https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-Notebooks/blob/main/Apple-FastVLM-0.5B-Live-...</description><pubDate>Wed, 03 Sep 2025 05:20:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/632863448558657</guid></item><item><title>Why am I amassing image features using seed 42?</title><link>https://huggingface.co/posts/AbstractPhil/395520128333007</link><description>Why am I amassing image features using seed 42? Simply put; training something with features gives a fair representative of the learning that you would get from running a model that has some random chance - using a single seed. Training with features does not need to wait for the representative model to actually generate; since you already generated everything ahead of time. Features are rich and utilizable within the spectrum of similarity assessments, classification accuracy, mass-deterministic normalization checks, and more. They are... put simply... exponentially faster and reusable for research. I'll include the notebooks used for imagenet and cifar100; as the cifar100 is much simpler since the cifar100 is much... smaller, I required less innovation. Imagenet is another beast though. This imagenet notebook is capable of running against much larger datasets with a few tweaks. clip-vit-bigG's imagenet feature set is complete, which means we're almost ready for full ablation. Note...</description><pubDate>Wed, 03 Sep 2025 05:20:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AbstractPhil/395520128333007</guid></item><item><title>Every time I run DEV Mode it stops my build from building its container. When deactivated my build runs normally. Why can't Dev Mode be a separate function from the container itself, and not run from the organization page or settings?</title><link>https://huggingface.co/posts/KingOfThoughtFleuren/373726456596353</link><description>Every time I run DEV Mode it stops my build from building its container. When deactivated my build runs normally. Why can't Dev Mode be a separate function from the container itself, and not run from the organization page or settings? See translation</description><pubDate>Wed, 03 Sep 2025 05:20:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/KingOfThoughtFleuren/373726456596353</guid></item></channel></rss>