<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Say hello to</title><link>https://huggingface.co/posts/Wauplin/921235032674409</link><description>Say hello to hf : a faster, friendlier Hugging Face CLI ‚ú® We are glad to announce a long-awaited quality-of-life improvement: the Hugging Face CLI has been officially renamed from huggingface-cli to hf! So... why this change? Typing huggingface-cli constantly gets old fast. More importantly, the CLI‚Äôs command structure became messy as new features were added over time (upload, download, cache management, repo management, etc.). Renaming the CLI is a chance to reorganize commands into a clearer, more consistent format. We decided not to reinvent the wheel and instead follow a well-known CLI pattern: hf &lt;resource&gt; &lt;action&gt;. Isn't hf auth login easier to type and remember? The full rationale, implementation details, and migration notes are in the blog post: https://huggingface.co/blog/hf-cli See translation</description><pubDate>Sun, 27 Jul 2025 09:26:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Wauplin/921235032674409</guid></item><item><title>Is it possible to apply for a resources grant for a whole organization, or do you need to apply for each repo individually? I think it'd be pretty cool to have something like the discord-community org for None-yet in terms of resource allocation (multiple spaces running on</title><link>https://huggingface.co/posts/nroggendorff/692994583869726</link><description>Is it possible to apply for a resources grant for a whole organization, or do you need to apply for each repo individually? I think it'd be pretty cool to have something like the discord-community org for None-yet in terms of resource allocation (multiple spaces running on cpu upgrade . I realize the scale of the community is just a tiny bit different, and that having this for a public org (one where anyone can join) isn't super fiscally responsible, but we'll be good. I promise we will! Right, guys? See translation</description><pubDate>Sun, 27 Jul 2025 09:26:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/692994583869726</guid></item><item><title>Excited to introduce the new experimental model "Qwen2.5-VL-7B-Abliterated-Caption-it", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.üß™ü§ó</title><link>https://huggingface.co/posts/prithivMLmods/432897219160306</link><description>Excited to introduce the new experimental model "Qwen2.5-VL-7B-Abliterated-Caption-it", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.üß™ü§ó ‚ú® Try the demo here : prithivMLmods/Qwen2.5-VL ‚ú® Qwen2.5-VL-7B-Abliterated-Caption-it : prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it ‚ú® Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 ‚ú® Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Sun, 27 Jul 2025 09:26:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/432897219160306</guid></item><item><title>Introducing Voxtral WebGPU: State-of-the-art audio transcription directly in your browser! ü§Ø</title><link>https://huggingface.co/posts/Xenova/793837995432659</link><description>Introducing Voxtral WebGPU: State-of-the-art audio transcription directly in your browser! ü§Ø üó£Ô∏è Transcribe videos, meeting notes, songs and more üîê Runs on-device, meaning no data is sent to a server üåé Multilingual (8 languages) ü§ó Completely free (forever) &amp; open source That's right, we're running Mistral's new Voxtral-Mini-3B model 100% locally in-browser on WebGPU, powered by Transformers.js and ONNX Runtime Web! üî• Try it out yourself! üëá webml-community/Voxtral-WebGPU See translation</description><pubDate>Sun, 27 Jul 2025 09:26:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Xenova/793837995432659</guid></item><item><title>üöÄ Deca 3 Ultra Alpha is coming in the next 72 hours! üöÄ</title><link>https://huggingface.co/posts/Blazgo/275361717193984</link><description>üöÄ Deca 3 Ultra Alpha is coming in the next 72 hours! üöÄ We're on the verge of something monumental. Right now, we're in the final stages of testing, and we're about to drop a game-changing milestone in the open-source AI community. üéâ In just two weeks, we've managed to almost 4x the size of the largest open-source LLM at that time (and we are still 2.6x bigger than the largest LLM). This is unprecedented and a testament to the power of collaboration, innovation, and the relentless pursuit of pushing AI to its limits. The future of open-source AI is now. Stay tuned for the release ‚Äì we‚Äôre just getting started. - Model testing finishes: 24hrs from now - Model gets uploaded: 30hrs from now - Related code/inference stack gets published: 70-90hrs from now See translation</description><pubDate>Sun, 27 Jul 2025 09:26:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Blazgo/275361717193984</guid></item><item><title>NEW ARTICLE: "Detecting Beyond Sight: Building AI-Enabled SAR Intelligence with Synthetic Data"</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/822376760397275</link><description>NEW ARTICLE: "Detecting Beyond Sight: Building AI-Enabled SAR Intelligence with Synthetic Data" Synthetic Aperture Radar (SAR) reveals what optical sensors can‚Äôt. AI can turn that information into actionable intelligence‚Äîbut only with the right training data. In our latest blog, we explore how Falcon‚Äôs new virtual SAR sensor solves the SAR data bottleneck for AI development. As the newest addition to Falcon‚Äôs sensor library, it models radar returns with precision‚Äîincluding azimuth, range resolution, signal intensity, and noise. This Falcon-specific, GPU-accelerated raytraced SAR model is exposed via Falcon‚Äôs Python API, giving teams precise, control over radar wave propagation and enabling physically grounded, highly customizable, and user-friendly SAR simulation. The result? High-fidelity, automatically labeled synthetic SAR imagery from any scenario‚Äîon demand. No custom setup. No external workflows. Just mission-ready data for building AI models across defense, disaster response,...</description><pubDate>Sun, 27 Jul 2025 09:26:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/822376760397275</guid></item><item><title>Big respect to the Qwen team! They just dropped another modelüî•</title><link>https://huggingface.co/posts/AdinaY/243707766122533</link><description>Big respect to the Qwen team! They just dropped another modelüî• Qwen3-235B-A22B-Thinking-2507 üß† new reasoning model by Qwen Qwen/Qwen3-235B-A22B-Thinking-2507 ‚ú® 235B total / 22B active (8 experts) ‚ú® 256K context window ‚ú® Agent-ready with tool use &amp; &lt;think&gt; reasoning mode Hope the team gets some well-deserved rest this weekend after all the massive releases üôå See translation</description><pubDate>Sun, 27 Jul 2025 09:26:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/243707766122533</guid></item><item><title>longer context doesn't generate better responses. it can even hurt your llm/agent.  1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it.</title><link>https://huggingface.co/posts/hesamation/830297477341251</link><description>longer context doesn't generate better responses. it can even hurt your llm/agent. 1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it. here are 4 types of context failure and why each one happens: 1. context poisoning: if hallucination finds its way into your context, the agent will rely on that false information to make its future moves. for example if the agent hallucinates about the "task description", all of its planning to solve the task would also be corrupt. 2. context distraction: when the context becomes too bloated, the model focuses too much on it rather than come up with novel ideas or to follow what it has learned during training. as Gemini 2.5 Pro technical report points out, as context grows significantly from 100K tokens, "the agent showed a tendency toward favoring repeating actions from its vast history rather than synthesizing novel plans". 3. context confusion: everyone lost it when MCPs became popular, it...</description><pubDate>Sun, 27 Jul 2025 09:26:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/830297477341251</guid></item><item><title>I run Qwen3-Coder 480B locally on my Z8, with a 1-million token context window. It‚Äôs the equivalent of parallel-parking a Nimitz-class carrier in a kiddie pool. Thanks to whatever dark pact the llama.cpp, CUDA, and kernel folks signed, hybrid inferencing + VRAM‚ÜîRAM offload let me stream the model‚Äôs synapses across Xeon, RAM, and four lonely A6000s without summoning either the OOM killer or a small house fire.</title><link>https://huggingface.co/posts/mitkox/940300076081193</link><description>I run Qwen3-Coder 480B locally on my Z8, with a 1-million token context window. It‚Äôs the equivalent of parallel-parking a Nimitz-class carrier in a kiddie pool. Thanks to whatever dark pact the llama.cpp, CUDA, and kernel folks signed, hybrid inferencing + VRAM‚ÜîRAM offload let me stream the model‚Äôs synapses across Xeon, RAM, and four lonely A6000s without summoning either the OOM killer or a small house fire. See translation</description><pubDate>Sun, 27 Jul 2025 09:26:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/940300076081193</guid></item><item><title>Implemented Test-Time Diffusion Deep Researcher (TTD-DR) in OptiLLM! üöÄ</title><link>https://huggingface.co/posts/codelion/260744865592486</link><description>Implemented Test-Time Diffusion Deep Researcher (TTD-DR) in OptiLLM! üöÄ Just shipped a game-changing feature that turns any LLM into a powerful research agent. TTD-DR applies diffusion-inspired techniques to iteratively refine research reports while grounding them in real web sources. How it works: ‚Ä¢ Generates initial draft ‚Ä¢ Identifies knowledge gaps ‚Ä¢ Searches web for missing info ‚Ä¢ Iteratively refines through "denoising" steps ‚Ä¢ Produces comprehensive reports with 15-30+ sources The magic? It works with ANY model so you can choose your favorite open-source models on HF! Key results: - 47 complex research queries tested - Every report backed by real web sources - Quality rivals human research analysts - No more hallucinations on current events! Try it: pip install optillm Then use "deep_research-your-model-name" as the model identifier - Implementation: https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research - Paper: https://arxiv.org/abs/2507.16075v1 - Sample...</description><pubDate>Sun, 27 Jul 2025 09:26:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/260744865592486</guid></item></channel></rss>