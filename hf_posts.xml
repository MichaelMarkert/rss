<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥</title><link>https://huggingface.co/posts/nouamanetazi/972464132222376</link><description>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥ Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious ğğ‚ğ‚ğ‹ ğğ«ğ«ğ¨ğ«ğ¬, or when your expensive GPU cluster is running at ğŸ”ğŸ% ğğŸğŸğ¢ğœğ¢ğğ§ğœğ², the problem isn't your model. It's most probably a ğ¦ğ¢ğ¬ğ®ğ¬ğ ğ¨ğŸ ğ­ğ¡ğ ğ¡ğšğ«ğğ°ğšğ«ğ. ğŸ› ï¸ Questions that seemed simple but had no clear answers: Why is ğŒğ¨ğ„ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğ¬ğ¥ğ¨ğ°ğğ« ğ­ğ¡ğšğ§ ğğğ§ğ¬ğ ğ¦ğ¨ğğğ¥ğ¬? Which ğğ‚ğ‚ğ‹ ğŸğ¥ğšğ ğ¬ should we actually set? How often should we checkpoint without killing throughput? That's why we built ğ“ğ¡ğ ğ’ğ¦ğ¨ğ¥ ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğğ¥ğšğ²ğ›ğ¨ğ¨ğ¤ ğŸ“–: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¥ğšğ²ğğ« that most teams get wrong. We validated real vs...</description><pubDate>Sun, 02 Nov 2025 17:18:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nouamanetazi/972464132222376</guid></item><item><title>Multilingual Tokenization Showdown</title><link>https://huggingface.co/posts/Norod78/977626760436669</link><description>Multilingual Tokenization Showdown Analyzing 12 LLM Tokenizers Across 204 Languages. First, I've created a dataset with Wikipedia's "Cat" article text in 272 languages: Norod78/WikiCat-Multilingual For each language entry with at least 100 words, I tokenized the text using 12 tokenizers and calculated the "Characters per token" ratio and "Word per token" ratio. The higher this ratio is, the more information each token represents on average for that language (and perhaps allowing the llm to potentially learn more per-parameter if trained on a dataset of that language). You can see a slideshow summary of the results here: https://norod.github.io/wikicat-tokenizer-eval/tokenizer-slideshow.html I hope I interpreted the results correctly, I've made the code available on GitHub so you can re-create the raw results jsonl with this repo: https://github.com/Norod/wikicat-tokenizer-eval Post on X: https://x.com/Norod78/status/1984366900550266999 See translation</description><pubDate>Sun, 02 Nov 2025 17:18:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Norod78/977626760436669</guid></item><item><title>ğŸš€ğŸ‘ŒğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ¤ŒğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/744756733617336</link><description>ğŸš€ğŸ‘ŒğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ¤ŒğŸš€ ğŸ“„ Title: Understanding Co-speech Gestures in-the-wild ğŸ” ğŸ“ Description: JEGAL is a tri-modal model that learns from gestures, speech and text simultaneously, enabling devices to interpret co-speech gestures in the wild. ğŸ‘¥ Authors: @ sindhuhegde , K R Prajwal, Taein Kwon, and Andrew Zisserman ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: Understanding Co-speech Gestures in-the-wild (2503.22668) ğŸŒ Web Page: https://www.robots.ox.ac.uk/~vgg/research/jegal ğŸ“ Repository: https://github.com/Sindhu-Hegde/jegal ğŸ“º Video: https://www.youtube.com/watch?v=TYFOLKfM-rM ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the Human Modeling Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/human-modeling.md ğŸ“š More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers curated by @ DmitryRyumin ğŸ” Keywords:...</description><pubDate>Sun, 02 Nov 2025 17:18:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/744756733617336</guid></item><item><title>*** Happy Halloween - Embrace the Horror ! ***</title><link>https://huggingface.co/posts/DavidAU/433692013361833</link><description>*** Happy Halloween - Embrace the Horror ! *** Unsloth fine tunes using in house horror dataset. Gemma 3 - 1B, 4B, two 12Bs and 27B (uploaded yesterday) Qwen 3 - 1.7B [two] - new today... and , 4B, 6B, 42B ... And 32 MORE horror models: https://huggingface.co/DavidAU/models?search=horror Collection: https://huggingface.co/collections/DavidAU/grand-horror-165b-horror-and-fiction-generation Enjoy ; See translation</description><pubDate>Sun, 02 Nov 2025 17:18:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DavidAU/433692013361833</guid></item><item><title>Starts erasing! ğŸ‰ ğŸ‰ ğŸ‰</title><link>https://huggingface.co/posts/piercus/167394123498038</link><description>Starts erasing! ğŸ‰ ğŸ‰ ğŸ‰ This is made with a one-step SD1.5 LBM [1] eraser ! Data is open. Data pipeline is open. Training code is open. On our LBM fork : https://github.com/finegrain-ai/LBM [1] LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) See translation</description><pubDate>Sun, 02 Nov 2025 17:18:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/piercus/167394123498038</guid></item><item><title>A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on â†—ï¸</title><link>https://huggingface.co/posts/prithivMLmods/710644146568512</link><description>A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on â†—ï¸ https://huggingface.co/blog/prithivMLmods/multimodal-ocr-vlms on behalf of strangervisionhf It discusses the latest trends in OCR models, the multilingual support offered by modern OCR systems, their unique capabilities, OCR benchmark model comparisons, transformer-based implementations, and strategies for streamlining transformers compatibility. See translation</description><pubDate>Sun, 02 Nov 2025 17:18:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/710644146568512</guid></item><item><title>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the</title><link>https://huggingface.co/posts/sergiopaniego/207791817757812</link><description>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the @ TogetherAgent x @ huggingface workshop we hosted in our Paris office ğŸƒ! Link: https://github.com/sergiopaniego/talks/blob/main/fine_tuning_with_trl/Fine%20tuning%20with%20TRL%20(Oct%2025).pdf See translation</description><pubDate>Sun, 02 Nov 2025 17:18:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/207791817757812</guid></item><item><title>I am dedicating this weekend to practicing/reading the latest b(ook)log from hugging face. It is meant to be a guide for anyone trying to go from â€œwe have a great dataset and GPUsâ€ to â€œwe built a really strong model.â€ Will share thoughts upon completion.</title><link>https://huggingface.co/posts/Shivansh000/941986646578616</link><description>I am dedicating this weekend to practicing/reading the latest b(ook)log from hugging face. It is meant to be a guide for anyone trying to go from â€œwe have a great dataset and GPUsâ€ to â€œwe built a really strong model.â€ Will share thoughts upon completion. Thanks for the treat @ eliebak @ ThomasWolf and HF team! HuggingFaceTB/smol-training-playbook See translation</description><pubDate>Sun, 02 Nov 2025 17:18:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Shivansh000/941986646578616</guid></item><item><title>Introducing the Medical-o1-Reasoning-SFT-Japanese dataset ğŸ‰</title><link>https://huggingface.co/posts/ronantakizawa/591564562942305</link><description>Introducing the Medical-o1-Reasoning-SFT-Japanese dataset ğŸ‰ This dataset is a Japanese dataset consisting questions, reasoning, and answer results for complex medical topics. #japanese #medical #dataset ronantakizawa/Medical-o1-Reasoning-SFT-Japanese See translation</description><pubDate>Sun, 02 Nov 2025 17:18:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/591564562942305</guid></item><item><title>Kimi K2 is a bit disappointing by my expectations. It is on a par with Codex mini.</title><link>https://huggingface.co/posts/onekq/456763679689481</link><description>Kimi K2 is a bit disappointing by my expectations. It is on a par with Codex mini. onekq-ai/WebApp1K-models-leaderboard See translation</description><pubDate>Sun, 02 Nov 2025 17:18:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/456763679689481</guid></item></channel></rss>