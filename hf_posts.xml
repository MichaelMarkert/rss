<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>You can now run Llama 4 on your own local device! ğŸ¦™</title><link>https://huggingface.co/posts/danielhanchen/859959880164586</link><description>You can now run Llama 4 on your own local device! ğŸ¦™ Run our Dynamic 1.78-bit and 2.71-bit Llama 4 GGUFs: unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF You can run them on llama.cpp and other inference engines. See our guide here: https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4 See translation</description><pubDate>Thu, 10 Apr 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/859959880164586</guid></item><item><title>Qwen 3 can launch very soon. ğŸ‘€</title><link>https://huggingface.co/posts/merterbak/235850739835485</link><description>Qwen 3 can launch very soon. ğŸ‘€ https://github.com/ggml-org/llama.cpp/pull/12828 See translation</description><pubDate>Thu, 10 Apr 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/235850739835485</guid></item><item><title>ğŸ”¥ Yesterday was a fire day!</title><link>https://huggingface.co/posts/jasoncorkill/726469711226418</link><description>ğŸ”¥ Yesterday was a fire day! We dropped two brand-new datasets capturing Human Preferences for text-to-video and text-to-image generations powered by our own crowdsourcing tool! Whether you're working on model evaluation, alignment, or fine-tuning, this is for you. 1. Text-to-Video Dataset (Pika 2.2 model): Rapidata/text-2-video-human-preferences-pika2.2 2. Text-to-Image Dataset (Reve-AI Halfmoon): Rapidata/Reve-AI-Halfmoon_t2i_human_preference Letâ€™s train AI on AI-generated content with humans in the loop. Letâ€™s make generative models that actually get us. See translation</description><pubDate>Thu, 10 Apr 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/726469711226418</guid></item><item><title>ğŸ‰ GitHub selected the ultralytics computer vision project, known for its YOLOv8/YOLO11 real-time SOTA computer vision models, as one of the top 5 open-source projects for first-time contributors in 2024!</title><link>https://huggingface.co/posts/fcakyon/248454580146320</link><description>ğŸ‰ GitHub selected the ultralytics computer vision project, known for its YOLOv8/YOLO11 real-time SOTA computer vision models, as one of the top 5 open-source projects for first-time contributors in 2024! Link to the project: https://github.com/ultralytics/ultralytics Link to the full GitHub 2024 recap report: https://github.blog/news-insights/octoverse/octoverse-2024/ See translation</description><pubDate>Thu, 10 Apr 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fcakyon/248454580146320</guid></item><item><title>I read the 456-page AI Index report so you don't have to (kidding). The wild part? While AI gets ridiculously more accessible, the power gap is actually widening:</title><link>https://huggingface.co/posts/fdaudens/650208950263848</link><description>I read the 456-page AI Index report so you don't have to (kidding). The wild part? While AI gets ridiculously more accessible, the power gap is actually widening: 1ï¸âƒ£ The democratization of AI capabilities is accelerating rapidly: - The gap between open and closed models is basically closed: difference in benchmarks like MMLU and HumanEval shrunk to just 1.7% in 2024 - The cost to run GPT-3.5-level performance dropped 280x in 2 years - Model size is shrinking while maintaining performance - Phi-3-mini hitting 60%+ MMLU at fraction of parameters of early models like PaLM 2ï¸âƒ£ But we're seeing concerning divides deepening: - Geographic: US private investment ($109B) dwarfs everyone else - 12x China's $9.3B - Research concentration: US and China dominate highly-cited papers (50 and 34 respectively in 2023), while next closest is only 7 - Gender: Major gaps in AI skill penetration rates - US shows 2.39 vs 1.71 male/female ratio The tech is getting more accessible but the benefits aren't...</description><pubDate>Thu, 10 Apr 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/650208950263848</guid></item><item><title>New in PawMatchAIğŸ¾ : Turn Your Dog Photos into Art!</title><link>https://huggingface.co/posts/DawnC/553378321840890</link><description>New in PawMatchAIğŸ¾ : Turn Your Dog Photos into Art! Iâ€™m excited to introduce a brand-new creative feature â€” Dog Style Transfer is now live on PawMatchAI! Just upload your dogâ€™s photo and transform it into 5 artistic styles: ğŸŒ¸ Japanese Anime ğŸ“š Classic Cartoon ğŸ–¼ï¸ Oil Painting ğŸ¨ Watercolor ğŸŒ† Cyberpunk All powered by Stable Diffusion and enhanced with smart prompt tuning to preserve your dogâ€™s unique traits and breed identity , so the artwork stays true to your furry friend. Whether you're creating a custom portrait or just having fun, this feature brings your pet photos to life in completely new ways. And hereâ€™s a little secret: although itâ€™s designed with dogs in mind, it actually works on any photo â€” cats, plush toys, even humans. Feel free to experiment! Results may not always be perfectly accurate, sometimes your photo might come back looking a little different, or even beyond your imagination. But thatâ€™s part of the fun! Itâ€™s all about creative surprises and letting the AI do its...</description><pubDate>Thu, 10 Apr 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/553378321840890</guid></item><item><title>What does it mean when models share the same bytes?</title><link>https://huggingface.co/posts/jsulz/855747629260036</link><description>What does it mean when models share the same bytes? We've investigated some quants and have seen that a considerable portion of quantizations of the same model share the same bytes and can be deduplicated to save considerable upload time for quantizers on the Hub. This space where we crack open a repo from @ bartowski shows we can get significant dedupe xet-team/quantization-dedup You can get a sense of why by reading this write-up: https://github.com/bartowski1182/llm-knowledge/blob/main/quantization/quantization.md But what about finetuned models? Since going into production the xet-team has migrated hundreds of repositories on the Hub to our storage layer, including classic "pre-Hub" open-source models like FacebookAI/xlm-roberta-large (XLM-R) from FacebookAI XLM-R, introduced in 2019, set new benchmarks for multilingual NLP by learning shared representations across 100 languages. It was then fine-tuned on English, Spanish, Dutch, and German, generating language-specific...</description><pubDate>Thu, 10 Apr 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/855747629260036</guid></item><item><title>Why the  'how many r's in strawberry' prompt "breaks" llama4? :D</title><link>https://huggingface.co/posts/csabakecskemeti/971611835182279</link><description>Why the 'how many r's in strawberry' prompt "breaks" llama4? :D Quants DevQuasar/meta-llama.Llama-4-Scout-17B-16E-Instruct-GGUF See translation</description><pubDate>Thu, 10 Apr 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/csabakecskemeti/971611835182279</guid></item><item><title>The Llama 4 release  -</title><link>https://huggingface.co/posts/jsulz/541804424324012</link><description>The Llama 4 release - meta-llama/llama-4-67f0c30d9fe03840bc9d0164 - was a big one for the xet-team with every model backed by the storage infrastructure of the future for the Hub. It's been a wild few days, and especially ğŸ¤¯ to see every tensor file with a Xet logo next to it instead of LFS. The attached graph shows requests per second to our content-addressed store (CAS) right as the release went live. yellow = GETs; dashed line = launch time. You can definitely tell when the community started downloading ğŸ‘€ h/t to @ rajatarya for the graph, the entire Xet crew to bring us to this point, and special shoutout to Rajat, @ port8080 , @ brianronan , @ seanses , and @ znation who made sure the bytes kept flying all weekend âš¡ï¸ See translation</description><pubDate>Thu, 10 Apr 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/541804424324012</guid></item><item><title>AI agents are transforming how we interact with technology, but how sustainable are they? ğŸŒ</title><link>https://huggingface.co/posts/BrigitteTousi/559995441481207</link><description>AI agents are transforming how we interact with technology, but how sustainable are they? ğŸŒ Design choices â€” like model size and structure â€” can massively impact energy use and cost. âš¡ğŸ’° The key takeaway: smaller, task-specific models can be far more efficient than large, general-purpose ones. ğŸ”‘ Open-source models offer greater transparency, allowing us to track energy consumption and make more informed decisions on deployment. ğŸŒ± Open-source = more efficient, eco-friendly, and accountable AI. Read our latest, led by @ sasha with assists from myself + @ yjernite ğŸ¤— https://huggingface.co/blog/sasha/ai-agent-sustainability See translation</description><pubDate>Thu, 10 Apr 2025 05:22:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/BrigitteTousi/559995441481207</guid></item></channel></rss>