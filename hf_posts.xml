<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Developing with ZeroGPU without a PRO account is painful. They give you so many requests at once, but then have like a 24 hour cooldown. I vote less requests in a batch, but then a shorter cooldown.</title><link>https://huggingface.co/posts/nroggendorff/877752190149689</link><description>Developing with ZeroGPU without a PRO account is painful. They give you so many requests at once, but then have like a 24 hour cooldown. I vote less requests in a batch, but then a shorter cooldown. or just less of a cooldown, but i understand if that is not allowed See translation</description><pubDate>Sun, 16 Nov 2025 09:22:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/877752190149689</guid></item><item><title>I built a demo on how to implement Cache-Augmented Generation (CAG) in an LLM and compare its performance gains to RAG (111 stars, 20 forks).</title><link>https://huggingface.co/posts/ronantakizawa/762624768908636</link><description>I built a demo on how to implement Cache-Augmented Generation (CAG) in an LLM and compare its performance gains to RAG (111 stars, 20 forks). https://github.com/ronantakizawa/cacheaugmentedgeneration CAG preloads document content into an LLM‚Äôs context as a precomputed key-value (KV) cache. This caching eliminates the need for real-time retrieval during inference, reducing token usage by up to 76% while maintaining answer quality. CAG is particularly effective for constrained knowledge bases like internal documentation, FAQs, and customer support systems, where all relevant information can fit within the model's extended context window. #rag #retrievalaugmentedgeneration See translation</description><pubDate>Sun, 16 Nov 2025 09:22:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/762624768908636</guid></item><item><title>Next level Realism with Qwen Image is now possible after new realism LoRA workflow - Top images are new realism workflow - Bottom ones are older default - Full tutorial published - 4+4 Steps only</title><link>https://huggingface.co/posts/MonsterMMORPG/695072229497754</link><description>Next level Realism with Qwen Image is now possible after new realism LoRA workflow - Top images are new realism workflow - Bottom ones are older default - Full tutorial published - 4+4 Steps only Tutorial of realism : https://youtu.be/XWzZ2wnzNuQ Tutorial of training : https://youtu.be/DPX3eBTuO_Y This is a full comprehensive step-by-step tutorial for how to train Qwen Image models. This tutorial covers how to do LoRA training and full Fine-Tuning / DreamBooth training on Qwen Image models. It covers both the Qwen Image base model and the Qwen Image Edit Plus 2509 model. This tutorial is the product of 21 days of full R&amp;D, costing over $800 in cloud services to find the best configurations for training. Furthermore, we have developed an amazing, ultra-easy-to-use Gradio app to use the legendary Kohya Musubi Tuner trainer with ease. You will be able to train locally on your Windows computer with GPUs with as little as 6 GB of VRAM for both LoRA and Fine-Tuning. See translation</description><pubDate>Sun, 16 Nov 2025 09:22:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/695072229497754</guid></item><item><title>Hugging Face MCP Server v0.2.46</title><link>https://huggingface.co/posts/evalstate/966306150906160</link><description>Hugging Face MCP Server v0.2.46 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - Add "discover" to Dynamic Space tool. Recommend deselecting "space_search" if using dynamic spaces. See translation</description><pubDate>Sun, 16 Nov 2025 09:22:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/evalstate/966306150906160</guid></item><item><title>OCR has absolutely blown up in 2025, and honestly, my perspective on document processing has completely changed.</title><link>https://huggingface.co/posts/RakshitAralimatti/154491437278082</link><description>OCR has absolutely blown up in 2025, and honestly, my perspective on document processing has completely changed. This year has been wild. Vision Language Models like Nanonets OCR2-3B hit the scene and suddenly we're getting accuracy on complex forms (vs for traditional OCR). We're talking handwritten checkboxes, watermarked documents, multi-column layouts, even LaTeX equations all handled in a single pass.‚Äã The market numbers say it all: OCR accuracy passed 98% for printed text, AI integration is everywhere, and real-time processing is now standard. The entire OCR market is hitting $25.13 billion in 2025 because this tech actually works now. I wrote a detailed Medium article walking through: 1. Why vision LMs changed the game 2. NVIDIA NeMo Retriever architecture 3. Complete code breakdown 4. Real government/healthcare use cases 5. Production deployment guide Article: https://medium.com/@rakshitaralimatti2001/nvidia-nemo-retriever-ocr-building-document-intelligence-systems-for-...</description><pubDate>Sun, 16 Nov 2025 09:22:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/154491437278082</guid></item><item><title>Fine-Tuning Qwen3 Embeddings for product category classification on the Large-Scale Product Corpus</title><link>https://huggingface.co/posts/aufklarer/281006786647431</link><description>Fine-Tuning Qwen3 Embeddings for product category classification on the Large-Scale Product Corpus Language-models such as GPT, Llama, DeepSeek, Qwen trained with a filtered slice of Common Crawl. For e-commerce work, though, we can start with the Web Data Commons (WDC), the project by the University of Mannheim. It extracts web pages that carry some metadata and publishes the result as the Large-Scale Product Corpus (LSPC). Search engines like Google reward pages that include detailed product markup, so merchants already populate their sites with SEO-friendly fields such as title, brand, GTIN, price ‚Äî and, crucially, category labels. Thanks to these built-in annotations, the WDC Large-Scale Product Corpus arrives almost fully self-labelled. I used those labels to fine-tune Qwen3 Embedding with Low-Rank Adaptation (LoRA), code is available on github. The resulting 615 million-parameter checkpoint fits comfortably in limited GPU memory yet updates the model‚Äôs representation space,...</description><pubDate>Sun, 16 Nov 2025 09:22:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aufklarer/281006786647431</guid></item><item><title>üéØ Introducing Chayan: A Calibrated 4-Model LLM Router Achieving 69% Accuracy on RouterArena</title><link>https://huggingface.co/posts/codelion/349113167339666</link><description>üéØ Introducing Chayan: A Calibrated 4-Model LLM Router Achieving 69% Accuracy on RouterArena We're excited to share Chayan, a cost-efficient LLM router that intelligently routes queries between 4 models to maximize accuracy while minimizing cost. Chayan just submitted to the RouterArena leaderboard and achieved 69.05% accuracy on the benchmark! üîó Model: adaptive-classifier/chayan üîó Dataset: RouteWorks/RouterArena üìä Performance Highlights Chayan achieves impressive results on the RouterArena benchmark: ‚Ä¢ 69.05% accuracy (would rank #1 on current leaderboard) ‚Ä¢ $0.333 per 1K queries ‚Ä¢ +12.07pp improvement over all-mini baseline (56.98%) ‚Ä¢ 99% of perfect 2-model oracle performance at 57% lower cost Compared to our previous 2-model router (61.43% accuracy), Chayan delivers +7.62pp improvement through smarter 4-model routing. üß† How It Works Chayan uses an Adaptive K-NN classifier with prototype memory to route between 4 models: ‚Ä¢ openai/gpt-4o-mini (fast &amp; cheap) ‚Ä¢...</description><pubDate>Sun, 16 Nov 2025 09:22:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/349113167339666</guid></item><item><title>Reached 1000+ total downloads across my models and datasets! üéâ</title><link>https://huggingface.co/posts/ronantakizawa/435117440357729</link><description>Reached 1000+ total downloads across my models and datasets! üéâ Follow me for more @ ronantakizawa See translation</description><pubDate>Sun, 16 Nov 2025 09:22:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/435117440357729</guid></item><item><title>The reaction on the QAT post is beyond expectations so below is my optimizer post as promised. But I found that I had lots of explanation to do about optimizer itself. So this post is actually a historical recount. The Muon optimizer  (used by Kimi) post (coming very soon) can only continue after this.</title><link>https://huggingface.co/posts/onekq/566639652632782</link><description>The reaction on the QAT post is beyond expectations so below is my optimizer post as promised. But I found that I had lots of explanation to do about optimizer itself. So this post is actually a historical recount. The Muon optimizer (used by Kimi) post (coming very soon) can only continue after this. https://huggingface.co/blog/onekq/adam-optimizer If you know Adam(W) optimizer already, you can just skip and sorry for the wait. Otherwise, it should be a useful read. See translation</description><pubDate>Sun, 16 Nov 2025 09:22:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/566639652632782</guid></item><item><title>Made a small write up and experimental finetuning guide for MetaCLIP2 for Image Classification on Downstream Tasks. The blog titled</title><link>https://huggingface.co/posts/prithivMLmods/462277719397337</link><description>Made a small write up and experimental finetuning guide for MetaCLIP2 for Image Classification on Downstream Tasks. The blog titled Fine Tuning MetaCLIP 2 for Image Classification on Downstream Tasks demonstrates the step by step finetuning using CIFAR10 and is also flexible for adapting to other datasets. For more details, check out the linked blog below. ü§ó‚ÜóÔ∏è ‚Æû Blog Article: https://huggingface.co/blog/prithivMLmods/metaclip2-downstream-finetune ‚Æû Demo Space[Zero-Shot Classification]: prithivMLmods/metaclip-2-demo Some other models ‚ï∞‚Ä∫ MetaCLIP-2-Cifar10: prithivMLmods/MetaCLIP-2-Cifar10 ‚ï∞‚Ä∫ MetaCLIP-2-Age-Range-Estimator: prithivMLmods/MetaCLIP-2-Age-Range-Estimator ‚ï∞‚Ä∫ MetaCLIP-2-Gender-Identifier: prithivMLmods/MetaCLIP-2-Gender-Identifier ‚ï∞‚Ä∫ MetaCLIP-2-Open-Scene: prithivMLmods/MetaCLIP-2-Open-Scene ‚Æû Collection: https://huggingface.co/collections/prithivMLmods/metaclip2-image-classification-experiments To know more about it, visit the app page or the respective model page! See...</description><pubDate>Sun, 16 Nov 2025 09:22:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/462277719397337</guid></item></channel></rss>