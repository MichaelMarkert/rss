<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>üöÄ Gemma3-R1984-27B: Next Generation Agentic AI Platform</title><link>https://huggingface.co/posts/openfree/214646053127729</link><description>üöÄ Gemma3-R1984-27B: Next Generation Agentic AI Platform Model Path: VIDraft/Gemma-3-R1984-27B Space: VIDraft/Gemma-3-R1984-27B git clone VIDraft/Gemma-3-R1984-27B üí´ A New Frontier in AI Innovation Gemma3-R1984-27B is a powerful agentic AI platform built on Google's Gemma-3-27B model. It integrates state-of-the-art deep research via web search with multimodal file processing capabilities and handles long contexts up to 8,000 tokens. Designed for local deployment on independent servers using NVIDIA A100 GPUs, it provides high security and prevents data leakage. üîì Uncensored and Unrestricted AI Experience Gemma3-R1984-27B comes with all censorship restrictions removed, allowing users to operate any persona without limitations. The model perfectly implements various roles and characters according to users' creative requests, providing unrestricted responses that transcend the boundaries of conventional AI. This unlimited interaction opens infinite possibilities across research, creative...</description><pubDate>Fri, 28 Mar 2025 05:23:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/214646053127729</guid></item><item><title>This is truly an inspirational story please help us spread the word,</title><link>https://huggingface.co/posts/giux78/831597804891353</link><description>This is truly an inspirational story please help us spread the word, @ clem , @ thomwolf and everyone who supports open source AI. A few weeks ago, @ mmuffo94 and @ cittiberto from indigo_ai launched the Chatbot Arena for the Italian language: https://indigo.ai/it/chatbot-arena-italia/ . To our surprise, among the top-ranked models is mii-llm/maestrale-chat-v0.4-beta a carefully fine-tuned version of mistralai/Mistral-7B-v0.1 , developed by @ efederici and @ mferraretto from https://huggingface.co/mii-llm , and released nearly a year ago. At this very moment, as shown in the screenshot, mii-llm/maestrale-chat-v0.4-beta is ranked 8th right between ChatGPT-4.5 and ChatGPT-4o. It's likely that for several months, the best Italian speaking LLM has been an open source 7B model created by open source contributors and hardly anyone knew it. See translation</description><pubDate>Fri, 28 Mar 2025 05:23:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giux78/831597804891353</guid></item><item><title>üî• ULTRA VIDEO COMPRESSION (300MB ‚Üí 3MB!)</title><link>https://huggingface.co/posts/luigi12345/403914274386316</link><description>üî• ULTRA VIDEO COMPRESSION (300MB ‚Üí 3MB!) ffmpeg - i input .mp4 -vcodec libx264 -crf 28 -vf "pad=ceil(iw/2)*2:ceil(ih/2)*2" -y output.mp4 -i ‚Üí Input ‚ö°Ô∏è -vcodec libx264 ‚Üí H.264 codec ‚ö°Ô∏è -crf 28 ‚Üí Compression (lower = better quality) ‚ö°Ô∏è-vf pad=... ‚Üí Even dimensions ‚ö°Ô∏è -y ‚Üí Overwrite See translation</description><pubDate>Fri, 28 Mar 2025 05:23:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/luigi12345/403914274386316</guid></item><item><title>We've all become experts at clicking "I agree" without a second thought. In my latest blog post, I explore why these traditional consent models are increasingly problematic in the age of generative AI.</title><link>https://huggingface.co/posts/giadap/214321702054817</link><description>We've all become experts at clicking "I agree" without a second thought. In my latest blog post, I explore why these traditional consent models are increasingly problematic in the age of generative AI. I found three fundamental challenges: - Scope problem: how can you know what you're agreeing to when AI could use your data in different ways? - Temporality problem: once an AI system learns from your data, good luck trying to make it "unlearn" it. - Autonomy trap: the data you share today could create systems that pigeonhole you tomorrow. Individual users shouldn't bear all the responsibility, while big tech holds all the cards. We need better approaches to level the playing field, from collective advocacy and stronger technological safeguards to establishing "data fiduciaries" with a legal duty to protect our digital interests. Available here: https://huggingface.co/blog/giadap/beyond-consent See translation</description><pubDate>Fri, 28 Mar 2025 05:23:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/214321702054817</guid></item><item><title>I'm not really doing much on HuggingFace right now due to their new Docker space policies, so if you want to keep up with most of what I'm up to, follow my [instagram](</title><link>https://huggingface.co/posts/nroggendorff/410679759202527</link><description>I'm not really doing much on HuggingFace right now due to their new Docker space policies, so if you want to keep up with most of what I'm up to, follow my [instagram]( https://sly.sh/ig ) See translation</description><pubDate>Fri, 28 Mar 2025 05:23:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/410679759202527</guid></item><item><title>üöÄ First Benchmark of</title><link>https://huggingface.co/posts/jasoncorkill/659246791111385</link><description>üöÄ First Benchmark of @ OpenAI 's 4o Image Generation Model! We've just completed the first-ever (to our knowledge) benchmarking of the new OpenAI 4o image generation model, and the results are impressive! In our tests, OpenAI 4o image generation absolutely crushed leading competitors, including @ black-forest-labs , @ google , @ xai-org , Ideogram, Recraft, and @ deepseek-ai , in prompt alignment and coherence! They hold a gap of more than 20% to the nearest competitor in terms of Bradley-Terry score, the biggest we have seen since the beginning of the benchmark! The benchmarks are based on 200k human responses collected through our API. However, the most challenging part wasn't the benchmarking itself, but generating and downloading the images: - 5 hours to generate 1000 images (no API available yet) - Just 10 minutes to set up and launch the benchmark - Over 200,000 responses rapidly collected While generating the images, we faced some hurdles that meant that we had to leave out...</description><pubDate>Fri, 28 Mar 2025 05:23:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/659246791111385</guid></item><item><title>Is it just me or are the ZeroGPU not refreshing/restoring/regen...?</title><link>https://huggingface.co/posts/Keltezaa/825870184562072</link><description>Is it just me or are the ZeroGPU not refreshing/restoring/regen...? Last time I used my allocated ZeroGPU was over 4 hours ago and I have only recovered 70 seconds? WTF..is going on? Correct me if I am wrong here but something is "OFF". And I still feel as a Subscriber, the regen is way to slow. Please Fix this for all of us. See translation</description><pubDate>Fri, 28 Mar 2025 05:23:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Keltezaa/825870184562072</guid></item><item><title>‚ÄºÔ∏èSentence Transformers v4.0 is out! You can now train and finetune reranker models with multi-GPU training, bf16 support, loss logging, callbacks &amp; much more. I also prove that finetuning on your domain helps much more than you might think.</title><link>https://huggingface.co/posts/tomaarsen/443265255291103</link><description>‚ÄºÔ∏èSentence Transformers v4.0 is out! You can now train and finetune reranker models with multi-GPU training, bf16 support, loss logging, callbacks &amp; much more. I also prove that finetuning on your domain helps much more than you might think. 1Ô∏è‚É£ Reranker Training Refactor Reranker models can now be trained using an extensive trainer with a lot of powerful features: - MultiGPU Training (Data Parallelism (DP) and Distributed Data Parallelism (DDP)) - bf16 training support; loss logging - Evaluation datasets + evaluation loss - Improved callback support + an excellent Weights &amp; Biases integration - Gradient checkpointing, gradient accumulation - Model card generation - Resuming from a training checkpoint without performance loss - Hyperparameter Optimization and much more! Read my detailed blogpost to learn about the components that make up this new training approach: https://huggingface.co/blog/train-reranker Notably, the release is fully backwards compatible: all deprecations are...</description><pubDate>Fri, 28 Mar 2025 05:23:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/443265255291103</guid></item><item><title>Besides being the coolest named benchmark in the game, HellaSwag is an important measurement of –∑–¥—Ä–∞–≤—ã–π —Å–º—ã—Å–ª—å (or common sense) in LLMs.</title><link>https://huggingface.co/posts/ZennyKenny/895817584228018</link><description>Besides being the coolest named benchmark in the game, HellaSwag is an important measurement of –∑–¥—Ä–∞–≤—ã–π —Å–º—ã—Å–ª—å (or common sense) in LLMs. - More on HellaSwag: https://github.com/rowanz/hellaswag I spent the afternoon benchmarking YandexGPT Pro 4th Gen, one of the Russian tech giant's premier models. - Yandex HF Org: https://huggingface.co/yandex - More on Yandex models: https://yandex.cloud/ru/docs/foundation-models/concepts/yandexgpt/models The eval notebook is available on GitHub and the resulting dataset is already on the HF Hub! - Eval Notebook: https://github.com/kghamilton89/ai-explorer/blob/main/yandex-hellaswag/hellaswag-assess.ipynb - Eval Dataset: ZennyKenny/yandexgptpro_4th_gen-hellaswag And of course, everyone wants to see the results so have a look at the results in the context of other zero-shot experiments that I was able to find! See translation</description><pubDate>Fri, 28 Mar 2025 05:23:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/895817584228018</guid></item><item><title>üìö Archive of Our Own (AO3) Dataset -</title><link>https://huggingface.co/posts/nyuuzyou/318268301919621</link><description>üìö Archive of Our Own (AO3) Dataset - nyuuzyou/archiveofourown Collection of approximately 12.6 million fanfiction works (from 63.2M processed IDs) featuring: - Full text content from diverse fandoms across television, film, books, anime, and more - Comprehensive metadata including warnings, relationships, characters, and tags - Multilingual content with works in 40+ languages though English predominant - Rich classification data preserving author-created folksonomy and content categorization P.S. This is the most expensive dataset I've created so far! And also, thank you all for the 100 followers on Hugging Face! See translation</description><pubDate>Fri, 28 Mar 2025 05:23:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nyuuzyou/318268301919621</guid></item></channel></rss>