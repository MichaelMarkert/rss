<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>âœ¨ DreamO Video: From Customized Images to Videos âœ¨</title><link>https://huggingface.co/posts/openfree/538970335354687</link><description>âœ¨ DreamO Video: From Customized Images to Videos âœ¨ Hello, AI creators! Today I'm introducing a truly special project. DreamO Video is an integrated framework that generates customized images based on reference images and transforms them into videos with natural movement. ğŸ¬âœ¨ openfree/DreamO-video ğŸ” Key Features Image Reference (IP): Maintain object appearance while applying to new backgrounds and situations ID Preservation: Retain facial features across various environments Style Transfer: Apply unique styles from reference images to other content ğŸï¸ Video Generation: Create natural 2-second videos from generated images ğŸ’¡ How to Use Upload Reference Images: One or two images (people, objects, landscapes, etc.) Select Task Type: Choose between IP (Image Preservation), ID (Face Feature Retention), or Style Enter Prompt: Describe your desired result (e.g., "a woman playing guitar on a cloud") Click Generate Image: âœ¨ Create customized AI images! Generate Video: Click the ğŸ¬ button on the...</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/538970335354687</guid></item><item><title>VLMS 2025 UPDATE ğŸ”¥</title><link>https://huggingface.co/posts/merve/544378273517703</link><description>VLMS 2025 UPDATE ğŸ”¥ We just shipped a blog on everything latest on vision language models, including ğŸ¤– GUI agents, agentic VLMs, omni models ğŸ“‘ multimodal RAG â¯ï¸ video LMs ğŸ¤ğŸ» smol models ..and more! https://huggingface.co/blog/vlms-2025 See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/544378273517703</guid></item><item><title>Iâ€™ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things.</title><link>https://huggingface.co/posts/ArturoNereu/644085701737970</link><description>Iâ€™ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things. I turned this into a GitHub repo: https://github.com/ArturoNereu/AI-Study-Group If youâ€™re just getting started, I recommend: ğŸ“˜ Deep Learning â€“ A Visual Approach: https://www.glassner.com/portfolio/deep-learning-a-visual-approach ğŸ¥ Dive into LLMs with Andrej Karpathy: https://youtu.be/7xTGNNLPyMI?si=aUTq_qUzyUx36BsT ğŸ§  The ğŸ¤— Agents course]( https://huggingface.co/learn/agents-course/ The repo has grown with help from the community (Reddit, Discord, etc.) and Iâ€™ll keep updating it. If you have any favorite resources, Iâ€™d love to include them. See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ArturoNereu/644085701737970</guid></item><item><title>Transfer Any Clothing Into A New Person &amp; Turn Any Person Into A 3D Figure - ComfyUI Tutorial</title><link>https://huggingface.co/posts/MonsterMMORPG/869555651580897</link><description>Transfer Any Clothing Into A New Person &amp; Turn Any Person Into A 3D Figure - ComfyUI Tutorial ComfyUI is super hard to use but I have literally prepared 1-click way to install and use 2 amazing workflows. First workflow is generating a person wearing any clothing. The second workflow is turning any person image into a 3D toy like figure image. Tutorial Link : https://youtu.be/ZzYnhKeaJBs Video Chapters 0:00:00 Intro: Two One-Click ComfyUI Workflows (Clothing Gen &amp; 3D Figure) 0:00:34 Effort &amp; Goal: Easy Installation &amp; Use of Complex Workflows 0:00:49 Setup Part 1: ComfyUI Prerequisite &amp; Downloading Project Zip File 0:01:06 Setup Part 2: Extracting Zip into ComfyUI Folder (WinRAR 'Extract Here' Tip) 0:01:18 Setup Part 3: Running update_comfyui.bat for Latest ComfyUI Version 0:01:37 Setup Part 4: Running install_clothing_and_3D.bat (Installs Nodes &amp; Requirements) 0:02:03 Model Downloads: Intro to Swarm UI Auto-Installer &amp; Automatic Updater 0:02:28 Using Swarm UI: Launching...</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/869555651580897</guid></item><item><title># ğŸŒŸ 3D Model to Video: Easy GLB Conversion Tool ğŸŒŸ</title><link>https://huggingface.co/posts/ginipick/766230066345476</link><description># ğŸŒŸ 3D Model to Video: Easy GLB Conversion Tool ğŸŒŸ demo link: ginigen/3D-VIDEO Hello there! Would you like to transform your 3D models into stunning animations? This space can help you! âœ¨ ## ğŸ” What Can It Do? This tool converts your uploaded GLB model into: 1. ğŸ® A transformed GLB file 2. ğŸ¬ An animated GIF preview 3. ğŸ“‹ A metadata JSON file ## âœ… Key Features * ğŸ–¥ï¸ Works in headless server environments (EGL + pyglet-headless â†’ pyrender fallback) * ğŸ” Objects in GIFs appear 3x larger (global scale Ã—3) * ğŸ¨ Clean interface with pastel background ## ğŸ® Animation Types * ğŸ”„ Rotate - Object rotates around the Y-axis * â¬†ï¸ Float - Object moves smoothly up and down * ğŸ’¥ Explode - Object moves sideways * ğŸ§© Assemble - Object returns to its original position * ğŸ’“ Pulse - Object changes in size * ğŸ”„ Swing - Object swings around the Z-axis ## ğŸ› ï¸ How to Use 1. Upload your GLB model ğŸ“¤ 2. Select your desired animation type ğŸ¬ 3. Adjust the duration and FPS â±ï¸ 4. Click the "Generate Animation" button â–¶ï¸ 5....</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/766230066345476</guid></item><item><title>Matrix Game ğŸ® an interactive foundation model for controllable game world generation, released by Skywork AI.</title><link>https://huggingface.co/posts/AdinaY/746841015963520</link><description>Matrix Game ğŸ® an interactive foundation model for controllable game world generation, released by Skywork AI. Skywork/Matrix-Game âœ¨ 17B with MIT licensed âœ¨ Diffusion-based image-to-world video generation via keyboard &amp; mouse input âœ¨ GameWorld Score benchmark for Minecraft world models âœ¨ Massive Matrix Game Dataset with fine-grained action labels See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/746841015963520</guid></item><item><title>ğŸš€ VisionScout Now Speaks More Like Me â€” Thanks to LLMs!</title><link>https://huggingface.co/posts/DawnC/683112818630492</link><description>ğŸš€ VisionScout Now Speaks More Like Me â€” Thanks to LLMs! I'm thrilled to share a major update to VisionScout, my end-to-end vision system. Beyond robust object detection (YOLOv8) and semantic context (CLIP), VisionScout now features a powerful LLM-based scene narrator (Llama 3.2), improving the clarity, accuracy, and fluidity of scene understanding. This isnâ€™t about replacing the pipeline , itâ€™s about giving it a better voice. âœ¨ â­ï¸ What the LLM Brings Fluent, Natural Descriptions: The LLM transforms structured outputs into human-readable narratives. Smarter Contextual Flow: It weaves lighting, objects, zones, and insights into a unified story. Grounded Expression: Carefully prompt-engineered to stay factual â€” it enhances, not hallucinates. Helpful Discrepancy Handling: When YOLO and CLIP diverge, the LLM adds clarity through reasoning. VisionScout Still Includes: ğŸ–¼ï¸ YOLOv8-based detection (Nano / Medium / XLarge) ğŸ“Š Real-time stats &amp; confidence insights ğŸ§  Scene understanding via...</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/683112818630492</guid></item><item><title>The era of local Computer Use AI Agents is here.</title><link>https://huggingface.co/posts/dhruv3006/692657412350660</link><description>The era of local Computer Use AI Agents is here. Meet UI-TARS-1.5-7B-6bit, now running natively on Apple Silicon via MLX. The video is of UI-TARS-1.5-7B-6bit completing the prompt "draw a line from the red circle to the green circle, then open reddit in a new tab" running entirely on MacBook. The video is just a replay, during actual usage it took between 15s to 50s per turn with 720p screenshots (on avg its ~30s per turn), this was also with many apps open so it had to fight for memory at times. Built using c/ua : https://github.com/trycua/cua Join us making them here: https://discord.gg/4fuebBsAUj Kudos to the MLX community here on huggingface : mlx-community See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/692657412350660</guid></item><item><title>Automatic Multi-Modal Research Agent</title><link>https://huggingface.co/posts/VirtualOasis/965866013655862</link><description>Automatic Multi-Modal Research Agent I am thinking of building an Automatic Research Agent that can boost creativity! Input: Topics or data sources Processing: Automated deep research Output: multimodal results (such as reports, videos, audio, diagrams) &amp; multi-platform publishing. There is a three-stage process In the initial Stage, output for text-based content in markdown format allows for user review before transformation into various other formats, such as PDF or HTML. The second stage transforms the output into other modalities, like audio, video, diagrams, and translations into different languages. The final stage focuses on publishing multi-modal content across multiple platforms like X, GitHub, Hugging Face, YouTube, and podcasts, etc. See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/VirtualOasis/965866013655862</guid></item><item><title>Very cool to see</title><link>https://huggingface.co/posts/clem/170733821735878</link><description>Very cool to see pytorch contributing on Hugging Face. Time to follow them to see what they're cooking! See translation</description><pubDate>Wed, 14 May 2025 13:33:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/170733821735878</guid></item></channel></rss>