<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Qwen3-Next can now be Run locally! (30GB RAM)</title><link>https://huggingface.co/posts/danielhanchen/212249714773740</link><description>Qwen3-Next can now be Run locally! (30GB RAM) Instruct GGUF: unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF The models come in Thinking and Instruct versions and utilize a new architecture, allowing it to have ~10x faster inference than Qwen32B. ğŸ’œ Step-by-step Guide: https://docs.unsloth.ai/models/qwen3-next Thinking GGUF: unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF See translation</description><pubDate>Sat, 29 Nov 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/212249714773740</guid></item><item><title>Introducing the  Super-OCRs Demo, a comparison of state-of-the-art multimodal OCR VLMs, including HunyuanOCR, DeepSeekOCR, Dots, and Nanonets in one space for performing OCR, rendering LaTeX and Markdown, and visual grounding (layout). Find the related Spaces and models below.ğŸ¤—ğŸ”¥</title><link>https://huggingface.co/posts/prithivMLmods/107899220924462</link><description>Introducing the Super-OCRs Demo, a comparison of state-of-the-art multimodal OCR VLMs, including HunyuanOCR, DeepSeekOCR, Dots, and Nanonets in one space for performing OCR, rendering LaTeX and Markdown, and visual grounding (layout). Find the related Spaces and models below.ğŸ¤—ğŸ”¥ âœ¨Super-OCRs[Demo]: prithivMLmods/Super-OCRs-Demo âœ¨Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations âœ¨GitHub: https://github.com/PRITHIVSAKTHIUR/Super-OCRs-Demo â­ Models Used: âœ¦ HunyuanOCR: tencent/HunyuanOCR âœ¦ DeepSeek-OCR: (-) deepseek-ai/DeepSeek-OCR (+) prithivMLmods/DeepSeek-OCR-Latest-BF16.I64 âœ¦ Dots.OCR: (-) rednote-hilab/dots.ocr (+) prithivMLmods/Dots.OCR-Latest-BF16 âœ¦ Nanonets-OCR2-3B: nanonets/Nanonets-OCR2-3B â­ Some Other Relevant Apps: âœ¦ Qwen3-VL-HF-Demo: prithivMLmods/Qwen3-VL-HF-Demo âœ¦ Qwen3-VL-Outpost: prithivMLmods/Qwen3-VL-Outpost âœ¦ Multimodal-OCR: prithivMLmods/Multimodal-OCR âœ¦ Multimodal-OCR2: prithivMLmods/Multimodal-OCR2 âœ¦ Multimodal-OCR3:...</description><pubDate>Sat, 29 Nov 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/107899220924462</guid></item><item><title>nanochat is now in transformers!</title><link>https://huggingface.co/posts/sergiopaniego/367599205240435</link><description>nanochat is now in transformers! The LLM by @ karpathy is officially in the library, and we wrote a blog covering: how did we port the model, differences from the original, and how to run or train it. go read it ğŸ¤“ nanochat-students/transformers See translation</description><pubDate>Sat, 29 Nov 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/367599205240435</guid></item><item><title>FLUX 2 vs FLUX SRPO, New FLUX Training Kohya SS GUI Premium App With Presets &amp; Features :</title><link>https://huggingface.co/posts/MonsterMMORPG/938257136422472</link><description>FLUX 2 vs FLUX SRPO, New FLUX Training Kohya SS GUI Premium App With Presets &amp; Features : https://youtu.be/RQHmyJVOHXo FLUX 2 has been published and I have compared it to the very best FLUX base model known as FLUX SRPO. Moreover, we have updated our FLUX Training APP and presets to the next level. Massive speed up gaings with 0 quality loss and lots of new features. I will show all of the new features we have with new SECourses Kohya SS GUI Premium app and compare FLUX SRPO trained model results with FLUX 2. https://youtu.be/RQHmyJVOHXo Get the SECourses Premium Kohya Trainer DreamBooth / Fine Tuning : [ https://www.patreon.com/posts/Kohya-FLUX-DreamBooth-Trainer-App-112099700 ] Get the SECourses Premium Kohya Trainer LoRA : [ https://www.patreon.com/posts/Kohya-FLUX-LoRA-Trainer-App-110879657 ] DreamBooth Training Tutorial: [ https://www.youtube.com/watch?v=FvpWy1x5etM ] LoRA Training Tutorial: [ https://www.youtube.com/watch?v=nySGu12Y05k ] Qwen Image Realism Tutorial: [...</description><pubDate>Sat, 29 Nov 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/938257136422472</guid></item><item><title>ğŸš€ I've just shipped a major update to the</title><link>https://huggingface.co/posts/Nymbo/982136402285178</link><description>ğŸš€ I've just shipped a major update to the Nymbo/Tools MCP server: the Agent_Terminal , a single "master tool" that cuts token usage by over 90%! Anthropic found 98.7% context savings using code execution with MCP, Cloudflare published similar findings. This is my open-source implementation of the same idea. # The Problem Traditional MCP exposes every tool definition directly to the model. With 12 tools, that's thousands of tokens consumed *before the conversation even starts*. Each tool call also passes intermediate results through the context window â€” a 10,000-row spreadsheet? That's all going into context just to sum a column. # The Solution: One Tool to Rule Them All Agent_Terminal wraps all 12 tools ( Web_Search , Web_Fetch , File_System , Generate_Image , Generate_Speech , Generate_Video , Deep_Research , Memory_Manager , Obsidian_Vault , Shell_Command , Code_Interpreter ) into a single Python code execution gateway. Instead of the model making individual tool calls, it writes...</description><pubDate>Sat, 29 Nov 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Nymbo/982136402285178</guid></item><item><title>AetherMind_SRL: How I beat 7B models on MMLU with 184M params and a $300 GPU</title><link>https://huggingface.co/posts/samerzaher80/506428935925502</link><description>AetherMind_SRL: How I beat 7B models on MMLU with 184M params and a $300 GPU Iâ€™m Sameer, a solo researcher from Iraq working on a single RTX 3050 8GB laptop.Today Iâ€™m releasing AetherMind_SRL â€“ a 184M-parameter NLI model that was trained only on tasks (SNLI, MNLI, ANLI, and a small clinical Alzheimerâ€™s dataset). It was never fine-tuned or even shown a single MMLU question during training.Yet here are the zero-shot MMLU (57 subjects) results:Model MMLU Zero-Shot Training Data AetherMind_SRL (me) 184M 36.05 % Only NLI (SNLI/MNLI/ANLI + ADNI) DeBERTa-v3-base 278M ~30.8 % General pre-training BERT-large 340M 27â€“30 % General pre-training LLaMA-1 7B 7B 34â€“35 % Massive text corpus LLaMA-2 7B 7B ~45 % Bigger + better data Yes â€“ my 184M model beats every classic 300â€“400M model and the original 7-billion-parameter LLaMA-1, all while running at 300+ samples/sec on a $300 laptop GPU.How did this happen?I built a standardized self-improvement loop called AetherMind Self-Reflective Learning (SRL)...</description><pubDate>Sat, 29 Nov 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/samerzaher80/506428935925502</guid></item><item><title>Why I think local, open-source models will eventually win.</title><link>https://huggingface.co/posts/abidlabs/941146046599374</link><description>Why I think local, open-source models will eventually win. The most useful AI applications are moving toward multi-turn agentic behavior: systems that take hundreds or even thousands of iterative steps to complete a task, e.g. Claude Code, computer-control agents that click, type, and test repeatedly. In these cases, the power of the model is not how smart it is per token, but in how quickly it can interact with its environment and tools across many steps. In that regime, model quality becomes secondary to latency. An open-source model that can call tools quickly, check that the right thing was clicked, or verify that a code change actually passes tests can easily outperform a slightly â€œsmarterâ€ closed model that has to make remote API calls for every move. Eventually, the balance tips: it becomes impractical for an agent to rely on remote inference for every micro-action. Just as no one would tolerate a keyboard that required a network request per keystroke, users wonâ€™t accept...</description><pubDate>Sat, 29 Nov 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/abidlabs/941146046599374</guid></item><item><title>4æœˆï¼Ÿã”ã‚ã«å‚åŠ ã—ãŸCerebrasã®ãƒãƒƒã‚«ã‚½ãƒ³ã‹ã‚‰ä½•æ•…ã‹Huggingfaceã®proãƒ—ãƒ©ãƒ³ãŒç¶šã„ã¦ã‚‹ã‚“ã§ã™ã‚ˆã­...</title><link>https://huggingface.co/posts/Holy-fox/916850799845292</link><description>4æœˆï¼Ÿã”ã‚ã«å‚åŠ ã—ãŸCerebrasã®ãƒãƒƒã‚«ã‚½ãƒ³ã‹ã‚‰ä½•æ•…ã‹Huggingfaceã®proãƒ—ãƒ©ãƒ³ãŒç¶šã„ã¦ã‚‹ã‚“ã§ã™ã‚ˆã­... å¤šåˆ†ãƒãƒƒã‚«ã‚½ãƒ³æœŸé–“ã ã‘ã®ã¯ãšãªã‚“ã ã‘ã©ã€å¤–ã‚Œãªã„ã®ã‚ˆã­ã€‚ ã¾ã‚ã€ã‚¯ãƒ¬ã‚«ã¨ã‹ã¯ç™»éŒ²ã—ã¦ãªã„ã‹ã‚‰å¤§ä¸ˆå¤«ã ã¨ã¯æ€ã†ã‘ã© See translation</description><pubDate>Sat, 29 Nov 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Holy-fox/916850799845292</guid></item><item><title>Accidentally made a totally blank, non-functioning HF Space while poking â€œwhat does this doâ€ buttons without reading anything.</title><link>https://huggingface.co/posts/Babsie/682016122905240</link><description>Accidentally made a totally blank, non-functioning HF Space while poking â€œwhat does this doâ€ buttons without reading anything. Iâ€™d already named it its-totally-supposed-to-do-that . It is, in fact, totally supposed to do that. Trickster QA passed. See translation</description><pubDate>Sat, 29 Nov 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Babsie/682016122905240</guid></item><item><title>I made a visualization based on the Prime Intellect INTELLECT-3 technical report.</title><link>https://huggingface.co/posts/anakin87/824755368598190</link><description>I made a visualization based on the Prime Intellect INTELLECT-3 technical report. Wild to see how far they pushed GLM-4.5-Air-Base with SFT + RL. SOTA for its size and competitive with models 3x larger. All open. Congrats on the release! Model: PrimeIntellect/INTELLECT-3 Technical report: https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf Chat: https://chat.primeintellect.ai/ See translation</description><pubDate>Sat, 29 Nov 2025 17:20:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/anakin87/824755368598190</guid></item></channel></rss>