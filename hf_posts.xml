<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>O(1) inference is the foundational design of Spartacus-1B-Instruct üõ°Ô∏è !</title><link>https://huggingface.co/posts/OzTianlu/866420978580038</link><description>O(1) inference is the foundational design of Spartacus-1B-Instruct üõ°Ô∏è ! NoesisLab/Spartacus-1B-Instruct We have successfully replaced the KV-cache bottleneck inherent in Softmax Attention with Causal Monoid State Compression. By defining the causal history as a monoid recurrence, , the entire prefix is lossily compressed into a fixed-size state matrix per head. The technical core of this architecture relies on the associativity of the monoid operator: Training: parallel prefix scan using Triton-accelerated JIT kernels to compute all prefix states simultaneously. Inference: True sequential updates. Memory and time complexity per token are decoupled from sequence length. Explicit Causality: We discard RoPE and attention masks. Causality is a first-class citizen, explicitly modeled through learned, content-dependent decay gates. Current zero-shot benchmarks demonstrate that Spartacus-1B-Instruct (1.3B) is already outperforming established sub-quadratic models like Mamba-1.4B and...</description><pubDate>Thu, 19 Feb 2026 14:10:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/OzTianlu/866420978580038</guid></item><item><title>üôãüèª‚Äç‚ôÇÔ∏èhello my lovelies ,</title><link>https://huggingface.co/posts/Tonic/772171916121212</link><description>üôãüèª‚Äç‚ôÇÔ∏èhello my lovelies , it is with great pleasure i present to you my working one-click deploy 16GB ram completely free huggingface spaces deployment. repo : Tonic/hugging-claw (use git clone to inspect) literally the one-click link : Tonic/hugging-claw you can also run it locally and see for yourself : docker run -it -p 7860:7860 --platform=linux/amd64 \ -e HF_TOKEN="YOUR_VALUE_HERE" \ -e OPENCLAW_GATEWAY_TRUSTED_PROXIES="YOUR_VALUE_HERE" \ -e OPENCLAW_GATEWAY_PASSWORD="YOUR_VALUE_HERE" \ -e OPENCLAW_CONTROL_UI_ALLOWED_ORIGINS="YOUR_VALUE_HERE" \ registry.hf.space/tonic-hugging-claw:latest just a few quite minor details i'll take care of but i wanted to share here first See translation</description><pubDate>Thu, 19 Feb 2026 14:10:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Tonic/772171916121212</guid></item><item><title>Hugging Face MCP Server v0.3.2</title><link>https://huggingface.co/posts/evalstate/567822573449139</link><description>Hugging Face MCP Server v0.3.2 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - Replace model_search and dataset_search with combined hub_repo_search tool. - Less distracting description for hf_doc_search - model_search and dataset_search tool calls will still function (plan to remove next release). See translation</description><pubDate>Thu, 19 Feb 2026 14:10:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/evalstate/567822573449139</guid></item><item><title>Gemma 3 (1b, 4b, 12b and 27b) - Uncensored full Reasoning/Thinking models fine tuned using top distill datasets.</title><link>https://huggingface.co/posts/DavidAU/658014279530502</link><description>Gemma 3 (1b, 4b, 12b and 27b) - Uncensored full Reasoning/Thinking models fine tuned using top distill datasets. 20 Gemma 3 models 1B, 4B, 12B and 27B with full reasoning using GLM 4.7 Flash, GPT, Claude and Gemini datasets and more fully fine tuned using Unsloth. Most models are Heretic'ed (uncensored) first, and tuned second. This vastly improves the model. Models are also bench marked and in almost all cases exceed org model metrics - and in some cases by a lot. Enjoy the freedom and more powerful THINKING/REASONING and UNCENSORED Gemma 3s ! https://huggingface.co/collections/DavidAU/gemma-3-reasoning-thinking-models-incl-uncensored See translation</description><pubDate>Thu, 19 Feb 2026 14:10:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DavidAU/658014279530502</guid></item><item><title>You can now run Qwen3.5 locally! üíú</title><link>https://huggingface.co/posts/danielhanchen/198614576061440</link><description>You can now run Qwen3.5 locally! üíú Qwen3.5-397B-A17B is an open MoE vision reasoning LLM for agentic coding &amp; chat. It performs on par with Gemini 3 Pro, Claude Opus 4.5 &amp; GPT-5.2. GGUF: unsloth/Qwen3.5-397B-A17B-GGUF Run Dynamic 3-bit on a 192GB Mac for 20 tokens/s. Guide: https://unsloth.ai/docs/models/qwen3.5 See translation</description><pubDate>Thu, 19 Feb 2026 14:10:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/198614576061440</guid></item><item><title>JavaScript-Code-Large</title><link>https://huggingface.co/posts/ajibawa-2023/551299945389829</link><description>JavaScript-Code-Large ajibawa-2023/JavaScript-Code-Large JavaScript-Code-Large is a large-scale corpus of JavaScript source code comprising around 5 million JavaScript files. The dataset is designed to support research in large language model (LLM) pretraining, code intelligence, software engineering automation, and program analysis for the JavaScript ecosystem. By providing a high-volume, language-specific corpus, JavaScript-Code-Large enables systematic experimentation in JavaScript-focused model training, domain adaptation, and downstream code understanding tasks. JavaScript-Code-Large addresses the need for a dedicated JavaScript-only dataset at substantial scale, enabling focused research across frontend, backend, and full-stack JavaScript environments. . See translation</description><pubDate>Thu, 19 Feb 2026 14:10:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ajibawa-2023/551299945389829</guid></item><item><title>NEW MODEL:</title><link>https://huggingface.co/posts/unmodeled-tyler/693994118177364</link><description>NEW MODEL: vanta-research/PE-Type-4-Solene-4B PE-Type-4-Solene-4B is the fourth release in Project Enneagram from VANTA Research, an initiative to study nuance in AI persona design wherein each of the 9 Enneagram types will be finetuned on the Gemma3 4B architecture. Solene is finetuned to exhibit the Individualist profile as defined by the Enneagram Institute; emotional honesty/depth, growth &amp; transformation intelligence, and creative expression. As with the other releases in this project, Solene is perfect for research applications, persona exploration, or self-improvement. Type 5 soon! See translation</description><pubDate>Thu, 19 Feb 2026 14:10:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/693994118177364</guid></item><item><title>We stress-test the FEETECH STS3215: real backlash (0.87¬∞ measured vs spec), repeatability, speed accuracy, stall torque above rating, and thermal overload behavior under continuous load. Practical implications for robot arms and grippers.</title><link>https://huggingface.co/posts/branikita/621296339305286</link><description>We stress-test the FEETECH STS3215: real backlash (0.87¬∞ measured vs spec), repeatability, speed accuracy, stall torque above rating, and thermal overload behavior under continuous load. Practical implications for robot arms and grippers. Full video: https://www.youtube.com/watch?v=UN5_fZVSWcw See translation</description><pubDate>Thu, 19 Feb 2026 14:10:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/621296339305286</guid></item><item><title>How do you prove a new AI privacy tool actually works? You test it against a world-class benchmark.</title><link>https://huggingface.co/posts/MikeDoes/749099860276897</link><description>How do you prove a new AI privacy tool actually works? You test it against a world-class benchmark. That's why we're proud our data played a key role in the research for "Rescriber," a new browser extension for user-led anonymization. To objectively measure their tool's performance against other methods, the researchers needed a diverse and challenging evaluation set. They built their benchmark using 240 samples from the Ai4Privacy open dataset. This is a win-win for the ecosystem: our open-source data helps researchers validate their innovative solutions, and in turn, their work pushes the entire field of privacy-preserving AI forward. The "Rescriber" tool is a fantastic step towards on-device, user-controlled privacy. üîó Learn more about their data-driven findings in the full paper: https://arxiv.org/pdf/2410.11876 üöÄ Stay updated on the latest in privacy-preserving AI‚Äîfollow us on LinkedIn: https://www.linkedin.com/company/ai4privacy/posts/ #DataPrivacy #AI #OpenSource...</description><pubDate>Thu, 19 Feb 2026 14:10:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/749099860276897</guid></item><item><title>3B vs 120B parameter open-source LLMs running locally on a Mac. Your Mac might be more powerful than you think!</title><link>https://huggingface.co/posts/EricFillion/199749719779775</link><description>3B vs 120B parameter open-source LLMs running locally on a Mac. Your Mac might be more powerful than you think! https://youtube.com/shorts/JdKtIdlnPs8?feature=share See translation</description><pubDate>Thu, 19 Feb 2026 14:10:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/EricFillion/199749719779775</guid></item></channel></rss>