<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Excited to introduce the Tiny VLMs Lab App for experiencing 15+ multimodal VLMs, ranging from a 250M parameter model to a 4B parameter model, for tasks like OCR, reasoning, small models for single-shot answering, and captioning (abliterated), across a broad range of visual categories including images with complex, sensitive, or nuanced content, while handling varying aspect ratios and resolutions.🧪</title><link>https://huggingface.co/posts/prithivMLmods/284574267701705</link><description>Excited to introduce the Tiny VLMs Lab App for experiencing 15+ multimodal VLMs, ranging from a 250M parameter model to a 4B parameter model, for tasks like OCR, reasoning, small models for single-shot answering, and captioning (abliterated), across a broad range of visual categories including images with complex, sensitive, or nuanced content, while handling varying aspect ratios and resolutions.🧪 🤗 Space/App: prithivMLmods/Tiny-VLMs-Lab ✦︎ Also introducing prithivMLmods/Qwen2.5-VL-3B-Abliterated-Caption-it , tailored for Abliterated Captioning / Uncensored Image Captioning. This release comes as a lighter alternative to the existing Qwen2.5-VL-7B-Abliterated-Caption-it prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it model, making it usable on mid-range GPUs and even experimental on T4 GPUs. ✦︎ Collection: prithivMLmods/vl-abliterated-caption-68a0443b63182e97a15c47a3 ✦︎ GitHub: https://github.com/PRITHIVSAKTHIUR/Tiny-VLMs-Lab . . . To know more about it, visit the app page or...</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/284574267701705</guid></item><item><title>Image-to-Prompt⚡</title><link>https://huggingface.co/posts/ovi054/657358125503535</link><description>Image-to-Prompt⚡ ovi054/image-to-prompt Extract text prompt from image. And you can reuse the prompt to generate similar images! Useful for prompt engineering, studying image-to-text alignment, making training datasets, or recreating similar outputs. Powered by: Gradio, Florence 2 👉 Try it now: ovi054/image-to-prompt See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/657358125503535</guid></item><item><title>When you ask ChatGPT, Claude, or Gemini a really tough question,</title><link>https://huggingface.co/posts/RakshitAralimatti/207934490136479</link><description>When you ask ChatGPT, Claude, or Gemini a really tough question, you might notice that little "thinking..." moment before it answers. But what does it actually mean when an LLM is “thinking”? Imagine a chess player pausing before their next move not because they don’t know how to play, but because they’re running through possibilities, weighing options, and choosing the best one. LLMs do something similar… except they’re not really thinking like us. Here’s the surprising part :- You might think these reasoning skills come from futuristic architectures or alien neural networks. In reality, most reasoning LLMs still use the same transformer decoder-only architecture as other models The real magic? It’s in how they’re trained and what data they learn from. Can AI actually think, or is it just insanely good at faking it? I broke it down in a simple, 4-minute Medium read. Bet you’ll walk away with at least one “aha!” moment. 🚀 Read here - https://lnkd.in/edZ8Ceyg See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/207934490136479</guid></item><item><title>✨ HairPick | Preview Your Perfect Hair Transformation in 360° ✨</title><link>https://huggingface.co/posts/ginipick/955296677233221</link><description>✨ HairPick | Preview Your Perfect Hair Transformation in 360° ✨ 🎊 Free Trial for Hugging Face Launch! Hurry! ⏰ Hello! Introducing an innovative AI service that helps you choose the perfect hairstyle without any regrets before visiting the salon! 🎯 Try It Now ginigen/Hair-Pick 🔄 What Makes HairPick Special? 360° Complete Preview! Other hair simulators only show the front view? 😑 HairPick is different! ✅ Front + 4 random angles = Total 5 multi-angle images generated ✅ Perfect check from side profile 👤 diagonal 📐 back view 👥! ✅ 100+ trendy hairstyle library 💇‍♀️ 💡 Highly Recommended For: 🎯 "I really don't want to fail this time!" → Check side volume and back lines thoroughly 🎯 "It's hard to explain exactly to my stylist" → Perfect communication with 360° result images! 🎯 "I have a profile photo/photoshoot coming up" → Preview your best look from every angle 🚀 Super Simple Usage (Just 1 Minute!) 1️⃣ One Selfie 📸 Take a front-facing photo in bright light (show your forehead and face...</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/955296677233221</guid></item><item><title>benchmarked 9 models in 3 days. they were mostly below average in AHA score. p(doom) probably increased :(</title><link>https://huggingface.co/posts/etemiz/891816438009932</link><description>benchmarked 9 models in 3 days. they were mostly below average in AHA score. p(doom) probably increased :( See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/891816438009932</guid></item><item><title>Want to quickly try Gemma 3 270m? 💎💬</title><link>https://huggingface.co/posts/anakin87/751707976654130</link><description>Want to quickly try Gemma 3 270m? 💎💬 I made a simple Space to do that: anakin87/gemma-3-270m-it ⚡ Fast: Flash Attention, Zero GPU ⚙️ Configurable See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/anakin87/751707976654130</guid></item><item><title>Qwen's latest Image Edit model has been implemented with lightx2v's LoRA for 8-step lightning fast inferencing. Still a WIP, so YMMV.</title><link>https://huggingface.co/posts/LPX55/307818881554669</link><description>Qwen's latest Image Edit model has been implemented with lightx2v's LoRA for 8-step lightning fast inferencing. Still a WIP, so YMMV. https://huggingface.co/spaces/LPX55/Qwen-Image-Edit-Lightning-Fast See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/LPX55/307818881554669</guid></item><item><title>✅ New Article: *Memory as Structured Time*</title><link>https://huggingface.co/posts/kanaria007/576453037371058</link><description>✅ New Article: *Memory as Structured Time* Title: 🧠 History: Memory Loops as Civilization Structure 🔗 https://huggingface.co/blog/kanaria007/memory-loops-as-civilization-structure --- Summary: Memory is often treated as *storage and retrieval*. Structured Intelligence reframes it as *time‑shaping architecture*: * *Loops that preserve context and continuity* * *Rollback paths that enable reflection and correction* * *Patterns that turn experience into adaptive structure* &gt; Memory isn’t static — &gt; *it’s how intelligence edits time.* --- Why It Matters: • Reveals *how memory enables learning, identity, and adaptation* • Supports *AI that can reflect, revise, and self‑align* • Connects *personal cognition and collective history* as structural processes --- What’s Inside: • Memory as *recursive structural loop* • *Failure and recovery* as part of adaptive recall • How *history and record‑keeping mirror cognitive memory* • Implications for *resilient AI and social knowledge systems* --- 📖...</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/576453037371058</guid></item><item><title>gpt-oss-120B scored 28 (one of the lowest) on AHA leaderboard. not very human aligned model.</title><link>https://huggingface.co/posts/etemiz/710778843328598</link><description>gpt-oss-120B scored 28 (one of the lowest) on AHA leaderboard. not very human aligned model. these kind of models are not really "free": they are costing you your freedom if you know what i mean. See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/710778843328598</guid></item><item><title>Added plug-and-play support for Qwen Image LoRA!  🤗⚡</title><link>https://huggingface.co/posts/prithivMLmods/366249407896156</link><description>Added plug-and-play support for Qwen Image LoRA! 🤗⚡ Try it here: ✦︎ Qwen-Image (with LoRA): prithivMLmods/Qwen-Image-Diffusion ✦︎ Collection: prithivMLmods/image-gen-apps-diffusion-lastupdated-08-18-68a2f4c5ef3e5e394eacc20a See translation</description><pubDate>Tue, 19 Aug 2025 09:25:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/366249407896156</guid></item></channel></rss>