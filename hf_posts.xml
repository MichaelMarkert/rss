<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>🔮 Mistral Perflexity AI - Local LLM Space with Web Search Capabilities 🌐</title><link>https://huggingface.co/posts/ginipick/917789522887291</link><description>🔮 Mistral Perflexity AI - Local LLM Space with Web Search Capabilities 🌐 Hello AI enthusiasts! Today I'm excited to introduce my special Hugging Face space! 🚀 ginigen/Mistral-Perflexity ✨ Key Features Powerful Model: Using Private-BitSix-Mistral-Small-3.1-24B-Instruct-2503, optimized through 6-bit quantization to run smoothly on local 4090 GPUs! 💪 Web Search Integration: Leveraging the Brave Search API to provide real-time web search results for user queries! 🔍 Customizable Responses: Shape AI personality and response format through system messages ⚙️ Multilingual Support: Perfect handling of both English and Korean! 🇺🇸🇰🇷 🛠️ Technical Highlights GGUF Format: Optimized quantized model with excellent memory efficiency Flash Attention: Applied optimization technology for faster inference speeds 8K Context Window: Capable of handling lengthy conversations and complex queries Streaming Responses: Watch text being generated in real-time 💡 Use Cases Complex Q&amp;A requiring real-time...</description><pubDate>Sun, 04 May 2025 17:18:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/917789522887291</guid></item><item><title>Forget everything you know about transcription models - NVIDIA's parakeet-tdt-0.6b-v2 changed the game for me!</title><link>https://huggingface.co/posts/fdaudens/694548457778636</link><description>Forget everything you know about transcription models - NVIDIA's parakeet-tdt-0.6b-v2 changed the game for me! Just tested it with Steve Jobs' Stanford speech and was speechless (pun intended). The video isn’t sped up. 3 things that floored me: - Transcription took just 10 seconds for a 15-min file - Got a CSV with perfect timestamps, punctuation &amp; capitalization - Stunning accuracy (correctly captured "Reed College" and other specifics) NVIDIA also released a demo where you can click any transcribed segment to play it instantly. The improvement is significant: number 1 on the ASR Leaderboard, 6% error rate (best in class) with complete commercial freedom (cc-by-4.0 license). Time to update those Whisper pipelines! H/t @ Steveeeeeeen for the finding! Model: nvidia/parakeet-tdt-0.6b-v2 Demo: nvidia/parakeet-tdt-0.6b-v2 ASR Leaderboard: hf-audio/open_asr_leaderboard See translation</description><pubDate>Sun, 04 May 2025 17:18:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/694548457778636</guid></item><item><title>🔥 Creating a qwen3-30b-a3b / qwen3-235b-a22b Chatbot with Deep Research Capabilities 🚀</title><link>https://huggingface.co/posts/openfree/174131256400578</link><description>🔥 Creating a qwen3-30b-a3b / qwen3-235b-a22b Chatbot with Deep Research Capabilities 🚀 openfree/qwen3-30b-a3b-research openfree/qwen3-235b-a22b-research Hello AI researchers! 👋 Today I'm introducing a powerful chatbot implementation with real-time web search capabilities. ✨ Key Features 🧠 Chatbot based on qwen3-30b-a3b and llama4-maverick models 🔍 LLM-based optimal keyword extraction 🌐 Real-time web search using SerpHouse API 💬 Streaming responses for natural conversation experience 🛠️ Technology Stack Gradio: Implementation of intuitive web interface Fireworks.ai API: Access to high-performance LLM models SerpHouse API: Collection of real-time search results 🌟 Application Areas Question answering systems requiring up-to-date information Providing current information beyond training data Delivering reliable information with accurate sources Add real-time search capabilities to your AI applications with this project! 🎉 Leave your questions or suggestions in the comments! Let's...</description><pubDate>Sun, 04 May 2025 17:18:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/174131256400578</guid></item><item><title>VisionScout — Now with Scene Understanding! 🚀</title><link>https://huggingface.co/posts/DawnC/822045713383062</link><description>VisionScout — Now with Scene Understanding! 🚀 I'm excited to share a major update to VisionScout, my interactive vision tool that combines powerful object detection with emerging scene understanding capabilities! 👀🔍 What can VisionScout do today? 🖼️ Upload any image and detect 80 object types using YOLOv8. 🔄 Instantly switch between Nano, Medium, and XLarge models depending on speed vs. accuracy needs. 🎯 Filter specific classes (people, vehicles, animals, etc.) to focus only on what matters to you. 📊 View detailed statistics on detected objects, confidence levels, and spatial distribution. ⭐️ NEW: Scene understanding layer now added! - Automatically interprets the scene based on detected objects. - Uses a combination of rule-based reasoning and CLIP-powered semantic validation. - Outputs descriptions, possible activities, and even safety concerns. What’s coming next? 🔎 Expanding YOLO’s object categories. 🎥 Adding video processing and multi-frame object tracking. ⚡ Faster real-time...</description><pubDate>Sun, 04 May 2025 17:18:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/822045713383062</guid></item><item><title>HOW TO ADD MCP SUPPORT TO ANY 🤗 SPACE</title><link>https://huggingface.co/posts/abidlabs/810486848644944</link><description>HOW TO ADD MCP SUPPORT TO ANY 🤗 SPACE Gradio now supports MCP! If you want to convert an existing Space, like this one hexgrad/Kokoro-TTS , so that you can use it with Claude Desktop / Cursor / Cline / TinyAgents / or any LLM that supports MCP, here's all you need to do: 1. Duplicate the Space (in the Settings Tab) 2. Upgrade the Gradio sdk_version to 5.28 (in the README.md ) 3. Set mcp_server=True in launch() 4. (Optionally) add docstrings to the function so that the LLM knows how to use it, like this: def generate ( text, speed= 1 ): """ Convert text to speech audio. Parameters: text (str): The input text to be converted to speech. speed (float, optional): Playback speed of the generated speech. That's it! Now your LLM will be able to talk to you 🤯 See translation</description><pubDate>Sun, 04 May 2025 17:18:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/abidlabs/810486848644944</guid></item><item><title>🖼️ OpenClipart SVG Dataset -</title><link>https://huggingface.co/posts/nyuuzyou/253978208604387</link><description>🖼️ OpenClipart SVG Dataset - nyuuzyou/openclipart Collection of 178,604 Public Domain Scalable Vector Graphics (SVG) clipart images featuring: - Comprehensive metadata: title, description, artist name, tags, original page URL, and more. - Contains complete SVG XML content (minified) for direct use or processing. - All images explicitly released into the public domain under the CC0 license. - Organized in a single train split with 178,604 entries. See translation</description><pubDate>Sun, 04 May 2025 17:18:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nyuuzyou/253978208604387</guid></item><item><title>Hi everyone,</title><link>https://huggingface.co/posts/mrfakename/455751106173013</link><description>Hi everyone, I just launched TTS Arena V2 - a platform for benchmarking TTS models by blind A/B testing. The goal is to make it easy to compare quality between open-source and commercial models, including conversational ones. What's new in V2: - **Conversational Arena**: Evaluate models like CSM-1B, Dia 1.6B, and PlayDialog in multi-turn settings - **Personal Leaderboard**: Optional login to see which models you tend to prefer - **Multi-speaker TTS**: Random voices per generation to reduce speaker bias - **Performance Upgrade**: Rebuilt from Gradio → Flask. Much faster with fewer failed generations. - **Keyboard Shortcuts**: Vote entirely via keyboard Also added models like MegaTTS 3, Cartesia Sonic, and ElevenLabs' full lineup. I'd love any feedback, feature suggestions, or ideas for models to include. TTS-AGI/TTS-Arena-V2 See translation</description><pubDate>Sun, 04 May 2025 17:18:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mrfakename/455751106173013</guid></item><item><title>you can easily fine-tune, quantize, play with sota vision LM InternVL3 now 🔥</title><link>https://huggingface.co/posts/merve/777073133757524</link><description>you can easily fine-tune, quantize, play with sota vision LM InternVL3 now 🔥 we have recently merged InternVL3 to Hugging Face transformers and released converted checkpoints 🤗 collection for converted checkpoints: merve/internvl3-hf-6814be2943b2ae0e711c92a5 notebook: https://colab.research.google.com/drive/1wAQ7cyjyaCwLXbMA_OjXZe7aCxCFm6sI?usp=sharing 📖 See translation</description><pubDate>Sun, 04 May 2025 17:18:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/777073133757524</guid></item><item><title>🚀 Excited to Share Our Latest Work: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer～</title><link>https://huggingface.co/posts/RiverZ/535015681556179</link><description>🚀 Excited to Share Our Latest Work: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer～ 🎨 Daily Paper: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer (2504.20690) 🔓 Code is now open source! 🔥 Huggingface DEMO: RiverZ/ICEdit 🌐 Project Website: https://river-zhang.github.io/ICEdit-gh-pages/ 🏠 GitHub Repository: https://github.com/River-Zhang/ICEdit/blob/main/scripts/gradio_demo.py 🤗 Huggingface: sanaka87/ICEdit-MoE-LoRA 📄 arxiv Paper: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer (2504.20690) 🔥 Why it’s cool: - Achieves high-quality, multi-task image editing. - Uses only 1% of the training parameters and 0.1% of the training data compared to existing methods — extremely efficient - Beats several commercial models on background preservation, ID control, and consistency - Open-...</description><pubDate>Sun, 04 May 2025 17:18:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RiverZ/535015681556179</guid></item><item><title>One of the biggest challenges I've been facing since I started developing [𝐏𝐝𝐟𝐈𝐭𝐃𝐨𝐰𝐧](</title><link>https://huggingface.co/posts/as-cle-bert/299436064475061</link><description>One of the biggest challenges I've been facing since I started developing [𝐏𝐝𝐟𝐈𝐭𝐃𝐨𝐰𝐧]( https://github.com/AstraBert/PdfItDown ) was handling correctly the conversion of files like Excel sheets and CSVs: table conversion was bad and messy, almost unusable for downstream tasks🫣 That's why today I'm excited to introduce 𝐫𝐞𝐚𝐝𝐞𝐫𝐬, the new feature of PdfItDown v1.4.0!🎉 With 𝘳𝘦𝘢𝘥𝘦𝘳𝘴, you can choose among three (for now👀) flavors of text extraction and conversion to PDF: - 𝗗𝗼𝗰𝗹𝗶𝗻𝗴, which does a fantastic work with presentations, spreadsheets and word documents🦆 - 𝗟𝗹𝗮𝗺𝗮𝗣𝗮𝗿𝘀𝗲 by LlamaIndex, suitable for more complex and articulated documents, with mixture of texts, images and tables🦙 - 𝗠𝗮𝗿𝗸𝗜𝘁𝗗𝗼𝘄𝗻 by Microsoft, not the best at handling highly structured documents, by extremly flexible in terms of input file format (it can even convert XML, JSON and ZIP files!)✒️ You can use this new feature in your python scripts (check the attached code snippet!😉) and in the command line interface as well!🐍...</description><pubDate>Sun, 04 May 2025 17:18:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/as-cle-bert/299436064475061</guid></item></channel></rss>