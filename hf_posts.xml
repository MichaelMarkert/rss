<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>üåà‚ú® FLUX 'Every Text Imaginator'</title><link>https://huggingface.co/posts/ginipick/804980229974999</link><description>üåà‚ú® FLUX 'Every Text Imaginator' Multilingual Text-Driven Image Generation and Editing Demo: ginigen/Every-Text üìù What is FLUX Text Imaginator? FLUX Text Imaginator is an innovative tool that leverages cutting-edge FLUX diffusion models to create and edit images with perfectly integrated multilingual text. Unlike other image generation models, FLUX possesses exceptional capability to naturally incorporate text in various languages including Korean, English, Chinese, Japanese, Russian, French, Spanish and more into images! ‚ú® FLUX's Multilingual Text Processing Strengths üî§ Superior Multilingual Text Rendering: FLUX renders text with amazing accuracy, including non-English languages and special characters üá∞üá∑ Perfect Korean Language Support: Accurately represents complex Korean combined characters üà∂ Excellent East Asian Language Handling: Naturally expresses complex Chinese characters and Japanese text üîç Sophisticated Text Placement: Precise text positioning using &lt;text1&gt;, &lt;text2&gt;,...</description><pubDate>Thu, 20 Mar 2025 17:20:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/804980229974999</guid></item><item><title>‚úÇÔ∏è AutoAbliteration</title><link>https://huggingface.co/posts/mlabonne/714992455492422</link><description>‚úÇÔ∏è AutoAbliteration I made a Colab notebook to automatically abliterate models. It's quite general, so you can do interesting stuff like blocking a given language in the model outputs. üíª Colab: https://colab.research.google.com/drive/1RmLv-pCMBBsQGXQIM8yF-OdCNyoylUR1?usp=sharing See translation</description><pubDate>Thu, 20 Mar 2025 17:20:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/714992455492422</guid></item><item><title>RWKV7-G1 0.1B üî• Pure RNN reasoning model released by RWKV</title><link>https://huggingface.co/posts/AdinaY/946974056180555</link><description>RWKV7-G1 0.1B üî• Pure RNN reasoning model released by RWKV Model: BlinkDL/rwkv7-g1 paper: RWKV-7 "Goose" with Expressive Dynamic State Evolution (2503.14456) ‚ú® Apache2.0 ‚ú® Supports 100+ languages ‚ú® 0.1 B runs smoothly on low power devices ‚ú® 0.4B/1.5B/2.9B are coming soon!! See translation</description><pubDate>Thu, 20 Mar 2025 17:20:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/946974056180555</guid></item><item><title>We open-sourced the</title><link>https://huggingface.co/posts/sharpenb/393986344403251</link><description>We open-sourced the pruna package that can be easily installed with pip install pruna :) It allows to easily ccompress and evaluate AI models including transformers and diffusers. - Github repo: https://github.com/PrunaAI/pruna - Documentation: https://docs.pruna.ai/en/stable/index.html With open-sourcing, people can now inspect and contribute to the open code. Beyond the code, we provide detailed readme, tutorials, benchmarks, and documentation to make transparent compression, evaluation, and saving/loading/serving of AI models. Happy to share it with you and always interested in collecting your feedback :) See translation</description><pubDate>Thu, 20 Mar 2025 17:20:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sharpenb/393986344403251</guid></item><item><title>Big companies are now training huge AI models with tons of data and billions of parameters, and the future seems to be about quantization‚Äîmaking those models smaller by turning big numbers into simpler ones, like going from 32-bit to 8-bit without reducing accuracy by +/- 0.01%. There should be some standard unit of measurement for the ratio of model size reduction to accuracy lost.</title><link>https://huggingface.co/posts/ritvik77/148515751469332</link><description>Big companies are now training huge AI models with tons of data and billions of parameters, and the future seems to be about quantization‚Äîmaking those models smaller by turning big numbers into simpler ones, like going from 32-bit to 8-bit without reducing accuracy by +/- 0.01%. There should be some standard unit of measurement for the ratio of model size reduction to accuracy lost. What do you all thing about this ? See translation</description><pubDate>Thu, 20 Mar 2025 17:20:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ritvik77/148515751469332</guid></item><item><title>üòä This program is designed to remove emojis from a given text. It uses a regular expression (regex) pattern to match and replace emojis with an empty string, effectively removing them from the text. The pattern includes a range of Unicode characters that correspond to various types of emojis, such as emoticons, symbols, and flags. By using this program, you can clean up text data by removing any emojis that may be present, which can be useful for text processing, analysis, or other applications where emojis are not desired. üíª</title><link>https://huggingface.co/posts/aifeifei798/845431141575759</link><description>üòä This program is designed to remove emojis from a given text. It uses a regular expression (regex) pattern to match and replace emojis with an empty string, effectively removing them from the text. The pattern includes a range of Unicode characters that correspond to various types of emojis, such as emoticons, symbols, and flags. By using this program, you can clean up text data by removing any emojis that may be present, which can be useful for text processing, analysis, or other applications where emojis are not desired. üíª import re def remove_emojis ( text ): # Define a broader emoji pattern emoji_pattern = re. compile ( "[" u"\U0001F600-\U0001F64F" # emoticons u"\U0001F300-\U0001F5FF" # symbols &amp; pictographs u"\U0001F680-\U0001F6FF" # transport &amp; map symbols u"\U0001F1E0-\U0001F1FF" # flags (iOS) u"\U00002702-\U000027B0" u"\U000024C2-\U0001F251" u"\U0001F900-\U0001F9FF" # supplemental symbols and pictographs u"\U0001FA00-\U0001FA6F" # chess symbols and more emojis...</description><pubDate>Thu, 20 Mar 2025 17:20:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aifeifei798/845431141575759</guid></item><item><title>Nice new space to see how fast your personal or organization followers are growing on HF:</title><link>https://huggingface.co/posts/clem/654976954929507</link><description>Nice new space to see how fast your personal or organization followers are growing on HF: julien-c/follow-history As you can see, I still have more followers than @ julien-c even if he's trying to change this by building such cool spaces üòùüòùüòù See translation</description><pubDate>Thu, 20 Mar 2025 17:20:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/654976954929507</guid></item><item><title>#PII Masking Tech that does not **** around!</title><link>https://huggingface.co/posts/MikeDoes/305810022878028</link><description>#PII Masking Tech that does not **** around! We are happy to release the OpenPII English Anonymiser ‚Äîthe most powerful open-source tool for redacting sensitive info from English text. Fine-tuned Modernbert on 5.7 million+ PII examples, it‚Äôs clocking 99%+ accuracy across emails, dates, social numbers, and more! Why it‚Äôs a big deal: ‚úÖ Top-tier precision: 100% for passport numbers, 99.96% for emails*. ‚úÖ Totally free: MIT license for personal or commercial use. ‚úÖ No secrets: Full metrics shared on Hugging Face. #AI #OpenSource #DataSecurity @ huggingface Day 2 out 7 of PII-Masking-1M Announcements Complete! *Accuracies reported from the new OpenPII-500k dataset ai4privacy/llama-ai4privacy-english-anonymiser-openpii See translation</description><pubDate>Thu, 20 Mar 2025 17:20:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/305810022878028</guid></item><item><title>Nvidia brings blue (from starwars droids) to life ü§Ø, supercute with flawless dexterity and droid voice. It's the result of their colab research with Google DeepMind and Disney, revealed as part of their new opensource physics engine for robotics simulation: NEWTON - which enables robots to learn how to complete complex tasks with greater precision.</title><link>https://huggingface.co/posts/Jaward/685498113959409</link><description>Nvidia brings blue (from starwars droids) to life ü§Ø, supercute with flawless dexterity and droid voice. It's the result of their colab research with Google DeepMind and Disney, revealed as part of their new opensource physics engine for robotics simulation: NEWTON - which enables robots to learn how to complete complex tasks with greater precision. ReadMore: https://developer.nvidia.com/blog/announcing-newton-an-open-source-physics-engine-for-robotics-simulation?ncid=so-twit-820797-vt48 See translation</description><pubDate>Thu, 20 Mar 2025 17:20:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/685498113959409</guid></item><item><title>If you've been following along with the Xet Team's (</title><link>https://huggingface.co/posts/jsulz/922959392704733</link><description>If you've been following along with the Xet Team's ( https://huggingface.co/xet-team ) work, you know we've been working to migrate the Hugging Face Hub from Git LFS and to Xet. Recently, we launched a waitlist to join the movement to Xet (join here! https://huggingface.co/join/xet ) but getting to this point was a journey. From the initial proof of concept in August, to launching on the Hub internally, to migrating a set of repositories and routing a small chunk of download traffic on the Hub through our infrastructure. Every step of the way has been full of challenges, big and small, and well worth the effort. Over the past few weeks, with real traffic flowing through our services we‚Äôve tackled some truly gnarly issues (unusual upload/download patterns, memory leaks, load imbalances, and more) and resolved each without major disruptions. If you're curious about how this sliver of Hub infrastructure looks as we routed traffic through it for the first time (and want a deep dive full...</description><pubDate>Thu, 20 Mar 2025 17:20:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/922959392704733</guid></item></channel></rss>