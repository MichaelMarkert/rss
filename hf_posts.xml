<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AI Just Made My Cat the King of Emojis 👑🐱😂</title><link>https://huggingface.co/posts/Monica997/874620000877286</link><description>AI Just Made My Cat the King of Emojis 👑🐱😂 Never thought I’d see this — but with iMini’s nano banana model, my cat is now a full emoji + sticker pack 🎨✨ Used the 9-grid meme template + cartoon sticker generator, and in just ONE click 👉 my ordinary cat photo turned into a hilarious, cute, and super shareable set of stickers 💬🔥 No need to master complicated nano banana prompts — iMini handles everything. Perfect for chats, socials, or just showing off your pet’s new “digital identity.” 👉 Try it here: https://imini.com/nano-banana Who else wants their pet to be the next emoji star? 🌟 See translation</description><pubDate>Thu, 25 Sep 2025 05:21:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Monica997/874620000877286</guid></item><item><title>Photo-Mate-i2i – a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages!</title><link>https://huggingface.co/posts/prithivMLmods/355225487543965</link><description>Photo-Mate-i2i – a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages! ⚡Demo: prithivMLmods/Photo-Mate-i2i ⚙️How to Use: prithivMLmods/Photo-Mate-i2i#2 👨‍🔧i2i-Kontext(Experimental LoRAs): prithivMLmods/i2i-kontext-exp-68ce573b5c0623476b636ec7 See translation</description><pubDate>Thu, 25 Sep 2025 05:21:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/355225487543965</guid></item><item><title>I'm sorry, what?</title><link>https://huggingface.co/posts/nroggendorff/916862110503909</link><description>I'm sorry, what?</description><pubDate>Thu, 25 Sep 2025 05:21:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/916862110503909</guid></item><item><title>YOLOv11 Complete On-device Study</title><link>https://huggingface.co/posts/yeonseok-zeticai/506441566129403</link><description>YOLOv11 Complete On-device Study - {NPU vs GPU vs CPU} Across All Model Variants We've just completed comprehensive benchmarking of the entire YOLOv11 family on ZETIC.MLange. Here's what every ML engineer needs to know. 📊 Key Findings Across 5 Model Variants (XL to Nano): 1. NPU Dominance in Efficiency: - YOLOv11n: 1.72ms on NPU vs 53.60ms on CPU (31x faster) - Memory footprint: 0-65MB across all variants - Consistent sub-10ms inference even on XL models 2. The Sweet Spot - YOLOv11s: - NPU: 3.23ms @ 95.57% mAP - Perfect balance: 36MB model, production-ready speed - 10x faster than GPU, 30x faster than CPU 3. Surprising Discovery: Medium models (YOLOv11m) show unusual GPU performance patterns - NPU outperforms GPU by 4x (9.55ms vs 35.82ms), suggesting current GPU kernels aren't optimized for mid-size architectures. 4. Production Insights: - XL/Large: GPU still competitive for batch processing - Small/Nano: NPU absolutely crushes everything else - Memory scaling: Linear from 10MB...</description><pubDate>Thu, 25 Sep 2025 05:21:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/506441566129403</guid></item><item><title>10 awesome advanced LoRA approaches</title><link>https://huggingface.co/posts/Kseniase/445000542637232</link><description>10 awesome advanced LoRA approaches Low-Rank Adaptation (LoRA) is the go-to method for efficient model fine-tuning that adds small low-rank matrices instead of retraining full models. The field isn’t standing still – new LoRA variants push the limits of efficiency, generalization, and personalization. So we’re sharing 10 of the latest LoRA approaches you should know about: 1. Mixture-of-LoRA-experts → Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection (2509.13878) Adds multiple low-rank adapters (LoRA) into a model’s layers, and a routing mechanism activates the most suitable ones for each input. This lets the model adapt better to new unseen conditions 2. Amortized Bayesian Meta-Learning for LoRA (ABMLL) → Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models (2508.14285) Balances global and task-specific parameters within a Bayesian framework to improve uncertainty calibration and generalization to new tasks without high...</description><pubDate>Thu, 25 Sep 2025 05:21:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/445000542637232</guid></item><item><title>🎯 RetinaFace On-Device Deployment Study: NPU Acceleration Breakthrough!</title><link>https://huggingface.co/posts/yeonseok-zeticai/752870941871415</link><description>🎯 RetinaFace On-Device Deployment Study: NPU Acceleration Breakthrough! (Check details at :https://mlange.zetic.ai/p/Steve/RetinaFace) TL;DR: Successfully deployed RetinaFace with ZETIC.MLange achieving 1.43ms inference on mobile NPU! 🔍 Complete Performance Analysis: Latency Comparison: - NPU: 1.43ms (Winner! 🏆) - GPU: 3.75ms - CPU: 21.42ms Accuracy Metrics - SNR: - FP16: 56.98 dB - Integer Quantized: 48.03 dB (Precision-Performance: Excellent trade-off maintained) Memory Footprint: - Model Size: 2.00 MB (highly compressed) - Runtime Memory: 14.58 MB peak - Deployment Ready: ✅ Production optimized 🛠 Technical Implementation: (Runnable with Copy &amp; Paste at the MLange link!) 📊 Device Compatibility Matrix: Tested on 50+ devices including Samsung Galaxy series, Google Pixel lineup, and Xiaomi devices, iPhones and iPads. Consistent sub-5ms performance across the board! 🚀 Applications Unlocked: - Real-time AR/VR face tracking - Privacy-preserving edge authentication - Live video...</description><pubDate>Thu, 25 Sep 2025 05:21:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/752870941871415</guid></item><item><title>BAAI has released ROME🔥 evaluating 30+ large reasoning models on text &amp; visual reasoning</title><link>https://huggingface.co/posts/AdinaY/290924120685458</link><description>BAAI has released ROME🔥 evaluating 30+ large reasoning models on text &amp; visual reasoning FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions (2509.17177) ✨Tests visual reasoning, not just recognition ✨Covers capability × alignment × safety × efficiency ✨More transparent &amp; reliable (less data contamination) ✨Helps make real-world deployment choices See translation</description><pubDate>Thu, 25 Sep 2025 05:21:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/290924120685458</guid></item><item><title>large AI labs open-sourced a ton of models last week 🔥</title><link>https://huggingface.co/posts/merve/604366247415617</link><description>large AI labs open-sourced a ton of models last week 🔥 here's few picks, find even more here merve/sep-16-releases-68d13ea4c547f02f95842f05 🤝 &gt; IBM released a new Docling model with 258M params based on Granite (A2.0) 📝 ibm-granite/granite-docling-258M &gt; Xiaomi released 7B audio LM with base and instruct variants (MIT) XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0 &gt; DecartAI released Lucy Edit, open Nano Banana 🍌 (NC) decart-ai/Lucy-Edit-Dev &gt; OpenGVLab released a family of agentic computer use models (3B/7B/32B) with the dataset 💻 OpenGVLab/scalecua-68c912cf56f7ff4c8e034003 &gt; Meituan Longcat released thinking version of LongCat-Flash 💭 meituan-longcat/LongCat-Flash-Thinking See translation</description><pubDate>Thu, 25 Sep 2025 05:21:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/604366247415617</guid></item><item><title>Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.🤗✨</title><link>https://huggingface.co/posts/prithivMLmods/322831563234696</link><description>Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.🤗✨ prithivMLmods/PhotoCleanser-i2i : Remove objects while preserving the rest of the image. prithivMLmods/Photo-Restore-i2i : Restore old photos into moderately colorized, detailed images. prithivMLmods/Polaroid-Warm-i2i : Seamless vintage Polaroid-style images with warm, faded tones. prithivMLmods/Yarn-Photo-i2i : Convert images into yarn-stitched artwork while retaining key details. prithivMLmods/Monochrome-Pencil : Turn images into monochrome pencil sketches while keeping original features. ✨Note: All the above models share the same auto-labeling multimodal VLM captioning model, prithivMLmods/DeepCaption-VLA-7B , which is used...</description><pubDate>Thu, 25 Sep 2025 05:21:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/322831563234696</guid></item><item><title>Qwen3Guard 🛡️ a series of safety moderation models built upon Qwen3</title><link>https://huggingface.co/posts/AdinaY/244715855875832</link><description>Qwen3Guard 🛡️ a series of safety moderation models built upon Qwen3 Qwen/qwen3guard-68d2729abbfae4716f3343a1 ✨ 0.6B/4B/8B - Apache2.0 ✨ Two variants: Gen &amp; Steam ✨ Trained on a dataset of 1.19 million prompts ✨ Classifies content into Safe / Unsafe / Controversial ✨ Supports 119 languages &amp; dialects See translation</description><pubDate>Thu, 25 Sep 2025 05:21:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/244715855875832</guid></item></channel></rss>