<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Multilingual Tokenization Showdown</title><link>https://huggingface.co/posts/Norod78/977626760436669</link><description>Multilingual Tokenization Showdown Analyzing 12 LLM Tokenizers Across 204 Languages. First, I've created a dataset with Wikipedia's "Cat" article text in 272 languages: Norod78/WikiCat-Multilingual For each language entry with at least 100 words, I tokenized the text using 12 tokenizers and calculated the "Characters per token" ratio and "Word per token" ratio. The higher this ratio is, the more information each token represents on average for that language (and perhaps allowing the llm to potentially learn more per-parameter if trained on a dataset of that language). You can see a slideshow summary of the results here: https://norod.github.io/wikicat-tokenizer-eval/tokenizer-slideshow.html I hope I interpreted the results correctly, I've made the code available on GitHub so you can re-create the raw results jsonl with this repo: https://github.com/Norod/wikicat-tokenizer-eval Post on X: https://x.com/Norod78/status/1984366900550266999 See translation</description><pubDate>Mon, 03 Nov 2025 13:34:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Norod78/977626760436669</guid></item><item><title>11 Fascinating new Policy Optimization techniques</title><link>https://huggingface.co/posts/Kseniase/468043722468280</link><description>11 Fascinating new Policy Optimization techniques Policy optimization (PO) algorithms are central to training AI models with preference-based feedback. In recent weeks, numerous new PO methods have emerged that build on or replace the popular PPO and GRPO, solving their issues. Here are 11 of them: 1. BAlanced Policy Optimization (BAPO) â†’ BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping (2510.18927) Dynamically adjusting the clipping bounds in PPO-style updates to balance positive and negative gradients and prevent entropy collapse 2. Training-Free GRPO â†’ Training-Free Group Relative Policy Optimization (2510.08191) Instead of using numeric rewards, it compares rollouts semantically to distill useful knowledge as a token prior, which is then applied during inference to guide the modelâ€™s behavior 3. Asymmetric Importance Sampling Policy Optimization (ASPO) â†’ ASPO: Asymmetric Importance Sampling Policy Optimization...</description><pubDate>Mon, 03 Nov 2025 13:34:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/468043722468280</guid></item><item><title>*** Happy Halloween - Embrace the Horror ! ***</title><link>https://huggingface.co/posts/DavidAU/433692013361833</link><description>*** Happy Halloween - Embrace the Horror ! *** Unsloth fine tunes using in house horror dataset. Gemma 3 - 1B, 4B, two 12Bs and 27B (uploaded yesterday) Qwen 3 - 1.7B [two] - new today... and , 4B, 6B, 42B ... And 32 MORE horror models: https://huggingface.co/DavidAU/models?search=horror Collection: https://huggingface.co/collections/DavidAU/grand-horror-165b-horror-and-fiction-generation Enjoy ; See translation</description><pubDate>Mon, 03 Nov 2025 13:34:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DavidAU/433692013361833</guid></item><item><title>I am dedicating this weekend to practicing/reading the latest b(ook)log from hugging face. It is meant to be a guide for anyone trying to go from â€œwe have a great dataset and GPUsâ€ to â€œwe built a really strong model.â€ Will share thoughts upon completion.</title><link>https://huggingface.co/posts/Shivansh000/941986646578616</link><description>I am dedicating this weekend to practicing/reading the latest b(ook)log from hugging face. It is meant to be a guide for anyone trying to go from â€œwe have a great dataset and GPUsâ€ to â€œwe built a really strong model.â€ Will share thoughts upon completion. Thanks for the treat @ eliebak @ ThomasWolf and HF team! HuggingFaceTB/smol-training-playbook See translation</description><pubDate>Mon, 03 Nov 2025 13:34:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Shivansh000/941986646578616</guid></item><item><title>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the</title><link>https://huggingface.co/posts/sergiopaniego/207791817757812</link><description>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the @ TogetherAgent x @ huggingface workshop we hosted in our Paris office ğŸƒ! Link: https://github.com/sergiopaniego/talks/blob/main/fine_tuning_with_trl/Fine%20tuning%20with%20TRL%20(Oct%2025).pdf See translation</description><pubDate>Mon, 03 Nov 2025 13:34:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/207791817757812</guid></item><item><title>Introducing the Medical-o1-Reasoning-SFT-Japanese dataset ğŸ‰</title><link>https://huggingface.co/posts/ronantakizawa/591564562942305</link><description>Introducing the Medical-o1-Reasoning-SFT-Japanese dataset ğŸ‰ This dataset is a Japanese dataset consisting questions, reasoning, and answer results for complex medical topics. #japanese #medical #dataset ronantakizawa/Medical-o1-Reasoning-SFT-Japanese See translation</description><pubDate>Mon, 03 Nov 2025 13:34:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/591564562942305</guid></item><item><title>New Datasets Published:</title><link>https://huggingface.co/posts/unmodeled-tyler/973176037226952</link><description>New Datasets Published: vanta-research/poetic-imagery-small vanta-research/excitement-small We are open sourcing two of our datasets today, which were used in the training of Apollo Astralis 8B and 4B. The first dataset, poetic-imagery-small is designed to give the model's responses a bit of "depth" to them in order to encourage curiosity and thought from the user. Additionally, the excitement-small dataset is designed to teach the model how to use "excited" language conversationally. This dataset was used on both Apollo Astralis models, which effectively demonstrate general excitement during user interaction. VANTA Research is an AI safety project which aims to research and develop language models aligned for all types of thinking. These datasets were created aligned with that mission, in addition to rigorous AI safety standards. See translation</description><pubDate>Mon, 03 Nov 2025 13:34:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/973176037226952</guid></item><item><title>A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on â†—ï¸</title><link>https://huggingface.co/posts/prithivMLmods/710644146568512</link><description>A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on â†—ï¸ https://huggingface.co/blog/prithivMLmods/multimodal-ocr-vlms on behalf of strangervisionhf It discusses the latest trends in OCR models, the multilingual support offered by modern OCR systems, their unique capabilities, OCR benchmark model comparisons, transformer-based implementations, and strategies for streamlining transformers compatibility. See translation</description><pubDate>Mon, 03 Nov 2025 13:34:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/710644146568512</guid></item><item><title>ğŸš€ğŸ‘ŒğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ¤ŒğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/744756733617336</link><description>ğŸš€ğŸ‘ŒğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ¤ŒğŸš€ ğŸ“„ Title: Understanding Co-speech Gestures in-the-wild ğŸ” ğŸ“ Description: JEGAL is a tri-modal model that learns from gestures, speech and text simultaneously, enabling devices to interpret co-speech gestures in the wild. ğŸ‘¥ Authors: @ sindhuhegde , K R Prajwal, Taein Kwon, and Andrew Zisserman ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: Understanding Co-speech Gestures in-the-wild (2503.22668) ğŸŒ Web Page: https://www.robots.ox.ac.uk/~vgg/research/jegal ğŸ“ Repository: https://github.com/Sindhu-Hegde/jegal ğŸ“º Video: https://www.youtube.com/watch?v=TYFOLKfM-rM ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the Human Modeling Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/human-modeling.md ğŸ“š More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers curated by @ DmitryRyumin ğŸ” Keywords:...</description><pubDate>Mon, 03 Nov 2025 13:34:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/744756733617336</guid></item><item><title>At</title><link>https://huggingface.co/posts/branikita/910220398337791</link><description>At Robonine , we applied topology optimization to enhance the stiffness and efficiency of a robotic manipulator. Using HyperMesh with the OptiStruct solver, we defined the design space where each element had a pseudo-density coefficient (0â€“1) controlling stiffness. This allowed the algorithm to continuously redistribute material toward regions with higher strain energy â€” much like how a fluid naturally flows to balance pressure. Results: - Aluminum bracket: displacement reduced by 0.16 mm - Steel bracket: displacement reduced from 1.05 mm â†’ 0.63 mm - Steel clamp: displacement reduced by 0.14 mm - Final structure: optimized geometry with improved load distribution and reduced deformation This project highlights how advanced structural optimization can significantly improve performance while minimizing material usage â€” shaping the next generation of robotic design. See translation</description><pubDate>Mon, 03 Nov 2025 13:34:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/910220398337791</guid></item></channel></rss>