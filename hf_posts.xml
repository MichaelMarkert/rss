<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Christmas came early this year</title><link>https://huggingface.co/posts/csabakecskemeti/277521964775652</link><description>Christmas came early this year See translation</description><pubDate>Mon, 20 Oct 2025 09:27:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/csabakecskemeti/277521964775652</guid></item><item><title>Introducing AWQ and GPTQ quantized versions of SmolVLM from Hugging Face!</title><link>https://huggingface.co/posts/ronantakizawa/523357349957866</link><description>Introducing AWQ and GPTQ quantized versions of SmolVLM from Hugging Face! These models only had their text models quantized, and had a 50% model size reduction (4GB~2GB) while keeping model degradation under 1% on the DocVQA benchmark. #huggingface #smolvlm #smollm ronantakizawa/SmolVLM-Instruct-awq ronantakizawa/SmolVLM-Instruct-gptq See translation</description><pubDate>Mon, 20 Oct 2025 09:27:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/523357349957866</guid></item><item><title>I'm excited to announce the release of Kanon 2 Embedder, the world's best legal embedding model, ranked first on the Massive Legal Embedding Benchmark üéâ</title><link>https://huggingface.co/posts/umarbutler/611899757397292</link><description>I'm excited to announce the release of Kanon 2 Embedder, the world's best legal embedding model, ranked first on the Massive Legal Embedding Benchmark üéâ This model is the product of quite literally months of painstaking work alongside @ abdurrahmanbutler collecting, cleaning, and processing terabytes of data as well as coming up with novel improvements to the standard embedder training recipe to push the limits of what's possible. Kanon 2 Embedder is my most advanced model to date. On MLEB, it benchmarks as 9% more accurate than OpenAI's best embedding model and 30% faster. Even when truncated from 1,792 to 768 dimensions, Kanon 2 Embedder continues to hold the number one spot on MLEB. Importantly, Kanon 2 Embedder is also privacy and security friendly ‚Äî unlike Voyage, Cohere and Jina, none of your data is used to train our models by default. Kanon 2 Embedder can also be self-hosted for enterprises with heightened security or reliability requirements. You can read the full...</description><pubDate>Mon, 20 Oct 2025 09:27:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/umarbutler/611899757397292</guid></item><item><title>today is going to be a great day for small models, are you ready?</title><link>https://huggingface.co/posts/appvoid/680866662633308</link><description>today is going to be a great day for small models, are you ready? See translation</description><pubDate>Mon, 20 Oct 2025 09:27:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/appvoid/680866662633308</guid></item><item><title>5 Lectures and keynotes defining AI right now</title><link>https://huggingface.co/posts/Kseniase/152348317273822</link><description>5 Lectures and keynotes defining AI right now If you want to understand the multifaceted AI landscape in 2025 and see where the field is heading ‚Äì start with (or revisit) these legendary talks. They can help you capture what‚Äôs happening in AI from multiple angles: 1. Andrej Karpathy: Software Is Changing (Again) ‚Üí https://www.youtube.com/watch?v=LCEmiRjPEtQ Unveils Software 3.0 ‚Äì a paradigm where LLMs are the new computers, programmed with prompts instead of code. The key: developers must now master coding, training, and prompting as AI becomes the heart of software building 2. Richard Sutton, The OaK Architecture: A Vision of SuperIntelligence from Experience ‚Üí https://www.youtube.com/watch?v=gEbbGyNkR2U Unveils the OaK (Options and Knowledge) architecture ‚Äì a model-based RL framework for continual intelligence, where every component learns, meta-learns &amp; builds hierarchical abstractions 3. GTC March 2025 Keynote with NVIDIA CEO Jensen Huang ‚Üí...</description><pubDate>Mon, 20 Oct 2025 09:27:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/152348317273822</guid></item><item><title>I love getting emails telling me when there's somebody else's active access token in one of my commit SHAs. HF should really only tell you if it is your token, otherwise I could just make a dataset with a bunch of random strings and wait for a valid token.</title><link>https://huggingface.co/posts/nroggendorff/141176924031866</link><description>I love getting emails telling me when there's somebody else's active access token in one of my commit SHAs. HF should really only tell you if it is your token, otherwise I could just make a dataset with a bunch of random strings and wait for a valid token. permission,token write,hf_. .. finegrained,hf_. .. ... , ... ... Also, don't comment about how unlikely this is. I've gotten a warning email about a token I 'leaked' at least four times. In all cases, it has been in the digest hash. See translation</description><pubDate>Mon, 20 Oct 2025 09:27:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/141176924031866</guid></item><item><title>üçå Huggwarts Banana - Nanobanana + Long-form Text Rendering Add-on</title><link>https://huggingface.co/posts/ginipick/606069092091867</link><description>üçå Huggwarts Banana - Nanobanana + Long-form Text Rendering Add-on üéØ The Critical Flaw in Nanobanana Google's Nanobanana is excellent, but has one critical weakness: Long-form text rendering breaks easily. Huggwarts Banana = Nanobanana + Long-form Text Rendering Add-on ginigen/AI ‚ú® Core Value üî• Nanobanana's Foundation Text-to-Image generation Multiple image models Style customization High-quality outputs üé® + Our Add-on Enhancement 100% Backward Compatible - Use it exactly like Nanobanana! üöÄ Extended Features üìå 1. Perfect Long-form Text Rendering Natural line-breaking for lengthy sentences Full multilingual support (Korean, English, Japanese, Chinese, etc.) Complex character combinations (double consonants, compound vowels, special symbols) handled flawlessly Mixed emoji üòä and special characters ‚ô• 8 optimized fonts üé¨ 2. Auto Image-to-Video Conversion One-click video generation Up to 12 seconds video length Auto-applied effects: fade-in, zoom, slide transitions SNS-optimized formats:...</description><pubDate>Mon, 20 Oct 2025 09:27:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/606069092091867</guid></item><item><title>üöÄ Introducing the xLLMs Dataset Collection</title><link>https://huggingface.co/posts/lamhieu/873520082917207</link><description>üöÄ Introducing the xLLMs Dataset Collection The xLLMs project is a growing suite of multilingual and multimodal dialogue datasets designed to train and evaluate advanced conversational LLMs. Each dataset focuses on a specific capability ‚Äî from long-context reasoning and factual grounding to STEM explanations, math Q&amp;A, and polite multilingual interaction. üåç Explore the full collection on Hugging Face: üëâ lamhieu/xllms-66cdfe34307bb2edc8c6df7d üí¨ Highlight: xLLMs ‚Äì Dialogue Pubs A large-scale multilingual dataset built from document-guided synthetic dialogues (Wikipedia, WikiHow, and technical sources). It‚Äôs ideal for training models on long-context reasoning, multi-turn coherence, and tool-augmented dialogue across 9 languages. üëâ lamhieu/xllms_dialogue_pubs üß† Designed for: - Long-context and reasoning models - Multilingual assistants - Tool-calling and structured response learning All datasets are open for research and development use ‚Äî free, transparent, and carefully curated to...</description><pubDate>Mon, 20 Oct 2025 09:27:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lamhieu/873520082917207</guid></item><item><title>How to Install and Use ComfyUI and SwarmUI on Massed Compute and RunPod Private Cloud GPU Services :</title><link>https://huggingface.co/posts/MonsterMMORPG/558002389274434</link><description>How to Install and Use ComfyUI and SwarmUI on Massed Compute and RunPod Private Cloud GPU Services : https://youtu.be/bBxgtVD3ek4 https://youtu.be/bBxgtVD3ek4 If your GPU is not strong enough to run Generative AI models this is the tutorial that you need. Or you want to scale your generation speed by using multiple GPUs at the same time again this is excellent tutorial. In this tutorial I will show how to setup ComfyUI and SwarmUI literally 1-click on Massed Compute and RunPod and use your most liked best image and video generation models like Qwen, FLUX, Wan 2.2 and more. üöÄ Unleash the full power of AI image and video generation on the cloud! This comprehensive tutorial is your step-by-step guide to installing and configuring SwarmUI and ComfyUI on two of the most popular cloud GPU platforms: Massed Compute and RunPod. Learn how to set up a powerful multi-GPU workflow to generate stunning, ultra-realistic images and videos at incredible speeds. We'll cover everything from deploying...</description><pubDate>Mon, 20 Oct 2025 09:27:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/558002389274434</guid></item><item><title>PaddleOCR VLüî• 0.9B Multilingual VLM by Baidu</title><link>https://huggingface.co/posts/AdinaY/411769057230600</link><description>PaddleOCR VLüî• 0.9B Multilingual VLM by Baidu PaddlePaddle/PaddleOCR-VL ‚ú® Ultra-efficient NaViT + ERNIE-4.5 architecture ‚ú® Supports 109 languages ü§Ø ‚ú® Accurately recognizes text, tables, formulas &amp; charts ‚ú® Fast inference and lightweight for deployment See translation</description><pubDate>Mon, 20 Oct 2025 09:27:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/411769057230600</guid></item></channel></rss>