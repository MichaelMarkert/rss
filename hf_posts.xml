<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Hunyuan-A13B 🔥 New MoE LLM by TencentHunyuan</title><link>https://huggingface.co/posts/AdinaY/115721386328243</link><description>Hunyuan-A13B 🔥 New MoE LLM by TencentHunyuan tencent/Hunyuan-A13B-Instruct ✨80B total / 13B active params ✨256K context window ✨Dual-mode reasoning: fast &amp; slow thinking ✨Efficient inference (GQA + quantization) See translation</description><pubDate>Sun, 29 Jun 2025 17:19:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/115721386328243</guid></item><item><title>Three big AI copyright updates this week alone. Tracking it all is getting almost impossible!</title><link>https://huggingface.co/posts/fdaudens/569299354714492</link><description>Three big AI copyright updates this week alone. Tracking it all is getting almost impossible! That’s why @ BrigitteTousi and I built this interactive tracker to keep you up to date fdaudens/ai-copyright-lawsuits (Prototyped in minutes with DeepSite!) See translation</description><pubDate>Sun, 29 Jun 2025 17:19:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/569299354714492</guid></item><item><title>Layer-wise and Pruned versions of Qwen/Qwen3-30B-A3B</title><link>https://huggingface.co/posts/eaddario/678649623001881</link><description>Layer-wise and Pruned versions of Qwen/Qwen3-30B-A3B * Tesor-wise: eaddario/Qwen3-30B-A3B-GGUF * Pruned: eaddario/Qwen3-30B-A3B-pruned-GGUF Even though the Perplexity scores of the pruned version are 3 times higher, the ARC, HellaSwag, MMLU, Truthful QA and WinoGrande scores are holding remarkably well, considering two layers were removed (5 and 39). This seems to support Xin Men et al conclusions in ShortGPT: Layers in Large Language Models are More Redundant Than You Expect (2403.03853) Results summary in the model's card and test results in the ./scores directory. Questions/feedback is always welcomed. See translation</description><pubDate>Sun, 29 Jun 2025 17:19:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/eaddario/678649623001881</guid></item><item><title>As you might have already heard, FLUX.1-Kontext-dev is now released and taken the generative community by storm!</title><link>https://huggingface.co/posts/a-r-r-o-w/674823997424742</link><description>As you might have already heard, FLUX.1-Kontext-dev is now released and taken the generative community by storm! In case you haven't come across it, you can get started with Kontext using 🤗 diffusers. See the official [model]( black-forest-labs/FLUX.1-Kontext-dev ) and [docs]( https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux#flux ). Want to know how inference companies like Fal &amp; Replicate are able to run the model so fast and in under 2 seconds per image? Check out this [gist]( https://gist.github.com/a-r-r-o-w/d08c37e8bd3e9c26b4ce80360be148c6 ) for some details! See translation</description><pubDate>Sun, 29 Jun 2025 17:19:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/a-r-r-o-w/674823997424742</guid></item><item><title>Mind2Web 2 is out - this time featuring eval and benchmark for deep research🔥</title><link>https://huggingface.co/posts/Jaward/802029609122095</link><description>Mind2Web 2 is out - this time featuring eval and benchmark for deep research🔥 Paper: Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge (2506.21506) Project: https://osu-nlp-group.github.io/Mind2Web-2/ See translation</description><pubDate>Sun, 29 Jun 2025 17:19:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/802029609122095</guid></item><item><title>Curated list of **Next Gen Code Generation** papers &amp; benchmarks! 🔥 with 100+ ⭐️ now!</title><link>https://huggingface.co/posts/YerbaPage/653320741659718</link><description>Curated list of **Next Gen Code Generation** papers &amp; benchmarks! 🔥 with 100+ ⭐️ now! Stay ahead with the latest in: ✅ Repo-level Issue Resolution (SWE-bench, Agents) ✅ Repo-level Code Completion (Repo understanding) ✅ Repo-level Code QA/Translation ✅ Datasets &amp; Benchmarks 👉 Check it out: https://github.com/YerbaPage/Awesome-Repo-Level-Code-Generation 🔥 💡PRs are welcomed! See translation</description><pubDate>Sun, 29 Jun 2025 17:19:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/653320741659718</guid></item><item><title>🧠 SOMA: The Core Architecture for AGI Level 1 🚀</title><link>https://huggingface.co/posts/openfree/674788441765421</link><description>🧠 SOMA: The Core Architecture for AGI Level 1 🚀 VIDraft/SOMA-AGI 🎯 The First Step Toward AGI SOMA (Self-Orchestrating Modular Architect) is a revolutionary architecture that fulfills the essential requirements for AGI (Artificial General Intelligence) Level 1. It perfectly implements the common AGI prerequisites emphasized by Yann LeCun (Meta), OpenAI, and Google DeepMind within a single LLM. 📋 AGI Level 1 Core Requirements = SOMA's Perfect Implementation ✅ 🎯 Planning Capability → Supervisor AI autonomously designs and executes comprehensive analysis roadmaps 🧩 Role Differentiation &amp; Modularity → A single LLM instantly differentiates into 5 expert AIs for collaboration 🔄 Self-reflection &amp; Feedback Loops → Evaluator AI continuously validates and directs improvements 🛠️ Tool-use &amp; Autonomy → Full automation from web search to report generation 🎮 Long-term Agency Structure → Completes complex 11-stage collaborative processes end-to-end 🔷 SOMA's Three Core Structures 🧭 Self-...</description><pubDate>Sun, 29 Jun 2025 17:19:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/674788441765421</guid></item><item><title>I find it really annoying that you can use a Zero GPU model that will throw an error but it’ll still take the processing time from your quota. So youre just left with less time and nothing to show for it.</title><link>https://huggingface.co/posts/wewittc/347088512345422</link><description>I find it really annoying that you can use a Zero GPU model that will throw an error but it’ll still take the processing time from your quota. So youre just left with less time and nothing to show for it. See translation</description><pubDate>Sun, 29 Jun 2025 17:19:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wewittc/347088512345422</guid></item><item><title>This is what efficient AI looks like: Gemma 3n just dropped - a natively multimodal model that runs entirely on your device. No cloud. No API calls.</title><link>https://huggingface.co/posts/fdaudens/514291892599532</link><description>This is what efficient AI looks like: Gemma 3n just dropped - a natively multimodal model that runs entirely on your device. No cloud. No API calls. 🧠 Text, image, audio, and video - handled locally. ⚡️Only needs 2B in GPU memory to run 🤯 First sub-10B model to hit 1300+ Elo ✅ Plug-and-play with Hugging Face, MLX, llama.cpp, and more. Plus: Multilingual out of the box (140+ languages), fine-tune in a free Colab notebook. google/gemma-3n-685065323f5984ef315c93f4 See translation</description><pubDate>Sun, 29 Jun 2025 17:19:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/514291892599532</guid></item><item><title>It's been a bit since I took a step back and looked at</title><link>https://huggingface.co/posts/jsulz/866221600890917</link><description>It's been a bit since I took a step back and looked at xet-team progress to migrate Hugging Face from Git LFS to Xet, but every time I do it boggles the mind. A month ago there were 5,500 users/orgs on Xet with 150K repos and 4PB. Today? 🤗 700,000 users/orgs 📈 350,000 repos 🚀 15PB Meanwhile, our migrations have pushed throughput to numbers that are bonkers. In June, we hit upload speeds of 577Gb/s (crossing 500Gb/s for the first time). These are hard numbers to put into context, but let's try: The latest run of the Common Crawl from commoncrawl was 471 TB. We now have ~32 crawls stored in Xet. At peak upload speed we could move the latest crawl into Xet in about two hours. We're moving to a new phase in the process, so stay tuned. This shift in gears means it's also time to roll up our sleeves and look at all the bytes we have and the value we're adding to the community. I already have some homework from @ RichardErkhov to look at the dedupe across their uploads, and I'll be doing...</description><pubDate>Sun, 29 Jun 2025 17:19:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/866221600890917</guid></item></channel></rss>