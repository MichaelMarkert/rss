<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸš€ğŸ’¡ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸª„ğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/213442382070723</link><description>ğŸš€ğŸ’¡ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸª„ğŸš€ ğŸ“„ Title: LoftUp: Learning a Coordinate-based Feature Upsampler for Vision Foundation Models ğŸ” ğŸ“ Description: LoftUp is a coordinate-based transformer that upscales the low-resolution features of VFMs (e.g. DINOv2 and CLIP) using cross-attention and self-distilled pseudo-ground truth (pseudo-GT) from SAM. ğŸ‘¥ Authors: Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models (2504.14032) ğŸŒ Github Page: https://andrehuang.github.io/loftup-site ğŸ“ Repository: https://github.com/andrehuang/loftup ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the Foundation Models and Representation Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/foundation-models-and-representation-learning.md ğŸ“š...</description><pubDate>Fri, 31 Oct 2025 17:21:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/213442382070723</guid></item><item><title>Starts erasing! ğŸ‰ ğŸ‰ ğŸ‰</title><link>https://huggingface.co/posts/piercus/167394123498038</link><description>Starts erasing! ğŸ‰ ğŸ‰ ğŸ‰ This is made with a one-step SD1.5 LBM [1] eraser ! Data is open. Data pipeline is open. Training code is open. On our LBM fork : https://github.com/finegrain-ai/LBM [1] LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) See translation</description><pubDate>Fri, 31 Oct 2025 17:21:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/piercus/167394123498038</guid></item><item><title>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥</title><link>https://huggingface.co/posts/nouamanetazi/972464132222376</link><description>After training ğ’ğ¦ğ¨ğ¥ğ‹ğŒğŸ‘ on ğŸ‘ğŸ–ğŸ’ ğ‡ğŸğŸğŸğ¬ for nearly a month, I've come to realize something most people overlook: ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¢ğ¬ ğ­ğ¡ğ ğ¦ğšğ¤ğ-ğ¨ğ«-ğ›ğ«ğğšğ¤ ğŸğšğœğ­ğ¨ğ« ğ¢ğ§ ğ‹ğ‹ğŒ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ . ğŸ”¥ Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious ğğ‚ğ‚ğ‹ ğğ«ğ«ğ¨ğ«ğ¬, or when your expensive GPU cluster is running at ğŸ”ğŸ% ğğŸğŸğ¢ğœğ¢ğğ§ğœğ², the problem isn't your model. It's most probably a ğ¦ğ¢ğ¬ğ®ğ¬ğ ğ¨ğŸ ğ­ğ¡ğ ğ¡ğšğ«ğğ°ğšğ«ğ. ğŸ› ï¸ Questions that seemed simple but had no clear answers: Why is ğŒğ¨ğ„ ğ­ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğ¬ğ¥ğ¨ğ°ğğ« ğ­ğ¡ğšğ§ ğğğ§ğ¬ğ ğ¦ğ¨ğğğ¥ğ¬? Which ğğ‚ğ‚ğ‹ ğŸğ¥ğšğ ğ¬ should we actually set? How often should we checkpoint without killing throughput? That's why we built ğ“ğ¡ğ ğ’ğ¦ğ¨ğ¥ ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğğ¥ğšğ²ğ›ğ¨ğ¨ğ¤ ğŸ“–: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the ğ¢ğ§ğŸğ«ğšğ¬ğ­ğ«ğ®ğœğ­ğ®ğ«ğ ğ¥ğšğ²ğğ« that most teams get wrong. We validated real vs...</description><pubDate>Fri, 31 Oct 2025 17:21:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nouamanetazi/972464132222376</guid></item><item><title>AI Speedpainting of a Tranquil Mountain Temple!</title><link>https://huggingface.co/posts/wang12390/946400201713761</link><description>AI Speedpainting of a Tranquil Mountain Temple! Just upload one image then it will generate hand-drawn video. Please watch till the end, if you like the result, please upvote. See translation</description><pubDate>Fri, 31 Oct 2025 17:21:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wang12390/946400201713761</guid></item><item><title>ğŸ¤– Did you know your voice might be cloned without your consent from just *one sentence* of audio?</title><link>https://huggingface.co/posts/meg/795374277994612</link><description>ğŸ¤– Did you know your voice might be cloned without your consent from just *one sentence* of audio? That's not great. So with @ frimelle , we brainstormed a new idea for developers who want to curb malicious use: âœ¨The Voice Consent Gate.âœ¨ Details, code, here: https://huggingface.co/blog/voice-consent-gate See translation</description><pubDate>Fri, 31 Oct 2025 17:21:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/meg/795374277994612</guid></item><item><title>What a fantastic community!</title><link>https://huggingface.co/posts/sourceoftruthdata/665062314942834</link><description>What a fantastic community! See translation</description><pubDate>Fri, 31 Oct 2025 17:21:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sourceoftruthdata/665062314942834</guid></item><item><title>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the</title><link>https://huggingface.co/posts/sergiopaniego/207791817757812</link><description>Sharing the slides from yesterday's talk about "Fine Tuning with TRL" from the @ TogetherAgent x @ huggingface workshop we hosted in our Paris office ğŸƒ! Link: https://github.com/sergiopaniego/talks/blob/main/fine_tuning_with_trl/Fine%20tuning%20with%20TRL%20(Oct%2025).pdf See translation</description><pubDate>Fri, 31 Oct 2025 17:21:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/207791817757812</guid></item><item><title>Implemented DeepSeek-OCR to support the latest transformers on the</title><link>https://huggingface.co/posts/prithivMLmods/821680680057976</link><description>Implemented DeepSeek-OCR to support the latest transformers on the strangervisionhf page. The page includes the model weights and corrected configuration, which fix the issues and allow transformers inference to run smoothly.ğŸ¤—ğŸ”¥ &gt; Model: strangervisionhf/deepseek-ocr-latest-transformers &gt; Demo Space: prithivMLmods/DeepSeek-OCR-experimental âœ…Supports the latest transformers âœ…You can also opt out of the attention implementation if needed. âœ…Supports torch version 2.6.0 or higher âœ…torch version cuda: 12.4 If you are interested in experimenting with new things and streamlining compatibility, the strangervisionhf organization is open for you, and you can join the community. &gt; Multimodal Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 , https://huggingface.co/collections/strangervisionhf/october-2025-models &gt; Thank you, @ merve , for assigning the blazing-fast Zero GPU support! &gt; Notebook : https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-...</description><pubDate>Fri, 31 Oct 2025 17:21:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/821680680057976</guid></item><item><title>Introducing Gliese-OCR-7B-Post2.0-final, a document content-structure retrieval VLM designed for content extraction (OCR), summarization, and document visual question answering. This is the fourth and final model in the Camel Doc OCR VLM series, following Gliese-OCR-7B-Post1.0. The model delivers superior accuracy across a wide range of document types, including scanned PDFs, handwritten pages, structured forms, and analytical reports.ğŸš€ğŸ¤—</title><link>https://huggingface.co/posts/prithivMLmods/213895647112348</link><description>Introducing Gliese-OCR-7B-Post2.0-final, a document content-structure retrieval VLM designed for content extraction (OCR), summarization, and document visual question answering. This is the fourth and final model in the Camel Doc OCR VLM series, following Gliese-OCR-7B-Post1.0. The model delivers superior accuracy across a wide range of document types, including scanned PDFs, handwritten pages, structured forms, and analytical reports.ğŸš€ğŸ¤— &gt; Gliese-OCR-7B-Post2.0-final : prithivMLmods/Gliese-OCR-7B-Post2.0-final &gt; Gliese-OCR-7B-Post1.0 (previous) : prithivMLmods/Gliese-OCR-7B-Post1.0 &gt; Gliese OCR Post-x.0 (collection) : https://huggingface.co/collections/prithivMLmods/gliese-ocr-post-x0 &gt; Multimodal Implementations (collection) : https://huggingface.co/collections/prithivMLmods/multimodal-implementations &gt; Qwen VL Captions (other-collection) : https://huggingface.co/collections/prithivMLmods/qwen-vl-captions &gt; Run Demo Here : prithivMLmods/Gliese-OCR-7B-Post2.0-final &gt; GitHub (4bit) :...</description><pubDate>Fri, 31 Oct 2025 17:21:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/213895647112348</guid></item><item><title>With</title><link>https://huggingface.co/posts/branikita/467826227099244</link><description>With Robonine team we recently verified the rotational speed of the Feetech STS3250 servo motor (12 V, 50 kgÂ·cm torque, magnetic encoder) to compare measured performance with the official specification. According to the datasheet: - Rated speed: 0.133 s per 60Â° Calculation: - 0.133 s Ã— 6 = 0.798 s per full rotation - 1 / 0.798 = 1.253 revolutions per second - 1.253 Ã— 60 = 75.2 RPM This confirms the official specification of approximately 75 RPM at 12 V under no load. Our measurement: - Encoder output: 5,300 values per second - Encoder resolution: 4,096 counts per revolution Calculation: - Revolutions per second = 5,300 Ã· 4,096 = 1.294 rev/s - RPM = 1.294 Ã— 60 = 77.6 RPM Result: The measured value differs by only about 2â€“3% from the datasheet specification, confirming that the STS3250 performs very close to its rated no-load speed. This close agreement validates both the servoâ€™s performance and our measurement approach. Video: https://youtube.com/shorts/_O_mVZvYQlQ?feature=share...</description><pubDate>Fri, 31 Oct 2025 17:21:34 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/467826227099244</guid></item></channel></rss>