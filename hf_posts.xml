<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Qwen releases Qwen3-Coder-Next! üíú Run the locally on 46GB RAM or less.</title><link>https://huggingface.co/posts/danielhanchen/824171868881117</link><description>Qwen releases Qwen3-Coder-Next! üíú Run the locally on 46GB RAM or less. Thhe model excels at agentic coding &amp; local use. With 256K context, it delivers similar performance to models with 10-20√ó more active parameters. GGUF: unsloth/Qwen3-Coder-Next-GGUF Guide: https://unsloth.ai/docs/models/qwen3-coder-next See translation</description><pubDate>Thu, 05 Feb 2026 05:59:12 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/824171868881117</guid></item><item><title>I‚Äôm excited to share PlaiTO, a reasoning-focused language model built on LLaMA 3.1 (8B) and optimized for humanities and social sciences.</title><link>https://huggingface.co/posts/alibidaran/992533889532684</link><description>I‚Äôm excited to share PlaiTO, a reasoning-focused language model built on LLaMA 3.1 (8B) and optimized for humanities and social sciences. PlaiTO is designed to go beyond surface-level text generation, emphasizing structured reasoning, conceptual clarity, and analytical depth‚Äîespecially in domains centered on human behavior and social systems. üéØ Focus Areas Psychology Management &amp; Organizational Studies Sociology üìä MMLU Benchmark Results (100 samples per domain) Professional Psychology: 76% Management: 74% Sociology: 75% These results highlight PlaiTO‚Äôs strong performance in abstract, theory-heavy, and reasoning-driven tasks. üí° Why PlaiTO? Strong analytical and reasoning capabilities Better handling of complex human-centered problems Suitable for academic, educational, and research use cases Balanced performance across multiple humanities disciplines PlaiTO is ideal for conceptual analysis, case reasoning, academic discussion, and decision-support scenarios‚Äîwhile still requiring...</description><pubDate>Thu, 05 Feb 2026 05:59:12 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/alibidaran/992533889532684</guid></item><item><title>A single lock on a door isn't enough. Real security is about layers.</title><link>https://huggingface.co/posts/MikeDoes/512575404125311</link><description>A single lock on a door isn't enough. Real security is about layers. The same is true for AI privacy. A new paper, "Whispered Tuning", offers a fantastic layered solution that aims to fortify LLMs against privacy infringements. We're proud that the first, essential layer, a high-precision PII redaction model was built on the foundation of the Ai4Privacy/pii-65k dataset. Our dataset provided the necessary training material for their initial anonymization step, which then enabled them to develop further innovations like differential privacy fine-tuning and output filtering. This is a win-win: our data helps create a solid base, and researchers build powerful, multi-stage privacy architectures on top of it. Together, we're making AI safer. üîó Read the full paper to see how a strong foundation enables a complete privacy solution: https://www.scirp.org/journal/paperinformation?paperid=130659 üöÄ Stay updated on the latest in privacy-preserving AI‚Äîfollow us on LinkedIn:...</description><pubDate>Thu, 05 Feb 2026 05:59:12 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/512575404125311</guid></item><item><title>Introducing the Qwen-Image-Edit-3D-Lighting-Control app, featuring 8√ó horizontal and 3√ó elevational lighting positions for precise 3D lighting control. It enables studio-level lighting using fast Qwen Image Edit fast inference, paired with Multi-Angle-Lighting adapters. üî¶</title><link>https://huggingface.co/posts/prithivMLmods/212829837698801</link><description>Introducing the Qwen-Image-Edit-3D-Lighting-Control app, featuring 8√ó horizontal and 3√ó elevational lighting positions for precise 3D lighting control. It enables studio-level lighting using fast Qwen Image Edit fast inference, paired with Multi-Angle-Lighting adapters. üî¶ üî• Space: prithivMLmods/Qwen-Image-Edit-3D-Lighting-Control ‚úÖ Collection: https://huggingface.co/collections/prithivMLmods/image-generation-apps-collection üìÇ GitHub: https://github.com/PRITHIVSAKTHIUR/Qwen-Image-Edit-3D-Lighting-Control See translation</description><pubDate>Thu, 05 Feb 2026 05:59:12 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/212829837698801</guid></item><item><title>We‚Äôve all had that moment where we watch a tutorial, nod along, but then realize we can‚Äôt actually do it ourselves because watching is just passive. At AIPrep, we are fixing this "watch and forget" cycle by building a foundational Generative Explanatory Model (GEM). GEM doesn't just give you a video or a wall of text; it builds an interactive lesson that asks you questions, catches your mistakes in real time, and adapts to your pace. We have just finished preparing our specialized datasets for this interactive logic, and you can already check them out on our profile to see how we are structuring this step-by-step reasoning. Training for the foundational model starts very soon, so stay in touch because something revolutionary is coming to the world of AI education. You can see our progress at aiprep.in.</title><link>https://huggingface.co/posts/AIPreplabs/635199649838795</link><description>We‚Äôve all had that moment where we watch a tutorial, nod along, but then realize we can‚Äôt actually do it ourselves because watching is just passive. At AIPrep, we are fixing this "watch and forget" cycle by building a foundational Generative Explanatory Model (GEM). GEM doesn't just give you a video or a wall of text; it builds an interactive lesson that asks you questions, catches your mistakes in real time, and adapts to your pace. We have just finished preparing our specialized datasets for this interactive logic, and you can already check them out on our profile to see how we are structuring this step-by-step reasoning. Training for the foundational model starts very soon, so stay in touch because something revolutionary is coming to the world of AI education. You can see our progress at aiprep.in. See translation</description><pubDate>Thu, 05 Feb 2026 05:59:12 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AIPreplabs/635199649838795</guid></item><item><title>SECourses Musubi Trainer upgraded to V27 and FLUX 2, FLUX Klein, Z-Image training added with demo configs - amazing VRAM optimized - read the news</title><link>https://huggingface.co/posts/MonsterMMORPG/876855019351468</link><description>SECourses Musubi Trainer upgraded to V27 and FLUX 2, FLUX Klein, Z-Image training added with demo configs - amazing VRAM optimized - read the news App is here : https://www.patreon.com/posts/137551634 Full tutorial how to use and train : https://youtu.be/DPX3eBTuO_Y See translation</description><pubDate>Thu, 05 Feb 2026 05:59:12 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/876855019351468</guid></item><item><title>I submitted a "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning" Paper by Tanyu Chen, Tairan Chen, Kai shen , Zhenghua Bao, Zhihui Zhang, Man Yuan, Yi Shi From</title><link>https://huggingface.co/posts/rajkumarrawal/904260944141642</link><description>I submitted a "FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning" Paper by Tanyu Chen, Tairan Chen, Kai shen , Zhenghua Bao, Zhihui Zhang, Man Yuan, Yi Shi From FlashLabs to Daily Papers on huggingface . Chroma 1.0 enables real time spoken dialogue with personalized voice cloning through discrete speech representations and interleaved text audio token scheduling. Chroma 1.0 , the world‚Äôs first open source, real time speech to speech model with voice cloning. FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning (2601.11141) See translation</description><pubDate>Thu, 05 Feb 2026 05:59:12 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/rajkumarrawal/904260944141642</guid></item><item><title>AI for science is moving fastüöÄ</title><link>https://huggingface.co/posts/AdinaY/629082711714950</link><description>AI for science is moving fastüöÄ Intern-S1-Pro üî¨ a MoE multimodal scientific reasoning model from Shanghai AI Lab internlm/Intern-S1-Pro ‚ú® 1T total / 22B active ‚ú® Apache 2.0 ‚ú® SoTA scientific reasoning performance ‚ú® FoPE enables scalable modeling of long physical time series (10‚Å∞‚Äì10‚Å∂) See translation</description><pubDate>Thu, 05 Feb 2026 05:59:12 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/629082711714950</guid></item><item><title>GLM-OCR: A Tiny 0.9B-Parameter Model That Punches Far Above Its Weight</title><link>https://huggingface.co/posts/Javedalam/705319100384927</link><description>GLM-OCR: A Tiny 0.9B-Parameter Model That Punches Far Above Its Weight Released today by Z.ai, GLM-OCR is a compact vision-language model designed specifically for document understanding. At just 0.9 billion parameters, it belongs to a new generation of lightweight AI systems proving that raw model size is no longer the only path to high performance. Despite its small footprint, GLM-OCR posts exceptionally strong results across major document benchmarks. It scores 94.6 on OmniDocBench, 94.0 on OCRBench, and an impressive 96.5 on UniMERNet for formula recognition‚Äînumbers that place it alongside, and in some cases ahead of, significantly larger specialized OCR models. The takeaway is clear: efficiency is rapidly becoming a defining feature of modern AI design. Developed by Z.ai, a research group focused on advancing multimodal foundation models, GLM-OCR reflects a broader shift toward highly optimized architectures that deliver serious capability without requiring massive compute...</description><pubDate>Thu, 05 Feb 2026 05:59:12 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Javedalam/705319100384927</guid></item><item><title>The 2025 Chinese LLM Showdown: Western Models Still Dominate Top 4, but China Leads the Open-Source Arena.</title><link>https://huggingface.co/posts/imnotkitty/790273915312125</link><description>The 2025 Chinese LLM Showdown: Western Models Still Dominate Top 4, but China Leads the Open-Source Arena. üèÜ The Champions: Claude-Opus-4.5, Gemini-3-Pro, GPT-5.2, and Gemini-3-Flash sweep the top four spots. üöÄ The Pursuers: Doubao and DeepSeek-V3.2 tie for first place among Chinese models; GLM-4.7, ERNIE-5.0, and Kimi secure their positions in the domestic top five. üî• The Biggest Highlight: The top three spots on the open-source leaderboard are entirely held by Team China (DeepSeek, GLM, Kimi), outperforming the best western open-source models. See translation</description><pubDate>Thu, 05 Feb 2026 05:59:12 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/imnotkitty/790273915312125</guid></item></channel></rss>