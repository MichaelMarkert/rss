<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸš€ğŸ’¡ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸª„ğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/213442382070723</link><description>ğŸš€ğŸ’¡ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸª„ğŸš€ ğŸ“„ Title: LoftUp: Learning a Coordinate-based Feature Upsampler for Vision Foundation Models ğŸ” ğŸ“ Description: LoftUp is a coordinate-based transformer that upscales the low-resolution features of VFMs (e.g. DINOv2 and CLIP) using cross-attention and self-distilled pseudo-ground truth (pseudo-GT) from SAM. ğŸ‘¥ Authors: Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models (2504.14032) ğŸŒ Github Page: https://andrehuang.github.io/loftup-site ğŸ“ Repository: https://github.com/andrehuang/loftup ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the Foundation Models and Representation Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/foundation-models-and-representation-learning.md ğŸ“š...</description><pubDate>Thu, 30 Oct 2025 05:23:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/213442382070723</guid></item><item><title>What a fantastic community!</title><link>https://huggingface.co/posts/sourceoftruthdata/665062314942834</link><description>What a fantastic community! See translation</description><pubDate>Thu, 30 Oct 2025 05:23:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sourceoftruthdata/665062314942834</guid></item><item><title>Trained a model for emotion-controllable TTS based on MiMo audio on LAION's dataset.</title><link>https://huggingface.co/posts/mrfakename/579691668854907</link><description>Trained a model for emotion-controllable TTS based on MiMo audio on LAION's dataset. Still very early and does have an issue with hallucinating but results seem pretty good so far, given that it is very early into the training run. Will probably kick off a new run later with some settings tweaked. Put up a demo here: mrfakename/EmoAct-MiMo (Turn ğŸ”Š on to hear audio samples) See translation</description><pubDate>Thu, 30 Oct 2025 05:23:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mrfakename/579691668854907</guid></item><item><title>I made a code sniping agent to detect when new AI papers with code (and weights) are released, and then automatically create a Gradio demo on Hugging Face ğŸ§™</title><link>https://huggingface.co/posts/jbilcke-hf/233313729271846</link><description>I made a code sniping agent to detect when new AI papers with code (and weights) are released, and then automatically create a Gradio demo on Hugging Face ğŸ§™ Here are some examples generated 100% automatically: https://huggingface.co/collections/jbilcke-hf/sniped I call this agent CheatCode ( https://github.com/jbilcke-hf/CheatCode ) because it skips so many steps that it kinda feels like breaking the rules of the AI tech release game ğŸ˜… As with any experimental technology, there is still room for improvement ğŸ‘©ğŸ»â€ğŸ”¬: - Currently the demos are all generated in one go and not built or tested by the agent itself. A more robust version should loop over the deployed app to fix build/runtime issues. - There is still a bit of human curation done to avoid making demos for things that canâ€™t really be demonstrated on ZeroGPU (eg. tasks taking several minutes) - Some papers can actually be showcased in a variety of ways, which isnâ€™t really supported (see Demo 2) See translation</description><pubDate>Thu, 30 Oct 2025 05:23:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jbilcke-hf/233313729271846</guid></item><item><title>With</title><link>https://huggingface.co/posts/branikita/467826227099244</link><description>With Robonine team we recently verified the rotational speed of the Feetech STS3250 servo motor (12 V, 50 kgÂ·cm torque, magnetic encoder) to compare measured performance with the official specification. According to the datasheet: - Rated speed: 0.133 s per 60Â° Calculation: - 0.133 s Ã— 6 = 0.798 s per full rotation - 1 / 0.798 = 1.253 revolutions per second - 1.253 Ã— 60 = 75.2 RPM This confirms the official specification of approximately 75 RPM at 12 V under no load. Our measurement: - Encoder output: 5,300 values per second - Encoder resolution: 4,096 counts per revolution Calculation: - Revolutions per second = 5,300 Ã· 4,096 = 1.294 rev/s - RPM = 1.294 Ã— 60 = 77.6 RPM Result: The measured value differs by only about 2â€“3% from the datasheet specification, confirming that the STS3250 performs very close to its rated no-load speed. This close agreement validates both the servoâ€™s performance and our measurement approach. Video: https://youtube.com/shorts/_O_mVZvYQlQ?feature=share...</description><pubDate>Thu, 30 Oct 2025 05:23:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/467826227099244</guid></item><item><title>ğŸš€ğŸ·ï¸ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ§©ğŸš€</title><link>https://huggingface.co/posts/DmitryRyumin/687304943131343</link><description>ğŸš€ğŸ·ï¸ğŸŒŸ New Research Alert - ICCV 2025 (Oral)! ğŸŒŸğŸ§©ğŸš€ ğŸ“„ Title: Heavy Labels Out! Dataset Distillation with Label Space Lightening ğŸ” ğŸ“ Description: The HeLlO framework is a new corpus distillation method that removes the need for large soft labels. It uses a lightweight, online image-to-label projector based on CLIP. This projector has been adapted using LoRA-style, parameter-efficient tuning. It has also been initialized with text embeddings. ğŸ‘¥ Authors: @ roseannelexie , @ Huage001 , Zigeng Chen, Jingwen Ye, and Xinchao Wang ğŸ“… Conference: ICCV, 19 â€“ 23 Oct, 2025 | Honolulu, Hawai'i, USA ğŸ‡ºğŸ‡¸ ğŸ“„ Paper: Heavy Labels Out! Dataset Distillation with Label Space Lightening (2408.08201) ğŸ“º Video: https://www.youtube.com/watch?v=kAyK_3wskgA ğŸš€ ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers ğŸš€ Added to the Efficient Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/efficient-learning.md ğŸ“š More Papers: more cutting-edge research...</description><pubDate>Thu, 30 Oct 2025 05:23:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DmitryRyumin/687304943131343</guid></item><item><title>Context rot is such a catchy phrase, but the problem has been identified 2+ years ago, called attention decay.</title><link>https://huggingface.co/posts/onekq/651277400400501</link><description>Context rot is such a catchy phrase, but the problem has been identified 2+ years ago, called attention decay. Lost in the Middle: How Language Models Use Long Contexts (2307.03172) I spotted the same problem in coding tasks, and documented in my book ( https://www.amazon.com/dp/9999331130 ). Why did this problem become hot again? This is because many of us thought the problem has been solved by long context models, which is not true. Here we were misled by benchmarks. Most long-context benchmarks build around the QA scenario, i.e. "finding needle in haystack". But in agentic scenarios, the model needs to find EVERYTHING in the haystack, and just can't afford enough attention for this challenge. See translation</description><pubDate>Thu, 30 Oct 2025 05:23:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/651277400400501</guid></item><item><title>AI Speedpainting of a Tranquil Mountain Temple!</title><link>https://huggingface.co/posts/wang12390/946400201713761</link><description>AI Speedpainting of a Tranquil Mountain Temple! Just upload one image then it will generate hand-drawn video. Please watch till the end, if you like the result, please upvote. See translation</description><pubDate>Thu, 30 Oct 2025 05:23:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wang12390/946400201713761</guid></item><item><title>Introducing Gliese-OCR-7B-Post2.0-final, a document content-structure retrieval VLM designed for content extraction (OCR), summarization, and document visual question answering. This is the fourth and final model in the Camel Doc OCR VLM series, following Gliese-OCR-7B-Post1.0. The model delivers superior accuracy across a wide range of document types, including scanned PDFs, handwritten pages, structured forms, and analytical reports.ğŸš€ğŸ¤—</title><link>https://huggingface.co/posts/prithivMLmods/213895647112348</link><description>Introducing Gliese-OCR-7B-Post2.0-final, a document content-structure retrieval VLM designed for content extraction (OCR), summarization, and document visual question answering. This is the fourth and final model in the Camel Doc OCR VLM series, following Gliese-OCR-7B-Post1.0. The model delivers superior accuracy across a wide range of document types, including scanned PDFs, handwritten pages, structured forms, and analytical reports.ğŸš€ğŸ¤— &gt; Gliese-OCR-7B-Post2.0-final : prithivMLmods/Gliese-OCR-7B-Post2.0-final &gt; Gliese-OCR-7B-Post1.0 (previous) : prithivMLmods/Gliese-OCR-7B-Post1.0 &gt; Gliese OCR Post-x.0 (collection) : https://huggingface.co/collections/prithivMLmods/gliese-ocr-post-x0 &gt; Multimodal Implementations (collection) : https://huggingface.co/collections/prithivMLmods/multimodal-implementations &gt; Qwen VL Captions (other-collection) : https://huggingface.co/collections/prithivMLmods/qwen-vl-captions &gt; Run Demo Here : prithivMLmods/Gliese-OCR-7B-Post2.0-final &gt; GitHub (4bit) :...</description><pubDate>Thu, 30 Oct 2025 05:23:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/213895647112348</guid></item><item><title>Implemented DeepSeek-OCR to support the latest transformers on the</title><link>https://huggingface.co/posts/prithivMLmods/821680680057976</link><description>Implemented DeepSeek-OCR to support the latest transformers on the strangervisionhf page. The page includes the model weights and corrected configuration, which fix the issues and allow transformers inference to run smoothly.ğŸ¤—ğŸ”¥ &gt; Model: strangervisionhf/deepseek-ocr-latest-transformers &gt; Demo Space: prithivMLmods/DeepSeek-OCR-experimental âœ…Supports the latest transformers âœ…You can also opt out of the attention implementation if needed. âœ…Supports torch version 2.6.0 or higher âœ…torch version cuda: 12.4 If you are interested in experimenting with new things and streamlining compatibility, the strangervisionhf organization is open for you, and you can join the community. &gt; Multimodal Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 , https://huggingface.co/collections/strangervisionhf/october-2025-models &gt; Thank you, @ merve , for assigning the blazing-fast Zero GPU support! &gt; Notebook : https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-...</description><pubDate>Thu, 30 Oct 2025 05:23:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/821680680057976</guid></item></channel></rss>