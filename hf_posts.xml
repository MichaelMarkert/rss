<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>gpt-oss-120B scored 28 (one of the lowest) on AHA leaderboard. not very human aligned model.</title><link>https://huggingface.co/posts/etemiz/710778843328598</link><description>gpt-oss-120B scored 28 (one of the lowest) on AHA leaderboard. not very human aligned model. these kind of models are not really "free": they are costing you your freedom if you know what i mean. See translation</description><pubDate>Mon, 18 Aug 2025 09:30:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/710778843328598</guid></item><item><title>Excited to introduce the Tiny VLMs Lab App for experiencing 15+ multimodal VLMs, ranging from a 250M parameter model to a 4B parameter model, for tasks like OCR, reasoning, small models for single-shot answering, and captioning (abliterated), across a broad range of visual categories including images with complex, sensitive, or nuanced content, while handling varying aspect ratios and resolutions.ğŸ§ª</title><link>https://huggingface.co/posts/prithivMLmods/284574267701705</link><description>Excited to introduce the Tiny VLMs Lab App for experiencing 15+ multimodal VLMs, ranging from a 250M parameter model to a 4B parameter model, for tasks like OCR, reasoning, small models for single-shot answering, and captioning (abliterated), across a broad range of visual categories including images with complex, sensitive, or nuanced content, while handling varying aspect ratios and resolutions.ğŸ§ª ğŸ¤— Space/App: prithivMLmods/Tiny-VLMs-Lab âœ¦ï¸ Also introducing prithivMLmods/Qwen2.5-VL-3B-Abliterated-Caption-it , tailored for Abliterated Captioning / Uncensored Image Captioning. This release comes as a lighter alternative to the existing Qwen2.5-VL-7B-Abliterated-Caption-it prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it model, making it usable on mid-range GPUs and even experimental on T4 GPUs. âœ¦ï¸ Collection: prithivMLmods/vl-abliterated-caption-68a0443b63182e97a15c47a3 âœ¦ï¸ GitHub: https://github.com/PRITHIVSAKTHIUR/Tiny-VLMs-Lab . . . To know more about it, visit the app page or...</description><pubDate>Mon, 18 Aug 2025 09:30:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/284574267701705</guid></item><item><title>Image-to-Promptâš¡</title><link>https://huggingface.co/posts/ovi054/657358125503535</link><description>Image-to-Promptâš¡ ovi054/image-to-prompt Extract text prompt from image. And you can reuse the prompt to generate similar images! Useful for prompt engineering, studying image-to-text alignment, making training datasets, or recreating similar outputs. Powered by: Gradio, Florence 2 ğŸ‘‰ Try it now: ovi054/image-to-prompt See translation</description><pubDate>Mon, 18 Aug 2025 09:30:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ovi054/657358125503535</guid></item><item><title>Want to quickly try Gemma 3 270m? ğŸ’ğŸ’¬</title><link>https://huggingface.co/posts/anakin87/751707976654130</link><description>Want to quickly try Gemma 3 270m? ğŸ’ğŸ’¬ I made a simple Space to do that: anakin87/gemma-3-270m-it âš¡ Fast: Flash Attention, Zero GPU âš™ï¸ Configurable See translation</description><pubDate>Mon, 18 Aug 2025 09:30:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/anakin87/751707976654130</guid></item><item><title>âœ¨ HairPick | Preview Your Perfect Hair Transformation in 360Â° âœ¨</title><link>https://huggingface.co/posts/ginipick/955296677233221</link><description>âœ¨ HairPick | Preview Your Perfect Hair Transformation in 360Â° âœ¨ ğŸŠ Free Trial for Hugging Face Launch! Hurry! â° Hello! Introducing an innovative AI service that helps you choose the perfect hairstyle without any regrets before visiting the salon! ğŸ¯ Try It Now ginigen/Hair-Pick ğŸ”„ What Makes HairPick Special? 360Â° Complete Preview! Other hair simulators only show the front view? ğŸ˜‘ HairPick is different! âœ… Front + 4 random angles = Total 5 multi-angle images generated âœ… Perfect check from side profile ğŸ‘¤ diagonal ğŸ“ back view ğŸ‘¥! âœ… 100+ trendy hairstyle library ğŸ’‡â€â™€ï¸ ğŸ’¡ Highly Recommended For: ğŸ¯ "I really don't want to fail this time!" â†’ Check side volume and back lines thoroughly ğŸ¯ "It's hard to explain exactly to my stylist" â†’ Perfect communication with 360Â° result images! ğŸ¯ "I have a profile photo/photoshoot coming up" â†’ Preview your best look from every angle ğŸš€ Super Simple Usage (Just 1 Minute!) 1ï¸âƒ£ One Selfie ğŸ“¸ Take a front-facing photo in bright light (show your forehead and face...</description><pubDate>Mon, 18 Aug 2025 09:30:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/955296677233221</guid></item><item><title>suppose someone is working on a reasoning model, which ends up unlocking achievements that lead to agi, should it be open source?</title><link>https://huggingface.co/posts/appvoid/589674942896129</link><description>suppose someone is working on a reasoning model, which ends up unlocking achievements that lead to agi, should it be open source? keep in mind everybody will have access to it: scientists, governments, terrorists, average people, etc... See translation</description><pubDate>Mon, 18 Aug 2025 09:30:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/appvoid/589674942896129</guid></item><item><title>âœ… New Article: *Memory as Structured Time*</title><link>https://huggingface.co/posts/kanaria007/576453037371058</link><description>âœ… New Article: *Memory as Structured Time* Title: ğŸ§  History: Memory Loops as Civilization Structure ğŸ”— https://huggingface.co/blog/kanaria007/memory-loops-as-civilization-structure --- Summary: Memory is often treated as *storage and retrieval*. Structured Intelligence reframes it as *timeâ€‘shaping architecture*: * *Loops that preserve context and continuity* * *Rollback paths that enable reflection and correction* * *Patterns that turn experience into adaptive structure* &gt; Memory isnâ€™t static â€” &gt; *itâ€™s how intelligence edits time.* --- Why It Matters: â€¢ Reveals *how memory enables learning, identity, and adaptation* â€¢ Supports *AI that can reflect, revise, and selfâ€‘align* â€¢ Connects *personal cognition and collective history* as structural processes --- Whatâ€™s Inside: â€¢ Memory as *recursive structural loop* â€¢ *Failure and recovery* as part of adaptive recall â€¢ How *history and recordâ€‘keeping mirror cognitive memory* â€¢ Implications for *resilient AI and social knowledge systems* --- ğŸ“–...</description><pubDate>Mon, 18 Aug 2025 09:30:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/576453037371058</guid></item><item><title>When you ask ChatGPT, Claude, or Gemini a really tough question,</title><link>https://huggingface.co/posts/RakshitAralimatti/207934490136479</link><description>When you ask ChatGPT, Claude, or Gemini a really tough question, you might notice that little "thinking..." moment before it answers. But what does it actually mean when an LLM is â€œthinkingâ€? Imagine a chess player pausing before their next move not because they donâ€™t know how to play, but because theyâ€™re running through possibilities, weighing options, and choosing the best one. LLMs do something similarâ€¦ except theyâ€™re not really thinking like us. Hereâ€™s the surprising part :- You might think these reasoning skills come from futuristic architectures or alien neural networks. In reality, most reasoning LLMs still use the same transformer decoder-only architecture as other models The real magic? Itâ€™s in how theyâ€™re trained and what data they learn from. Can AI actually think, or is it just insanely good at faking it? I broke it down in a simple, 4-minute Medium read. Bet youâ€™ll walk away with at least one â€œaha!â€ moment. ğŸš€ Read here - https://lnkd.in/edZ8Ceyg See translation</description><pubDate>Mon, 18 Aug 2025 09:30:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/207934490136479</guid></item><item><title>benchmarked 9 models in 3 days. they were mostly below average in AHA score. p(doom) probably increased :(</title><link>https://huggingface.co/posts/etemiz/891816438009932</link><description>benchmarked 9 models in 3 days. they were mostly below average in AHA score. p(doom) probably increased :( See translation</description><pubDate>Mon, 18 Aug 2025 09:30:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/891816438009932</guid></item><item><title>Want to learn to build an AI Agent? I put together a cookbook for creating your own news research agent with OpenAI GPT-OSS:</title><link>https://huggingface.co/posts/fdaudens/770107969696647</link><description>Want to learn to build an AI Agent? I put together a cookbook for creating your own news research agent with OpenAI GPT-OSS: - Searches headlines &amp; specific sites - Pulls full articles when you need depth - Summarizes with clickable sources - Runs in a simple Gradio chat UI - No GPU, no local setup â€” just open-weight GPT-OSS models via Hugging Face If youâ€™ve been wanting to try agents but werenâ€™t sure where to start, this is an end-to-end example you can fork, run, and adapt. Full guide + code https://huggingface.co/blog/fdaudens/openai-gpt-oss-agent-inference-providers See translation</description><pubDate>Mon, 18 Aug 2025 09:30:47 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/770107969696647</guid></item></channel></rss>