<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>When you're looking for data, what's your focus (use the reactions below to vote):</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/147929225613950</link><description>When you're looking for data, what's your focus (use the reactions below to vote): ğŸš€ Getting as many images as you can ğŸ¤¯ Getting the right type of images (framing, domain, lighting, etc) I know both are very important, but I'm curious what people would put as #1 See translation</description><pubDate>Thu, 31 Jul 2025 13:41:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/147929225613950</guid></item><item><title>ğŸ’¬ From Replika to everyday chatbots, millions of people are forming emotional bonds with AI, sometimes seeking comfort, sometimes seeking intimacy. But what happens when an AI tells you "I understand how you feel" and you actually believe it?</title><link>https://huggingface.co/posts/giadap/164607097500775</link><description>ğŸ’¬ From Replika to everyday chatbots, millions of people are forming emotional bonds with AI, sometimes seeking comfort, sometimes seeking intimacy. But what happens when an AI tells you "I understand how you feel" and you actually believe it? At Hugging Face, together with @ frimelle and @ yjernite , we dug into something we felt wasn't getting enough attention: the need to evaluate AI companionship behaviors. These are the subtle ways AI systems validate us, engage with us, and sometimes manipulate our emotional lives. Here's what we found: ğŸ‘‰ Existing benchmarks (accuracy, helpfulness, safety) completely miss this emotional dimension. ğŸ‘‰ We mapped how leading AI systems actually respond to vulnerable prompts. ğŸ‘‰ We built the Interactions and Machine Attachment Benchmark (INTIMA): a first attempt at evaluating how models handle emotional dependency, boundaries, and attachment (with a full paper coming soon). Check out the blog post: https://huggingface.co/blog/giadap/evaluating-...</description><pubDate>Thu, 31 Jul 2025 13:41:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/164607097500775</guid></item><item><title>I got 370 tokens/sec of Qwen3-30B-A3B 2507 on my desktop Z8 GPU workstation. My target is 400 t/s, and the last 10 % always tastes like victory!</title><link>https://huggingface.co/posts/mitkox/340947617819121</link><description>I got 370 tokens/sec of Qwen3-30B-A3B 2507 on my desktop Z8 GPU workstation. My target is 400 t/s, and the last 10 % always tastes like victory! See translation</description><pubDate>Thu, 31 Jul 2025 13:41:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/340947617819121</guid></item><item><title>We just released TRL v0.20 with major multimodal upgrades!</title><link>https://huggingface.co/posts/sergiopaniego/236970560808195</link><description>We just released TRL v0.20 with major multimodal upgrades! ğŸ‘ï¸ VLM support for GRPO (highly requested by the community!) ğŸï¸ New GSPO trainer (from @ Qwen , released last week, VLM-ready) ğŸ™ New MPO trainer (multimodal by design, as in the paper) ğŸ“ Full release notes here: https://github.com/huggingface/trl/releases/tag/v0.20.0 See translation</description><pubDate>Thu, 31 Jul 2025 13:41:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/236970560808195</guid></item><item><title>Qwen3-30B-A3B-Thinking-2507 ğŸ”¥ latest step in scaling thinking capabilities from  Alibaba Qwen team.</title><link>https://huggingface.co/posts/AdinaY/263963364371914</link><description>Qwen3-30B-A3B-Thinking-2507 ğŸ”¥ latest step in scaling thinking capabilities from Alibaba Qwen team. Qwen/Qwen3-30B-A3B-Thinking-2507-FP8 âœ¨ 30B total / 3B active - Apache 2.0 âœ¨ Native 256K context âœ¨ SOTA coding, alignment, agentic reasoning See translation</description><pubDate>Thu, 31 Jul 2025 13:41:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/263963364371914</guid></item><item><title>Skywork UniPic ğŸ”¥a unified autoregressive multimodal model for image understanding, generation, &amp; editing, by Skywork å¤©å·¥</title><link>https://huggingface.co/posts/AdinaY/618670304840857</link><description>Skywork UniPic ğŸ”¥a unified autoregressive multimodal model for image understanding, generation, &amp; editing, by Skywork å¤©å·¥ Skywork/skywork-unipic-6888c0789cdb82457b2acf32 âœ¨ 1.5 B - MIT License âœ¨ Runs on RTX 4090 âœ¨ Truly unified architecture See translation</description><pubDate>Thu, 31 Jul 2025 13:41:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/618670304840857</guid></item><item><title>past week in open AI was insane ğŸ”¥ here's some of picks, find more here</title><link>https://huggingface.co/posts/merve/514822650680483</link><description>past week in open AI was insane ğŸ”¥ here's some of picks, find more here merve/releases-july-25-688768ca47fe3693407e02d1 ğŸ’¬ LLMs &amp; VLMs &gt; Qwen/Qwen3-235B-A22B-Thinking-2507 had a new update (OS) &gt; Qwen/Qwen3-Coder-480B-A35B-Instruct is out with 480B total 35B active params ğŸ¤¯ (OS) &gt; AllenAI dropped an update to allenai/olmOCR-7B-0725 ğŸ“ &gt; InternLM released internlm/Intern-S1 - 235B Qwen3 MoE + 6B InternViT encoder (OS) &gt; OmniSVG/OmniSVG is a new SVG generation VLM (OS) ğŸ–¼ï¸ image/video/3D generation &gt; WanAI released Wan2.2 series - both T2V and I2V 14B models for high-quality video generation (OS) multimodalart/wan-22-688767e313337b434ed55112 &gt; Tencent dropped tencent/HunyuanWorld-1 - image-to-3D scene generation model See translation</description><pubDate>Thu, 31 Jul 2025 13:41:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/514822650680483</guid></item><item><title>How to achieve 100% Pass Rate on HumanEval ? ğŸ”¥</title><link>https://huggingface.co/posts/YerbaPage/459269562265928</link><description>How to achieve 100% Pass Rate on HumanEval ? ğŸ”¥ Meet MGDebugger if you are tired of LLMs failing on complex bugs ğŸ¤” Our MGDebugger, just hit 100% accuracy on HumanEval using the DeepSeek-R1 model. ğŸš€ âœ¨ Demo: learnmlf/MGDebugger ğŸ“ Paper: From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging (2410.01215) ğŸ’» Code: https://github.com/YerbaPage/MGDebugger HumanEval may be retired, we're ready for the next challenge In more complex scenarios! You may also take look at this repo for a collection of awesome repo-level coding tasks! ğŸ–¥ï¸ https://github.com/YerbaPage/Awesome-Repo-Level-Code-Generation See translation</description><pubDate>Thu, 31 Jul 2025 13:41:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/459269562265928</guid></item><item><title>Qwen just released Qwen3-30B-A3B-Instruct-2507 ğŸ”¥ an upgrade to the non-thinking mode model</title><link>https://huggingface.co/posts/AdinaY/246049488455702</link><description>Qwen just released Qwen3-30B-A3B-Instruct-2507 ğŸ”¥ an upgrade to the non-thinking mode model Qwen/Qwen3-30B-A3B-Instruct-2507 âœ¨ 30B MoE / 3.3B active - Apache 2.0 âœ¨ Strong gains in reasoning, math, coding, &amp; multilingual tasks âœ¨ Native support for 256K long-context inputs See translation</description><pubDate>Thu, 31 Jul 2025 13:41:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/246049488455702</guid></item><item><title>Hugging Face just made life easier with the new hf CLI!</title><link>https://huggingface.co/posts/ImranzamanML/854877876171123</link><description>Hugging Face just made life easier with the new hf CLI! huggingface-cli to hf With renaming the CLI, there are new features added like hf jobs. We can now run any script or Docker image on dedicated Hugging Face infrastructure with a simple command. It's a good addition for running experiments and jobs on the fly. To get started, just run: pip install -U huggingface_hub List of hf CLI Commands Main Commands hf auth: Manage authentication (login, logout, etc.). hf cache: Manage the local cache directory. hf download: Download files from the Hub. hf jobs: Run and manage Jobs on the Hub. hf repo: Manage repos on the Hub. hf upload: Upload a file or a folder to the Hub. hf version: Print information about the hf version. hf env: Print information about the environment. Authentication Subcommands (hf auth) login: Log in using a Hugging Face token. logout: Log out of your account. whoami: See which account you are logged in as. switch: Switch between different stored access...</description><pubDate>Thu, 31 Jul 2025 13:41:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ImranzamanML/854877876171123</guid></item></channel></rss>