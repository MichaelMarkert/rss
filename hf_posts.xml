<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸš€ DeepSeek V3-0324 + Real-time Research Power! ğŸŒ</title><link>https://huggingface.co/posts/openfree/198818765852761</link><description>ğŸš€ DeepSeek V3-0324 + Real-time Research Power! ğŸŒ Hello there! Today I'm excited to introduce an amazing tool based on the DeepSeek V3-0324 latest model. This isn't just another AI chatbotâ€”it's a true "research assistant" capable of real-time information retrieval and analysis! openfree/Deepseek-v3-0324-Research ğŸ§  Key Strengths of DeepSeek V3-0324 DeepSeek V3-0324, provided by Fireworks AI, comes with these powerful advantages: ğŸ¯ Superior Reasoning: Excellent ability to solve complex problems step-by-step ğŸ“š Extensive Knowledge: Deep understanding across various topics from comprehensive training ğŸ§© Context Awareness: Maintains long conversation contexts for consistent responses ğŸŒ Multilingual Support: Processes various languages effectively ğŸ” Added Real-time "Deep Research" Capability! The most exciting feature of this project is the implementation of real-time search functionality similar to ChatGPT's Browse with Bing or Perplexity AI! ğŸŒŸ How does it work? ğŸ“‹ Query Analysis: Analyzes...</description><pubDate>Tue, 25 Mar 2025 13:30:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/198818765852761</guid></item><item><title>ğŸ‘‹ Hi all!</title><link>https://huggingface.co/posts/hanzla/334929914214979</link><description>ğŸ‘‹ Hi all! For any AI agent, internet search ğŸ” is an important tool. However, with APIs like Tavily and Exa, it becomes really difficult to keep up with the cost. In some cases, these Internet APIs cost more than the LLM. To solve, this, I am making a playwright wrapper API on top of publicly available searXNG instances. This will enable agent applications to fetch internet results for free. Currently, I have set up a basic GitHub repo, and I will continue developing advanced search features, such as image search ğŸ–¼ï¸ Github: https://github.com/HanzlaJavaid/Free-Search/tree/main ğŸš€ Try the deployed version: https://freesearch.replit.app/docs If you find this useful, consider starring â­ï¸ the GitHub repository to support further development! See translation</description><pubDate>Tue, 25 Mar 2025 13:30:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hanzla/334929914214979</guid></item><item><title>8 types of RoPE</title><link>https://huggingface.co/posts/Kseniase/498106595218801</link><description>8 types of RoPE As we always use Transformers, it's helpful to understand RoPEâ€”Rotary Position Embedding. Since token order matters, RoPE encodes it by rotating token embeddings based on their position, so the model knows how to interpret which token comes first, second, and so on. Here are 8 types of RoPE that can be implemented in different cases: 1. Original RoPE -&gt; RoFormer: Enhanced Transformer with Rotary Position Embedding (2104.09864) Encodes token positions by rotating token embeddings in the complex plane via a position-based rotation matrix, thereby providing the self-attention mechanism with relative positional info. 2. LongRoPE -&gt; LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens (2402.13753) Extends the context window of pre-trained LLMs to 2048k tokens, leveraging non-uniformities in positional interpolation with an efficient search. 3. LongRoPE2 -&gt; LongRoPE2: Near-Lossless LLM Context Window Scaling (2502.20082) Extends the effective context window of...</description><pubDate>Tue, 25 Mar 2025 13:30:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/498106595218801</guid></item><item><title>So many open releases at Hugging Face past week ğŸ¤¯ recapping all here â¤µï¸</title><link>https://huggingface.co/posts/merve/746832157330905</link><description>So many open releases at Hugging Face past week ğŸ¤¯ recapping all here â¤µï¸ merve/march-21-releases-67dbe10e185f199e656140ae ğŸ‘€ Multimodal &gt; Mistral AI released a 24B vision LM, both base and instruction FT versions, sota ğŸ”¥ (OS) &gt; with IBM we released SmolDocling, a sota 256M document parser with Apache 2.0 license (OS) &gt; SpatialLM is a new vision LM that outputs 3D bounding boxes, comes with 0.5B (QwenVL based) and 1B (Llama based) variants &gt; SkyWork released SkyWork-R1V-38B, new vision reasoning model (OS) ğŸ’¬ LLMs &gt; NVIDIA released new Nemotron models in 49B and 8B with their post-training dataset &gt; LG released EXAONE, new reasoning models in 2.4B, 7.8B and 32B &gt; Dataset: Glaive AI released a new reasoning dataset of 22M+ examples &gt; Dataset: NVIDIA released new helpfulness dataset HelpSteer3 &gt; Dataset: OpenManusRL is a new agent dataset based on ReAct framework (OS) &gt; Open-R1 team released OlympicCoder, new competitive coder model in 7B and 32B &gt; Dataset: GeneralThought-430K is a new...</description><pubDate>Tue, 25 Mar 2025 13:30:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/746832157330905</guid></item><item><title>I shared my view on Qwen vs DeepSeek (student vs genius), and I forgot to mention this: they are neighbors in the same city.</title><link>https://huggingface.co/posts/onekq/124053264899473</link><description>I shared my view on Qwen vs DeepSeek (student vs genius), and I forgot to mention this: they are neighbors in the same city. https://en.wikipedia.org/wiki/Hangzhou See translation</description><pubDate>Tue, 25 Mar 2025 13:30:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/124053264899473</guid></item><item><title>Implemented a custom multimodal GRPO trainer that scales for Small VLMs, supports cpu and gpu with vllm + flash attention. Using SmolVLM-256M-Instruct reference &amp; reward model, wasnâ€™t trained for long btw, still got some sparks of â€œthinkingâ€:)</title><link>https://huggingface.co/posts/Jaward/890536870890791</link><description>Implemented a custom multimodal GRPO trainer that scales for Small VLMs, supports cpu and gpu with vllm + flash attention. Using SmolVLM-256M-Instruct reference &amp; reward model, wasnâ€™t trained for long btw, still got some sparks of â€œthinkingâ€:) Code: https://github.com/Jaykef/ai-algorithms/blob/main/grpo_multimodal_reasoner.ipynb See translation</description><pubDate>Tue, 25 Mar 2025 13:30:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/890536870890791</guid></item><item><title>ğŸš€ Just released a PoC: Kurtis-E1 MLX Voice Agent</title><link>https://huggingface.co/posts/mrs83/394905068174905</link><description>ğŸš€ Just released a PoC: Kurtis-E1 MLX Voice Agent An offline, privacy-first voice assistant built for macOS (Apple Silicon), designed for empathetic, short-form interactions. ğŸ§  Powered by: - Whisper (via MLX) for speech-to-text: https://pypi.org/project/mlx-whisper/ - Kurtis-E1 (a custom SmolLM2 LLM) via Ollama - Coqui-TTS XTTSv2 for multilingual TTS - Optional translation layer via TowerInstruct-13B-v0.1 for non-English voice input/output: Unbabel/TowerInstruct-13B-v0.1 ğŸ§ Everything runs entirely on-device (Mac Mini M4 Max - 24gb) â€” no cloud, no remote API calls, no data leakage. ğŸ’¡ Code is fully handcrafted (no AI-generated code), and designed to showcase whatâ€™s possible with local models, even on laptops. ğŸ› ï¸ Open to contributions, ideas (e.g., LM Studio for MLX inference, MLX worker subprocess, optimize for latency and VRAM usage). ğŸ‘‰ Video demo (Italian): https://www.youtube.com/watch?v=8-1PcmUStaI PoC: https://github.com/ethicalabs-ai/Kurtis-E1-MLX-Voice-Agent Kurtis-E1:...</description><pubDate>Tue, 25 Mar 2025 13:30:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mrs83/394905068174905</guid></item><item><title>ğŸŒŸ Day 4: Two Models, One Privacy Mission! ğŸŒŸ</title><link>https://huggingface.co/posts/MikeDoes/593610719403706</link><description>ğŸŒŸ Day 4: Two Models, One Privacy Mission! ğŸŒŸ The PII-Masking-1M series rolls on with two gems: Categorical: ai4privacy/llama-ai4privacy-multilingual-categorical-anonymiser-openpii Redaction: ai4privacy/llama-ai4privacy-multilingual-anonymiser-openpii Join us in protecting data everywhere! #AI #Privacy #OpenSource #Multilingual See translation</description><pubDate>Tue, 25 Mar 2025 13:30:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/593610719403706</guid></item><item><title>I'm collecting llama-bench results for inference with a llama 3.1 8B q4 and q8 reference models on varoius GPUs. The results are average of 5 executions.</title><link>https://huggingface.co/posts/csabakecskemeti/287842366376256</link><description>I'm collecting llama-bench results for inference with a llama 3.1 8B q4 and q8 reference models on varoius GPUs. The results are average of 5 executions. The system varies (different motherboard and CPU ... but that probably that has little effect on the inference performance). https://devquasar.com/gpu-gguf-inference-comparison/ the exact models user are in the page I'd welcome results from other GPUs is you have access do anything else you've need in the post. Hopefully this is useful information everyone. See translation</description><pubDate>Tue, 25 Mar 2025 13:30:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/csabakecskemeti/287842366376256</guid></item><item><title>Dropping Downstream tasks using newly initialized parameters and weights ([classifier.bias &amp; weights]) support domain-specific ğ—¶ğ—ºğ—®ğ—´ğ—² ğ—°ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—». Based on siglip2-base-patch16-224 and DomainNet (single-domain, multi-source adaptation), with Fashion-MNIST for experimental testing. ğŸ§¤â˜„ï¸</title><link>https://huggingface.co/posts/prithivMLmods/636017629605073</link><description>Dropping Downstream tasks using newly initialized parameters and weights ([classifier.bias &amp; weights]) support domain-specific ğ—¶ğ—ºğ—®ğ—´ğ—² ğ—°ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—». Based on siglip2-base-patch16-224 and DomainNet (single-domain, multi-source adaptation), with Fashion-MNIST for experimental testing. ğŸ§¤â˜„ï¸ Fashion-Mnist : prithivMLmods/Fashion-Mnist-SigLIP2 Multisource-121 : prithivMLmods/Multisource-121-DomainNet Painting-126 : prithivMLmods/Painting-126-DomainNet Sketch-126 : prithivMLmods/Sketch-126-DomainNet Clipart-126 : prithivMLmods/Clipart-126-DomainNet Models are trained with different parameter settings for experimental purposes only, with the intent of further development. Refer to the model page below for instructions on running it with Transformers ğŸ¤—. Collection : prithivMLmods/domainnet-0324-67e0e3c934c03cc40c6c8782 Citations : SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features https://arxiv.org/pdf/2502.14786 &amp; Moment...</description><pubDate>Tue, 25 Mar 2025 13:30:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/636017629605073</guid></item></channel></rss>