<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>15 types of attention mechanisms</title><link>https://huggingface.co/posts/Kseniase/624548696865407</link><description>15 types of attention mechanisms Attention mechanisms allow models to dynamically focus on specific parts of their input when performing tasks. In our recent article, we discussed Multi-Head Latent Attention (MLA) in detail and now it's time to summarize other existing types of attention. Here is a list of 15 types of attention mechanisms used in AI models: 1. Soft attention (Deterministic attention) -&gt; Neural Machine Translation by Jointly Learning to Align and Translate (1409.0473) Assigns a continuous weight distribution over all parts of the input. It produces a weighted sum of the input using attention weights that sum to 1. 2. Hard attention (Stochastic attention) -&gt; Effective Approaches to Attention-based Neural Machine Translation (1508.04025) Makes a discrete selection of some part of the input to focus on at each step, rather than attending to everything. 3. Self-attention -&gt; Attention Is All You Need (1706.03762) Each element in the sequence "looks" at other elements and...</description><pubDate>Tue, 18 Mar 2025 13:30:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/624548696865407</guid></item><item><title>‚úÇÔ∏è Gemma 3 Abliterated</title><link>https://huggingface.co/posts/mlabonne/443122762320210</link><description>‚úÇÔ∏è Gemma 3 Abliterated I noticed that Gemma 3 was much more resilient to refusal removal than other models like Qwen 2.5. I experimented with different recipes and improved the abliteration technique I wrote about last year. It's still experimental but the refusal rate is super low in my tests. Enjoy! mlabonne/gemma-3-4b-it-abliterated mlabonne/gemma-3-12b-it-abliterated mlabonne/gemma-3-27b-it-abliterated See translation</description><pubDate>Tue, 18 Mar 2025 13:30:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/443122762320210</guid></item><item><title>There seems to multiple paid apps shared here that are based on models on hf, but some ppl sell their wrappers as "products" and promote them here. For a long time, hf was the best and only platform to do oss model stuff but with the recent AI website builders anyone can create a product (really crappy ones btw) and try to sell it with no contribution to oss stuff. Please dont do this, or try finetuning the models you use...</title><link>https://huggingface.co/posts/AtAndDev/784658782191587</link><description>There seems to multiple paid apps shared here that are based on models on hf, but some ppl sell their wrappers as "products" and promote them here. For a long time, hf was the best and only platform to do oss model stuff but with the recent AI website builders anyone can create a product (really crappy ones btw) and try to sell it with no contribution to oss stuff. Please dont do this, or try finetuning the models you use... Sorry for filling yall feed with this bs but yk... See translation</description><pubDate>Tue, 18 Mar 2025 13:30:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AtAndDev/784658782191587</guid></item><item><title>At Rapidata, we compared DeepL with LLMs like DeepSeek-R1, Llama, and Mixtral for translation quality using feedback from over 51,000 native speakers. Despite the costs, the performance makes it a valuable investment, especially in critical applications where translation quality is paramount. Now we can say that Europe is more than imposing regulations.</title><link>https://huggingface.co/posts/jasoncorkill/521063845119914</link><description>At Rapidata, we compared DeepL with LLMs like DeepSeek-R1, Llama, and Mixtral for translation quality using feedback from over 51,000 native speakers. Despite the costs, the performance makes it a valuable investment, especially in critical applications where translation quality is paramount. Now we can say that Europe is more than imposing regulations. Our dataset, based on these comparisons, is now available on Hugging Face. This might be useful for anyone working on AI translation or language model evaluation. Rapidata/Translation-deepseek-llama-mixtral-v-deepl See translation</description><pubDate>Tue, 18 Mar 2025 13:30:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/521063845119914</guid></item><item><title>üì¢ Our EMOVA paper has been accepted by CVPR 2025, and we are glad to release all resources, including code (training &amp; inference), datasets (training &amp; evaluation), and checkpoints (EMOVA-3B/7B/72B)!</title><link>https://huggingface.co/posts/KaiChen1998/260621109030490</link><description>üì¢ Our EMOVA paper has been accepted by CVPR 2025, and we are glad to release all resources, including code (training &amp; inference), datasets (training &amp; evaluation), and checkpoints (EMOVA-3B/7B/72B)! ü§ó EMOVA is a novel end-to-end omni-modal LLM that can see, hear and speak. Given omni-modal (i.e., textual, visual and speech) inputs, EMOVA can generate both textual and speech responses with vivid emotional controls by utilizing the speech decoder and a style controller. ‚ú® EMOVA Highlights ‚úÖ State-of-the-art omni-modality: EMOVA achieves SoTA comparable results on both vision-language and speech benchmarks simultaneously. ‚úÖ Device adaptation: our codebase supports training/inference on both NVIDIA GPUs (e.g., A800 &amp; H20) and Ascend NPUs (e.g., 910B3)! ‚úÖ Modular design: we integrate multiple implementations of vision encoder, vision projector, and language model, even including the most recent DeepSeekMoE-tiny! üî• You are all welcome to try and star! - Project page: https://emova-...</description><pubDate>Tue, 18 Mar 2025 13:30:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/KaiChen1998/260621109030490</guid></item><item><title>smolagents now support vLLM! ü•≥</title><link>https://huggingface.co/posts/m-ric/783111989290994</link><description>smolagents now support vLLM! ü•≥ As one of the most popular local inference solutions, the community had been asking us to integrate vLLM: after a heavy refactoring of our LLM classes, we've just released smolagents 1.11.0, with a brand new VLLMModel class. Go try it and tell us what you think! https://github.com/huggingface/smolagents/blob/45b2c86857b7f7657daaa74e4d17d347e9e2c4a4/src/smolagents/models.py#L497 See translation</description><pubDate>Tue, 18 Mar 2025 13:30:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/783111989290994</guid></item><item><title>@ mii-llm with</title><link>https://huggingface.co/posts/giux78/524924454012414</link><description>@ mii-llm with @ efederici @ mferraretto @ FinancialSupport and @ DeepMount00 we just released #Propaganda a framework designed to evaluate and train LLMs on political opinions and bias. We aim to analyze both open-source and closed-source LLMs to understand the political positions and biases expressed in their outputs. Moreover we provide a set of recipes to enforce political positions into the models by creating ad hoc curated datasets and by applying fine tuning techniques. By releasing our work in the open, we hope to foster contributions: https://github.com/mii-llm/propaganda This framework offers opportunities for expansion in various directions and could become the standard reference for evaluating LLMs on political topics, particularly those that influence public opinion. See translation</description><pubDate>Tue, 18 Mar 2025 13:30:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giux78/524924454012414</guid></item><item><title>üòä This program is designed to remove emojis from a given text. It uses a regular expression (regex) pattern to match and replace emojis with an empty string, effectively removing them from the text. The pattern includes a range of Unicode characters that correspond to various types of emojis, such as emoticons, symbols, and flags. By using this program, you can clean up text data by removing any emojis that may be present, which can be useful for text processing, analysis, or other applications where emojis are not desired. üíª</title><link>https://huggingface.co/posts/aifeifei798/845431141575759</link><description>üòä This program is designed to remove emojis from a given text. It uses a regular expression (regex) pattern to match and replace emojis with an empty string, effectively removing them from the text. The pattern includes a range of Unicode characters that correspond to various types of emojis, such as emoticons, symbols, and flags. By using this program, you can clean up text data by removing any emojis that may be present, which can be useful for text processing, analysis, or other applications where emojis are not desired. üíª import re def remove_emojis ( text ): # Define a broader emoji pattern emoji_pattern = re. compile ( "[" u"\U0001F600-\U0001F64F" # emoticons u"\U0001F300-\U0001F5FF" # symbols &amp; pictographs u"\U0001F680-\U0001F6FF" # transport &amp; map symbols u"\U0001F1E0-\U0001F1FF" # flags (iOS) u"\U00002702-\U000027B0" u"\U000024C2-\U0001F251" u"\U0001F900-\U0001F9FF" # supplemental symbols and pictographs u"\U0001FA00-\U0001FA6F" # chess symbols and more emojis...</description><pubDate>Tue, 18 Mar 2025 13:30:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/aifeifei798/845431141575759</guid></item><item><title>200</title><link>https://huggingface.co/posts/nroggendorff/216238217580963</link><description>200</description><pubDate>Tue, 18 Mar 2025 13:30:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/216238217580963</guid></item><item><title>Hello community,</title><link>https://huggingface.co/posts/hanzla/186651630886849</link><description>Hello community, I want to share my work of creating a reasoning mamba model I used GRPO over Falcon3 Mamba Instruct to make this model. It generates blazing fast response while building good logic to answer challenging questions. Give it a try: Model repo: hanzla/Falcon3-Mamba-R1-v0 Space: hanzla/Falcon3MambaReasoner Looking forward to community feedback. See translation</description><pubDate>Tue, 18 Mar 2025 13:30:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hanzla/186651630886849</guid></item></channel></rss>