<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Datasets Convertor üöÄ</title><link>https://huggingface.co/posts/openfree/251902401084370</link><description>Datasets Convertor üöÄ openfree/Datasets-Convertor Welcome to Datasets Convertor, the cutting-edge solution engineered for seamless and efficient data format conversion. Designed with both data professionals and enthusiasts in mind, our tool simplifies the transformation process between CSV, Parquet, and JSONL, XLS file formats, ensuring that your data is always in the right shape for your next analytical or development challenge. üíª‚ú® Why Choose Datasets Convertor? In today‚Äôs data-driven world, managing and converting large datasets can be a daunting task. Our converter is built on top of robust technologies like Pandas and Gradio, delivering reliable performance with a modern, intuitive interface. Whether you‚Äôre a data scientist, analyst, or developer, Datasets Convertor empowers you to effortlessly switch between formats while maintaining data integrity and optimizing storage. Key Features and Capabilities: CSV ‚áÜ Parquet Conversion: Easily transform your CSV files into the highly...</description><pubDate>Mon, 24 Feb 2025 05:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/251902401084370</guid></item><item><title>It's really interesting about the deployment of a new state of matter in Majorana 1: the world‚Äôs first quantum processor powered by topological qubits. If you missed this news this week, here are some links for you:</title><link>https://huggingface.co/posts/prithivMLmods/319015417669347</link><description>It's really interesting about the deployment of a new state of matter in Majorana 1: the world‚Äôs first quantum processor powered by topological qubits. If you missed this news this week, here are some links for you: üÖ±Ô∏èTopological qubit arrays: https://arxiv.org/pdf/2502.12252 ‚öõÔ∏è Quantum Blog: https://azure.microsoft.com/en-us/blog/quantum/2025/02/19/microsoft-unveils-majorana-1-the-worlds-first-quantum-processor-powered-by-topological-qubits/ üìñ Read the story: https://news.microsoft.com/source/features/innovation/microsofts-majorana-1-chip-carves-new-path-for-quantum-computing/ üìù Majorana 1 Intro: https://youtu.be/Q4xCR20Dh1E?si=Z51DbEYnZFp_88Xp üåÄThe Path to a Million Qubits: https://youtu.be/wSHmygPQukQ?si=TS80EhI62oWiMSHK See translation</description><pubDate>Mon, 24 Feb 2025 05:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/319015417669347</guid></item><item><title>8 Free Sources about AI Agents:</title><link>https://huggingface.co/posts/Kseniase/557379551700019</link><description>8 Free Sources about AI Agents: Agents seem to be everywhere and this collection is for a deep dive into the theory and practice: 1. "Agents" Google's whitepaper by Julia Wiesinger, Patrick Marlow and Vladimir Vuskovic -&gt; https://www.kaggle.com/whitepaper-agents Covers agents, their functions, tool use and how they differ from models 2. "Agents in the Long Game of AI. Computational Cognitive Modeling for Trustworthy, Hybrid AI" book by Marjorie McShane, Sergei Nirenburg, and Jesse English -&gt; https://direct.mit.edu/books/oa-monograph/5833/Agents-in-the-Long-Game-of-AIComputational Explores building AI agents, using Hybrid AI, that combines ML with knowledge-based reasoning 3. "AI Engineer Summit 2025: Agent Engineering" 8-hour video -&gt; https://www.youtube.com/watch?v=D7BzTxVVMuw Experts' talks that share insights on the freshest Agent Engineering advancements, such as Google Deep Research, scaling tips and more 4. AI Agents Course from Hugging Face -&gt;...</description><pubDate>Mon, 24 Feb 2025 05:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/557379551700019</guid></item><item><title>SmolVLM-2 and SigLIP-2 are now part of</title><link>https://huggingface.co/posts/lysandre/966361810633890</link><description>SmolVLM-2 and SigLIP-2 are now part of transformers in dedicated releases! They're added on top of the v4.49.0 release, and can be installed from the following tags: v4.49.0-SmolVLM-2 and v4.49.0-SigLIP-2 . This marks a new beginning for the release process of transformers. For the past five years, we've been doing monthly releases featuring many models (v4.49.0, the latest release, features 9 new architectures). Starting with SmolVLM-2 &amp; SigLIP2, we'll now additionally release tags supporting new models on a stable branch. These models are therefore directly available for use by installing from the tag itself. These tags will continue to be updated with fixes applied to these models. Going forward, continue expecting software releases following semantic versioning: v4.50.0 will have ~10 new architectures compared to v4.49.0, as well as a myriad of new features, improvements and bug fixes. Accompanying these software releases, we'll release tags offering brand new models as fast as...</description><pubDate>Mon, 24 Feb 2025 05:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lysandre/966361810633890</guid></item><item><title>The past few years have been a blast for artificial intelligence, with large language models (LLMs) stunning everyone with their capabilities and powering everything from chatbots to code assistants. However, not all applications demand the massive size and complexity of LLMs, the computational power required makes them impractical for many use cases. This is why Small Language Models (SLMs) entered the scene to make powerful AI models more accessible by shrinking in size.</title><link>https://huggingface.co/posts/jjokah/811590134220421</link><description>The past few years have been a blast for artificial intelligence, with large language models (LLMs) stunning everyone with their capabilities and powering everything from chatbots to code assistants. However, not all applications demand the massive size and complexity of LLMs, the computational power required makes them impractical for many use cases. This is why Small Language Models (SLMs) entered the scene to make powerful AI models more accessible by shrinking in size. In this article we went through what SLMs are, how they are made small, their benefits and limitations, real-world use cases, and how they can be used on mobile and desktop devices. https://huggingface.co/blog/jjokah/small-language-model See translation</description><pubDate>Mon, 24 Feb 2025 05:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jjokah/811590134220421</guid></item><item><title>üì¢ If you're looking for translating massive dataset of JSON-lines / CSV data with various set of source fields, then the following update would be relevant. So far and experimenting with adapting language specific Sentiment Analysis model, got a change to reforge and relaese bulk-translate 0.25.2.</title><link>https://huggingface.co/posts/nicolay-r/986619870856670</link><description>üì¢ If you're looking for translating massive dataset of JSON-lines / CSV data with various set of source fields, then the following update would be relevant. So far and experimenting with adapting language specific Sentiment Analysis model, got a change to reforge and relaese bulk-translate 0.25.2. ‚≠êÔ∏è https://github.com/nicolay-r/bulk-translate/releases/tag/0.25.2 The update has the following major features - Supporting schemas: all the columns to be translated are now could be declared within the same prompt-style format. using json this automatically allows to map them onto output fields - The related updates for shell execution mode: schema parameter is now available alongside with just a prompt usage before. Benefit is that your output is invariant. You can extend and stack various translators with separated shell laucnhes. Screenshot below is the application of the google-translate engine in manual batching mode. üöÄ Performance: 2.5 it / sec (in the case of a single field...</description><pubDate>Mon, 24 Feb 2025 05:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nicolay-r/986619870856670</guid></item><item><title>Trying something new to keep you ahead of the curve: The 5 AI stories of the week - a weekly curation of the most important AI news you need to know. Do you like it?</title><link>https://huggingface.co/posts/fdaudens/422173269922572</link><description>Trying something new to keep you ahead of the curve: The 5 AI stories of the week - a weekly curation of the most important AI news you need to know. Do you like it? For more AI stories and deeper analysis, check out my newsletter: https://open.substack.com/pub/fdaudens/p/ai-competition-heats-up-grok-3-iphone See translation</description><pubDate>Mon, 24 Feb 2025 05:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/422173269922572</guid></item><item><title>üéâ We're excited to introduce MemoryCode, a novel synthetic dataset designed to rigorously evaluate LLMs' ability to track and execute coding instructions across multiple sessions. MemoryCode simulates realistic workplace scenarios where a mentee (the LLM) receives coding instructions from a mentor amidst a stream of both relevant and irrelevant information.</title><link>https://huggingface.co/posts/mmhamdy/257109472269745</link><description>üéâ We're excited to introduce MemoryCode, a novel synthetic dataset designed to rigorously evaluate LLMs' ability to track and execute coding instructions across multiple sessions. MemoryCode simulates realistic workplace scenarios where a mentee (the LLM) receives coding instructions from a mentor amidst a stream of both relevant and irrelevant information. üí° But what makes MemoryCode unique?! The combination of the following: ‚úÖ Multi-Session Dialogue Histories: MemoryCode consists of chronological sequences of dialogues between a mentor and a mentee, mirroring real-world interactions between coworkers. ‚úÖ Interspersed Irrelevant Information: Critical instructions are deliberately interspersed with unrelated content, replicating the information overload common in office environments. ‚úÖ Instruction Updates: Coding rules and conventions can be updated multiple times throughout the dialogue history, requiring LLMs to track and apply the most recent information. ‚úÖ Prospective Memory:...</description><pubDate>Mon, 24 Feb 2025 05:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mmhamdy/257109472269745</guid></item><item><title>Do you want ArcticTraining at</title><link>https://huggingface.co/posts/stas/738116252437953</link><description>Do you want ArcticTraining at @ SnowflakeDB to add an ability to post-train DeepSeek V3/R1 models with DPO using just a few GPU nodes? Please vote here and tell others about it: https://github.com/snowflakedb/ArcticTraining/discussions/58 ArcticTraining is an open-source, easy to use post-training framework for NVIDIA GPUs built on top of DeepSpeed. See translation</description><pubDate>Mon, 24 Feb 2025 05:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/stas/738116252437953</guid></item><item><title>ü§óWelcome to the Doge Edge Device Small language Model.</title><link>https://huggingface.co/posts/JingzeShi/354750943862398</link><description>ü§óWelcome to the Doge Edge Device Small language Model. SmallDoge/Doge-160M-Instruct See translation</description><pubDate>Mon, 24 Feb 2025 05:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/JingzeShi/354750943862398</guid></item></channel></rss>