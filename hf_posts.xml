<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Got to 1199.8 tokens/sec with Devstral Small -2 on my desktop GPU workstation. vLLM nightly.</title><link>https://huggingface.co/posts/mitkox/706030667212965</link><description>Got to 1199.8 tokens/sec with Devstral Small -2 on my desktop GPU workstation. vLLM nightly. Works out of the box with Mistral Vibe. Next is time to test the big one. See translation</description><pubDate>Thu, 11 Dec 2025 17:30:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/706030667212965</guid></item><item><title>Currently having a blast learning the transformers library.</title><link>https://huggingface.co/posts/melvindave/358781763788308</link><description>Currently having a blast learning the transformers library. I noticed that model cards usually have Transformers code as usage examples. So I tried to figure out how to load a model just using the transformers library without using ollama, lmstudio, or llamacpp. Learned how to install dependencies required to make it work like pytorch and CUDA. I also used Conda for python environment dependencies. Once I got the model loaded and sample inference working, I made an API to serve it. I know it's very basic stuff for machine learning experts here in HF but I'm completely new to this so I'm happy to get it working! Model used: Qwen/Qwen3-VL-8B-Instruct GPU: NVIDIA GeForce RTX 3090 Here's the result of my experimentation See translation</description><pubDate>Thu, 11 Dec 2025 17:30:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/melvindave/358781763788308</guid></item><item><title>ICYMI, you can fine-tune open LLMs using Claude Code</title><link>https://huggingface.co/posts/sergiopaniego/384415208092213</link><description>ICYMI, you can fine-tune open LLMs using Claude Code just tell it: ‚ÄúFine-tune Qwen3-0.6B on open-r1/codeforces-cots‚Äù and Claude submits a real training job on HF GPUs using TRL. it handles everything: &gt; dataset validation &gt; GPU selection &gt; training + Trackio monitoring &gt; job submission + cost estimation when it‚Äôs done, your model is on the Hub, ready to use read more about the process: https://huggingface.co/blog/hf-skills-training See translation</description><pubDate>Thu, 11 Dec 2025 17:30:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/384415208092213</guid></item><item><title>We just released TRL v0.26.0!</title><link>https://huggingface.co/posts/sergiopaniego/627155943228357</link><description>We just released TRL v0.26.0! It comes packed with updates: &gt; Agent training with tools in GRPO &gt; New CISPO &amp; SAPO losses + reasoning rewards &gt; vLLM quantization in colocate mode &gt; Dataset shuffling in SFT &gt; Lots of NEW examples &gt; Tons of fixes and documentation improvements See translation</description><pubDate>Thu, 11 Dec 2025 17:30:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/627155943228357</guid></item><item><title>üê¶‚Äçüî• I've just published Sentence Transformers v5.2.0! It introduces multi-processing for CrossEncoder (rerankers), multilingual NanoBEIR evaluators, similarity score outputs in mine_hard_negatives, Transformers v5 support and more. Details:</title><link>https://huggingface.co/posts/tomaarsen/853653818134091</link><description>üê¶‚Äçüî• I've just published Sentence Transformers v5.2.0! It introduces multi-processing for CrossEncoder (rerankers), multilingual NanoBEIR evaluators, similarity score outputs in mine_hard_negatives, Transformers v5 support and more. Details: - CrossEncoder multi-processing: Similar to SentenceTransformer and SparseEncoder, you can now use multi-processing with CrossEncoder rerankers. Useful for multi-GPU and CPU settings, and simple to configure: just device=["cuda:0", "cuda:1"] or device=["cpu"]*4 on the model.predict or model.rank calls. - Multilingual NanoBEIR Support: You can now use community translations of the tiny NanoBEIR retrieval benchmark instead of only the English one, by passing dataset_id , e.g. dataset_id="lightonai/NanoBEIR-de" for the German benchmark. - Similarity scores in Hard Negatives Mining: When mining for hard negatives to create a strong training dataset, you can now pass output_scores=True to get similarity scores returned. This can be useful for some...</description><pubDate>Thu, 11 Dec 2025 17:30:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/853653818134091</guid></item><item><title>installama.sh at the TigerBeetle 1000x World Tour !</title><link>https://huggingface.co/posts/angt/186034800220690</link><description>installama.sh at the TigerBeetle 1000x World Tour ! Last week I had the chance to give a short talk during the TigerBeetle 1000x World Tour (organized by @ jedisct1 üëè ) a fantastic event celebrating high-performance engineering and the people who love pushing systems to their limits! In the talk, I focused on the CPU and Linux side of things, with a simple goal in mind: making the installation of llama.cpp instant, automatic, and optimal, no matter your OS or hardware setup. For the curious, here are the links worth checking out: Event page: https://tigerbeetle.com/event/1000x GitHub repo: https://github.com/angt/installama.sh Talk: https://youtu.be/pg5NOeJZf0o?si=9Dkcfi2TqjnT_30e More improvements are coming soon. Stay tuned! See translation</description><pubDate>Thu, 11 Dec 2025 17:30:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/angt/186034800220690</guid></item><item><title>Hey all  üëã</title><link>https://huggingface.co/posts/StJohnDeakins/147365886941567</link><description>Hey all üëã A Quick one for any founders building with Small Language Models in mobile apps: We‚Äôre opening 10 Innovation Partner spots this month for our Device Native AI (DNA) platform. What you get: - Device Native AI SDK ‚Å†(AI processes data on-device, not cloud üì≤) - 99% off for 3 months, then 90% off for the rest of the year (no lock-in) - Direct engineering access + feature releases - It's an Innovation community, so at least some participation is required Perfect if you're building consumer apps and want: ‚úì Hyper-personalization without privacy risks ‚úì Zero cloud AI token costs ‚úì Early access to next-gen mobile AI Limited spots, and on a first-come basis, so DM me "DNA" for more info and an access code. Cheers Singe üêµ See translation</description><pubDate>Thu, 11 Dec 2025 17:30:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/StJohnDeakins/147365886941567</guid></item><item><title>I built something crazy you never saw before.</title><link>https://huggingface.co/posts/RakshitAralimatti/606573836468382</link><description>I built something crazy you never saw before. Please check - https://huggingface.co/blog/RakshitAralimatti/streaming-data-rag A real-time Streaming Data to RAG system that listens to live radio, transcribes it on-the-fly, and lets you query across TIME. Not just "what was discussed" ‚Äì but "what happened in the last 10 minutes on channel 0?" or "at 9 AM, what was the breaking news?" This is RAG that understands temporal context. See translation</description><pubDate>Thu, 11 Dec 2025 17:30:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/606573836468382</guid></item><item><title>We just released our latest Shisa V2.1 Japanese multi-lingual models:</title><link>https://huggingface.co/posts/leonardlin/280604558183575</link><description>We just released our latest Shisa V2.1 Japanese multi-lingual models: https://huggingface.co/collections/shisa-ai/shisa-v21 Besides updates to our 14B, and 70B, we have a new LFM2-based 1.2B, Llama 3.2-based 3B, and Qwen 3-based 8B, all with class-leading Japanese language capabilities. Per usual, lots of details in the Model Cards for those interested. See translation</description><pubDate>Thu, 11 Dec 2025 17:30:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/leonardlin/280604558183575</guid></item><item><title>What do you think of my LLM Chat app so far?</title><link>https://huggingface.co/posts/NicoBBQ1/471268005218525</link><description>What do you think of my LLM Chat app so far? Here are some of the features already included (and more are coming): - Chat with AI models ‚Äì Local inference via Ollama - Reasoning support ‚Äì View model thinking process (DeepSeek-R1, Qwen-QwQ, etc.) - Vision models ‚Äì Analyze images with llava, bakllava, moondream - Image generation ‚Äì Local GGUF models with GPU acceleration (CUDA) - Fullscreen images ‚Äì Click generated images to view in fullscreen - Image attachments ‚Äì File picker or clipboard paste (Ctrl+V) - DeepSearch ‚Äì Web search with tool use - Inference Stats ‚Äì Token counts, speed, duration (like Ollama verbose) - Regenerate ‚Äì Re-run any AI response - Copy ‚Äì One-click copy AI responses See translation</description><pubDate>Thu, 11 Dec 2025 17:30:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/NicoBBQ1/471268005218525</guid></item></channel></rss>