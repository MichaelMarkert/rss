<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>üöÄ Dhanishtha-2.0-preview-0825 Is Here</title><link>https://huggingface.co/posts/Abhaykoul/625756342268823</link><description>üöÄ Dhanishtha-2.0-preview-0825 Is Here The Intermediate Thinking Model just leveled up again. With sharper reasoning, better tool use, and expanded capabilities, Dhanishtha-2.0-preview-0825 is now live and ready to impress. üß† What Makes Dhanishtha Special? Unlike typical CoT models that only thinks one time, Dhanishtha thinks iteratively: &gt; Think ‚Üí Answer ‚Üí Rethink ‚Üí Improve ‚Üí Rethink again if needed. üîó Try it now: HelpingAI/Dhanishtha-2.0-preview-0825 üîû Dhanishtha NSFW Preview For those exploring more expressive and immersive roleplay scenarios, we‚Äôre also releasing: HelpingAI/Dhanishtha-nsfw A specialized version tuned for adult-themed interactions and character-driven roleplay. üîó Explore it here: HelpingAI/Dhanishtha-nsfw üí¨ You can also try all of these live at chat.helpingai.co See translation</description><pubDate>Mon, 04 Aug 2025 05:39:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/625756342268823</guid></item><item><title>We‚Äôve reached a point where on device AI coding that is free, offline, and capable isn‚Äôt just a theoretical possibility; it‚Äôs sitting on my lap, barely warming my thighs.</title><link>https://huggingface.co/posts/mitkox/598805408500117</link><description>We‚Äôve reached a point where on device AI coding that is free, offline, and capable isn‚Äôt just a theoretical possibility; it‚Äôs sitting on my lap, barely warming my thighs. My local MacBook Air setup includes a Qwen3 Coder Flash with a 1M context, Cline in a VSCode IDE. No internet, no cloud, no ID verification- this is the forbidden tech. Current stats: All agentic tools work great local, sandboxed, and MCP OK model output precision 17 tokens/sec. Not great, not terrible 65K tokens context, the model can do 1M, but let‚Äôs be real, my MacBook Air would probably achieve fusion before hitting that smoothly Standard backend and cache off for the test All inference and function calling happen locally, offline, untethered. The cloud didn‚Äôt even get a memo. See translation</description><pubDate>Mon, 04 Aug 2025 05:39:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/598805408500117</guid></item><item><title>XBai o4 claims to beat Claude Opus 4 and o3-mini, and they provide verifiable proof. My skepticism circuits overloaded, but my local AI FOMO module screamed louder.</title><link>https://huggingface.co/posts/mitkox/604222926195865</link><description>XBai o4 claims to beat Claude Opus 4 and o3-mini, and they provide verifiable proof. My skepticism circuits overloaded, but my local AI FOMO module screamed louder. I've thrown this 33B monoblock LLM onto a single GPU and used Roo Code for some‚Ä¶ let‚Äôs call it ‚Äúvibe testing‚Äù. It‚Äôs terrifyingly competent. As an architect, it‚Äôs the best open-weight model I‚Äôve touched this side of 2025. See translation</description><pubDate>Mon, 04 Aug 2025 05:39:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/604222926195865</guid></item><item><title>Wan 2.2 &amp; FLUX Krea Full Tutorial - Automated Install - Ready Perfect Presets - SwarmUI with ComfyUI - Install Wan 2.2 and FLUX Krea with literally 1-click and use our pre-made most amazing quality presets :</title><link>https://huggingface.co/posts/MonsterMMORPG/425722660691765</link><description>Wan 2.2 &amp; FLUX Krea Full Tutorial - Automated Install - Ready Perfect Presets - SwarmUI with ComfyUI - Install Wan 2.2 and FLUX Krea with literally 1-click and use our pre-made most amazing quality presets : https://youtu.be/8MvvuX4YPeo https://youtu.be/8MvvuX4YPeo Video Chapters 0:00 Introduction: The Ultimate Wan 2.2 Tutorial with Optimized Presets 1:03 Free Prompt Generation Tool &amp; Introducing the New FLUX Krea Dev Model 2:01 How SwarmUI &amp; ComfyUI Enable Video Generation on Low-End Hardware 2:46 Quick Start Guide: Downloading the Latest SwarmUI &amp; ComfyUI Installers 3:10 Step-by-Step: How to Update or Perform a Fresh Installation of ComfyUI 3:51 Step-by-Step: How to Update or Perform a Fresh Installation of SwarmUI 4:18 Essential Setup: Configuring the SwarmUI Backend for ComfyUI 4:53 One-Click Setup: Downloading All Required Wan 2.2 Models Automatically 5:46 Importing the Ultimate SwarmUI Presets Pack for Best Results 6:22 Wan 2.2 Image-to-Video Generation: A Complete Step-by-...</description><pubDate>Mon, 04 Aug 2025 05:39:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/425722660691765</guid></item><item><title>Introducing Camel-Doc-OCR-080125(v2), a document content-structure retrieval VLM designed for content extraction and summarization. This is the second model in the Camel Doc OCR VLM series, following Camel-Doc-OCR-062825(v1). The new version fixes formal table reconstruction issues in both en and zh language, achieving optimal performance for long-context inferences.ü§óüê™</title><link>https://huggingface.co/posts/prithivMLmods/210600524016945</link><description>Introducing Camel-Doc-OCR-080125(v2), a document content-structure retrieval VLM designed for content extraction and summarization. This is the second model in the Camel Doc OCR VLM series, following Camel-Doc-OCR-062825(v1). The new version fixes formal table reconstruction issues in both en and zh language, achieving optimal performance for long-context inferences.ü§óüê™ ‚§∑ Camel-Doc-OCR(v2) : prithivMLmods/Camel-Doc-OCR-080125 ‚§∑ Camel-Doc-OCR(v1) : prithivMLmods/Camel-Doc-OCR-062825 ‚§∑ Demo : prithivMLmods/core-OCR Multimodal Model Collections and Spaces: ‚ûù Camel-Doc-OCR : prithivMLmods/camel-doc-ocr-080125-688c0c61c5dba648756f31f8 ‚ûù Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a ‚ûù Multimodal Spaces : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 ‚ûù Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 . . . To know more about it, visit the model card of the respective model. !! See...</description><pubDate>Mon, 04 Aug 2025 05:39:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/210600524016945</guid></item><item><title>ü´° I am the first and only one to like the French Tax Code Dataset</title><link>https://huggingface.co/posts/Tonic/788576464887071</link><description>ü´° I am the first and only one to like the French Tax Code Dataset that's it , that's the post find the dataset here : louisbrulenaudet/code-impots follow : @ louisbrulenaudet See translation</description><pubDate>Mon, 04 Aug 2025 05:39:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Tonic/788576464887071</guid></item><item><title>Introducing Completionist, an open-source command-line tool that automates synthetic dataset generation.</title><link>https://huggingface.co/posts/mrs83/268112975581936</link><description>Introducing Completionist, an open-source command-line tool that automates synthetic dataset generation. It works by iterating over an existing HF dataset and by using a LLM to create completions. - Problem: You need a fast way to create custom datasets for fine-tuning or RAG, but you want the flexibility to use different LLM backends or your own infrastructure. - Solution: Completionist connects with any OpenAI-compatible endpoint, including Ollama and LM Studio, or a Hugging Face inference endpoint. A simple CLI like Completionist gives you the possibility to take full control of your synthetic data generation workflow. üëâ Check out Completionist on GitHub: https://github.com/ethicalabs-ai/completionist Synthetic Dataset Example: ethicalabs/kurtis-mental-health-v2-sft-reasoning See translation</description><pubDate>Mon, 04 Aug 2025 05:39:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mrs83/268112975581936</guid></item><item><title>Latest work on SWE-Bench üêõ</title><link>https://huggingface.co/posts/YerbaPage/437092763804097</link><description>Latest work on SWE-Bench üêõ Our two new papers from the SJTU &amp; Huawei: Powered by DeepSeek-V3, we've achieved a new SOTA on the SWE-Bench benchmark! We introduce two innovative approaches: ‚öîÔ∏è SWE-Debate: AI agents compete and "debate" to generate the best code fix. üß† SWE-Exp: An AI agent learns from past repair "experience" to solve new issues more efficiently. üëá Explore the future of software development: SWE-Debate üìÑ Paper: https://arxiv.org/abs/2507.23348 üíª Code: https://github.com/YerbaPage/SWE-Debate SWE-Exp üìÑ Paper: https://arxiv.org/abs/2507.23361 üíª Code: https://github.com/YerbaPage/SWE-Exp See translation</description><pubDate>Mon, 04 Aug 2025 05:39:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/437092763804097</guid></item><item><title>Qwen team did it again!!</title><link>https://huggingface.co/posts/AdinaY/564352975503737</link><description>Qwen team did it again!! They just released Qwen3-Coder-30B-A3B-Instruct on the hubüî• Qwen/Qwen3-Coder-30B-A3B-Instruct ‚ú® Apache 2.0 ‚ú®30B total / 3.3B active (128 experts, 8 top-k) ‚ú® Native 256K context, extendable to 1M via Yarn ‚ú® Built for Agentic Coding See translation</description><pubDate>Mon, 04 Aug 2025 05:39:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/564352975503737</guid></item><item><title>Extended the ICM paper to show cross-model capability transfer - used Qwen3's mathematical reasoning to improve Gemma3 without any human supervision.</title><link>https://huggingface.co/posts/codelion/145733180850232</link><description>Extended the ICM paper to show cross-model capability transfer - used Qwen3's mathematical reasoning to improve Gemma3 without any human supervision. Key results: Qwen3-0.6B: 63.2 ‚Üí 66.0 on MATH-500 (+4%) Gemma3-1B: 41.0 ‚Üí 45.6 on MATH-500 (+11%) The method extracts coherent reasoning patterns from one model via Internal Coherence Maximization, converts them to DPO training data, and uses that to improve a completely different model architecture. This goes beyond the original ICM paper which only improved models using their own labels. We're showing you can transfer capabilities between any models - imagine extracting capabilities from strong models to improve your local ones. Models available: codelion/Qwen3-0.6B-ICM-DPO codelion/gemma-3-1b-it-ICM-DPO Complete collection with code and datasets: codelion/internal-coherence-maximization-687a1bd1c1f5f1d6f76e9b3b Full methodology and results: https://huggingface.co/blog/codelion/internal-coherence-maximization Planning to extend this...</description><pubDate>Mon, 04 Aug 2025 05:39:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/145733180850232</guid></item></channel></rss>