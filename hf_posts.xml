<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸŒŠ CycleNavigator: Visualizing Economic and Political Cycles Through AI at a Glance! ğŸ§ ğŸ’¹</title><link>https://huggingface.co/posts/openfree/905523908666849</link><description>ğŸŒŠ CycleNavigator: Visualizing Economic and Political Cycles Through AI at a Glance! ğŸ§ ğŸ’¹ ğŸ’« Strategic Intelligence Tool for Navigating Historical Waves and Forecasting the Future Hello there! ğŸ™Œ CycleNavigator brings you an innovative fusion of economic history, data visualization, and generative AI. This open-source project revolutionizes decision-making by displaying four major economic and political cycles through interactive visualizations! ğŸ“Š Experience Four Major Cycles in One View: Business Cycle (â‰ˆ9 years) â±ï¸ - The 'heartbeat' of investment and inventory Kondratiev Wave (â‰ˆ50 years) ğŸŒ - Long technological innovation waves Finance Cycle (â‰ˆ80 years) ğŸ’° - Rhythm of debt and financial crises Hegemony Cycle (â‰ˆ250 years) ğŸ›ï¸ - Transitions in global order âœ¨ Cutting-Edge Features: Interactive Wave Visualization ğŸ¯ - Intuitive graphs powered by Plotly AI-Powered Historical Similarity Mapping ğŸ§© - Connecting past events via SBERT embeddings Real-time News Integration ğŸ“° - Linking current issues...</description><pubDate>Thu, 15 May 2025 09:25:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/905523908666849</guid></item><item><title>Iâ€™ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things.</title><link>https://huggingface.co/posts/ArturoNereu/644085701737970</link><description>Iâ€™ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things. I turned this into a GitHub repo: https://github.com/ArturoNereu/AI-Study-Group If youâ€™re just getting started, I recommend: ğŸ“˜ Deep Learning â€“ A Visual Approach: https://www.glassner.com/portfolio/deep-learning-a-visual-approach ğŸ¥ Dive into LLMs with Andrej Karpathy: https://youtu.be/7xTGNNLPyMI?si=aUTq_qUzyUx36BsT ğŸ§  The ğŸ¤— Agents course]( https://huggingface.co/learn/agents-course/ The repo has grown with help from the community (Reddit, Discord, etc.) and Iâ€™ll keep updating it. If you have any favorite resources, Iâ€™d love to include them. See translation</description><pubDate>Thu, 15 May 2025 09:25:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ArturoNereu/644085701737970</guid></item><item><title>Tried something new: an AI-generated podcast that breaks down the top research paper each day. Fully automated, now live on Spotify.</title><link>https://huggingface.co/posts/fdaudens/617387724043904</link><description>Tried something new: an AI-generated podcast that breaks down the top research paper each day. Fully automated, now live on Spotify. I built this prototype to help keep up with the rapid pace of AI developments and, hopefully, make cutting-edge research more accessible. I donâ€™t know about you, but just listening to a conversation about a paper really helps the content sink in for me. This build taught me a lot about full automation. If youâ€™re into the technical weeds: Qwen3 runs on Inference to handle the script, Kokoro does the voice, and the whole thing gets published automatically thanks to the Hugging Face Jobs API and Gradio deployment. Itâ€™s not perfect yet â€” Iâ€™ll be monitoring for hallucinations and incoherence. The voice model still needs polish, but itâ€™s a promising start. Would love to build this with the community â€” submit a PR or send feedback. Itâ€™s just a beta of an experimental idea! Big kudos to @ m-ric , whose Open NotebookLM this is based on, and to @ nielsr for his...</description><pubDate>Thu, 15 May 2025 09:25:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/617387724043904</guid></item><item><title>Very cool to see</title><link>https://huggingface.co/posts/clem/170733821735878</link><description>Very cool to see pytorch contributing on Hugging Face. Time to follow them to see what they're cooking! See translation</description><pubDate>Thu, 15 May 2025 09:25:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/170733821735878</guid></item><item><title>ğŸ”¥ Hidream I1 is online! ğŸ”¥</title><link>https://huggingface.co/posts/jasoncorkill/313090798327696</link><description>ğŸ”¥ Hidream I1 is online! ğŸ”¥ We just added Hidream I1 to our T2I leaderboard ( https://www.rapidata.ai/leaderboard/image-models ) benchmarked using 195k+ human responses from 38k+ annotators, all collected in under 24 hours. It landed #3 overall, right behind: - @ openai 4o - @ black-forest-labs Flux 1 Pro ...and just ahead of @ black-forest-labs Flux 1.1 Pro, @ xai-org Aurora and @ google Imagen3. Want to dig into the data? Check out our dataset here: Rapidata/Hidream_t2i_human_preference What model should we benchmark next? See translation</description><pubDate>Thu, 15 May 2025 09:25:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/313090798327696</guid></item><item><title>Transcribing 1 hour of audio for less than $0.01 ğŸ¤¯</title><link>https://huggingface.co/posts/jeffboudier/904543868043384</link><description>Transcribing 1 hour of audio for less than $0.01 ğŸ¤¯ @ mfuntowicz cooked with 8x faster Whisper speech recognition - whisper-large-v3-turbo transcribes at 100x real time on a $0.80/hr L4 GPU! How they did it: https://huggingface.co/blog/fast-whisper-endpoints 1-click deploy with HF Inference Endpoints: https://endpoints.huggingface.co/new?repository=openai%2Fwhisper-large-v3-turbo&amp;vendor=aws&amp;region=us-east&amp;accelerator=gpu&amp;instance_id=aws-us-east-1-nvidia-l4-x1&amp;task=automatic-speech-recognition&amp;no_suggested_compute=true See translation</description><pubDate>Thu, 15 May 2025 09:25:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jeffboudier/904543868043384</guid></item><item><title>ğ—”ğ—¯ğ˜€ğ—¼ğ—¹ğ˜‚ğ˜ğ—² ğ—­ğ—²ğ—¿ğ—¼: ğ—Ÿğ—Ÿğ— ğ˜€ ğ—°ğ—®ğ—» ğ˜ğ—¿ğ—®ğ—¶ğ—» ğ˜„ğ—¶ğ˜ğ—µğ—¼ğ˜‚ğ˜ ğ—®ğ—»ğ˜† ğ—²ğ˜…ğ˜ğ—²ğ—¿ğ—»ğ—®ğ—¹ ğ—±ğ—®ğ˜ğ—® ğŸ¤¯</title><link>https://huggingface.co/posts/m-ric/402808163323191</link><description>ğ—”ğ—¯ğ˜€ğ—¼ğ—¹ğ˜‚ğ˜ğ—² ğ—­ğ—²ğ—¿ğ—¼: ğ—Ÿğ—Ÿğ— ğ˜€ ğ—°ğ—®ğ—» ğ˜ğ—¿ğ—®ğ—¶ğ—» ğ˜„ğ—¶ğ˜ğ—µğ—¼ğ˜‚ğ˜ ğ—®ğ—»ğ˜† ğ—²ğ˜…ğ˜ğ—²ğ—¿ğ—»ğ—®ğ—¹ ğ—±ğ—®ğ˜ğ—® ğŸ¤¯ Has the "data wall" just been breached? Recent RL paradigms often relied on a set of questions an answers that needs to be manually curated. Researchers from Tsinghua University went like "why though". ğŸ¤” Indeed, why learn from question designed by a human teacher, when the model can start from their base knowledge and learn by experimenting in a code environment, proposing coding tasks themselves and trying to solve them? Thus they created â€œAbsolute Zero Reasoningâ€ (AZR), an approach that removes any need for human curated data. ğŸ­ ğ——ğ˜‚ğ—®ğ—¹ ğ—¿ğ—¼ğ—¹ğ—²ğ˜€: â€£ Proposer: Generates challenging but solvable coding tasks â€£ Solver: Attempts to solve those self-proposed tasks ğŸ§ª ğ—§ğ—µğ—¿ğ—²ğ—² ğ˜ğ—®ğ˜€ğ—¸ ğ˜ğ˜†ğ—½ğ—²ğ˜€: all types are defined as triplets of program, input and output â€£ Deduction: Give model an input and program, it must deduce the output â€£ Abduction: Give model an program and output, it must find the input that gave said output â€£ Induction: Synthesize a...</description><pubDate>Thu, 15 May 2025 09:25:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/402808163323191</guid></item><item><title>ğŸš€ VisionScout Now Speaks More Like Me â€” Thanks to LLMs!</title><link>https://huggingface.co/posts/DawnC/683112818630492</link><description>ğŸš€ VisionScout Now Speaks More Like Me â€” Thanks to LLMs! I'm thrilled to share a major update to VisionScout, my end-to-end vision system. Beyond robust object detection (YOLOv8) and semantic context (CLIP), VisionScout now features a powerful LLM-based scene narrator (Llama 3.2), improving the clarity, accuracy, and fluidity of scene understanding. This isnâ€™t about replacing the pipeline , itâ€™s about giving it a better voice. âœ¨ â­ï¸ What the LLM Brings Fluent, Natural Descriptions: The LLM transforms structured outputs into human-readable narratives. Smarter Contextual Flow: It weaves lighting, objects, zones, and insights into a unified story. Grounded Expression: Carefully prompt-engineered to stay factual â€” it enhances, not hallucinates. Helpful Discrepancy Handling: When YOLO and CLIP diverge, the LLM adds clarity through reasoning. VisionScout Still Includes: ğŸ–¼ï¸ YOLOv8-based detection (Nano / Medium / XLarge) ğŸ“Š Real-time stats &amp; confidence insights ğŸ§  Scene understanding via...</description><pubDate>Thu, 15 May 2025 09:25:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/683112818630492</guid></item><item><title>âœ¨ DreamO Video: From Customized Images to Videos âœ¨</title><link>https://huggingface.co/posts/openfree/538970335354687</link><description>âœ¨ DreamO Video: From Customized Images to Videos âœ¨ Hello, AI creators! Today I'm introducing a truly special project. DreamO Video is an integrated framework that generates customized images based on reference images and transforms them into videos with natural movement. ğŸ¬âœ¨ openfree/DreamO-video ğŸ” Key Features Image Reference (IP): Maintain object appearance while applying to new backgrounds and situations ID Preservation: Retain facial features across various environments Style Transfer: Apply unique styles from reference images to other content ğŸï¸ Video Generation: Create natural 2-second videos from generated images ğŸ’¡ How to Use Upload Reference Images: One or two images (people, objects, landscapes, etc.) Select Task Type: Choose between IP (Image Preservation), ID (Face Feature Retention), or Style Enter Prompt: Describe your desired result (e.g., "a woman playing guitar on a cloud") Click Generate Image: âœ¨ Create customized AI images! Generate Video: Click the ğŸ¬ button on the...</description><pubDate>Thu, 15 May 2025 09:25:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/538970335354687</guid></item><item><title>Transfer Any Clothing Into A New Person &amp; Turn Any Person Into A 3D Figure - ComfyUI Tutorial</title><link>https://huggingface.co/posts/MonsterMMORPG/869555651580897</link><description>Transfer Any Clothing Into A New Person &amp; Turn Any Person Into A 3D Figure - ComfyUI Tutorial ComfyUI is super hard to use but I have literally prepared 1-click way to install and use 2 amazing workflows. First workflow is generating a person wearing any clothing. The second workflow is turning any person image into a 3D toy like figure image. Tutorial Link : https://youtu.be/ZzYnhKeaJBs Video Chapters 0:00:00 Intro: Two One-Click ComfyUI Workflows (Clothing Gen &amp; 3D Figure) 0:00:34 Effort &amp; Goal: Easy Installation &amp; Use of Complex Workflows 0:00:49 Setup Part 1: ComfyUI Prerequisite &amp; Downloading Project Zip File 0:01:06 Setup Part 2: Extracting Zip into ComfyUI Folder (WinRAR 'Extract Here' Tip) 0:01:18 Setup Part 3: Running update_comfyui.bat for Latest ComfyUI Version 0:01:37 Setup Part 4: Running install_clothing_and_3D.bat (Installs Nodes &amp; Requirements) 0:02:03 Model Downloads: Intro to Swarm UI Auto-Installer &amp; Automatic Updater 0:02:28 Using Swarm UI: Launching...</description><pubDate>Thu, 15 May 2025 09:25:49 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/869555651580897</guid></item></channel></rss>