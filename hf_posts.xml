<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>WAN 2.1 FusionX + Self Forcing LoRA are the New Best of Local Video Generation with Only 8 Steps + FLUX Upscaling Guide :</title><link>https://huggingface.co/posts/MonsterMMORPG/682969091190201</link><description>WAN 2.1 FusionX + Self Forcing LoRA are the New Best of Local Video Generation with Only 8 Steps + FLUX Upscaling Guide : https://www.youtube.com/watch?v=Xbn93GRQKsQ Tutorial : https://www.youtube.com/watch?v=Xbn93GRQKsQ Video Chapters 0:00 Introduction to the New FusionX Video Model &amp; FLUX Upscaling 0:30 One-Click Presets &amp; The SwarmUI Model Downloader Explained 1:07 Achieving Hyper-Realism with the FLUX 2x Latent Upscale Preset 1:58 How to Download &amp; Install the SwarmUI Model Downloader 2:49 Downloading Full Models vs. Downloading Just The LoRAs 3:48 Final Setup: Updating SwarmUI &amp; Importing The New Presets 4:32 Generating a Video: Applying the FusionX Image-to-Video Preset 5:03 Critical Step: Correcting The Model's Native Resolution Metadata 5:55 Finalizing Image-to-Video Settings (Frame Count &amp; RIFE Interpolation) 6:49 Troubleshooting Performance: Identifying Low GPU Usage &amp; Shared VRAM Bug 8:35 The Solution: Disabling Sage Attention for Image-to-Video Models 10:02 Final Result:...</description><pubDate>Fri, 20 Jun 2025 13:34:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/682969091190201</guid></item><item><title>Releases of the past week are here</title><link>https://huggingface.co/posts/merve/828394748215710</link><description>Releases of the past week are here merve/releases-june-13-6852c3c1eaf1e0c24c958860 Here's our picks ü§ì So many interesting models released past week in open AI! ü§ñ üñºÔ∏è Computer Vision/VLMs &gt; nanonets/Nanonets-OCR-s is the new state-of-the-art OCR model that can handle checkboxes, watermarks, tables (OS) &gt; Meta released facebook/v-jepa-2-6841bad8413014e185b497a6 , new sota video embeddings with two new classification models (OS) &gt; ByteDance-Seed/SeedVR2-3B is a new 3B video restoration model (OS) Audio &gt; Stepfun released stepfun-ai/Step-Audio-AQAA , new large (137B ü§Ø) audio language model that takes in audio and generates audio (OS) ü§ñ Robotics &gt; nvidia released nvidia/GR00T-N1.5-3B , new open foundation vision language action model 3D &gt; tencent/Hunyuan3D-2.1 is the new version of Hunyuan by Tencent that can generate 3D assets from text and image prompts See translation</description><pubDate>Fri, 20 Jun 2025 13:34:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/828394748215710</guid></item><item><title>We got a visitor to the office today!</title><link>https://huggingface.co/posts/clem/645090569186989</link><description>We got a visitor to the office today! pollen-robotics , lerobot , unitreerobotics meetings! See translation</description><pubDate>Fri, 20 Jun 2025 13:34:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/645090569186989</guid></item><item><title>The demo for the MonkeyOCR Recognition model, which adopts a Structure-Recognition-Relation (SRR) triplet paradigm &amp; Nanonets-OCR-s a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction and other experimental document OCR models, is combined into a single space.</title><link>https://huggingface.co/posts/prithivMLmods/854747069091698</link><description>The demo for the MonkeyOCR Recognition model, which adopts a Structure-Recognition-Relation (SRR) triplet paradigm &amp; Nanonets-OCR-s a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction and other experimental document OCR models, is combined into a single space. ‚ú¶ Try the demo here : prithivMLmods/core-OCR ‚ú¶ Try Nanonets-OCR-s demo here : prithivMLmods/Multimodal-OCR ‚§∑ MonkeyOCR Recognition : echo840/MonkeyOCR ‚§∑ docscopeOCR-7B-050425-exp : prithivMLmods/docscopeOCR-7B-050425-exp ‚§∑ coreOCR-7B-050325-preview : prithivMLmods/coreOCR-7B-050325-preview ‚§∑ Nanonets-OCR-s : nanonets/Nanonets-OCR-s ‚§∑ Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Also, include a sample OCR test using the VisionOCR-3B-061125 model and the Qwen2-VL-OCR-2B-Instruct model. ‚§∑ Blog : https://huggingface.co/blog/prithivMLmods/visionocr-3b-061125-vs-qwen2-vl-ocr-2b-instruct To know more about it, visit the model card...</description><pubDate>Fri, 20 Jun 2025 13:34:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/854747069091698</guid></item><item><title>Self-Forcing - a real-time video distilled model from Wan 2.1 by</title><link>https://huggingface.co/posts/multimodalart/420236527922092</link><description>Self-Forcing - a real-time video distilled model from Wan 2.1 by @ adobe is out, and they open sourced it üêê I've built a live real time demo on Spaces üìπüí® multimodalart/self-forcing See translation</description><pubDate>Fri, 20 Jun 2025 13:34:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/multimodalart/420236527922092</guid></item><item><title>üéØ Open GAMMA - AI PPT Generator 'GamJa'</title><link>https://huggingface.co/posts/openfree/667885440717862</link><description>üéØ Open GAMMA - AI PPT Generator 'GamJa' üöÄ Project Introduction Revolutionary AI presentation generator presented by OpenFree AI Community! Create professional-level PPTs with just a few clicks. üÜì Completely FREE! Create Premium PPTs with Free GAMMA! üéâ DEMO: openfree/Open-GAMMA ‚ú® Key Features ü§ñ Powered by FACTS Grounding Leaderboard 2nd RANK LLM Base Model: vidraft/gemma-3-R1984-27B Perfect support for English/Korean/Multi-language Automatic speaker notes generation üé® Premium Visuals 3D style AI image generation 5 design themes (Professional, Modern, Nature, Creative, Minimal) FLUX style diagram images Automatic emoji bullet points üìä Smart Diagrams Process Flow, Concept Map, WBS, Radial, Synoptic Chart Content analysis-based automatic diagram generation Perfect Korean font support üí° Main Features üìù Intelligent Content Generation Auto-generate 3-20 slides just by entering a topic Latest information through web search Reference PDF, CSV, TXT files üñºÔ∏è Visual Automation 3D images for...</description><pubDate>Fri, 20 Jun 2025 13:34:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/667885440717862</guid></item><item><title>DeepThink Plugin: Bringing Gemini 2.5's Parallel Reasoning to Open Models</title><link>https://huggingface.co/posts/codelion/412568329805459</link><description>DeepThink Plugin: Bringing Gemini 2.5's Parallel Reasoning to Open Models Just released an open-source plugin that implements Google's "Deep Think" reasoning approach for models like DeepSeek R1, Qwen3, and other open models. Google's recent Gemini 2.5 report introduced Deep Think - a technique where models generate multiple hypotheses in parallel and critique them before arriving at final answers. It achieves SOTA results on math olympiads and competitive coding benchmarks. Our implementation works by modifying the inference pipeline to explore multiple solution paths simultaneously, then synthesizing the best approach. Instead of single-pass generation, models run an internal debate before responding. Key features: - Works with any model that supports structured reasoning patterns - Implements parallel thinking during response generation - Particularly effective for complex reasoning tasks, math, and coding problems - Increases inference time but significantly improves answer...</description><pubDate>Fri, 20 Jun 2025 13:34:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/412568329805459</guid></item><item><title>not sure of what to make of this but solving autonomous/selective reflection seems like a big deal in current agent frameworks. We did hit on this with iterative self-refinement in our AutoAgents framework (</title><link>https://huggingface.co/posts/Jaward/804892295096878</link><description>not sure of what to make of this but solving autonomous/selective reflection seems like a big deal in current agent frameworks. We did hit on this with iterative self-refinement in our AutoAgents framework ( https://ijcai.org/proceedings/2024/0003.pdf ). Nice read, looking forward to the code. Paper: Scaling Test-time Compute for LLM Agents (2506.12928) See translation</description><pubDate>Fri, 20 Jun 2025 13:34:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/804892295096878</guid></item><item><title># Reinforcement Learning societal impact: A Deep Dive</title><link>https://huggingface.co/posts/ghostai1/378992241286931</link><description># Reinforcement Learning societal impact: A Deep Dive Artificial Intelligence, or AI, is revolutionizing the way we live, work, and interact with our environment. With advancements in Reinforcement Learning (RL), machines are becoming increasingly intelligent and capable of making decisions autonomously. This shift is having a significant impact on society as we know it. One of the most notable aspects of RL is its ability to learn from experience. By observing and interacting with its surroundings, an AI-driven RL system can adapt to new situations and make decisions based on real-world data. This has huge implications for industries like healthcare, where AI can be used to analyze patient data and provide personalized treatment plans, or finance, where it can help predict market trends and make more informed investment decisions. Furthermore, RL is driving innovation in robotics and automation. Autonomous vehicles, for example, rely on RL to navigate complex environments safely...</description><pubDate>Fri, 20 Jun 2025 13:34:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ghostai1/378992241286931</guid></item><item><title>üó£Ô∏è Whose voice do we hear when AI speaks?</title><link>https://huggingface.co/posts/giadap/887797954746272</link><description>üó£Ô∏è Whose voice do we hear when AI speaks? Every language carries its own cultural values and worldviews. So, when we build AI systems, we're not just deciding how they speak but also whose perspectives they represent. Even choosing which dialect to train on in Norway becomes a question of inclusion and power. In Kenya, will AI speak Swahili from Nairobi or coastal regions? What about indigenous languages with rich oral traditions but limited written text, like Quechua in Peru or Cherokee in North America? The path forward? Building WITH communities, not just FOR them. Working with local partners (libraries, universities, civil society), testing for cultural alignment, and asking hard questions about representation. Just published some thoughts on this after my keynote in Norway a few weeks ago: https://huggingface.co/blog/giadap/when-ai-speaks See translation</description><pubDate>Fri, 20 Jun 2025 13:34:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/887797954746272</guid></item></channel></rss>