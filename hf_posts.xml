<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>I wanted to share a technique that's been working really well for recovering performance after INT4 quantization.</title><link>https://huggingface.co/posts/codelion/741062673202173</link><description>I wanted to share a technique that's been working really well for recovering performance after INT4 quantization. Typically, quantizing the LLM to INT4 (unlike say INT8) for inference can incur some accuracy loss. Instead of accepting the quality loss, we used the FP16 model as a teacher to train a tiny LoRA adapter (rank=16) for the quantized model. The cool part: the model generates its own training data using the Magpie technique so no external datasets needed. This is critical because we want to remain as much as possible in the distribution of the model's natural responses. Last year Apple's foundational models paper ( https://arxiv.org/pdf/2407.21075 ) had proposed a similar technique and found "By using accuracy-recovery LoRA adapters with only rank 16, Alpaca win rate can be improved by 7-18%, GMS8K accuracy is boosted by 5-10%." (page 47). We saw similar results on Qwen3-0.6B: Perplexity: 2.40 â†’ 2.09 (only 5.7% degradation from FP16 baseline) Memory: Only 0.28GB vs 1.0GB...</description><pubDate>Sat, 30 Aug 2025 05:20:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/741062673202173</guid></item><item><title>I'm currently looking into what makes a scientific paper more popular than others on a platform like Hugging Face. I conducted a huge array of tests, content length, time based information even semantic feature extraction to get to some sort of answer around...</title><link>https://huggingface.co/posts/takarajordan/290509757896054</link><description>I'm currently looking into what makes a scientific paper more popular than others on a platform like Hugging Face. I conducted a huge array of tests, content length, time based information even semantic feature extraction to get to some sort of answer around... What actually drives popularity of these papers, why do some papers get zero upvotes and why do some get thousands? The answer is absolutely nothing. Yes that's right. Nothing about the actual paper itself drives popularity, the paper's popularity is driven by external factors like it's authors, external marketing and others. So next time you see a research paper with a lot of upvotes, just remember it's not because of the efforts of the authors. Remain objective. See translation</description><pubDate>Sat, 30 Aug 2025 05:20:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/takarajordan/290509757896054</guid></item><item><title>ğŸŒ Nano Banana + Video: AI Image Style Transfer &amp; Video Generation Tool</title><link>https://huggingface.co/posts/ginipick/948503650396274</link><description>ğŸŒ Nano Banana + Video: AI Image Style Transfer &amp; Video Generation Tool ğŸ¨ Key Features 1ï¸âƒ£ Image Style Transfer ginigen/Nano-Banana-Video ğŸ“¸ Upload up to 2 images for style fusion âœ¨ High-quality image generation with Google Nano Banana model ğŸ­ Apply desired styles with text prompts 2ï¸âƒ£ Video Generation ğŸ¬ Convert generated images to videos ğŸ“ Maintain original aspect ratio option â±ï¸ Adjustable duration (1-4 seconds) ğŸš€ How to Use Step-by-Step Guide Step 1: Image Generation ğŸ–¼ï¸ Enter style description Upload 1-2 images (optional) Click "Generate Magic âœ¨" Step 2: Video Creation ğŸ“¹ Send generated image to video tab Set animation style Generate video! ğŸ’¡ Use Cases ğŸï¸ Transform landscape photos into artistic masterpieces ğŸ¤– Bring static images to life ğŸ¨ Mix styles from two different images ğŸ“± Create short videos for social media âš¡ Tech Stack Google Nano Banana Stable Video Diffusion Gradio Replicate API #AIVideoGenerator #ImageToVideoConverter #StyleTransferAI #GoogleNanoBanana...</description><pubDate>Sat, 30 Aug 2025 05:20:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/948503650396274</guid></item><item><title>Pair a vision grounding model with a reasoning LLM with Cua</title><link>https://huggingface.co/posts/dhruv3006/852628837357270</link><description>Pair a vision grounding model with a reasoning LLM with Cua Cua just shipped v0.4 of the Cua Agent framework with Composite Agents - you can now pair a vision/grounding model with a reasoning LLM using a simple modelA+modelB syntax. Best clicks + best plans. The problem: every GUI model speaks a different dialect. â€¢ some want pixel coordinates â€¢ others want percentages â€¢ a few spit out cursed tokens like &lt;|loc095|&gt; We built a universal interface that works the same across Anthropic, OpenAI, Hugging Face, etc.: agent = ComputerAgent( model="anthropic/claude-3-5-sonnet-20241022", tools=[computer] ) But hereâ€™s the fun part: you can combine models by specialization. Grounding model (sees + clicks) + Planning model (reasons + decides) â†’ agent = ComputerAgent( model="huggingface-local/HelloKKMe/GTA1-7B+openai/gpt-4o", tools=[computer] ) This gives GUI skills to models that were never built for computer use. One handles the eyes/hands, the other the brain. Think driver + navigator working...</description><pubDate>Sat, 30 Aug 2025 05:20:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/852628837357270</guid></item><item><title>ğŸŒ Nano Banana: Google AI Completely Free!</title><link>https://huggingface.co/posts/openfree/636576339128278</link><description>ğŸŒ Nano Banana: Google AI Completely Free! ğŸ‰ Finally, Google's Nano Banana AI is available for everyone - absolutely FREE! ğŸ¯ Choose Your Perfect Version! ğŸŒŸ Free Nano Banana - For Everyone Transform images with AI - It's that simple! ğŸš€ Start in 3 Seconds 1ï¸âƒ£ Click Here 2ï¸âƒ£ Upload Image 3ï¸âƒ£ Enter Style â†’ Done! âœ¨ No Sign-up âŒ | No Payment âŒ | No Ads âŒ | Just Free â­• ğŸ“¸ Simple drag &amp; drop upload âœï¸ Describe styles in any language âš¡ Results in under 30 seconds ğŸ¨ Perfect for SNS, blogs, presentations ğŸ‘‰ Start Now: openfree/Free-Nano-Banana ğŸ” Nano Banana Upscale - For Designers Professional high-resolution output when you need it! ğŸ–¼ï¸ 4x resolution upscaling (Real-ESRGAN) ğŸ¯ Optimized for print &amp; large displays ğŸ’ Premium quality with preserved details ğŸ“ Professional quality without Photoshop ğŸ‘‰ Create in HD: openfree/Nano-Banana-Upscale ğŸ’» Nano Banana API - For Developers Power your app with AI! ğŸ”§ Instant RESTful API integration ğŸ“¦ Python, JS, Java code examples included âš™ï¸ Batch processing &amp;...</description><pubDate>Sat, 30 Aug 2025 05:20:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/636576339128278</guid></item><item><title>Cosmos Reason just topped Physical Reasoning Leaderboard on Hugging Face. ğŸ‘ğŸ”¥</title><link>https://huggingface.co/posts/tsungyi/147340620272288</link><description>Cosmos Reason just topped Physical Reasoning Leaderboard on Hugging Face. ğŸ‘ğŸ”¥ Cosmos Reason is an open, customizable, commercial-ready 7B-parameter, reasoning vision language model (VLM) for physical AI and robotics. The VLM empowers robots and vision AI agents to reason like humans, leveraging prior knowledge, physics understanding, and common sense to understand and operate intelligently in the real world. This model unlocks advanced capabilities for robotics, autonomous vehicles, and real-world operationsâ€”from cities to high-tech factories. Key use cases include: Data curation &amp; annotation: Automate high-quality dataset curation and annotation at scale. Robot planning &amp; reasoning: Serve as the "brain" for deliberate, methodical decision-making with vision language action (VLA) models. Video analytics AI agents: Extract actionable insights and perform root-cause analysis on massive video datasets. Ready to build the next generation of physical AI? Get started ğŸ‘‰ nvidia/Cosmos-...</description><pubDate>Sat, 30 Aug 2025 05:20:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tsungyi/147340620272288</guid></item><item><title>Nano Banana (Gemini 2.5 Flash Image) Full Tutorial â€” 27 Unique Cases vs Qwen Image Edit â€” Free 2 Use :</title><link>https://huggingface.co/posts/MonsterMMORPG/626834107736478</link><description>Nano Banana (Gemini 2.5 Flash Image) Full Tutorial â€” 27 Unique Cases vs Qwen Image Edit â€” Free 2 Use : https://youtu.be/qPUreQxB8zQ Tutorial link : https://youtu.be/qPUreQxB8zQ Nano Banana AI image editing model was published by Google today. It is officially named the Google Gemini 2.5 Flash Image model. It is the most advanced zero-shot image editing model ever made. I have conducted a thorough, in-depth review of this model with 27 unique cases. All prompts, images used, and results are demonstrated in real-timeâ€”live in this tutorial. Moreover, I have compared each result with the state-of-the-art (SOTA) best open-source, locally available, and free-to-use Qwen Image Edit model, so we can see which model performs better at which tasks. Video Chapters 0:00 Introduction to Google's "Nano Banana" (Gemini 2.5 Flash) 0:28 Comparing Gemini vs. Qwen Image Edit Model (27 Test Cases) 1:33 Solving Gemini's Low Resolution with SUPIR Upscaling 2:28 Teaser: Upcoming Qwen Image LoRA Training...</description><pubDate>Sat, 30 Aug 2025 05:20:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/626834107736478</guid></item><item><title>MiniCPM-V 4.5 ğŸš€ New MLLM for image, multi-image &amp; video understanding, running even on your phone, released by OpenBMB</title><link>https://huggingface.co/posts/AdinaY/473496396970919</link><description>MiniCPM-V 4.5 ğŸš€ New MLLM for image, multi-image &amp; video understanding, running even on your phone, released by OpenBMB openbmb/MiniCPM-V-4_5 âœ¨ SOTA vision language capability âœ¨ 96Ã— video token compression &gt; high-FPS &amp; long video reasoning âœ¨ Switchable fast vs deep thinking modes âœ¨ Strong OCR, document parsing, supports 30+ languages See translation</description><pubDate>Sat, 30 Aug 2025 05:20:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/473496396970919</guid></item><item><title>Introducing</title><link>https://huggingface.co/posts/prithivMLmods/604588784783928</link><description>Introducing prithivMLmods/DeepCaption-VLA-7B , a multimodal VLM designed for reasoning with long-shot captions (Captioning and Vision-Language Attribution). It focuses on defining visual properties, object attributes, and scene details across a wide spectrum of images and aspect ratios, generating attribute-rich image captions. The model supports creative, artistic, and technical applications that require detailed descriptions. ğŸ¤—ğŸ”¥ âœ¦ï¸ Models: prithivMLmods/DeepCaption-VLA-7B , also includes prithivMLmods/DeepAttriCap-VLA-3B , an experimental model for vision-language attribution. âœ¦ï¸ Try the demo here: prithivMLmods/VisionScope-R2 âœ¦ï¸ Try it now on Google Colab, with support for T4 GPUs in 4-bit quant_type: https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-Notebooks/blob/main/DeepCaption-VLA-7B%5B4bit%20-%20notebook%20demo%5D/DeepCaption-VLA-7B.ipynb âœ¦ï¸ Collection: prithivMLmods/deepcaption-attr-68b041172ebcb867e45c556a . . . To know more about it, visit the model card of the...</description><pubDate>Sat, 30 Aug 2025 05:20:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/604588784783928</guid></item><item><title>These are the current best Generative 3D</title><link>https://huggingface.co/posts/dylanebert/577211626996199</link><description>These are the current best Generative 3D Render: #1 - CSM #2 - TRELLIS (open-source) #3 - Zaohaowu3D Topology: #1 - Hunyuan3D-2 #2 - TRELLIS (open-source) #3 - Hunyuan3D-2.1 as voted/submitted openly on dylanebert/3d-arena See translation</description><pubDate>Sat, 30 Aug 2025 05:20:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dylanebert/577211626996199</guid></item></channel></rss>