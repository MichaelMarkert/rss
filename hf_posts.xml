<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Is 100% Pass Rate on HumanEval possible? Yes! ✅</title><link>https://huggingface.co/posts/YerbaPage/727846915147423</link><description>Is 100% Pass Rate on HumanEval possible? Yes! ✅ Meet MGDebugger if you are tired of LLMs failing on complex bugs 🤔 Our MGDebugger, just hit 100% accuracy on HumanEval using the DeepSeek-R1 model. 🚀 ✨ Demo: learnmlf/MGDebugger 📝 Paper: From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging (2410.01215) 💻 Code: https://github.com/YerbaPage/MGDebugger HumanEval may be retired, we're ready for the next challenge In more complex scenarios! You may also take look at this repo for a collection of awesome repo-level coding tasks! 🖥️ https://github.com/YerbaPage/Awesome-Repo-Level-Code-Generation See translation</description><pubDate>Mon, 07 Jul 2025 05:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/727846915147423</guid></item><item><title>13 Outstanding MCP Servers</title><link>https://huggingface.co/posts/Kseniase/251572743842280</link><description>13 Outstanding MCP Servers MCP is redefining how AI assistants connect to the world of data and tools, so no wonder MCP servers are in high demand now. That’s why we’ve curated 13 cool MCP servers to upgrade your workflow: 1. Hugging Face Official MCP Server -&gt; https://github.com/evalstate/hf-mcp-server Provides an access and interaction with Hugging Face models, datasets, and Gradio Spaces for dynamic tool integration and configuration across environments. 2. Browser MCP -&gt; https://browsermcp.io/ An MCP server +Chrome extension. It allows to automate your browser with AI apps like VS Code, Claude, Cursor, and Windsurf. 3. Bright Data MCP -&gt; https://github.com/brightdata/brightdata-mcp This one is for working with data in real-time: searching the web, navigating websites, taking action and retrieving data. 4. JSON MCP -&gt; https://github.com/VadimNastoyashchy/json-mcp Interact with JSON files: split, merge, find specific data, and validate content within them. 5. Octagon Deep Research...</description><pubDate>Mon, 07 Jul 2025 05:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/251572743842280</guid></item><item><title>Hey all</title><link>https://huggingface.co/posts/zamal/249688504470467</link><description>Hey all Finally it's happening. DeepGit lite is back now, running on cpu only devices. Just smartly search across Github and spin up conversational agents in the background and have grounded conversation with repositories Try it out now!!!! zamal/DeepGit See translation</description><pubDate>Mon, 07 Jul 2025 05:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/zamal/249688504470467</guid></item><item><title>🧠 MathX-5M by XenArcAI — Scalable Math Reasoning for Smarter LLMs</title><link>https://huggingface.co/posts/Parveshiiii/373499845630863</link><description>🧠 MathX-5M by XenArcAI — Scalable Math Reasoning for Smarter LLMs Introducing MathX-5M, a high-quality, instruction-tuned dataset built to supercharge mathematical reasoning in large language models. With 5 million rigorously filtered examples, it spans everything from basic arithmetic to advanced calculus—curated from public sources and enhanced with synthetic data. 🔍 Key Highlights: - Step-by-step reasoning with verified answers - Covers algebra, geometry, calculus, logic, and more - RL-validated correctness and multi-stage filtering - Ideal for fine-tuning, benchmarking, and educational AI 📂 - XenArcAI/MathX-5M See translation</description><pubDate>Mon, 07 Jul 2025 05:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Parveshiiii/373499845630863</guid></item><item><title>🚀 We just released the WASM Agent Blueprint!</title><link>https://huggingface.co/posts/stefan-french/381121888136998</link><description>🚀 We just released the WASM Agent Blueprint! It shows how to run Python-based AI agents directly in your browser using WebAssembly (WASM) via Pyodide and the OpenAI Agents SDK. There are no installs, it runs straight in your browser. Try it out and explore the code 👉 https://github.com/mozilla-ai/wasm-agents-blueprint See translation</description><pubDate>Mon, 07 Jul 2025 05:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/stefan-french/381121888136998</guid></item><item><title>I'd like to share my fine-tuning notebook collection for anyone interested in adapting the</title><link>https://huggingface.co/posts/atasoglu/551144063116472</link><description>I'd like to share my fine-tuning notebook collection for anyone interested in adapting the ytu-ce-cosmos/Turkish-LLaVA-v0.1 model to their specific domain. I hope you find it helpful! You can find it in this repository: https://github.com/atasoglu/turkish-llava-notebooks?tab=readme-ov-file Happy fine-tuning! 🤗 See translation</description><pubDate>Mon, 07 Jul 2025 05:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/atasoglu/551144063116472</guid></item><item><title>Layer-wise and Pruned versions of cognitivecomputations/Dolphin-Mistral-24B-Venice-Edition</title><link>https://huggingface.co/posts/eaddario/837008676792926</link><description>Layer-wise and Pruned versions of cognitivecomputations/Dolphin-Mistral-24B-Venice-Edition * Tesor-wise: eaddario/Dolphin-Mistral-24B-Venice-Edition-GGUF * Pruned: eaddario/Dolphin-Mistral-24B-Venice-Edition-pruned-GGUF Summary in the model's card and test results in the ./scores directory. Questions/feedback is always welcomed. See translation</description><pubDate>Mon, 07 Jul 2025 05:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/eaddario/837008676792926</guid></item><item><title>Multimodal OCR with ReportLab? On Colab T4? (Nanonets OCR, Monkey OCR, OCRFlux 3B, Typhoo OCR 3B?) .. Yeah, it’s possible. I’ve made a dedicated Colab notebook to experiment with these models (all built on top of Qwen2.5 VL). 🤗🚀</title><link>https://huggingface.co/posts/prithivMLmods/606950317422768</link><description>Multimodal OCR with ReportLab? On Colab T4? (Nanonets OCR, Monkey OCR, OCRFlux 3B, Typhoo OCR 3B?) .. Yeah, it’s possible. I’ve made a dedicated Colab notebook to experiment with these models (all built on top of Qwen2.5 VL). 🤗🚀 Download notebooks here : ✦︎ NanonetsOCR : https://colab.research.google.com/drive/1VvA-amvSVxGdWgIsh4_by6KWOtEs_Iqp ✦︎ MonkeyOCR : https://colab.research.google.com/drive/1vPCojbmlXjDFUt06FJ1tjgnj_zWK4mUo ✦︎ OCRFluxOCR : https://colab.research.google.com/drive/1TDoCXzWdF2hxVLbISqW6DjXAzOyI7pzf ✦︎ TyphoonOCR : https://colab.research.google.com/drive/1_59zvLNnn1kvbiSFxzA1WiqhpbW8RKbz 🜲 Github : https://github.com/PRITHIVSAKTHIUR/OCR-ReportLab What does it do? 1. Performs OCR on the input image 2. Generates a DOCX or PDF file with the input image and the extracted text . . . To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Mon, 07 Jul 2025 05:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/606950317422768</guid></item><item><title>🧠👁️ Can AI visualize solutions?</title><link>https://huggingface.co/posts/andito/535116877809522</link><description>🧠👁️ Can AI visualize solutions? Humans often solve visual problems by sketching ideas in our minds. What if Vision-Language Models (VLMs) could do something similar, not by generating full images, but by using internal “mental sketches”? That’s the idea behind Mirage, a new framework that empowers VLMs to reason using latent visual tokens. Instead of just thinking in words, Mirage mixes in abstract visual representations that help the model solve complex tasks. These aren't photorealistic images. They're compact, internal representations optimized purely to support reasoning. 🔧 Mirage is trained in two phases: 1) Grounding: It learns to produce latent tokens anchored in real images. 2) Refinement: The model drops the images and learns to generate visual tokens on its own. 📈 And yes, it works! On challenging benchmarks like Visual Spatial Planning, Jigsaw puzzles, and Spatial Attention Tasks, Mirage clearly outperforms GPT-4o and other strong baselines. Smart sketches &gt; empty words....</description><pubDate>Mon, 07 Jul 2025 05:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/andito/535116877809522</guid></item><item><title>Updated my HF Space for vibe testing smol VLMs on object detection, visual grounding, keypoint detection &amp; counting! 👓</title><link>https://huggingface.co/posts/sergiopaniego/776509639548053</link><description>Updated my HF Space for vibe testing smol VLMs on object detection, visual grounding, keypoint detection &amp; counting! 👓 🆕 Compare Qwen2.5 VL 3B vs Moondream 2B side-by-side with annotated images &amp; text outputs. Try examples or test your own images! 🏃 📱Space: sergiopaniego/vlm_object_understanding See translation</description><pubDate>Mon, 07 Jul 2025 05:27:26 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/776509639548053</guid></item></channel></rss>