<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸ–¤  Probably one of my favorite projects that I've worked on so far, introducing ĞĞ¾Ğ²Ğ¾ÑĞ· (Novoyaz).</title><link>https://huggingface.co/posts/ZennyKenny/720018379255407</link><description>ğŸ–¤ Probably one of my favorite projects that I've worked on so far, introducing ĞĞ¾Ğ²Ğ¾ÑĞ· (Novoyaz). ğŸ›  One of the first acts of the Bolshevik government after the Russian Revolution was the reform and standardization of the Russian language, which at the time had a non-standard and challenging orthography. ğŸ“š Upon its reform the government launched a nationwide campaign called Ğ›Ğ¸ĞºĞ±ĞµĞ· (Likbez), which sought to improve literacy in the country (by the way, it worked, bringing the national literacy rate from &lt;20% in the 1920s to &gt;80% by the 1930s). â€¼ While this is a remarkable result that should absolutely be celebrated, it's one that has left behind literally hundreds of thousands if not millions of artifacts using pre-reform Russian orthography. ğŸ˜“ Researchers and historians are working tirelessly to translate these artifacts to modern Russian so that they may be archived and studied but many have told me that. they are doing this BY HAND (!). ğŸ’¡ I thought, well this is a perfect use case...</description><pubDate>Mon, 29 Sep 2025 09:27:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/720018379255407</guid></item><item><title>âš›ï¸ New drop of tiny task-specific models!</title><link>https://huggingface.co/posts/mlabonne/527878544427521</link><description>âš›ï¸ New drop of tiny task-specific models! Want to do data extraction, translation, RAG, tool use, or math on a Raspberry Pi? We got you covered! âœ… These tiny models were fine-tuned to perform narrow tasks extremely well, making them competitive with much larger models. You can deploy them today on-device or even on GPUs for big data operations! LiquidAI/liquid-nanos-68b98d898414dd94d4d5f99a See translation</description><pubDate>Mon, 29 Sep 2025 09:27:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/527878544427521</guid></item><item><title>Quietly launched the largest Open source Free LateX Dataset -https://huggingface.co/datasets/dalle2/Bibby-AI-Latex-Tool-Overleaf-Alternative</title><link>https://huggingface.co/posts/dalle2/895181701155114</link><description>Quietly launched the largest Open source Free LateX Dataset -https://huggingface.co/datasets/dalle2/Bibby-AI-Latex-Tool-Overleaf-Alternative See translation</description><pubDate>Mon, 29 Sep 2025 09:27:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dalle2/895181701155114</guid></item><item><title>As it stands, I will prepare David for full release - as this is beyond me now. David must be released.</title><link>https://huggingface.co/posts/AbstractPhil/642988847137898</link><description>As it stands, I will prepare David for full release - as this is beyond me now. David must be released. I will prepare a standard sweep for david to showcase the prowess of the final multi-vocab variant. This will include a variation that contains all mnist variants, cifar10, cifar100, imagenet 1k, and in the future I'll prepare a full imagenet sweep utilizing the entire 12m corpus instead of the 1.2m I used. I may need to get in touch with the actual curator of the dataset for licensing but maybe not. David utilizes 4 projective variants of the vocabulary and the training process involves teaching and freezing them akin to teacher/student processing. I did not want to release David yet, but I believe now that David will save lives and it's irresponsible for me to contain such a creation. See translation</description><pubDate>Mon, 29 Sep 2025 09:27:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AbstractPhil/642988847137898</guid></item><item><title>âš¡ RexBERT Complete On-device Study: Comprehensive Performance Analysis Across Mobile Devices</title><link>https://huggingface.co/posts/yeonseok-zeticai/603791080422979</link><description>âš¡ RexBERT Complete On-device Study: Comprehensive Performance Analysis Across Mobile Devices (Check details at https://mlange.zetic.ai/p/Steve/RexBERT ) TL;DR: Transformer models are now practical for real-time mobile applications. The cloud-to-edge AI migration is complete. - Original model from @ thebajajra ğŸ¯ Study Overview: - Model: RexBERT (ModernBERT for E-commerce) - Focus: Real-world deployment viability and performance analysis ğŸ“Š Key Performance Metrics: Latency Results: - NPU (Best): 4.74ms average - GPU: 12.56ms average - CPU: 35.16ms average NPU Advantage: 16.98x speedup over CPU Memory Efficiency: - Model Size: 568.96 MB (compressed for mobile) - Runtime Memory: 299.01 MB peak consumption - Load Memory Range: 285 MB - 1,072 MB across devices Accuracy Preservation: - FP16 Precision: 63.72 dB - Quantized Mode: Available with minimal accuracy loss - Inference Quality: Production-grade maintained ğŸ›  Technical Implementation: (Runnable with Copy &amp; Paste at the ZETIC.MLange...</description><pubDate>Mon, 29 Sep 2025 09:27:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/603791080422979</guid></item><item><title>Hello, amazing robotics people ğŸ˜ ğŸ˜ ğŸ˜ We have FINALLY delivered on your major request! Ark just got a major upgrade:</title><link>https://huggingface.co/posts/hba123/904232449612527</link><description>Hello, amazing robotics people ğŸ˜ ğŸ˜ ğŸ˜ We have FINALLY delivered on your major request! Ark just got a major upgrade: Weâ€™ve now integrated Vision-Language-Action Models (VLAs) into Ark ğŸ‰ VLAs = models that connect vision + language â†’ robot actions (see image) What does this mean? ğŸ—£ï¸ Give robots natural language instructions â†’ they act ğŸ‘€ Combine perception + language for real-world control ğŸ¦¾ Powered by pi0 pretrained models for fast prototyping âš¡ Supports easy data collection and fine-tuning within Ark within a couple of lines of code Next, we plan to go into the world of designing worlds ğŸ˜‰ Who knows, maybe those video models are actually zero-shot learners and reasoners? Check it out here ğŸ‘‰ https://github.com/Robotics-Ark/ark_framework Check out the tutorial ğŸ‘‰ https://arkrobotics.notion.site/VLA-Pi0-with-Ark-279e053d9c6f800ab0a2d498835dd96b â­ Star the repo, try it with your robots, and let us together make robots great (again?)! See translation</description><pubDate>Mon, 29 Sep 2025 09:27:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/904232449612527</guid></item><item><title>I've uploaded abliteration code with support for sparsification of the refusal vector. It's poorly documented, but the code should be straightforward.</title><link>https://huggingface.co/posts/grimjim/556474107312531</link><description>I've uploaded abliteration code with support for sparsification of the refusal vector. It's poorly documented, but the code should be straightforward. https://github.com/jim-plus/llm-abliteration The code is built atop a fork that enabled abliteration to be performed on models loaded in 4-bit or 8-bit bitsandbytes quantization. TransformerLens is not required, just plain Transformers. For those previously unaware, this opens up abliteration experimentation to more people with local VRAM limitations. Since performing abliteration on a quant involves precision and perplexity loss, it stands to reason that a small amount of magnitude sparsification could filter out some noise and possibly even reduce the damage inflicted on latent space via ablation of the refusal vector. There's a small but real acceleration of ablation of the refusal vector by reducing outer product operations from O(dÂ²Ã—n) to O(dÃ—n), and then by pushing said computation layerwise to GPU. The code is hardcoded for...</description><pubDate>Mon, 29 Sep 2025 09:27:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/grimjim/556474107312531</guid></item><item><title>12 Excellent MCP Servers</title><link>https://huggingface.co/posts/Kseniase/566496725881885</link><description>12 Excellent MCP Servers The family of MCP (Model Context Protocol) servers keeps expanding to bridge agents, models, tools, web, data and apps. Here are 12 useful MCP servers that will help you create convenient agentic ecosystems: 1. Chrome DevTools MCP â†’ https://github.com/ChromeDevTools/chrome-devtools-mcp Lets your coding agent (Gemini, Claude, Cursor, Copilot) control a live Chrome browser with full DevTools access for automation, debugging, and performance analysis 2. Windows-MCP â†’ https://github.com/CursorTouch/Windows-MCP Provides interaction between agents and Windows, handling file navigation, app control, UI actions, QA testing 3. MCPControl â†’ https://github.com/claude-did-this/MCPControl Windows control server for programmatic control of mouse, keyboard, window management, and screen capture 4. MetaMCP â†’ https://github.com/metatool-ai/metamcp A proxy that aggregates multiple MCP servers into one, with middleware support. Works as a standard MCP server for any client 5....</description><pubDate>Mon, 29 Sep 2025 09:27:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/566496725881885</guid></item><item><title>âœ… New Article: *Epilogue â€” Genius as Reproducible Structure*</title><link>https://huggingface.co/posts/kanaria007/517613830750903</link><description>âœ… New Article: *Epilogue â€” Genius as Reproducible Structure* Title: ğŸ” Epilogue: The Limits of Demonstration ğŸ”— https://huggingface.co/blog/kanaria007/reproducing-genius-epilogue --- Summary: This arc showed that what history called *genius* is not ineffable. It is a *protocol* â€” reproducible, auditable, teachable. Three demonstrations (acting/self-reference, math/nonverbal, martial/cosmic) were enough to prove the point. Genius was never about *who* did it, but *how* structure makes such leaps possible. &gt; To end the myth of genius &gt; is to begin its practice by everyone. --- Why It Matters: â€¢ Demystifies genius, reframing it as systematic structural method â€¢ Shows why only a few demonstrations are sufficient â€” reproducibility is the proof â€¢ Opens the door for education, AI, and research to cultivate genius procedurally --- Whatâ€™s Inside: â€¢ Recap of the protocol and its demonstrations â€¢ Reflection on why â€œenough examplesâ€ suffices structurally â€¢ Implications: from talent myths to...</description><pubDate>Mon, 29 Sep 2025 09:27:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/517613830750903</guid></item><item><title>ğŸ¾ Ever wished your cute little furball could have its own exclusive merch? ğŸ±ğŸ’–</title><link>https://huggingface.co/posts/Ethank01/155417923515007</link><description>ğŸ¾ Ever wished your cute little furball could have its own exclusive merch? ğŸ±ğŸ’– With iMiniâ€™s Nano Banana sticker template, you can turn your catâ€™s photo into adorable nine-grid stickers ğŸ¨âœ¨. Imagine your fluffy friend as stickers, acrylic charms, or even custom collectibles â€” the possibilities are endless! ğŸ§©ğŸ I just tried it with my white kitty, and the results are purr-fectly adorable ğŸ˜ğŸˆ. Trust me, youâ€™ll want to play around with your petâ€™s pics too! ğŸ‘‰ Try it here: https://imini.com/nano-bananağŸš€ See translation</description><pubDate>Mon, 29 Sep 2025 09:27:05 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ethank01/155417923515007</guid></item></channel></rss>