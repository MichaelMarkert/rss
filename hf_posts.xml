<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>today is going to be a great day for small models, are you ready?</title><link>https://huggingface.co/posts/appvoid/680866662633308</link><description>today is going to be a great day for small models, are you ready? See translation</description><pubDate>Tue, 21 Oct 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/appvoid/680866662633308</guid></item><item><title>5 Lectures and keynotes defining AI right now</title><link>https://huggingface.co/posts/Kseniase/152348317273822</link><description>5 Lectures and keynotes defining AI right now If you want to understand the multifaceted AI landscape in 2025 and see where the field is heading ‚Äì start with (or revisit) these legendary talks. They can help you capture what‚Äôs happening in AI from multiple angles: 1. Andrej Karpathy: Software Is Changing (Again) ‚Üí https://www.youtube.com/watch?v=LCEmiRjPEtQ Unveils Software 3.0 ‚Äì a paradigm where LLMs are the new computers, programmed with prompts instead of code. The key: developers must now master coding, training, and prompting as AI becomes the heart of software building 2. Richard Sutton, The OaK Architecture: A Vision of SuperIntelligence from Experience ‚Üí https://www.youtube.com/watch?v=gEbbGyNkR2U Unveils the OaK (Options and Knowledge) architecture ‚Äì a model-based RL framework for continual intelligence, where every component learns, meta-learns &amp; builds hierarchical abstractions 3. GTC March 2025 Keynote with NVIDIA CEO Jensen Huang ‚Üí...</description><pubDate>Tue, 21 Oct 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/152348317273822</guid></item><item><title>deepseek-ai/DeepSeek-OCR</title><link>https://huggingface.co/posts/merve/349000009530858</link><description>deepseek-ai/DeepSeek-OCR is out! üî• my take ‚§µÔ∏è &gt; pretty insane it can parse and re-render charts in HTML &gt; it uses CLIP and SAM features concatenated, so better grounding &gt; very efficient per vision tokens/performance ratio &gt; covers 100 languages See translation</description><pubDate>Tue, 21 Oct 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/349000009530858</guid></item><item><title>Introducing AWQ and GPTQ quantized versions of SmolVLM from Hugging Face!</title><link>https://huggingface.co/posts/ronantakizawa/523357349957866</link><description>Introducing AWQ and GPTQ quantized versions of SmolVLM from Hugging Face! These models only had their text models quantized, and had a 50% model size reduction (4GB~2GB) while keeping model degradation under 1% on the DocVQA benchmark. #huggingface #smolvlm #smollm ronantakizawa/SmolVLM-Instruct-awq ronantakizawa/SmolVLM-Instruct-gptq See translation</description><pubDate>Tue, 21 Oct 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/523357349957866</guid></item><item><title>I'm excited to announce the release of Kanon 2 Embedder, the world's best legal embedding model, ranked first on the Massive Legal Embedding Benchmark üéâ</title><link>https://huggingface.co/posts/umarbutler/611899757397292</link><description>I'm excited to announce the release of Kanon 2 Embedder, the world's best legal embedding model, ranked first on the Massive Legal Embedding Benchmark üéâ This model is the product of quite literally months of painstaking work alongside @ abdurrahmanbutler collecting, cleaning, and processing terabytes of data as well as coming up with novel improvements to the standard embedder training recipe to push the limits of what's possible. Kanon 2 Embedder is my most advanced model to date. On MLEB, it benchmarks as 9% more accurate than OpenAI's best embedding model and 30% faster. Even when truncated from 1,792 to 768 dimensions, Kanon 2 Embedder continues to hold the number one spot on MLEB. Importantly, Kanon 2 Embedder is also privacy and security friendly ‚Äî unlike Voyage, Cohere and Jina, none of your data is used to train our models by default. Kanon 2 Embedder can also be self-hosted for enterprises with heightened security or reliability requirements. You can read the full...</description><pubDate>Tue, 21 Oct 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/umarbutler/611899757397292</guid></item><item><title>üß† Introducing Ellora Recipe #6: Execution-Aware World Model for Qwen3-4B-Thinking</title><link>https://huggingface.co/posts/codelion/654123549897898</link><description>üß† Introducing Ellora Recipe #6: Execution-Aware World Model for Qwen3-4B-Thinking Teaching LLMs to understand not just what code does, but HOW it executes at runtime! Inspired by Meta's CWM (Code World Model) research, this LoRA adapter adds execution awareness to Qwen3-4B-Thinking-2507. The model learns to predict variable states, trace program execution step-by-step, and debug code by understanding runtime behavior. üîç Key Innovation: We combine Qwen3's native thinking capabilities with real Python execution traces captured via sys.settrace(). The model is trained using GRPO with a custom reward function that scores execution prediction accuracy. üìä Training Approach: - Hybrid Magpie-style code generation - Real execution tracing for ground truth - Self-supervised learning (no manual annotations!) - 298 training samples with execution traces ‚ú® What it does: - Predicts variable states at each line of code - Explains execution flow with thinking tags - Helps debug by understanding...</description><pubDate>Tue, 21 Oct 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/654123549897898</guid></item><item><title>ü§ñ New article: In-depth testing of the Feetech STS3215 servomotor ‚Äî the popular choice for open-source robotic projects like SO-ARM 100 and others. We've conducted comprehensive performance analysis covering backlash, repeatability, torque characteristics, and thermal behavior. Discover how this affordable servo performs in real-world robotic applications, including its 12-bit magnetic encoder, metal gearbox design, and overload protection mechanisms.</title><link>https://huggingface.co/posts/branikita/979883770066556</link><description>https://robonine.com/testing-of-feetech-sts3215-servomotor-backlash-repeatability-and-torque/ ü§ñ New article: In-depth testing of the Feetech STS3215 servomotor ‚Äî the popular choice for open-source robotic projects like SO-ARM 100 and others. We've conducted comprehensive performance analysis covering backlash, repeatability, torque characteristics, and thermal behavior. Discover how this affordable servo performs in real-world robotic applications, including its 12-bit magnetic encoder, metal gearbox design, and overload protection mechanisms. #Robotics #Servomotor #EngineeringTesting #Automation #FeetechSTS3215 #OpenSourceRobotics #SOArm #RoboticSystems #ControlSystems #MotorTesting #AffordableRobotics #ClosedLoopControl See translation</description><pubDate>Tue, 21 Oct 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/979883770066556</guid></item><item><title>I benchmarked embedding APIs for speed, compared local vs hosted models, and tuned USearch for sub-millisecond retrieval on 143k chunks using only CPU. The post walks through the results, trade-offs, and what I learned about embedding API terms of service.</title><link>https://huggingface.co/posts/adlumal/408694709656049</link><description>I benchmarked embedding APIs for speed, compared local vs hosted models, and tuned USearch for sub-millisecond retrieval on 143k chunks using only CPU. The post walks through the results, trade-offs, and what I learned about embedding API terms of service. The main motivation for using USearch is that CPU compute is cheap and easy to scale. Blog post: https://huggingface.co/blog/adlumal/lightning-fast-vector-search-for-legal-documents See translation</description><pubDate>Tue, 21 Oct 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/adlumal/408694709656049</guid></item><item><title>üöÄ Introducing the xLLMs Dataset Collection</title><link>https://huggingface.co/posts/lamhieu/873520082917207</link><description>üöÄ Introducing the xLLMs Dataset Collection The xLLMs project is a growing suite of multilingual and multimodal dialogue datasets designed to train and evaluate advanced conversational LLMs. Each dataset focuses on a specific capability ‚Äî from long-context reasoning and factual grounding to STEM explanations, math Q&amp;A, and polite multilingual interaction. üåç Explore the full collection on Hugging Face: üëâ lamhieu/xllms-66cdfe34307bb2edc8c6df7d üí¨ Highlight: xLLMs ‚Äì Dialogue Pubs A large-scale multilingual dataset built from document-guided synthetic dialogues (Wikipedia, WikiHow, and technical sources). It‚Äôs ideal for training models on long-context reasoning, multi-turn coherence, and tool-augmented dialogue across 9 languages. üëâ lamhieu/xllms_dialogue_pubs üß† Designed for: - Long-context and reasoning models - Multilingual assistants - Tool-calling and structured response learning All datasets are open for research and development use ‚Äî free, transparent, and carefully curated to...</description><pubDate>Tue, 21 Oct 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lamhieu/873520082917207</guid></item><item><title>Christmas came early this year</title><link>https://huggingface.co/posts/csabakecskemeti/277521964775652</link><description>Christmas came early this year See translation</description><pubDate>Tue, 21 Oct 2025 09:26:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/csabakecskemeti/277521964775652</guid></item></channel></rss>