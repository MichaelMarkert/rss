<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸ¯ RetinaFace On-Device Deployment Study: NPU Acceleration Breakthrough!</title><link>https://huggingface.co/posts/yeonseok-zeticai/752870941871415</link><description>ğŸ¯ RetinaFace On-Device Deployment Study: NPU Acceleration Breakthrough! (Check details at :https://mlange.zetic.ai/p/Steve/RetinaFace) TL;DR: Successfully deployed RetinaFace with ZETIC.MLange achieving 1.43ms inference on mobile NPU! ğŸ” Complete Performance Analysis: Latency Comparison: - NPU: 1.43ms (Winner! ğŸ†) - GPU: 3.75ms - CPU: 21.42ms Accuracy Metrics - SNR: - FP16: 56.98 dB - Integer Quantized: 48.03 dB (Precision-Performance: Excellent trade-off maintained) Memory Footprint: - Model Size: 2.00 MB (highly compressed) - Runtime Memory: 14.58 MB peak - Deployment Ready: âœ… Production optimized ğŸ›  Technical Implementation: (Runnable with Copy &amp; Paste at the MLange link!) ğŸ“Š Device Compatibility Matrix: Tested on 50+ devices including Samsung Galaxy series, Google Pixel lineup, and Xiaomi devices, iPhones and iPads. Consistent sub-5ms performance across the board! ğŸš€ Applications Unlocked: - Real-time AR/VR face tracking - Privacy-preserving edge authentication - Live video...</description><pubDate>Fri, 26 Sep 2025 17:19:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/752870941871415</guid></item><item><title>ğŸ’¥ Tons of new material just landed in the smol-course! ğŸ§‘â€ğŸ’»</title><link>https://huggingface.co/posts/sergiopaniego/566597314485869</link><description>ğŸ’¥ Tons of new material just landed in the smol-course! ğŸ§‘â€ğŸ’» &gt; evaluation &gt; alignment &gt; VLMs &gt; quizzes &gt; assignments! &gt; certificates!ğŸ‘©â€ğŸ“ go learn! ğŸ‘‰ https://huggingface.co/learn/smol-course/unit0/1 See translation</description><pubDate>Fri, 26 Sep 2025 17:19:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/566597314485869</guid></item><item><title>AI Just Made My Cat the King of Emojis ğŸ‘‘ğŸ±ğŸ˜‚</title><link>https://huggingface.co/posts/Monica997/874620000877286</link><description>AI Just Made My Cat the King of Emojis ğŸ‘‘ğŸ±ğŸ˜‚ Never thought Iâ€™d see this â€” but with iMiniâ€™s nano banana model, my cat is now a full emoji + sticker pack ğŸ¨âœ¨ Used the 9-grid meme template + cartoon sticker generator, and in just ONE click ğŸ‘‰ my ordinary cat photo turned into a hilarious, cute, and super shareable set of stickers ğŸ’¬ğŸ”¥ No need to master complicated nano banana prompts â€” iMini handles everything. Perfect for chats, socials, or just showing off your petâ€™s new â€œdigital identity.â€ ğŸ‘‰ Try it here: https://imini.com/nano-banana Who else wants their pet to be the next emoji star? ğŸŒŸ See translation</description><pubDate>Fri, 26 Sep 2025 17:19:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Monica997/874620000877286</guid></item><item><title>Weâ€™re excited to share that Cosmos Reason has surpassed 1 million downloads on Hugging Face!</title><link>https://huggingface.co/posts/tsungyi/865184416328763</link><description>Weâ€™re excited to share that Cosmos Reason has surpassed 1 million downloads on Hugging Face! Cosmos Reason is an open, customizable, commercial-ready 7B-parameter reasoning vision language model (VLM) designed for physical AI. By combining physics understanding, prior knowledge, and common sense reasoning, Cosmos Reason empowers AI agents and robots to operate intelligently in real-world environments. Key applications already unlocked include: âœ… Automating large-scale dataset curation and annotation ğŸ¤– Powering robot planning and vision-language action (VLA) decision-making ğŸ“Š Driving advanced video analytics and actionable insight generation Weâ€™re proud to see a global community of developers using Cosmos Reason to teach robots to think like humansâ€”and weâ€™re just getting started. âš¡ Get started with Cosmos Reason 1 NIM, an easy-to-use microservice for AI model deployment: https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/cosmos-reason1-7b?version=1 ğŸ“ˆ See the leaderboard:...</description><pubDate>Fri, 26 Sep 2025 17:19:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tsungyi/865184416328763</guid></item><item><title>Hello, amazing robotics people ğŸ˜ ğŸ˜ ğŸ˜ We have FINALLY delivered on your major request! Ark just got a major upgrade:</title><link>https://huggingface.co/posts/hba123/904232449612527</link><description>Hello, amazing robotics people ğŸ˜ ğŸ˜ ğŸ˜ We have FINALLY delivered on your major request! Ark just got a major upgrade: Weâ€™ve now integrated Vision-Language-Action Models (VLAs) into Ark ğŸ‰ VLAs = models that connect vision + language â†’ robot actions (see image) What does this mean? ğŸ—£ï¸ Give robots natural language instructions â†’ they act ğŸ‘€ Combine perception + language for real-world control ğŸ¦¾ Powered by pi0 pretrained models for fast prototyping âš¡ Supports easy data collection and fine-tuning within Ark within a couple of lines of code Next, we plan to go into the world of designing worlds ğŸ˜‰ Who knows, maybe those video models are actually zero-shot learners and reasoners? Check it out here ğŸ‘‰ https://github.com/Robotics-Ark/ark_framework Check out the tutorial ğŸ‘‰ https://arkrobotics.notion.site/VLA-Pi0-with-Ark-279e053d9c6f800ab0a2d498835dd96b â­ Star the repo, try it with your robots, and let us together make robots great (again?)! See translation</description><pubDate>Fri, 26 Sep 2025 17:19:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/904232449612527</guid></item><item><title>ShareGPT? How about ShareGPT-X?</title><link>https://huggingface.co/posts/AnSungJae3489/522189375080147</link><description>ShareGPT? How about ShareGPT-X? We release **92K** Human with LLM conversations as a refresh and update over the original ShareGPT Dataset. DSULT-Core/ShareGPT-X See translation</description><pubDate>Fri, 26 Sep 2025 17:19:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AnSungJae3489/522189375080147</guid></item><item><title>ğŸŒ Our Summer of Workflows finale! New release Embedding Generation for Videos.</title><link>https://huggingface.co/posts/dmoxy/581267174720703</link><description>ğŸŒ Our Summer of Workflows finale! New release Embedding Generation for Videos. With our latest ApertureDB AI workflow, you can now generate embeddings for video frames &amp; clips and store them directly in a multimodal databaseâ€”ready for semantic search, RAG, or agentic use cases. ğŸ¬ See It In Action: https://youtu.be/X2ZXE0EEAkk ğŸ” Example use cases: Natural language search across video libraries Highlight reel creation &amp; scene retrieval Safer content moderation Lecture indexing + video-to-text alignment Multimodal RAG (text + images + video) âœ¨ Check out the demo notebook to see how you can: Import videos from S3 Generate embeddings per frame/clip Query videos with natural language (â€œshow me a babyâ€) ğŸ‘‰ Try it here: https://shorturl.at/jSNtu Turn your video library into a fully searchable knowledge base. No scrubbingâ€”just instant, semantic video results. Weâ€™d love your feedback and ideas on how you would use it. See translation</description><pubDate>Fri, 26 Sep 2025 17:19:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dmoxy/581267174720703</guid></item><item><title>We're joining the</title><link>https://huggingface.co/posts/salma-remyx/828425996651513</link><description>We're joining the @ ag2 team in discord to present a deep-dive into how we've used the framework to build GitRank in their Community Talks The GitRank pipeline is used to: ğŸ“° power personalized paper recommendations ğŸ³ build environments as Docker Images ğŸ¯ implement core-methods as PRs for your target repo Don't miss it! Tomorrow, Sept 25 at 9:00 am PST: https://calendar.app.google/3soCpuHupRr96UaF8 See translation</description><pubDate>Fri, 26 Sep 2025 17:19:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/828425996651513</guid></item><item><title>**Wikipedia Monthly's September edition is now live ğŸ‰**</title><link>https://huggingface.co/posts/omarkamali/930832276115275</link><description>**Wikipedia Monthly's September edition is now live ğŸ‰** Highlights of this edition: Â· ğŸ—£ï¸ 341 languages Â· ğŸ“š 63.1M articles Â· ğŸ“¦ 86.5GB of data This update also solves upload issues in the August edition where some languages had missing parts. Happy data engineering! omarkamali/wikipedia-monthly See translation</description><pubDate>Fri, 26 Sep 2025 17:19:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/omarkamali/930832276115275</guid></item><item><title>I'm sorry, what?</title><link>https://huggingface.co/posts/nroggendorff/916862110503909</link><description>I'm sorry, what?</description><pubDate>Fri, 26 Sep 2025 17:19:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/916862110503909</guid></item></channel></rss>