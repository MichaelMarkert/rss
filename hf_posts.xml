<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Is it possible to apply for a resources grant for a whole organization, or do you need to apply for each repo individually? I think it'd be pretty cool to have something like the discord-community org for None-yet in terms of resource allocation (multiple spaces running on</title><link>https://huggingface.co/posts/nroggendorff/692994583869726</link><description>Is it possible to apply for a resources grant for a whole organization, or do you need to apply for each repo individually? I think it'd be pretty cool to have something like the discord-community org for None-yet in terms of resource allocation (multiple spaces running on cpu upgrade . I realize the scale of the community is just a tiny bit different, and that having this for a public org (one where anyone can join) isn't super fiscally responsible, but we'll be good. I promise we will! Right, guys? See translation</description><pubDate>Mon, 28 Jul 2025 13:42:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nroggendorff/692994583869726</guid></item><item><title>Say hello to</title><link>https://huggingface.co/posts/Wauplin/921235032674409</link><description>Say hello to hf : a faster, friendlier Hugging Face CLI ‚ú® We are glad to announce a long-awaited quality-of-life improvement: the Hugging Face CLI has been officially renamed from huggingface-cli to hf! So... why this change? Typing huggingface-cli constantly gets old fast. More importantly, the CLI‚Äôs command structure became messy as new features were added over time (upload, download, cache management, repo management, etc.). Renaming the CLI is a chance to reorganize commands into a clearer, more consistent format. We decided not to reinvent the wheel and instead follow a well-known CLI pattern: hf &lt;resource&gt; &lt;action&gt;. Isn't hf auth login easier to type and remember? The full rationale, implementation details, and migration notes are in the blog post: https://huggingface.co/blog/hf-cli See translation</description><pubDate>Mon, 28 Jul 2025 13:42:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Wauplin/921235032674409</guid></item><item><title>9 new policy optimization techniques</title><link>https://huggingface.co/posts/Kseniase/659752309280422</link><description>9 new policy optimization techniques Reinforcement Learning (RL) won't stuck in the same old PPO loop - in the last two months alone, researchers have introduced a new wave of techniques, reshaping how we train and fine-tune LLMs, VLMs, and agents. Here are 9 fresh policy optimization techniques worth knowing: 1. GSPO: Group Sequence Policy Optimization ‚Üí Group Sequence Policy Optimization (2507.18071) Shifts from token-level to sequence-level optimization, clipping, and rewarding to capture the full picture and increase stability compared to GRPO. GSPO-token variation also allows token-level fine-tuning. 2. LAPO: Length-Adaptive Policy Optimization ‚Üí LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization (2507.15758) A two-stage RL framework that trains models to adaptively control reasoning length by learning typical solution lengths for shorter and more efficient reasoning. 3. HBPO: Hierarchical Budget Policy Optimization ‚Üí Hierarchical Budget Policy...</description><pubDate>Mon, 28 Jul 2025 13:42:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/659752309280422</guid></item><item><title>Implemented Test-Time Diffusion Deep Researcher (TTD-DR) in OptiLLM! üöÄ</title><link>https://huggingface.co/posts/codelion/260744865592486</link><description>Implemented Test-Time Diffusion Deep Researcher (TTD-DR) in OptiLLM! üöÄ Just shipped a game-changing feature that turns any LLM into a powerful research agent. TTD-DR applies diffusion-inspired techniques to iteratively refine research reports while grounding them in real web sources. How it works: ‚Ä¢ Generates initial draft ‚Ä¢ Identifies knowledge gaps ‚Ä¢ Searches web for missing info ‚Ä¢ Iteratively refines through "denoising" steps ‚Ä¢ Produces comprehensive reports with 15-30+ sources The magic? It works with ANY model so you can choose your favorite open-source models on HF! Key results: - 47 complex research queries tested - Every report backed by real web sources - Quality rivals human research analysts - No more hallucinations on current events! Try it: pip install optillm Then use "deep_research-your-model-name" as the model identifier - Implementation: https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research - Paper: https://arxiv.org/abs/2507.16075v1 - Sample...</description><pubDate>Mon, 28 Jul 2025 13:42:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/260744865592486</guid></item><item><title>The development of SnowflakeCore-G1-7B-MoE. I can't say when it would be publish yet because it's big and it requires a lot of computational power.</title><link>https://huggingface.co/posts/FlameF0X/804120995868939</link><description>The development of SnowflakeCore-G1-7B-MoE. I can't say when it would be publish yet because it's big and it requires a lot of computational power. See translation</description><pubDate>Mon, 28 Jul 2025 13:42:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/FlameF0X/804120995868939</guid></item><item><title>üöÄ Deca 3 Ultra Alpha is coming in the next 72 hours! üöÄ</title><link>https://huggingface.co/posts/Blazgo/275361717193984</link><description>üöÄ Deca 3 Ultra Alpha is coming in the next 72 hours! üöÄ We're on the verge of something monumental. Right now, we're in the final stages of testing, and we're about to drop a game-changing milestone in the open-source AI community. üéâ In just two weeks, we've managed to almost 4x the size of the largest open-source LLM at that time (and we are still 2.6x bigger than the largest LLM). This is unprecedented and a testament to the power of collaboration, innovation, and the relentless pursuit of pushing AI to its limits. The future of open-source AI is now. Stay tuned for the release ‚Äì we‚Äôre just getting started. - Model testing finishes: 24hrs from now - Model gets uploaded: 30hrs from now - Related code/inference stack gets published: 70-90hrs from now See translation</description><pubDate>Mon, 28 Jul 2025 13:42:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Blazgo/275361717193984</guid></item><item><title>Excited to introduce the new experimental model "Qwen2.5-VL-7B-Abliterated-Caption-it", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.üß™ü§ó</title><link>https://huggingface.co/posts/prithivMLmods/432897219160306</link><description>Excited to introduce the new experimental model "Qwen2.5-VL-7B-Abliterated-Caption-it", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.üß™ü§ó ‚ú® Try the demo here : prithivMLmods/Qwen2.5-VL ‚ú® Qwen2.5-VL-7B-Abliterated-Caption-it : prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it ‚ú® Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 ‚ú® Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Mon, 28 Jul 2025 13:42:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/432897219160306</guid></item><item><title>Explore OCR, Captioning, and Visual Understanding with Cutting-Edge Models on Hugging Face. ü§óüß™</title><link>https://huggingface.co/posts/prithivMLmods/591329511468798</link><description>Explore OCR, Captioning, and Visual Understanding with Cutting-Edge Models on Hugging Face. ü§óüß™ I‚Äôve put together a collection of Google Colab notebooks to experiment with some of the most exciting models available on the Hugging Face Hub focused on OCR, image captioning, and visual understanding tasks. [Image-to-Text] / [Image-Text-to-Text] &gt; üìñ OCR-ReportLab-Notebooks : prithivMLmods/OCR-ReportLab-Notebooks These notebooks are built for quick prototyping and run on free T4 GPUs, making them perfect for experimentation, testing ideas, or just exploring what‚Äôs possible with modern vision-language models. Note: The experimental notebooks are compiled with models that fit within the T4 GPU (free-tier) limits. More models along with their notebooks will be added over time. See translation</description><pubDate>Mon, 28 Jul 2025 13:42:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/591329511468798</guid></item><item><title>üì¢ For those who planning to start a PhD or research in the UK üá¨üáß (including AI field in particular) but facing ATAS (Academic Technology Approval Scheme) issues.</title><link>https://huggingface.co/posts/nicolay-r/579222924328271</link><description>üì¢ For those who planning to start a PhD or research in the UK üá¨üáß (including AI field in particular) but facing ATAS (Academic Technology Approval Scheme) issues. Excited to share the ultimate guide for dealing with ATAS refusals and how to write effective rebuttal letters. üé¨ https://youtu.be/bfknM3n-SHs üîç From the video you will find: 1. Why appealing an ATAS decision matters even if your visa is approved 2. Which docments to use in understanding the principles behind sponsorship decisions 3. Key tips for proper rebuttal letter structuring See translation</description><pubDate>Mon, 28 Jul 2025 13:42:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nicolay-r/579222924328271</guid></item><item><title>ü§Ø 241B VLM with apache-2.0 license</title><link>https://huggingface.co/posts/merve/838684266814336</link><description>ü§Ø 241B VLM with apache-2.0 license internlm/Intern-S1 internlm released Intern-S1: multimodal reasoning model based on 235B MoE Qwen3 and 6B InternViT üòç benchmarks look great (üëë best model ‚úÖ best open model) See translation</description><pubDate>Mon, 28 Jul 2025 13:42:46 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/838684266814336</guid></item></channel></rss>