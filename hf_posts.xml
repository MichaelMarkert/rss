<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>üåà‚ú® FLUX 'Every Text Imaginator'</title><link>https://huggingface.co/posts/ginipick/804980229974999</link><description>üåà‚ú® FLUX 'Every Text Imaginator' Multilingual Text-Driven Image Generation and Editing Demo: ginigen/Every-Text üìù What is FLUX Text Imaginator? FLUX Text Imaginator is an innovative tool that leverages cutting-edge FLUX diffusion models to create and edit images with perfectly integrated multilingual text. Unlike other image generation models, FLUX possesses exceptional capability to naturally incorporate text in various languages including Korean, English, Chinese, Japanese, Russian, French, Spanish and more into images! ‚ú® FLUX's Multilingual Text Processing Strengths üî§ Superior Multilingual Text Rendering: FLUX renders text with amazing accuracy, including non-English languages and special characters üá∞üá∑ Perfect Korean Language Support: Accurately represents complex Korean combined characters üà∂ Excellent East Asian Language Handling: Naturally expresses complex Chinese characters and Japanese text üîç Sophisticated Text Placement: Precise text positioning using &lt;text1&gt;, &lt;text2&gt;,...</description><pubDate>Fri, 21 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/804980229974999</guid></item><item><title>Should we assemble affordable open-source robots at Hugging Face for the community. Would you buy them? At what price?</title><link>https://huggingface.co/posts/clem/968928866217294</link><description>Should we assemble affordable open-source robots at Hugging Face for the community. Would you buy them? At what price? See translation</description><pubDate>Fri, 21 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/968928866217294</guid></item><item><title>We open-sourced the</title><link>https://huggingface.co/posts/sharpenb/393986344403251</link><description>We open-sourced the pruna package that can be easily installed with pip install pruna :) It allows to easily ccompress and evaluate AI models including transformers and diffusers. - Github repo: https://github.com/PrunaAI/pruna - Documentation: https://docs.pruna.ai/en/stable/index.html With open-sourcing, people can now inspect and contribute to the open code. Beyond the code, we provide detailed readme, tutorials, benchmarks, and documentation to make transparent compression, evaluation, and saving/loading/serving of AI models. Happy to share it with you and always interested in collecting your feedback :) See translation</description><pubDate>Fri, 21 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sharpenb/393986344403251</guid></item><item><title>‚úÇÔ∏è AutoAbliteration</title><link>https://huggingface.co/posts/mlabonne/714992455492422</link><description>‚úÇÔ∏è AutoAbliteration I made a Colab notebook to automatically abliterate models. It's quite general, so you can do interesting stuff like blocking a given language in the model outputs. üíª Colab: https://colab.research.google.com/drive/1RmLv-pCMBBsQGXQIM8yF-OdCNyoylUR1?usp=sharing See translation</description><pubDate>Fri, 21 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/714992455492422</guid></item><item><title>Nice new space to see how fast your personal or organization followers are growing on HF:</title><link>https://huggingface.co/posts/clem/654976954929507</link><description>Nice new space to see how fast your personal or organization followers are growing on HF: julien-c/follow-history As you can see, I still have more followers than @ julien-c even if he's trying to change this by building such cool spaces üòùüòùüòù See translation</description><pubDate>Fri, 21 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/654976954929507</guid></item><item><title>ü§ñ üó∫Mapped all(?) the swimming pools Ô∏èüèä around another town with</title><link>https://huggingface.co/posts/daavoo/876890642000188</link><description>ü§ñ üó∫Mapped all(?) the swimming pools Ô∏èüèä around another town with https://github.com/mozilla-ai/osm-ai-helper . This time, I have mapped and contributed to https://www.openstreetmap.org more than 100 swimming pools around my wife's hometown. Only took about 20min to find them all (+~3 min verification) in a free Colab GPUüöÄ Try it yourself around a single point: mozilla-ai/osm-ai-helper See translation</description><pubDate>Fri, 21 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/daavoo/876890642000188</guid></item><item><title>Mistral AI Small 3.1 24B is not only commercial free but also the best model in a single GPU deployment.</title><link>https://huggingface.co/posts/chansung/317347704788964</link><description>Mistral AI Small 3.1 24B is not only commercial free but also the best model in a single GPU deployment. I packed up all the information you need to know in a single picture. Hope this helps! :) See translation</description><pubDate>Fri, 21 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/chansung/317347704788964</guid></item><item><title>Started fine tuning Gemma 3 using evolutionary approach. It is not the worst model according to AHA leaderboard and it is one of the smart according to lmarena.ai. My objective is to make it based, anti woke, wise, beneficial and then some.</title><link>https://huggingface.co/posts/etemiz/560030136297214</link><description>Started fine tuning Gemma 3 using evolutionary approach. It is not the worst model according to AHA leaderboard and it is one of the smart according to lmarena.ai. My objective is to make it based, anti woke, wise, beneficial and then some. Several GPUs are fine tuning it at the same time, each using a different dataset and using QLoRA and the successful ones are merged later. Compared to LoRa this allows faster training and also reduced overfitting because the merge operation heals overfitting. The problem with this could be the 4 bit quantization may make models dumber. But I am not looking for sheer IQ. Too much mind is a problem anyway :) Has anyone tried parallel QLoRa and merge before? I also automated the dataset selection and benchmarking and converging to objectives (the fit function, the reward). It is basically trying to get higher score in AHA Leaderboard as fast as possible with a diverse set of organisms that "evolve by training". I want to release some cool stuff when...</description><pubDate>Fri, 21 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/560030136297214</guid></item><item><title>We introduce FAT5 (Flash Attention T5) ‚ö°</title><link>https://huggingface.co/posts/lbourdois/751427866406946</link><description>We introduce FAT5 (Flash Attention T5) ‚ö° An implementation of T5 in PyTorch with UL2 objective optimized for GPGPU for both training and inference thanks to 13 different optimizations. The main one is that we have designed a CUDA kernel to expand the Flash Attention by @ tridao with RPE biases and supports other PE such as RoPE, ALiBi or FIRE. The result kernel is 2 times faster than a SPDA implementation. We also use Triton kernels to optimize certain parts of the architecture, such as the cross-entropy and RMSNorm layer. The various kernels have been carefully built to be compatible with BF16 and torch.compile to go even faster and achieve efficient pretraining. All other optimizations are described in a üìù subsequent blog post available on @ huggingface ü§ó: CATIE-AQ/FAT5-report . This methodology enabled us to efficiently pretrain as a proof of concept a FAT5 with 147M parameters in French in a reasonable time (1,461H for 419B tokens), with limited resources (1 A100 i.e. a...</description><pubDate>Fri, 21 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lbourdois/751427866406946</guid></item><item><title>The Hugging Face Agents Course now includes three major agent frameworks!</title><link>https://huggingface.co/posts/burtenshaw/105046709529701</link><description>The Hugging Face Agents Course now includes three major agent frameworks! üîó https://huggingface.co/agents-course This includes LlamaIndex, LangChain, and our very own smolagents. We've worked to integrate the three frameworks in distinctive ways so that learners can reflect on when and where to use each. This also means that you can follow the course if you're already familiar with one of these frameworks, and soak up some of the fundamental knowledge in earlier units. Hopefully, this makes the agents course as open to as many people as possible. See translation</description><pubDate>Fri, 21 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/105046709529701</guid></item></channel></rss>