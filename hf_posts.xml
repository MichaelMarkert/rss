<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>It's been a bit since I took a step back and looked at</title><link>https://huggingface.co/posts/jsulz/866221600890917</link><description>It's been a bit since I took a step back and looked at xet-team progress to migrate Hugging Face from Git LFS to Xet, but every time I do it boggles the mind. A month ago there were 5,500 users/orgs on Xet with 150K repos and 4PB. Today? ü§ó 700,000 users/orgs üìà 350,000 repos üöÄ 15PB Meanwhile, our migrations have pushed throughput to numbers that are bonkers. In June, we hit upload speeds of 577Gb/s (crossing 500Gb/s for the first time). These are hard numbers to put into context, but let's try: The latest run of the Common Crawl from commoncrawl was 471 TB. We now have ~32 crawls stored in Xet. At peak upload speed we could move the latest crawl into Xet in about two hours. We're moving to a new phase in the process, so stay tuned. This shift in gears means it's also time to roll up our sleeves and look at all the bytes we have and the value we're adding to the community. I already have some homework from @ RichardErkhov to look at the dedupe across their uploads, and I'll be doing...</description><pubDate>Fri, 27 Jun 2025 17:21:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/866221600890917</guid></item><item><title>Introducing Dhanishtha 2.0: World's first Intermediate Thinking Model</title><link>https://huggingface.co/posts/Abhaykoul/997219525730173</link><description>Introducing Dhanishtha 2.0: World's first Intermediate Thinking Model Dhanishtha 2.0 is the world's first LLM designed to think between the responses. Unlike other Reasoning LLMs, which think just once. Dhanishtha can think, rethink, self-evaluate, and refine in between responses using multiple &lt;think&gt; blocks. This technique makes it Hinghlt Token efficient it Uses up to 79% fewer tokens than DeepSeek R1 --- You can try our model from: https://helpingai.co/chat Also, we're gonna Open-Source Dhanistha on July 1st. --- For Devs: üîë Get your API key at https://helpingai.co/dashboard from HelpingAI import HAI # pip install HelpingAI ==1.1.1 from rich import print hai = HAI( api_key = "hl-***********************" ) response = hai.chat.completions.create( model = "Dhanishtha-2.0-preview" , messages=[{ "role" : "user" , "content" : "What is the value of ‚à´0‚àûùë•3/ùë•‚àí1ùëëùë• ?" }], stream = True , hide_think = False # Hide or show models thinking ) for chunk in response: print...</description><pubDate>Fri, 27 Jun 2025 17:21:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Abhaykoul/997219525730173</guid></item><item><title>üî•BEST DEBUG PROMPT FOR CLAUDE_CODE</title><link>https://huggingface.co/posts/samihalawa/553684674682033</link><description>üî•BEST DEBUG PROMPT FOR CLAUDE_CODE üò≤FIXES ANY REPO How can I prompt you so there will not be any More bug or thing? So list the 20 kind of errors and bugs more common in a codebase like this one and create a comprehensive table list of each of them and all the main files and fucnionality and fix until cross all out . Don 't add complexity. Ignore security. Only fatal problems Proceed in the most exhaustive and comprehensive way possible. Don' t miss a line of code. SYSTEMATICALLY FIX EVERYTHING See translation</description><pubDate>Fri, 27 Jun 2025 17:21:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/samihalawa/553684674682033</guid></item><item><title>üöÄ Real-Time On-Device AI Agent with Polaris-4B ‚Äî Run It Yourself, No Cloud, No Cost</title><link>https://huggingface.co/posts/yeonseok-zeticai/894951796717498</link><description>üöÄ Real-Time On-Device AI Agent with Polaris-4B ‚Äî Run It Yourself, No Cloud, No Cost We just deployed a real-time on-device AI agent using the Polaris-4B-Preview model ‚Äî one of the top-performing &lt;6B open LLMs on Hugging Face. üì± What‚Äôs remarkable? This model runs entirely on a mobile device, without cloud, and without any manual optimization. It was built using ZETIC.MLange, and the best part? ‚û°Ô∏è It‚Äôs totally automated, free to use, and anyone can do it. You don‚Äôt need to write deployment code, tweak backends, or touch device-specific SDKs. Just upload your model ‚Äî and ZETIC.MLange handles the rest. üß† About the Model - Model: Polaris-4B-Preview - Size: ~4B parameters - Ranking: Top 3 on Hugging Face LLM Leaderboard (&lt;6B) - Tokenizer: Token-incremental inference supported - Modifications: None ‚Äî stock weights, just optimized for mobile ‚öôÔ∏è What ZETIC.MLange Does ZETIC.MLange is a fully automated deployment framework for On-Device AI, built for AI engineers who want to focus on models ‚Äî...</description><pubDate>Fri, 27 Jun 2025 17:21:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/894951796717498</guid></item><item><title>SnowflakeCore-G1 development update: We're building a 24-layer transformer with 32K context and 1024 embedding dimensions - pretty ambitious! Even running at batch_size=1 with heavy gradient accumulation, we're hitting memory walls at 300GB RAM. Scaling up to ~1TB will take some time, but the architecture is looking promising. Thanks for following along with the journey! üòÖ</title><link>https://huggingface.co/posts/FlameF0X/436788445658511</link><description>SnowflakeCore-G1 development update: We're building a 24-layer transformer with 32K context and 1024 embedding dimensions - pretty ambitious! Even running at batch_size=1 with heavy gradient accumulation, we're hitting memory walls at 300GB RAM. Scaling up to ~1TB will take some time, but the architecture is looking promising. Thanks for following along with the journey! üòÖ See translation</description><pubDate>Fri, 27 Jun 2025 17:21:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/FlameF0X/436788445658511</guid></item><item><title>Hi everyone,</title><link>https://huggingface.co/posts/yeonseok-zeticai/727857049396772</link><description>Hi everyone, I‚Äôve been running small language models (SLLMs) directly on smartphones ‚Äî completely offline, with no cloud backend or server API calls. I wanted to share: 1. ‚ö° Tokens/sec performance across several SLLMs 2. ü§ñ Observations on hardware utilization (where the workload actually runs) 3. üìè Trade-offs between model size, latency, and feasibility for mobile apps There are reports for below models - QWEN3 0.6B - NVIDIA/Nemotron QWEN 1.5B - SimpleScaling S1 - TinyLlama - Unsloth tuned Llama 3.2 1B - Naver HyperClova 0.5B üìúComparable Benchmark reports (no cloud, all on-device): I‚Äôd really value your thoughts on: - Creative ideas to further optimize inference under these hardware constraints - Other compact LLMs worth testing on-device - Experiences you‚Äôve had trying to deploy LLMs at the edge If there‚Äôs interest, I‚Äôm happy to share more details on the test setup, hardware specs, or the tooling we used for these comparisons. Thanks for taking a look, and you can build your own...</description><pubDate>Fri, 27 Jun 2025 17:21:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/727857049396772</guid></item><item><title>Hackathons in Paris on July 5th and 6th!</title><link>https://huggingface.co/posts/pagezyhf/610118153924016</link><description>Hackathons in Paris on July 5th and 6th! Hugging Face just wrapped 4 months of deep work with AMD to push kernel-level optimization on their MI300X GPUs. Now, it's time to share everything we learned. Join us in Paris at STATION F for a hands-on weekend of workshops and a hackathon focused on making open-source LLMs faster and more efficient on AMD. Prizes, amazing host speakers, ... if you want more details, navigate to https://lu.ma/fmvdjmur ! See translation</description><pubDate>Fri, 27 Jun 2025 17:21:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/pagezyhf/610118153924016</guid></item><item><title>Awesome intro to LLM course "Language Modeling from Scratch" by stanford. love the aesthetics behind the lecture notes, notes-in-code genius ideaüëç</title><link>https://huggingface.co/posts/Jaward/445538723467397</link><description>Awesome intro to LLM course "Language Modeling from Scratch" by stanford. love the aesthetics behind the lecture notes, notes-in-code genius ideaüëç Course site: https://stanford-cs336.github.io/spring2025/ Repo: https://github.com/stanford-cs336/spring2025-lectures Videos: https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_ See translation</description><pubDate>Fri, 27 Jun 2025 17:21:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/445538723467397</guid></item><item><title>As you might have already heard, FLUX.1-Kontext-dev is now open source and taken the generative community by storm!</title><link>https://huggingface.co/posts/a-r-r-o-w/674823997424742</link><description>As you might have already heard, FLUX.1-Kontext-dev is now open source and taken the generative community by storm! In case you haven't come across it, you can get started with Kontext using ü§ó diffusers. See the official [model]( black-forest-labs/FLUX.1-Kontext-dev ) and [docs]( https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux#flux ). Want to know how inference companies like Fal &amp; Replicate are able to run the model so fast and in under 2 seconds per image? Check out this [gist]( https://gist.github.com/a-r-r-o-w/d08c37e8bd3e9c26b4ce80360be148c6 ) for some details! See translation</description><pubDate>Fri, 27 Jun 2025 17:21:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/a-r-r-o-w/674823997424742</guid></item><item><title>Hunyuan-A13B üî• New MoE LLM by TencentHunyuan</title><link>https://huggingface.co/posts/AdinaY/115721386328243</link><description>Hunyuan-A13B üî• New MoE LLM by TencentHunyuan tencent/Hunyuan-A13B-Instruct ‚ú®80B total / 13B active params ‚ú®256K context window ‚ú®Dual-mode reasoning: fast &amp; slow thinking ‚ú®Efficient inference (GQA + quantization) See translation</description><pubDate>Fri, 27 Jun 2025 17:21:03 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/115721386328243</guid></item></channel></rss>