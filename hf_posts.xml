<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>longer context doesn't generate better responses. it can even hurt your llm/agent.  1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it.</title><link>https://huggingface.co/posts/hesamation/830297477341251</link><description>longer context doesn't generate better responses. it can even hurt your llm/agent. 1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it. here are 4 types of context failure and why each one happens: 1. context poisoning: if hallucination finds its way into your context, the agent will rely on that false information to make its future moves. for example if the agent hallucinates about the "task description", all of its planning to solve the task would also be corrupt. 2. context distraction: when the context becomes too bloated, the model focuses too much on it rather than come up with novel ideas or to follow what it has learned during training. as Gemini 2.5 Pro technical report points out, as context grows significantly from 100K tokens, "the agent showed a tendency toward favoring repeating actions from its vast history rather than synthesizing novel plans". 3. context confusion: everyone lost it when MCPs became popular, it...</description><pubDate>Fri, 25 Jul 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/830297477341251</guid></item><item><title>I run Qwen3-Coder 480B locally on my Z8, with a 1-million token context window. Itâ€™s the equivalent of parallel-parking a Nimitz-class carrier in a kiddie pool. Thanks to whatever dark pact the llama.cpp, CUDA, and kernel folks signed, hybrid inferencing + VRAMâ†”RAM offload let me stream the modelâ€™s synapses across Xeon, RAM, and four lonely A6000s without summoning either the OOM killer or a small house fire.</title><link>https://huggingface.co/posts/mitkox/940300076081193</link><description>I run Qwen3-Coder 480B locally on my Z8, with a 1-million token context window. Itâ€™s the equivalent of parallel-parking a Nimitz-class carrier in a kiddie pool. Thanks to whatever dark pact the llama.cpp, CUDA, and kernel folks signed, hybrid inferencing + VRAMâ†”RAM offload let me stream the modelâ€™s synapses across Xeon, RAM, and four lonely A6000s without summoning either the OOM killer or a small house fire. See translation</description><pubDate>Fri, 25 Jul 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/940300076081193</guid></item><item><title>olmOCR [Allen AI] just got an upgrade! ğŸ“ˆğŸ§‘â€ğŸ³</title><link>https://huggingface.co/posts/prithivMLmods/906521786731164</link><description>olmOCR [Allen AI] just got an upgrade! ğŸ“ˆğŸ§‘â€ğŸ³ The allenai/olmOCR-7B-0725 â€” fine-tuned with allenai/olmOCR-mix-0225 on top of Qwen/Qwen2.5-VL-7B-Instruct , pushing the boundaries of OCR technology. It takes a single document image as input, with the longest side resized to 1288 pixels. High-quality, openly available approach to parsing pdfs and other complex documents optical character recognition. Try the demo here: prithivMLmods/Multimodal-OCR âœ¨ Model: allenai/olmOCR-7B-0725 âœ¨ Model [fp8]: allenai/olmOCR-7B-0725-FP8 âœ¨ Multimodal Implementations Space Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Fri, 25 Jul 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/906521786731164</guid></item><item><title>Introducing Voxtral WebGPU: State-of-the-art audio transcription directly in your browser! ğŸ¤¯</title><link>https://huggingface.co/posts/Xenova/793837995432659</link><description>Introducing Voxtral WebGPU: State-of-the-art audio transcription directly in your browser! ğŸ¤¯ ğŸ—£ï¸ Transcribe videos, meeting notes, songs and more ğŸ” Runs on-device, meaning no data is sent to a server ğŸŒ Multilingual (8 languages) ğŸ¤— Completely free (forever) &amp; open source That's right, we're running Mistral's new Voxtral-Mini-3B model 100% locally in-browser on WebGPU, powered by Transformers.js and ONNX Runtime Web! ğŸ”¥ Try it out yourself! ğŸ‘‡ webml-community/Voxtral-WebGPU See translation</description><pubDate>Fri, 25 Jul 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Xenova/793837995432659</guid></item><item><title>Many VLMs claim to process hours of video. But can they follow the story?ğŸ¤”</title><link>https://huggingface.co/posts/andito/542123544707457</link><description>Many VLMs claim to process hours of video. But can they follow the story?ğŸ¤” Today, we introduce TimeScope: The benchmark that separates true temporal understanding from marketing hype. Let's see how much VLMs really understand!â³ We test three skills that matter for real-world use: ğŸ” Localized Retrieval: Find a specific action. ğŸ§© Information Synthesis: Piece together scattered clues. ğŸƒ Fine-Grained Perception: Analyze detailed motion (e.g., count how many times a person swings an axe). The results are in, and they're revealing. Only Gemini 2.5 pro handles 1-hour-long videos. Performance drops sharply with duration, proving that long video understanding is still challenging. We've found the breaking pointsâ€”now the community can start fixing them.ğŸ“ˆ Want to learn more? TimeScope is 100% open-source. Benchmark your model and help us build the next generation of video AI. ğŸ“– Blog: https://huggingface.co/blog/timescope-video-lmm-benchmark ğŸ‘©â€ğŸ’» Leaderboard &amp; Demo: Apollo-LMMs/TimeScope ğŸ“Š...</description><pubDate>Fri, 25 Jul 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/andito/542123544707457</guid></item><item><title>It's Qwen3 week! ğŸ’œ We uploaded Dynamic 2-bit GGUFs for:</title><link>https://huggingface.co/posts/danielhanchen/754522453041743</link><description>It's Qwen3 week! ğŸ’œ We uploaded Dynamic 2-bit GGUFs for: Qwen3-Coder: unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF Qwen3-2507: unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF So you can run them both locally! Guides are in model cards. See translation</description><pubDate>Fri, 25 Jul 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/754522453041743</guid></item><item><title>Say hello to</title><link>https://huggingface.co/posts/Wauplin/921235032674409</link><description>Say hello to hf : a faster, friendlier Hugging Face CLI âœ¨ We are glad to announce a long-awaited quality-of-life improvement: the Hugging Face CLI has been officially renamed from huggingface-cli to hf! So... why this change? Typing huggingface-cli constantly gets old fast. More importantly, the CLIâ€™s command structure became messy as new features were added over time (upload, download, cache management, repo management, etc.). Renaming the CLI is a chance to reorganize commands into a clearer, more consistent format. We decided not to reinvent the wheel and instead follow a well-known CLI pattern: hf &lt;resource&gt; &lt;action&gt;. Isn't hf auth login easier to type and remember? The full rationale, implementation details, and migration notes are in the blog post: https://huggingface.co/blog/hf-cli See translation</description><pubDate>Fri, 25 Jul 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Wauplin/921235032674409</guid></item><item><title>Excited to introduce the new experimental model "Qwen2.5-VL-7B-Abliterated-Caption-it", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.ğŸ§ªğŸ¤—</title><link>https://huggingface.co/posts/prithivMLmods/432897219160306</link><description>Excited to introduce the new experimental model "Qwen2.5-VL-7B-Abliterated-Caption-it", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.ğŸ§ªğŸ¤— âœ¨ Try the demo here : prithivMLmods/Qwen2.5-VL âœ¨ Qwen2.5-VL-7B-Abliterated-Caption-it : prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it âœ¨ Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 âœ¨ Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Fri, 25 Jul 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/432897219160306</guid></item><item><title>ğŸš€ AutoRound(</title><link>https://huggingface.co/posts/wenhuach/293322564041371</link><description>ğŸš€ AutoRound( https://github.com/intel/auto-round ) Now Supports GGUF Export &amp; Custom Bit Settings! We're excited to announce that AutoRound now supports: âœ… GGUF format export â€“ for seamless compatibility with popular inference engines. âœ… Custom bit settings â€“ tailor quantization to your needs for optimal performance. Check out these newly released models: ğŸ”¹Intel/Qwen3-235B-A22B-Instruct-2507-gguf-q4km-AutoRound ğŸ”¹Intel/Qwen3-235B-A22B-Instruct-2507-gguf-q2ks-mixed-AutoRound ğŸ”¹Intel/Kimi-K2-Instruct-gguf-q2ks-mixed-AutoRound Stay tuned! An even more advanced algorithm for some configurations is coming soon. See translation</description><pubDate>Fri, 25 Jul 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wenhuach/293322564041371</guid></item><item><title>Yet Another New Multimodal Fine-Tuning Recipe ğŸ¥§</title><link>https://huggingface.co/posts/sergiopaniego/657914327935230</link><description>Yet Another New Multimodal Fine-Tuning Recipe ğŸ¥§ ğŸ§‘â€ğŸ³ In this @ HuggingFace Face Cookbook notebook, we demonstrate how to align a multimodal model (VLM) using Mixed Preference Optimization (MPO) using trl. ğŸ’¡ This recipe is powered by the new MPO support in trl, enabled through a recent upgrade to the DPO trainer! We align the multimodal model using multiple optimization objectives (losses), guided by a preference dataset (chosen vs. rejected multimodal pairs). Check it out! â¡ï¸ https://huggingface.co/learn/cookbook/fine_tuning_vlm_mpo See translation</description><pubDate>Fri, 25 Jul 2025 17:24:30 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/657914327935230</guid></item></channel></rss>