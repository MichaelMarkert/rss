<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Exciting New Tool for Knowledge Graph Extraction from Plain Text!</title><link>https://huggingface.co/posts/singhsidhukuldeep/815565847250252</link><description>Exciting New Tool for Knowledge Graph Extraction from Plain Text! I just came across a groundbreaking new tool called KGGen that's solving a major challenge in the AI world - the scarcity of high-quality knowledge graph data. KGGen is an open-source Python package that leverages language models to extract knowledge graphs (KGs) from plain text. What makes it special is its innovative approach to clustering related entities, which significantly reduces sparsity in the extracted KGs. The technical approach is fascinating: 1. KGGen uses a multi-stage process involving an LLM (GPT-4o in their implementation) to extract entities and relations from source text 2. It aggregates graphs across sources to reduce redundancy 3. Most importantly, it applies iterative LM-based clustering to refine the raw graph The clustering stage is particularly innovative - it identifies which nodes and edges refer to the same underlying entities or concepts. This normalizes variations in tense, plurality,...</description><pubDate>Wed, 05 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/singhsidhukuldeep/815565847250252</guid></item><item><title>Super happy to welcome Nvidia as our latest enterprise hub customer. They have almost 2,000 team members using Hugging Face, and close to 20,000 followers of their org. Can't wait to see what they'll open-source for all of us in the coming months!</title><link>https://huggingface.co/posts/clem/866977064333227</link><description>Super happy to welcome Nvidia as our latest enterprise hub customer. They have almost 2,000 team members using Hugging Face, and close to 20,000 followers of their org. Can't wait to see what they'll open-source for all of us in the coming months! Nvidia's org: https://huggingface.co/nvidia Enterprise hub: https://huggingface.co/enterprise See translation</description><pubDate>Wed, 05 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/866977064333227</guid></item><item><title>ü•ä Epic Agent Framework Showdown! Available today!</title><link>https://huggingface.co/posts/davidberenstein1957/655300080970392</link><description>ü•ä Epic Agent Framework Showdown! Available today! üîµ In the blue corner, the versatile challenger with a proven track record of knowledge retrieval: LlamaIndex! üõë In the red corner, the defender, weighing in with lightweight efficiency: Hugging Face smolagents! üîó URL: https://huggingface.co/agents-course We just published the LlamaIndex unit for the agents course, and it is set to offer a great contrast between the smolagents unit by looking at - What makes llama-index stand-out - How the LlamaHub is used for integrations - Creating QueryEngine components - Using agents and tools - Agentic and multi-agent workflows The team has been working flat-out on this for a few weeks. Supported by Logan Markewich and Laurie Voss over at LlamaIndex. Who won? You decide! See translation</description><pubDate>Wed, 05 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/davidberenstein1957/655300080970392</guid></item><item><title>Hi there!</title><link>https://huggingface.co/posts/Undi95/824593315166092</link><description>Hi there! If you want to create your own thinking model or do a better MistralThinker, I just uploaded my entire dataset made on Deepseek R1 and the axolotl config. (well I made them public) Axolotl config : Undi95/MistralThinker-v1.1 The dataset : Undi95/R1-RP-ShareGPT3 You can also read all I did on those two discord screenshot from two days ago, I'm a little lazy to rewrite all kek. Hope you will use them! See translation</description><pubDate>Wed, 05 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Undi95/824593315166092</guid></item><item><title>Duality.ai just released a 1000 image dataset used to train a YOLOv8 model in multiclass object detection -- and it's 100% free!</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/775086617889949</link><description>Duality.ai just released a 1000 image dataset used to train a YOLOv8 model in multiclass object detection -- and it's 100% free! duality-robotics/YOLOv8-Multiclass-Object-Detection-Dataset Access the full size dataset by creating an EDU account here- https://falcon.duality.ai/secure/documentation/ex3-dataset?sidebarMode=learn Or check it out in the linked HuggingFace dataset! What makes this dataset unique, useful, and capable of bridging the Sim2Real gap? üí† The digital twins are not generated by AI, but instead crafted by 3D artists to be INDISTINGUISHABLE from the physical-world objects. This allows the training from this data to transfer into real-world applicability üí† The simulation software, called FalconEditor, can easily create thousands of images with varying lighting, posing, occlusions, backgrounds, camera positions, and more. This enables robust model training. üí† The labels are created along with the data. This not only saves large amounts of time, but also ensures the...</description><pubDate>Wed, 05 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/775086617889949</guid></item><item><title>Published a stable version of Ukrainian Text-to-Speech library on GitHub and PyPI.</title><link>https://huggingface.co/posts/Yehor/619825346186306</link><description>Published a stable version of Ukrainian Text-to-Speech library on GitHub and PyPI. Features: - Multi-speaker model: 2 female (Tetiana, Lada) + 1 male (Mykyta) voices; - Fine-grained control over speech parameters, including duration, fundamental frequency (F0), and energy; - High-fidelity speech generation using the RAD-TTS++ acoustic model; - Fast vocoding using Vocos; - Synthesizes long sentences effectively; - Supports a sampling rate of 44.1 kHz; - Tested on Linux environments and Windows/WSL; - Python API (requires Python 3.9 or later); - CUDA-enabled for GPU acceleration. Repository: https://github.com/egorsmkv/tts_uk See translation</description><pubDate>Wed, 05 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Yehor/619825346186306</guid></item><item><title>ü•≥ü•≥Just achieved 25m 59s of research with plain ChatGPT üî• Had it doing a complete internet search in just ONE call visiting 443 websites! Hard to beat huh!</title><link>https://huggingface.co/posts/luigi12345/685788730899562</link><description>ü•≥ü•≥Just achieved 25m 59s of research with plain ChatGPT üî• Had it doing a complete internet search in just ONE call visiting 443 websites! Hard to beat huh! PROMPT IN COMMENTS Check out the Massive Article created by the prompt: https://huggingface.co/blog/luigi12345/automating-lead-generation-with-ai See translation</description><pubDate>Wed, 05 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/luigi12345/685788730899562</guid></item><item><title>ü´∏ New release to push vector search to the Hub with vicinity and work with any serialisable objects.</title><link>https://huggingface.co/posts/davidberenstein1957/915880767531433</link><description>ü´∏ New release to push vector search to the Hub with vicinity and work with any serialisable objects. üßë‚Äçüè´ KNN, HNSW, USEARCH, ANNOY, PYNNDESCENT, FAISS, and VOYAGER. üîó Example Repo: minishlab/my-vicinity-repo See translation</description><pubDate>Wed, 05 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/davidberenstein1957/915880767531433</guid></item><item><title>9 types of "Chain-of-..." approaches:</title><link>https://huggingface.co/posts/Kseniase/433849056207490</link><description>9 types of "Chain-of-..." approaches: Chain-of-Thought (CoT) prompting enhances reasoning in AI models by breaking down complex problems into step-by-step logical sequences. It continues proving its effectiveness, especially in top-performing reasoning models. However, there are other similar methods, that expand CoT and can be used for different purposes. Here are 9 of them: 1. Chain-of-Action-Thought (COAT) -&gt; Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search (2502.02508) Helps model decide when to keep thinking, double-check their work, or try a different approach, using special guiding tokens. 2. Chain of Draft (CoD) -&gt; Chain of Draft: Thinking Faster by Writing Less (2502.18600) It helps model generate short but meaningful reasoning steps, cutting costs and making processing faster 3. Chain-of-Agents -&gt; Chain of Agents: Large Language Models Collaborating on Long-Context Tasks (2406.02818) Uses multi-agent...</description><pubDate>Wed, 05 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/433849056207490</guid></item><item><title>CogView-4 is outüî•üöÄ The SoTa OPEN text to image model by ZhipuAI</title><link>https://huggingface.co/posts/AdinaY/339226274130426</link><description>CogView-4 is outüî•üöÄ The SoTa OPEN text to image model by ZhipuAI Model: THUDM/CogView4-6B Demo: THUDM-HF-SPACE/CogView4 ‚ú® 6B with Apache2.0 ‚ú® Supports Chinese &amp; English Prompts by ANY length ‚ú® Generate Chinese characters within images ‚ú® Creates images at any resolution within a given range See translation</description><pubDate>Wed, 05 Mar 2025 17:19:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/339226274130426</guid></item></channel></rss>