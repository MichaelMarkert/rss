<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>10 awesome advanced LoRA approaches</title><link>https://huggingface.co/posts/Kseniase/445000542637232</link><description>10 awesome advanced LoRA approaches Low-Rank Adaptation (LoRA) is the go-to method for efficient model fine-tuning that adds small low-rank matrices instead of retraining full models. The field isn‚Äôt standing still ‚Äì new LoRA variants push the limits of efficiency, generalization, and personalization. So we‚Äôre sharing 10 of the latest LoRA approaches you should know about: 1. Mixture-of-LoRA-experts ‚Üí Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection (2509.13878) Adds multiple low-rank adapters (LoRA) into a model‚Äôs layers, and a routing mechanism activates the most suitable ones for each input. This lets the model adapt better to new unseen conditions 2. Amortized Bayesian Meta-Learning for LoRA (ABMLL) ‚Üí Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models (2508.14285) Balances global and task-specific parameters within a Bayesian framework to improve uncertainty calibration and generalization to new tasks without high...</description><pubDate>Tue, 23 Sep 2025 13:31:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/445000542637232</guid></item><item><title>large AI labs open-sourced a ton of models last week üî•</title><link>https://huggingface.co/posts/merve/604366247415617</link><description>large AI labs open-sourced a ton of models last week üî• here's few picks, find even more here merve/sep-16-releases-68d13ea4c547f02f95842f05 ü§ù &gt; IBM released a new Docling model with 258M params based on Granite (A2.0) üìù ibm-granite/granite-docling-258M &gt; Xiaomi released 7B audio LM with base and instruct variants (MIT) XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0 &gt; DecartAI released Lucy Edit, open Nano Banana üçå (NC) decart-ai/Lucy-Edit-Dev &gt; OpenGVLab released a family of agentic computer use models (3B/7B/32B) with the dataset üíª OpenGVLab/scalecua-68c912cf56f7ff4c8e034003 &gt; Meituan Longcat released thinking version of LongCat-Flash üí≠ meituan-longcat/LongCat-Flash-Thinking See translation</description><pubDate>Tue, 23 Sep 2025 13:31:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/604366247415617</guid></item><item><title>Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.ü§ó‚ú®</title><link>https://huggingface.co/posts/prithivMLmods/322831563234696</link><description>Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.ü§ó‚ú® prithivMLmods/PhotoCleanser-i2i : Remove objects while preserving the rest of the image. prithivMLmods/Photo-Restore-i2i : Restore old photos into moderately colorized, detailed images. prithivMLmods/Polaroid-Warm-i2i : Seamless vintage Polaroid-style images with warm, faded tones. prithivMLmods/Yarn-Photo-i2i : Convert images into yarn-stitched artwork while retaining key details. prithivMLmods/Monochrome-Pencil : Turn images into monochrome pencil sketches while keeping original features. ‚ú®Note: All the above models share the same auto-labeling multimodal VLM captioning model, prithivMLmods/DeepCaption-VLA-7B , which is used...</description><pubDate>Tue, 23 Sep 2025 13:31:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/322831563234696</guid></item><item><title>Discussing  iMini Nano Banana ‚Äì Practical AI Templates for Creative Workflows</title><link>https://huggingface.co/posts/Ethank01/541492789102968</link><description>Discussing iMini Nano Banana ‚Äì Practical AI Templates for Creative Workflows üéôÔ∏èI‚Äôve been experimenting with imini nano banana recently, and the new batch of 100+ AI templates feels like a real shift in how we handle creative workflows. üé®One example: the photo-to-hand-drawn sketch template. I tested it with a travel photo (a blonde girl in front of the Eiffel Tower) ‚Üí the output was a clean, consistent hand-drawn sketch in seconds. Normally, this would take manual editing in Photoshop/Illustrator, but the AI template cut the process down to a single click. üí°From a workflow perspective, this could streamline tasks for: Designers: quick iterations on style variations Marketers: generating campaign visuals at scale Content creators: producing unique artwork without advanced editing skills üñåÔ∏èIf anyone here has tested imini nano banana in professional projects, I‚Äôd like to hear how it fits into your pipeline. Full details: https://imini.com/nano-banana See translation</description><pubDate>Tue, 23 Sep 2025 13:31:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ethank01/541492789102968</guid></item><item><title>YOLOv11 Complete On-device Study</title><link>https://huggingface.co/posts/yeonseok-zeticai/506441566129403</link><description>YOLOv11 Complete On-device Study - {NPU vs GPU vs CPU} Across All Model Variants We've just completed comprehensive benchmarking of the entire YOLOv11 family on ZETIC.MLange. Here's what every ML engineer needs to know. üìä Key Findings Across 5 Model Variants (XL to Nano): 1. NPU Dominance in Efficiency: - YOLOv11n: 1.72ms on NPU vs 53.60ms on CPU (31x faster) - Memory footprint: 0-65MB across all variants - Consistent sub-10ms inference even on XL models 2. The Sweet Spot - YOLOv11s: - NPU: 3.23ms @ 95.57% mAP - Perfect balance: 36MB model, production-ready speed - 10x faster than GPU, 30x faster than CPU 3. Surprising Discovery: Medium models (YOLOv11m) show unusual GPU performance patterns - NPU outperforms GPU by 4x (9.55ms vs 35.82ms), suggesting current GPU kernels aren't optimized for mid-size architectures. 4. Production Insights: - XL/Large: GPU still competitive for batch processing - Small/Nano: NPU absolutely crushes everything else - Memory scaling: Linear from 10MB...</description><pubDate>Tue, 23 Sep 2025 13:31:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/506441566129403</guid></item><item><title>Photo-Mate-i2i ‚Äì a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages!</title><link>https://huggingface.co/posts/prithivMLmods/355225487543965</link><description>Photo-Mate-i2i ‚Äì a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages! ‚ö°Demo: prithivMLmods/Photo-Mate-i2i ‚öôÔ∏èHow to Use: prithivMLmods/Photo-Mate-i2i#2 üë®‚Äçüîßi2i-Kontext(Experimental LoRAs): prithivMLmods/i2i-kontext-exp-68ce573b5c0623476b636ec7 See translation</description><pubDate>Tue, 23 Sep 2025 13:31:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/355225487543965</guid></item><item><title>Rolling Benchmarks - Evaluating AI Agents on Unseen GitHub Repos</title><link>https://huggingface.co/posts/salma-remyx/953215218627820</link><description>Rolling Benchmarks - Evaluating AI Agents on Unseen GitHub Repos Static benchmarks are prone to leaderboard hacking and training data contamination, so how about a dynamic/rolling benchmark? By limiting submissions to only freshly published code, we could evaluate based on consistency over time with rolling averages instead of finding agents overfit to a static benchmark. Can rolling benchmarks bring us closer to evaluating agents in a way more closely aligned with their real-world applications? Perhaps a new direction for agent evaluation? Would love to hear what you think about this! More on reddit: https://www.reddit.com/r/LocalLLaMA/comments/1nmvw7a/rolling_benchmarks_evaluating_ai_agents_on_unseen/ See translation</description><pubDate>Tue, 23 Sep 2025 13:31:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/953215218627820</guid></item><item><title>Trustworthy AI evals has been an industry challenge for the last few years, so what's missing?</title><link>https://huggingface.co/posts/salma-remyx/977986978639462</link><description>Trustworthy AI evals has been an industry challenge for the last few years, so what's missing? Causal Reasoning. Model based eval frameworks can't tell you if your changes actually improved user outcomes - you need to take a systems level approach. At Remyx, we‚Äôre building the intelligence layer for AI experimentation. Check out this example on how we start laying the scaffolding to launch controlled experiments to turn your hypotheses into insights on what drives performance for your application. Check out the latest at Remyx in our docs: https://docs.remyx.ai Try your first experiment today! https://engine.remyx.ai See translation</description><pubDate>Tue, 23 Sep 2025 13:31:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/977986978639462</guid></item><item><title>BAAI has released ROMEüî• evaluating 30+ large reasoning models on text &amp; visual reasoning</title><link>https://huggingface.co/posts/AdinaY/290924120685458</link><description>BAAI has released ROMEüî• evaluating 30+ large reasoning models on text &amp; visual reasoning FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions (2509.17177) ‚ú®Tests visual reasoning, not just recognition ‚ú®Covers capability √ó alignment √ó safety √ó efficiency ‚ú®More transparent &amp; reliable (less data contamination) ‚ú®Helps make real-world deployment choices See translation</description><pubDate>Tue, 23 Sep 2025 13:31:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/290924120685458</guid></item><item><title>The reactions to</title><link>https://huggingface.co/posts/ZennyKenny/159427530316943</link><description>The reactions to mostlyai/synthetic-sdk-demo have been incredible! üî• Some users wrote that they were having performance issues on larger datasets, so I've capped the Space's input to 5000 rows and 10 columns, but you can always use the open source SDK that powers the space any time you want on datasets of arbitrary size and shape! Check it out: https://github.com/mostly-ai/mostlyai üëà See translation</description><pubDate>Tue, 23 Sep 2025 13:31:29 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/159427530316943</guid></item></channel></rss>