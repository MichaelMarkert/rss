<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>I recently worked on a LoRA that improves tool use in LLM. Thought the approach might interest folks here.</title><link>https://huggingface.co/posts/codelion/510406818109359</link><description>I recently worked on a LoRA that improves tool use in LLM. Thought the approach might interest folks here. The issue I have had when trying to use some of the local LLMs with coding agents is this: Me: "Find all API endpoints with authentication in this codebase" LLM: "You should look for @ app .route decorators and check if they have auth middleware..." But I often want it to search the files and show me but the LLM doesn't trigger a tool use call. To fine-tune it for tool use I combined two data sources: 1. Magpie scenarios - 5000+ diverse tasks (bug hunting, refactoring, security audits) 2. Real execution - Ran these on actual repos (FastAPI, Django, React) to get authentic tool responses This ensures the model learns both breadth (many scenarios) and depth (real tool behavior). Tools We Taught: - read_file - Actually read file contents - search_files - Regex/pattern search across codebases - find_definition - Locate classes/functions - analyze_imports - Dependency tracking -...</description><pubDate>Mon, 01 Sep 2025 13:31:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/510406818109359</guid></item><item><title>It‚Äôs absolutely mind blowing - the work Dynamics Lab is doing!!</title><link>https://huggingface.co/posts/Jaward/864148450814843</link><description>It‚Äôs absolutely mind blowing - the work Dynamics Lab is doing!! With just a single input image and in a few seconds, their new world engine model (Mirage 2) can generate a whole new interactive world that‚Äôs physics informed and fully explorable in real-timeü§Ø Try it yourself: https://demo.dynamicslab.ai/chaos See translation</description><pubDate>Mon, 01 Sep 2025 13:31:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/864148450814843</guid></item><item><title>üå≤üçÑ LLM Forest Orchestra: Turning Hidden States into Music</title><link>https://huggingface.co/posts/Locutusque/640139873710354</link><description>üå≤üçÑ LLM Forest Orchestra: Turning Hidden States into Music Hello everyone! I'm excited to introduce a new Space I've been developing called LLM Forest Orchestra. This project converts the hidden states and attention patterns of transformer models into layered MIDI compositions. The concept draws inspiration from mushrooms and mycelial networks in forests. Fungi create underground connections linking plants and trees, establishing what some call a "wood-wide web" where signals and nutrients travel. Researchers have discovered that these exchanges form patterns resembling rhythms and pulses. When translated appropriately, these patterns can become music. Transformers operate through remarkably similar principles: tokens share signals via hidden states and attention heads. This Space transforms those invisible information flows into notes, chords, and rhythms, treating the model as a digital forest orchestra. üéõ Features * Two compute modes: - Full model operates on a Hugging Face model...</description><pubDate>Mon, 01 Sep 2025 13:31:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Locutusque/640139873710354</guid></item><item><title>Introducing</title><link>https://huggingface.co/posts/prithivMLmods/604588784783928</link><description>Introducing prithivMLmods/DeepCaption-VLA-7B , a multimodal VLM designed for reasoning with long-shot captions (Captioning and Vision-Language Attribution). It focuses on defining visual properties, object attributes, and scene details across a wide spectrum of images and aspect ratios, generating attribute-rich image captions. The model supports creative, artistic, and technical applications that require detailed descriptions. ü§óüî• ‚ú¶Ô∏é Models: prithivMLmods/DeepCaption-VLA-7B , also includes prithivMLmods/DeepAttriCap-VLA-3B , an experimental model for vision-language attribution. ‚ú¶Ô∏é Try the demo here: prithivMLmods/VisionScope-R2 ‚ú¶Ô∏é Try it now on Google Colab, with support for T4 GPUs in 4-bit quant_type: https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-Notebooks/blob/main/DeepCaption-VLA-7B%5B4bit%20-%20notebook%20demo%5D/DeepCaption-VLA-7B.ipynb ‚ú¶Ô∏é Collection: prithivMLmods/deepcaption-attr-68b041172ebcb867e45c556a . . . To know more about it, visit the model card of the...</description><pubDate>Mon, 01 Sep 2025 13:31:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/604588784783928</guid></item><item><title>Huge updates made for SECourses Musubi Tuner - 1-Click to Install App for LoRA Training and Full Fine Tuning Qwen Image, Qwen Image Edit, Wan 2.1 and Wan 2.2 Models with Musubi Tuner with Ready Presets</title><link>https://huggingface.co/posts/MonsterMMORPG/262579091589206</link><description>Huge updates made for SECourses Musubi Tuner - 1-Click to Install App for LoRA Training and Full Fine Tuning Qwen Image, Qwen Image Edit, Wan 2.1 and Wan 2.2 Models with Musubi Tuner with Ready Presets 1-Click to install app link : https://www.patreon.com/posts/137551634 Check all the screenshots 30 August 2025 Update V7 Dataset TOML file generate error fixed Qwen2.5-VL image captioning turns out working perfect on Windows It turns out my model file was corrupted even though it was same size Therefore I have updated the model downloader and now it will check and verify SHA 256 of files therefore it will be 100% accurate Prompt file selection folder icon issue fixed Downloader file will use generated venv of installation Make sure to run it after installation completed Fixed skip existing captions functionality in Image Captioning with Qwen2.5-VL Previously skipping was happening after caption generation which was destroying the skip logic Now properly checks for existing captions...</description><pubDate>Mon, 01 Sep 2025 13:31:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/262579091589206</guid></item><item><title>Nano Banana (Gemini 2.5 Flash Image) Full Tutorial ‚Äî 27 Unique Cases vs Qwen Image Edit ‚Äî Free 2 Use :</title><link>https://huggingface.co/posts/MonsterMMORPG/626834107736478</link><description>Nano Banana (Gemini 2.5 Flash Image) Full Tutorial ‚Äî 27 Unique Cases vs Qwen Image Edit ‚Äî Free 2 Use : https://youtu.be/qPUreQxB8zQ Tutorial link : https://youtu.be/qPUreQxB8zQ Nano Banana AI image editing model was published by Google today. It is officially named the Google Gemini 2.5 Flash Image model. It is the most advanced zero-shot image editing model ever made. I have conducted a thorough, in-depth review of this model with 27 unique cases. All prompts, images used, and results are demonstrated in real-time‚Äîlive in this tutorial. Moreover, I have compared each result with the state-of-the-art (SOTA) best open-source, locally available, and free-to-use Qwen Image Edit model, so we can see which model performs better at which tasks. Video Chapters 0:00 Introduction to Google's "Nano Banana" (Gemini 2.5 Flash) 0:28 Comparing Gemini vs. Qwen Image Edit Model (27 Test Cases) 1:33 Solving Gemini's Low Resolution with SUPIR Upscaling 2:28 Teaser: Upcoming Qwen Image LoRA Training...</description><pubDate>Mon, 01 Sep 2025 13:31:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/626834107736478</guid></item><item><title>The combination of Gemini Nano (Google's AI model) and the Tensor G5 chip (Google's AI processor), built into the Pixel 10 (Google's Smartphone), provides Google with a strong foundation to continue pushing the limits of edge AI ‚Üí üîÆMagic Cue.</title><link>https://huggingface.co/posts/jjokah/677752694420450</link><description>The combination of Gemini Nano (Google's AI model) and the Tensor G5 chip (Google's AI processor), built into the Pixel 10 (Google's Smartphone), provides Google with a strong foundation to continue pushing the limits of edge AI ‚Üí üîÆMagic Cue. Magic Cue digs through your device (Gmail, Calendar, Messages, Photos, screenshots, notes, and more) to surface what‚Äôs useful at that moment. Ref (Magic Cue): https://store.google.com/intl/en/ideas/articles/magic-cue/ See translation</description><pubDate>Mon, 01 Sep 2025 13:31:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jjokah/677752694420450</guid></item><item><title>Over 40 percent of AI-generated code contains security vulnerabilities. We recently worked on a LoRA to write secure code by default using automated Semgrep analysis and GRPO, achieving 97 percent reduction in vulnerabilities without requiring security-specific prompts.</title><link>https://huggingface.co/posts/codelion/329352982390313</link><description>Over 40 percent of AI-generated code contains security vulnerabilities. We recently worked on a LoRA to write secure code by default using automated Semgrep analysis and GRPO, achieving 97 percent reduction in vulnerabilities without requiring security-specific prompts. Technical Approach: Automated security training pipeline combining Semgrep vulnerability detection with preference learning. Generate multiple solutions with varying security awareness, automatically analyze for vulnerabilities, create preference pairs based on security scores, train using GRPO with multi-factor scoring. Scoring System (100 points total): - Functionality: 40 points - Does the code work correctly - Security patterns: 40 points - Uses secure coding practices - Low vulnerabilities: 20 points - Semgrep score below threshold This balanced scoring prevents reward hacking where models generate empty functions to avoid vulnerabilities. Real Transformation Examples: Database query before: query = f"SELECT *...</description><pubDate>Mon, 01 Sep 2025 13:31:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/329352982390313</guid></item><item><title>Although more and more code editors are aligning themselves with the AGENTS.md file standard, some still use specific nomenclatures that can make it difficult to maintain different configuration files when several people are working on the same project with different agents.</title><link>https://huggingface.co/posts/louisbrulenaudet/410111970297370</link><description>Although more and more code editors are aligning themselves with the AGENTS.md file standard, some still use specific nomenclatures that can make it difficult to maintain different configuration files when several people are working on the same project with different agents. Bodyboard addresses this by generating canonical instructions for code helpers from a single AGENTS.md file, thereby streamlining the production of adapter outputs for Gemini CLI, Copilot, Cline, Claude, Rules, Windsurf, and OpenAI Codex integrations. You just have to: npm install -g bodyboard Then run, at the root of your project: bodyboard all Link to npm: https://www.npmjs.com/package/bodyboard Link to the GitHub repo: https://github.com/louisbrulenaudet/bodyboard It's a very simple project, but it addresses certain issues I've encountered, so why not make it available to everyone... If you have other ideas for adapters to create, feel free to open a PR on the GitHub repo. See translation</description><pubDate>Mon, 01 Sep 2025 13:31:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/louisbrulenaudet/410111970297370</guid></item><item><title>Massive Updates and Improvements to SECourses Musubi Trainer and Here Updated Screenshots of all features - check all 16 images to see all</title><link>https://huggingface.co/posts/MonsterMMORPG/476621372022731</link><description>Massive Updates and Improvements to SECourses Musubi Trainer and Here Updated Screenshots of all features - check all 16 images to see all App download and install link : https://www.patreon.com/posts/137551634 Images as gallery : https://www.reddit.com/r/SECourses/comments/1n4qq8y/massive_updates_and_improvements_to_secourses/ See translation</description><pubDate>Mon, 01 Sep 2025 13:31:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/476621372022731</guid></item></channel></rss>