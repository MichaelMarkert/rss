<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Bu post'u çevirebilirsiniz 🤗💗</title><link>https://huggingface.co/posts/merve/523189303979360</link><description>Bu post'u çevirebilirsiniz 🤗💗 See translation</description><pubDate>Fri, 23 May 2025 13:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/523189303979360</guid></item><item><title>🧬 Hey everyone! Just released **OpenEvolve** - an open-source implementation of Google DeepMind's AlphaEvolve system.</title><link>https://huggingface.co/posts/codelion/735622263233891</link><description>🧬 Hey everyone! Just released **OpenEvolve** - an open-source implementation of Google DeepMind's AlphaEvolve system. It's an evolutionary coding agent that uses LLMs to discover and optimize algorithms. I successfully replicated DeepMind's results on circle packing (99.97% match!) and evolved a random search into a simulated annealing algorithm. ✨ Key features: - Evolves entire codebases (not just single functions) - Works with any OpenAI-compatible API - LLM ensemble approach for better results - Multi-objective optimization 👉 Check it out: GitHub: https://github.com/codelion/openevolve Blog post: https://huggingface.co/blog/codelion/openevolve Would love to hear your thoughts or answer any questions about it! See translation</description><pubDate>Fri, 23 May 2025 13:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/735622263233891</guid></item><item><title>Meet our new agentic model : 𝗗𝗲𝘃𝘀𝘁𝗿𝗮𝗹</title><link>https://huggingface.co/posts/Jofthomas/656021086131496</link><description>Meet our new agentic model : 𝗗𝗲𝘃𝘀𝘁𝗿𝗮𝗹 Devstral is an open-source LLM built software engineering tasks built under a collaboration between Mistral AI and All Hands AI 🙌. 𝗞𝗲𝘆 𝗳𝗲𝗮𝘁𝘂𝗿𝗲𝘀 : • 🤖 𝗔𝗴𝗲𝗻𝘁𝘀 : perfect for Agentic coding • 🍃 𝗹𝗶𝗴𝗵𝘁𝘄𝗲𝗶𝗴𝗵𝘁: Devstral is a 𝟮𝟰𝗕 parameter based on Mistral small. • ©️ 𝗔𝗽𝗮𝗰𝗵𝗲 𝟮.𝟬, meaning fully open-source ! • 📄 A 𝟭𝟮𝟴𝗸 context window. 📚Blog : https://mistral.ai/news/devstral ⚡API : The model is also available on our API under the name 𝗱𝗲𝘃𝘀𝘁𝗿𝗮𝗹-𝘀𝗺𝗮𝗹𝗹-𝟮𝟱𝟬𝟱 🤗 repo : mistralai/Devstral-Small-2505 Can't wait to see what you will build with it ! See translation</description><pubDate>Fri, 23 May 2025 13:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jofthomas/656021086131496</guid></item><item><title>🌾 NH Prediction: AI System for Korean Agricultural Price Forecasting 🌾</title><link>https://huggingface.co/posts/openfree/102455854917725</link><description>🌾 NH Prediction: AI System for Korean Agricultural Price Forecasting 🌾 📊 Project Introduction Price volatility in agricultural markets has significant impacts from producers to consumers! NH Prediction is an innovative system that utilizes cutting-edge AI technology to predict Korean agricultural wholesale prices based on extensive data spanning 40 years. 🚀 VIDraft/NH-Prediction ginipick/NH-Korea 🧠 VIDraft's 14 Enhanced Prediction Models The VIDraft research team has developed 14 advanced prediction models by reinforcing existing forecasting approaches: 🔮 VID-SARIMA Series: Precisely models seasonality and trends (up to 99.99% accuracy) ⚖️ VID-ETS Series: Captures multiplicative/additive variation patterns 📈 VID-Holt/Holt-Winters: Simultaneous analysis of linear trends and seasonality 📉 VID-MovingAverage/WeightedMA: Noise removal and medium-term trend identification 🔍 VID-Fourier+LR: Hybrid approach capturing complex periodicity ✨ Key Features 🌟 Item-Specific Optimization:...</description><pubDate>Fri, 23 May 2025 13:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/102455854917725</guid></item><item><title>Google released MedGemma on I/O'25 👏</title><link>https://huggingface.co/posts/merve/962316386830239</link><description>Google released MedGemma on I/O'25 👏 google/medgemma-release-680aade845f90bec6a3f60c4 &gt; 4B and 27B instruction fine-tuned vision LMs and a 4B pre-trained vision LM for medicine &gt; available with transformers from the get-go 🤗 they also released a cool demo for scan reading ➡️ google/rad_explain use with transformers ⤵️ See translation</description><pubDate>Fri, 23 May 2025 13:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/962316386830239</guid></item><item><title>Playing with Veo3 this morning. Share your prompt if you want me to create videos for you (bonus point if they funnily reference HF/open-source). These videos are "a cat on the moon rapping "I love Hugging Face""!</title><link>https://huggingface.co/posts/clem/670042306060895</link><description>Playing with Veo3 this morning. Share your prompt if you want me to create videos for you (bonus point if they funnily reference HF/open-source). These videos are "a cat on the moon rapping "I love Hugging Face""! See translation</description><pubDate>Fri, 23 May 2025 13:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/670042306060895</guid></item><item><title>ByteDance is absolutely cooking lately🔥</title><link>https://huggingface.co/posts/AdinaY/709690582361356</link><description>ByteDance is absolutely cooking lately🔥 BAGEL 🥯 7B active parameter open multimodal foundation model by Bytedance Seed team. ByteDance-Seed/BAGEL-7B-MoT ✨ Apache 2.0 ✨ Outperforms top VLMs (Qwen2.5-VL &amp; InternVL-2.5) ✨ Mixture-of-Transformer-Experts + dual encoders ✨ Trained on trillions of interleaved tokens See translation</description><pubDate>Fri, 23 May 2025 13:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/709690582361356</guid></item><item><title>&gt; New Model</title><link>https://huggingface.co/posts/KaraKaraWitch/569360445188531</link><description>&gt; New Model &gt; Looks at Model Card &gt; "Open-Weights" See translation</description><pubDate>Fri, 23 May 2025 13:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/KaraKaraWitch/569360445188531</guid></item><item><title>tis the year of any-to-any/omni models 🤠</title><link>https://huggingface.co/posts/merve/870882250701193</link><description>tis the year of any-to-any/omni models 🤠 ByteDance-Seed/BAGEL-7B-MoT 7B native multimodal model that understands and generates both image + text it outperforms leading VLMs like Qwen 2.5-VL 👏 and has Apache 2.0 license 😱 See translation</description><pubDate>Fri, 23 May 2025 13:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/870882250701193</guid></item><item><title>Despite the emergence of combining LLM and DiT architectures for T2I synthesis, its design remains severely understudied.</title><link>https://huggingface.co/posts/sayakpaul/592772325865027</link><description>Despite the emergence of combining LLM and DiT architectures for T2I synthesis, its design remains severely understudied. This was done long ago and got into CVPR25 -- super excited to finally share it now, along with the data and code ♥️ We explore several architectural choices that affect this design. We provide an open &amp; reproducible training recipe that works at scale. Works like Playground v3 have already explored a deep fusion between an LLM and a DiT, sharing their representations through layerwise attention. They exhibit excellent performance on T2I. Despite its compelling results and other performance virtues, it remains unexplored, which is what we want to improve in our work. Specifically, we take a pre-trained LLM (Gemma-2B) and trainable DiT, and set out to explore what makes a "good deep fusion" between the two for T2I. We explore several key questions in the work, such as: Q1: How should we do attention? We considered several alternatives. PixArt-Alpha like attention...</description><pubDate>Fri, 23 May 2025 13:33:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sayakpaul/592772325865027</guid></item></channel></rss>