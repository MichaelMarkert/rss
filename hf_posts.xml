<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ðŸ¤— Sentence Transformers is joining Hugging Face! ðŸ¤— This formalizes the existing maintenance structure, as I've personally led the project for the past two years on behalf of Hugging Face! Details:</title><link>https://huggingface.co/posts/tomaarsen/527498981313495</link><description>ðŸ¤— Sentence Transformers is joining Hugging Face! ðŸ¤— This formalizes the existing maintenance structure, as I've personally led the project for the past two years on behalf of Hugging Face! Details: Today, the Ubiquitous Knowledge Processing (UKP) Lab is transferring the project to Hugging Face. Sentence Transformers will remain a community-driven, open-source project, with the same open-source license (Apache 2.0) as before. Contributions from researchers, developers, and enthusiasts are welcome and encouraged. The project will continue to prioritize transparency, collaboration, and broad accessibility. Read our full announcement for more details and quotes from UKP and Hugging Face leadership: https://huggingface.co/blog/sentence-transformers-joins-hf We see an increasing wish from companies to move from large LLM APIs to local models for better control and privacy, reflected in the library's growth: in just the last 30 days, Sentence Transformer models have been downloaded &gt;270...</description><pubDate>Fri, 24 Oct 2025 17:20:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/527498981313495</guid></item><item><title>Say hello to my little friends! I just unboxed this trio of HP Z2 G1a!</title><link>https://huggingface.co/posts/mitkox/390180686171042</link><description>Say hello to my little friends! I just unboxed this trio of HP Z2 G1a! Three is always better than one! 3x AMD Ryzen AI Max+ Pro 395 384GB RAM 24TB of RAID storage Ubuntu 24.04 ROCm 7.0.2 llama cpp, vLLM and Aibrix Small, cheap GPUs are about to become the Raspberry Pi of edge AI inference. Sprinkle some kubectl fairy dust on top, and suddenly it's a high-availability, self-healing, cloud-native, enterprise-grade AI cluster camping in a closet. Make sure you own your AI. AI in the cloud is not aligned with you; itâ€™s aligned with the company that owns it. See translation</description><pubDate>Fri, 24 Oct 2025 17:20:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/390180686171042</guid></item><item><title>We've built over 10K containerized reproductions of papers from arXiv!</title><link>https://huggingface.co/posts/salma-remyx/787120125285076</link><description>We've built over 10K containerized reproductions of papers from arXiv! Instead of spending all day trying to build an environment to test that new idea, just pull the Docker container from the Remyx registry. And with Remyx, you can start experimenting faster by generating a test PR in your codebase based on the ideas found in your paper of choice. Hub: https://hub.docker.com/u/remyxai Remyx docs: https://docs.remyx.ai/resources/ideate Coming soon, explore reproduced papers with AG2 + Remyx: https://github.com/ag2ai/ag2/pull/2141 See translation</description><pubDate>Fri, 24 Oct 2025 17:20:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/787120125285076</guid></item><item><title>Hello from PyTorch Conference 2025! ðŸ‘‹</title><link>https://huggingface.co/posts/sondhiArm/743734106671454</link><description>Hello from PyTorch Conference 2025! ðŸ‘‹ The energy is high, and weâ€™re excited to connect with the community to showcase how developers can build high-performance GenAI applications across cloud, mobile, and edge using PyTorch and ExecuTorch. Visit us at Booth #P1 to explore hands-on demos, join a one-on-one workshop, or catch one of our insightful sessions throughout the day. https://developer.arm.com/developer-partners/pytorch See translation</description><pubDate>Fri, 24 Oct 2025 17:20:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sondhiArm/743734106671454</guid></item><item><title>HunyuanWorld MirrorðŸ”¥a versatile feed forward model for universal 3D world reconstruction by Tencent</title><link>https://huggingface.co/posts/AdinaY/459235610615805</link><description>HunyuanWorld MirrorðŸ”¥a versatile feed forward model for universal 3D world reconstruction by Tencent tencent/HunyuanWorld-Mirror âœ¨ Any prior in â†’ 3D world out âœ¨ Mix camera, intrinsics, depth as priors âœ¨ Predict point clouds, normals, Gaussians &amp; more in one pass âœ¨ Unified architecture for all 3D task See translation</description><pubDate>Fri, 24 Oct 2025 17:20:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/459235610615805</guid></item><item><title>Which is the best model to use as a signal for investment?</title><link>https://huggingface.co/posts/SelmaNajih001/829546518885653</link><description>Which is the best model to use as a signal for investment? Here who is gaining the most: SelmaNajih001/InvestmentStrategyBasedOnSentiment The Space uses titles from this dataset: ðŸ“Š SelmaNajih001/Cnbc_MultiCompany Given a news title, it calculates a sentiment score : if the score crosses a certain threshold, the strategy decides to buy or sell. Each trade lasts one day, and the strategy then computes the daily return. For Tesla the best model seems to be the regression ðŸ‘€ Just a quick note: the model uses the closing price as the buy price, meaning it already reflects the impact of the news. See translation</description><pubDate>Fri, 24 Oct 2025 17:20:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/SelmaNajih001/829546518885653</guid></item><item><title>Meet OpenEnv ðŸ‘‹, an open ecosystem of environments for intelligent agents. Build, share, and test agents safely and consistently.</title><link>https://huggingface.co/posts/sergiopaniego/527159223589050</link><description>Meet OpenEnv ðŸ‘‹, an open ecosystem of environments for intelligent agents. Build, share, and test agents safely and consistently. Ideal for training with TRL (we include examplesðŸ¤“), deployment, and community collaboration via the HF Hub Blog: https://huggingface.co/blog/openenv Hub for Environments: openenv OpenEnv repo: https://github.com/meta-pytorch/OpenEnv Try it out using TRL: https://huggingface.co/docs/trl/main/en/openenv See translation</description><pubDate>Fri, 24 Oct 2025 17:20:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/527159223589050</guid></item><item><title>I see all Chinese labs are turning TL;DR into TL;DRGB</title><link>https://huggingface.co/posts/mitkox/331863384538017</link><description>I see all Chinese labs are turning TL;DR into TL;DRGB Problem: 1M text tokens == 1 M opportunities for your GPU to file worker-comp Solution: donâ€™t feed the model War &amp; Peaceâ€”feed it the movie poster. This is Glyph, Zaiâ€™s new visual-text compression voodoo: â€¢ 10 k words â†’ 3 PNGs â‰ˆ 3 k visual tokens â€¢ Compression ratio: 4.3Ã— â€¢ Throughput: 40-60 tok/s i.e. your context window now finishes before my coffee does So I did the only reasonable thing: asked GLM-4.6 to port Glyph for Qwen3-VL-8B-Thinking. Translation: I made one model compress a novel into a comic strip, then made another model read the comic strip and still ace QA. Itâ€™s basically passing notes in class, except the note is a 1920Ã—1080 meme and the teacher is a transformer. We've gone from "Attention is All You Need" to "Attention is Too Expensive, Just Use Your Eyes." Remember kids: in 2025 literacy is optional, but JPEG is forever. See translation</description><pubDate>Fri, 24 Oct 2025 17:20:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/331863384538017</guid></item><item><title>I'm excited to announce the release of Kanon 2 Embedder, the world's best legal embedding model, ranked first on the Massive Legal Embedding Benchmark ðŸŽ‰</title><link>https://huggingface.co/posts/umarbutler/611899757397292</link><description>I'm excited to announce the release of Kanon 2 Embedder, the world's best legal embedding model, ranked first on the Massive Legal Embedding Benchmark ðŸŽ‰ This model is the product of quite literally months of painstaking work alongside @ abdurrahmanbutler collecting, cleaning, and processing terabytes of data as well as coming up with novel improvements to the standard embedder training recipe to push the limits of what's possible. Kanon 2 Embedder is my most advanced model to date. On MLEB, it benchmarks as 9% more accurate than OpenAI's best embedding model and 30% faster. Even when truncated from 1,792 to 768 dimensions, Kanon 2 Embedder continues to hold the number one spot on MLEB. Importantly, Kanon 2 Embedder is also privacy and security friendly â€” unlike Voyage, Cohere and Jina, none of your data is used to train our models by default. Kanon 2 Embedder can also be self-hosted for enterprises with heightened security or reliability requirements. You can read the full...</description><pubDate>Fri, 24 Oct 2025 17:20:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/umarbutler/611899757397292</guid></item><item><title>Excited to announce 4 AWQ quantized models from #AllenAI! ðŸŽ‰</title><link>https://huggingface.co/posts/ronantakizawa/860085077429074</link><description>Excited to announce 4 AWQ quantized models from #AllenAI! ðŸŽ‰ Molmo-7B-D AWQ (14GBâ†’5GB): Efficient VLM performing between GPT-4V and GPT-4o on academic benchmarks, with just 6.1% perplexity degradation. MolmoAct-7B-D AWQ (14GBâ†’6GB): Specialized robotic manipulation model reduced by ~57%. Molmo-72B AWQ (145GBâ†’38GB): VLM with Qwen2-72B decoder that performs competitively with GPT-4, achieving only 10.5% perplexity degradation while saving 107GB of memory. OLMo-2-32B-Instruct AWQ (64GBâ†’17GB): LLM post-trained on TÃ¼lu 3 with 3% perplexity degradation while saving ~50GB. All VLMs only had their text models quantized. ronantakizawa/molmo-7b-d-awq ronantakizawa/molmoact-7b-d-awq ronantakizawa/molmo-72b-awq ronantakizawa/olmo2-32b-instruct-awq See translation</description><pubDate>Fri, 24 Oct 2025 17:20:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ronantakizawa/860085077429074</guid></item></channel></rss>