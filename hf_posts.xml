<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>How to compress long code context? üìö</title><link>https://huggingface.co/posts/YerbaPage/558970453952386</link><description>How to compress long code context? üìö Check out our LongCodeZip! Paper just got accepted to ASE 2025. üî• Code: https://github.com/YerbaPage/LongCodeZip Paper: LongCodeZip: Compress Long Context for Code Language Models (2510.00446) See translation</description><pubDate>Sat, 04 Oct 2025 09:20:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/558970453952386</guid></item><item><title>Try the Hugging Face Space demo for</title><link>https://huggingface.co/posts/prithivMLmods/704561076669428</link><description>Try the Hugging Face Space demo for Logics-MLLM/Logics-Parsing , the latest multimodal VLM from the Logics Team at Alibaba Group. It enables end-to-end document parsing with precise content extraction in markdown format, and it also generates a clean HTML representation of the document while preserving its logical structure. ü§óüî• Additionally, I‚Äôve integrated one of my recent works ‚Äî prithivMLmods/Gliese-OCR-7B-Post1.0 ‚Äî which also excels at document comprehension. ‚≠ê Space / App : prithivMLmods/Logics-Parsing-VLM üìÑ Technical Report by the Logics Team, Alibaba Group : Logics-Parsing Technical Report (2509.19760) ‚ö° Collections : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Other Pages: ‚ûî Multimodal VLMs - July'25 : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 ‚ûî Multimodal VLMs - Aug'25 : prithivMLmods/multimodal-vlms-aug25-68a56aac39fe8084f3c168bd ‚ûî VL caption ‚Äî &lt; Sep 15 ‚Äô25 : prithivMLmods/vl-caption-sep-15-25-68c7f6d737985c63c13e2391 . . ....</description><pubDate>Sat, 04 Oct 2025 09:20:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/704561076669428</guid></item><item><title>üöÄ Exciting News! We've released a Performance Marketing Expert Dataset from Hawky.ai [www.hawky.ai]</title><link>https://huggingface.co/posts/Sri-Vigneshwar-DJ/952078344859787</link><description>üöÄ Exciting News! We've released a Performance Marketing Expert Dataset from Hawky.ai [www.hawky.ai] Hawky-ai This dataset empowers AI models with cutting-edge strategies for Meta, Google Ads, and TikTok campaigns. It includes: 1. Multi-platform strategies for e-commerce, DTC, B2B, and more 2. Creative optimization and audience targeting insights 3. ROI-driven recommendations based on 2025 best practices Sri-Vigneshwar-DJ/Performance-Marketing-Data See translation</description><pubDate>Sat, 04 Oct 2025 09:20:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Sri-Vigneshwar-DJ/952078344859787</guid></item><item><title>Introducing</title><link>https://huggingface.co/posts/SelmaNajih001/721687692996128</link><description>Introducing SelmaNajih001/StockPredictionExplanation , built with GRPO and RAG: -GRPO trains the model to predict and explain stock direction. -RAG grounds explanations in historical financial news and central bank speeches. Together, they create a system that forecasts stock movements and shows the reasoning behind them. Full article: Explainable Financial Predictions ‚Äî https://huggingface.co/blog/SelmaNajih001/explainable-financial-predictions Try it here: StockPredictionExplanation Space ‚Äî SelmaNajih001/StockPredictionExplanation See translation</description><pubDate>Sat, 04 Oct 2025 09:20:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/SelmaNajih001/721687692996128</guid></item><item><title>üöÄ Big news from XenArcAI!</title><link>https://huggingface.co/posts/Parveshiiii/228189451590505</link><description>üöÄ Big news from XenArcAI! We‚Äôve just released our new dataset: **Bhagwat‚ÄëGita‚ÄëInfinity** üå∏üìñ ‚ú® What‚Äôs inside: - Verse‚Äëaligned Sanskrit, Hindi, and English - Clean, structured, and ready for ML/AI projects - Perfect for research, education, and open‚Äësource exploration üîó Hugging Face: XenArcAI/Bhagwat-Gita-Infinity Let‚Äôs bring timeless wisdom into modern AI together üôå See translation</description><pubDate>Sat, 04 Oct 2025 09:20:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Parveshiiii/228189451590505</guid></item><item><title>I have a few Sora-2 invites - 15509N</title><link>https://huggingface.co/posts/Nymbo/667159047100186</link><description>I have a few Sora-2 invites - 15509N See translation</description><pubDate>Sat, 04 Oct 2025 09:20:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Nymbo/667159047100186</guid></item><item><title>Ovi - Generate Videos With Audio Like VEO 3 or SORA 2 - Run Locally - Open Source for Free</title><link>https://huggingface.co/posts/MonsterMMORPG/329376915558167</link><description>Ovi - Generate Videos With Audio Like VEO 3 or SORA 2 - Run Locally - Open Source for Free Download and install : https://www.patreon.com/posts/140393220 Quick demo tutorial : https://youtu.be/uE0QabiHmRw Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation Project page : https://aaxwaz.github.io/Ovi/ SECourses Ovi Pro Premium App Features Full scale ultra advanced app for Ovi - an open source project that can generate videos from both text prompts and image + text prompts with real audio. Project page is here : https://aaxwaz.github.io/Ovi/ I have developed an ultra advanced Gradio app and much better pipeline that fully supports block swapping Now we can generate full quality videos with as low as 8.2 GB VRAM Hopefully I will work on dynamic on load FP8_Scaled tomorrow to improve VRAM even further So more VRAM optimizations will come hopefully tomorrow Our implemented block swapping is the very best one out there - I took the approach from famous Kohya Musubi tuner The...</description><pubDate>Sat, 04 Oct 2025 09:20:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/329376915558167</guid></item><item><title>‚úÖ New Article: *Cosmic Exploration PoC ‚Äî Part 4*</title><link>https://huggingface.co/posts/kanaria007/107464223968156</link><description>‚úÖ New Article: *Cosmic Exploration PoC ‚Äî Part 4* Title: üöÄ PoC for Space Exploration with Structured Intelligence Computers (SIC) ‚Äî Long-Horizon Starship Simulation üîó https://huggingface.co/blog/kanaria007/cosmic-exploration-starship --- Summary: Exploration across stars is not a question of distance ‚Äî it is a question of *time*. This PoC tests whether mission identity, ethics, and governance can endure across centuries of generational drift and uncertainty. A starship becomes more than transport: it is a *self-sustaining arc of civilization*. &gt; The challenge is not propulsion. &gt; *It is continuity of mind and mission.* --- Why It Matters: ‚Ä¢ Demonstrates structural continuity over simulated 500-year journeys ‚Ä¢ Validates conflict mediation and governance stability without collapse ‚Ä¢ Proves that ethics and identity can persist across generations --- What‚Äôs Inside: ‚Ä¢ Starship OS integrating resilience, ethics, and memory loops ‚Ä¢ Simulation of generational conflict, scarcity events, value...</description><pubDate>Sat, 04 Oct 2025 09:20:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/107464223968156</guid></item><item><title>ü•ä  Big Code Arena is live!</title><link>https://huggingface.co/posts/ZennyKenny/911264518414164</link><description>ü•ä Big Code Arena is live! bigcode/arena üí° bigcode is an open scientific collaboration working on responsible training of large language models for coding applications. üëâ The Arena ranks LLMs based on their ability to support natural language vibe coding requests in a competitive format, based on feedback from human reviewers. üß† It was a pleasure to contribute to this project led by @ terryyz and appear as an additional contributor in the Big Code Arena paper. See translation</description><pubDate>Sat, 04 Oct 2025 09:20:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZennyKenny/911264518414164</guid></item><item><title>WebApp1K measures an oldest and simplest kind of task predated ChatGPT. It is code completion, you can also consider it a translation task mapping test spec into code. It requires no conversation,  reasoning (which helps sometimes), or RL.</title><link>https://huggingface.co/posts/onekq/612802221121002</link><description>WebApp1K measures an oldest and simplest kind of task predated ChatGPT. It is code completion, you can also consider it a translation task mapping test spec into code. It requires no conversation, reasoning (which helps sometimes), or RL. I don't think it is on the roadmap of top labs. Otherwise, you can't explain why Claude 4 has the same 70+ score on SweBench, which is way more challenging than this benchmark. Neither do I encourage model builders to optimize towards my benchmark, which in itself won't be too hard to top the leaderboard. I just argue that we're still in a very early phase. What I witness now is still the same pattern: the dropping of generic models strategically optimized towards famous benchmarks. Meanwhile, agent builders (top labs and startups alike) painfully prompt these models to follow their expectations, and pray they won't drift overnight. See translation</description><pubDate>Sat, 04 Oct 2025 09:20:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/612802221121002</guid></item></channel></rss>