<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Today in Privacy &amp; AI Tooling - introducing a nifty new tool to examine where data goes in open-source apps on  🤗</title><link>https://huggingface.co/posts/yjernite/333157611385452</link><description>Today in Privacy &amp; AI Tooling - introducing a nifty new tool to examine where data goes in open-source apps on 🤗 HF Spaces have tons (100Ks!) of cool demos leveraging or examining AI systems - and because most of them are OSS we can see exactly how they handle user data 📚🔍 That requires actually reading the code though, which isn't always easy or quick! Good news: code LMs have gotten pretty good at automatic review, so we can offload some of the work - here I'm using Qwen/Qwen2.5-Coder-32B-Instruct to generate reports and it works pretty OK 🙌 The app works in three stages: 1. Download all code files 2. Use the Code LM to generate a detailed report pointing to code where data is transferred/(AI-)processed (screen 1) 3. Summarize the app's main functionality and data journeys (screen 2) 4. Build a Privacy TLDR with those inputs It comes with a bunch of pre-reviewed apps/Spaces, great to see how many process data locally or through (private) HF endpoints 🤗 Note that this is a POC,...</description><pubDate>Sat, 19 Apr 2025 05:21:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yjernite/333157611385452</guid></item><item><title>FramePack Full Tutorial: 1-Click to Install on Windows - Up to 120 Second Image-to-Videos with 6GB &gt;</title><link>https://huggingface.co/posts/MonsterMMORPG/401294184812659</link><description>FramePack Full Tutorial: 1-Click to Install on Windows - Up to 120 Second Image-to-Videos with 6GB &gt; https://youtu.be/HwMngohRmHg Tutorial video : https://youtu.be/HwMngohRmHg FramePack from legendary lllyasviel full Windows local tutorial with a very advanced Gradio app to generate consistent videos from images with as long as 120 seconds and as low as 6 GB GPUs. This tutorial will show you step by step how to install and use FramePack locall with a very advanced Graido app. Moreover, I have published installers for cloud services such as RunPod and Massed Compute for those GPU poor and who wants to scale. 🔗 Full Instructions, Installers and Links Shared Post (the one used in the tutorial) ⤵️ ▶️ https://www.patreon.com/posts/click-to-open-post-used-in-tutorial-126855226 🔗 SECourses Official Discord 10500+ Members ⤵️ ▶️ https://discord.com/servers/software-engineering-courses-secourses-772774097734074388 🔗 Stable Diffusion, FLUX, Generative AI Tutorials and Resources GitHub ⤵️ ▶️...</description><pubDate>Sat, 19 Apr 2025 05:21:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/401294184812659</guid></item><item><title>OpenAI just released a 34-page practical guide to building agents,</title><link>https://huggingface.co/posts/hesamation/750913380201236</link><description>OpenAI just released a 34-page practical guide to building agents, Here's 10 things it teaches us: 1➜ agents are different from workflows: they are complete autonomous systems that perform tasks on your behalf. many applications use LLMs for workflows, but this is not an agent. 2➜ use them for tricky stuff: complex decision making, dynamic rules, unstructured data 3➜ core recipe: each agent has three main components: Model (the brain), Tools, Instructions on how to behave 4➜ choose the right brain: set up evals to get a baseline performance, use a smart model to see what's possible, gradually downgrade the model for cost and speed 5➜ tools are key: choose well-defined and tested tools. an agent needs tools to retrieve data and context, and take actions. 6➜ instruction matters A LOT: be super clear telling the agent its goals, steps, and rules. Vague instructions = unpredictable agent. Be explicit. 7➜ start simple, then scale: often a single agent with several tools is ok. don't jump...</description><pubDate>Sat, 19 Apr 2025 05:21:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/750913380201236</guid></item><item><title>New king of open VLMs: InternVL3 takes Qwen 2.5's crown! 👑</title><link>https://huggingface.co/posts/m-ric/531366391123392</link><description>New king of open VLMs: InternVL3 takes Qwen 2.5's crown! 👑 InternVL have been a wildly successful series of model : and the latest iteration has just taken back their crown thanks to their superior, natively multimodal vision training pipeline. ➡️ Most of the vision language models (VLMs) these days are built like Frankenstein : take a good text-only Large Language Model (LLM) backbone, stitch a specific vision transformer (ViT) on top of it. Then the training is sequential 🔢 : 1. Freeze the LLM weights while you train the ViT only to work with the LLM part, then 2. Unfreeze all weights to train all weights in order to work together. 💫 The Shanghai Lab decided to challenge this paradigm and chose this approach that they call "native". For each of their model sizes, they still start from a good LLM (mostly Qwen-2.5 series, did I tell you I'm a huge fan of Qwen? ❤️), and stitch the ViT, but they don't freeze anything : they train all weights together with interleaved text and image...</description><pubDate>Sat, 19 Apr 2025 05:21:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/531366391123392</guid></item><item><title>Gemini 2.5 Flash is here! We excited launch our first hybrid reasoning Gemini model. In Flash 2.5 developer can turn thinking off.</title><link>https://huggingface.co/posts/philschmid/318540305385241</link><description>Gemini 2.5 Flash is here! We excited launch our first hybrid reasoning Gemini model. In Flash 2.5 developer can turn thinking off. **TL;DR:** - 🧠 Controllable "Thinking" with thinking budget with up to 24k token - 🌌 1 Million multimodal input context for text, image, video, audio, and pdf - 🛠️ Function calling, structured output, google search &amp; code execution. - 🏦 $0.15 1M input tokens; $0.6 or $3.5 (thinking on) per million output tokens (thinking tokens are billed as output tokens) - 💡 Knowledge cut of January 2025 - 🚀 Rate limits - Free 10 RPM 500 req/day - 🏅Outperforms 2.0 Flash on every benchmark Try it ⬇️ https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-preview-04-17 See translation</description><pubDate>Sat, 19 Apr 2025 05:21:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/philschmid/318540305385241</guid></item><item><title>Dropping an entire collection of Style Intermixing Adapters on StrangerZone HF — including Realism, Anime, Sketch, Texture-Rich 3D Experimentals, Automotive Concept Images, and LoRA models based on Flux.1, SD 3.5 Turbo/Large, Stable Diffusion XL 🎨</title><link>https://huggingface.co/posts/prithivMLmods/567717355691306</link><description>Dropping an entire collection of Style Intermixing Adapters on StrangerZone HF — including Realism, Anime, Sketch, Texture-Rich 3D Experimentals, Automotive Concept Images, and LoRA models based on Flux.1, SD 3.5 Turbo/Large, Stable Diffusion XL 🎨 ╰┈➤Collection : ➜ sketch : strangerzonehf/sketch-fav-675ba869c7ceaec7e652ee1c ➜ sketch2 : strangerzonehf/q-series-sketch-678e3503bf3a661758429717 ➜ automotive : strangerzonehf/automotive-3d-675bb31a491d8c264d45d843 ➜ texture 3d : strangerzonehf/flux-3dxl-engine-674833c14a001d5b1fdb5139 ➜ super 3d : strangerzonehf/super-3d-engine-6743231d69f496df97addd2b ➜ style mix : strangerzonehf/mixer-engine-673582c9c5939d8aa5bf9533 ➜ realism : strangerzonehf/realism-engine-67343495b6daf0fbdb904cc1 ╰┈➤The Entire Collection : ➜ flux.1 : prithivMLmods/flux-lora-collections-66dd5908be2206cfaa8519be ➜ flux-ultimate-lora-collection : strangerzonehf/Flux-Ultimate-LoRA-Collection ➜ sd 3.5 large / turbo : prithivMLmods/sd-35-large-lora-671b39d7bc2e7f71a446b163...</description><pubDate>Sat, 19 Apr 2025 05:21:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/567717355691306</guid></item><item><title>𝐌𝐄𝐒𝐀 🏔️ 𝐓𝐞𝐱𝐭-𝐛𝐚𝐬𝐞𝐝 𝐭𝐞𝐫𝐫𝐚𝐢𝐧 𝐠𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧 𝐦𝐨𝐝𝐞𝐥</title><link>https://huggingface.co/posts/mikonvergence/838006705758028</link><description>𝐌𝐄𝐒𝐀 🏔️ 𝐓𝐞𝐱𝐭-𝐛𝐚𝐬𝐞𝐝 𝐭𝐞𝐫𝐫𝐚𝐢𝐧 𝐠𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧 𝐦𝐨𝐝𝐞𝐥 MESA is a novel generative model based on latent denoising diffusion capable of generating 2.5D representations (co-registered colour and depth maps) of terrains based on text prompt conditioning. Work developed by Paul Borne–Pons ( @ NewtNewt ) during his joint internship at Adobe &amp; ESA, and in collaboration with asterisk labs. 🏔️ 𝐏𝐫𝐨𝐣𝐞𝐜𝐭 𝐏𝐚𝐠𝐞 : https://paulbornep.github.io/mesa-terrain/ 📝 𝐏𝐫𝐞𝐩𝐫𝐢𝐧𝐭 : https://arxiv.org/abs/2504.07210 🤗 𝐌𝐨𝐝𝐞𝐥 𝐖𝐞𝐢𝐠𝐡𝐭𝐬 : NewtNewt/MESA 💾 𝐃𝐚𝐭𝐚𝐬𝐞𝐭 : Major-TOM/Core-DEM 🧑🏻‍💻​𝐂𝐨𝐝𝐞 : https://github.com/PaulBorneP/MESA 𝐇𝐅 𝐒𝐩𝐚𝐜𝐞: mikonvergence/MESA See translation</description><pubDate>Sat, 19 Apr 2025 05:21:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mikonvergence/838006705758028</guid></item><item><title>Reasoning models like o3 and o4-mini are advancing faster than ever, but imagine what will be possible when they can run locally in your browser! 🤯</title><link>https://huggingface.co/posts/Xenova/811708183292240</link><description>Reasoning models like o3 and o4-mini are advancing faster than ever, but imagine what will be possible when they can run locally in your browser! 🤯 Well, with 🤗 Transformers.js, you can do just that! Here's Zyphra's new ZR1 model running at over 100 tokens/second on WebGPU! ⚡️ Giving models access to browser APIs (like File System, Screen Capture, and more) could unlock an entirely new class of web experiences that are personalized, interactive, and run locally in a secure, sandboxed environment. For now, try out the demo! 👇 webml-community/Zyphra-ZR1-WebGPU See translation</description><pubDate>Sat, 19 Apr 2025 05:21:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Xenova/811708183292240</guid></item><item><title>Wan2.1-FLF2V🎥  a 14B start-end frame video generation model just released by Alibaba_Wan🔥</title><link>https://huggingface.co/posts/AdinaY/926684469376880</link><description>Wan2.1-FLF2V🎥 a 14B start-end frame video generation model just released by Alibaba_Wan🔥 Wan-AI/Wan2.1-FLF2V-14B-720P ✨ Give it two images (start &amp; end), it generates a smooth, high-quality video in between. ✨ Apache 2.0 licensed ✨ Built on DiT + Flow Matching See translation</description><pubDate>Sat, 19 Apr 2025 05:21:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/926684469376880</guid></item><item><title>Introducing BioClinicalBERT-Triage: A Medical Triage Classification Model</title><link>https://huggingface.co/posts/VolodymyrPugachov/428721082134198</link><description>Introducing BioClinicalBERT-Triage: A Medical Triage Classification Model I'm excited to share my latest project: a fine-tuned model for medical triage classification! What is BioClinicalBERT-Triage? BioClinicalBERT-Triage is a specialized model that classifies patient-reported symptoms into appropriate triage categories. Built on the foundation of emilyalsentzer/Bio_ClinicalBERT, this model helps healthcare providers prioritize patient care by analyzing symptom descriptions and medical history. Why I Built This As healthcare systems face increasing demands, efficient triage becomes crucial. This model aims to support healthcare professionals in quickly assessing the urgency of medical situations, particularly in telehealth and high-volume settings. Model Performance The model was trained on 42,513 medical symptom descriptions, using an 80:20 train/test split. After 3 epochs of training, the model achieved: Final training loss: 0.3246 Processing speed: 13.99 samples/second The loss...</description><pubDate>Sat, 19 Apr 2025 05:21:56 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/VolodymyrPugachov/428721082134198</guid></item></channel></rss>