<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>NEW: Real-time conversational AI models can now run 100% locally in your browser! ü§Ø</title><link>https://huggingface.co/posts/Xenova/927328273503233</link><description>NEW: Real-time conversational AI models can now run 100% locally in your browser! ü§Ø üîê Privacy by design (no data leaves your device) üí∞ Completely free... forever üì¶ Zero installation required, just visit a website ‚ö°Ô∏è Blazingly-fast WebGPU-accelerated inference Try it out: webml-community/conversational-webgpu For those interested, here's how it works: - Silero VAD for voice activity detection - Whisper for speech recognition - SmolLM2-1.7B for text generation - Kokoro for text to speech Powered by Transformers.js and ONNX Runtime Web! ü§ó I hope you like it! See translation</description><pubDate>Fri, 06 Jun 2025 09:26:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Xenova/927328273503233</guid></item><item><title>We just dropped SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics!</title><link>https://huggingface.co/posts/danaaubakirova/558502564618988</link><description>We just dropped SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics! check out the blog: https://huggingface.co/blog/smolvla read the technical report: SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics (2506.01844) access the model weights: lerobot/smolvla_base See translation</description><pubDate>Fri, 06 Jun 2025 09:26:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danaaubakirova/558502564618988</guid></item><item><title>Hi everyone, we‚Äôve got big news! Starting today, all Langfuse product features are available as free OSS (MIT license).</title><link>https://huggingface.co/posts/MJannik/975422002507458</link><description>Hi everyone, we‚Äôve got big news! Starting today, all Langfuse product features are available as free OSS (MIT license). You can now upgrade your self-hosted Langfuse to access features like: - Managed LLM-as-a-Judge evaluations - Annotation queues - Prompt experiments - LLM playground We‚Äôre incredibly grateful for the support of this amazing community and can‚Äôt wait to hear your feedback on the new features! More on this change here: https://langfuse.com/blog/2025-06-04-open-sourcing-langfuse-product See translation</description><pubDate>Fri, 06 Jun 2025 09:26:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MJannik/975422002507458</guid></item><item><title>We have been working on a project called</title><link>https://huggingface.co/posts/danieldk/385505075920135</link><description>We have been working on a project called kernels . kernels makes it possible to load compute kernels directly from the Hub! üöÄ We plan to give kernels a more proper introduction soon. But for those who have been following along, we are happy to announce a new release: - New layer API with torch.compile support. - Experimental support for loading Apple Silicon Metal ü§ò Kernels. - Generate wheels from Hub kernels for legacy deployments. Full release notes here: https://github.com/huggingface/kernels/releases/tag/v0.6.0 See translation</description><pubDate>Fri, 06 Jun 2025 09:26:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danieldk/385505075920135</guid></item><item><title>Past week was insanely packed for open AI! üò±</title><link>https://huggingface.co/posts/merve/599865137438975</link><description>Past week was insanely packed for open AI! üò± Luckily we picked some highlights for you ‚ù§Ô∏è lfg! üí¨ LLMs/VLMs &gt; Deepseek üê≥ released deepseek-ai/DeepSeek-R1-0528 , 38B model, only 0.2 and 1.4 points behind o3 in AIME 24/25 ü§Ø they also released an 8B distilled version based on Qwen3 (OS) deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d &gt; Xiaomi released MiMo-7B-RL (LLM for code and math) and MiMo-VL-7B-RL (VLM for visual reasoning, GUI agentic task and general use) (OS) üòç XiaomiMiMo/mimo-vl-68382ccacc7c2875500cd212 &gt; NVIDIA released , new reasoning model nvidia/Nemotron-Research-Reasoning-Qwen-1.5B &gt; DS: MiniMax released https://huggingface.co/MiniMaxAI/SynLogic , new 49k logical reasoning examples across 35 tasks including solving cipher, sudoku and more! üñºÔ∏è Image/Video Generation &gt; tencent released tencent/HunyuanPortrait , a new model for consistent portrait generation with SVD Research license. They also released tencent/HunyuanVideo-Avatar , audio driven avatar generation (OS) &gt;...</description><pubDate>Fri, 06 Jun 2025 09:26:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/599865137438975</guid></item><item><title>Yesterday was the day of vision language action models (VLAs)!</title><link>https://huggingface.co/posts/merve/820895577634325</link><description>Yesterday was the day of vision language action models (VLAs)! &gt; SmolVLA: open-source small VLA for robotics by Hugging Face LeRobot team ü§ñ Blog: https://huggingface.co/blog/smolvla Model: lerobot/smolvla_base &gt; Holo-1: 3B &amp; 7B web/computer use agentic VLAs by H Company üíª Model family: Hcompany/holo1-683dd1eece7eb077b96d0cbd Demo: https://huggingface.co/spaces/multimodalart/Holo1 Blog: https://huggingface.co/blog/Hcompany/holo1 super exciting times!! See translation</description><pubDate>Fri, 06 Jun 2025 09:26:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/820895577634325</guid></item><item><title>Qwen2.5-Omni is soooo good that people build multimodal reasoning models off of it ü•π</title><link>https://huggingface.co/posts/merve/361903268457703</link><description>Qwen2.5-Omni is soooo good that people build multimodal reasoning models off of it ü•π &gt; KE-Team/Ke-Omni-R-3B is open-source audio reasoning model sota on average of benchmarks, based on Qwen/Qwen2.5-Omni-3B üó£Ô∏è &gt; Haoz0206/Omni-R1 is a video reasoning model with pixel level grounding (see below) and it's super competitive ‚èØÔ∏è based on Qwen/Qwen2.5-Omni-7B See translation</description><pubDate>Fri, 06 Jun 2025 09:26:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/361903268457703</guid></item><item><title>OpenAudio S1-mini üîä a new OPEN multilingual TTS model trained on 2M+ hours of data, by FishAudio</title><link>https://huggingface.co/posts/AdinaY/854108171347548</link><description>OpenAudio S1-mini üîä a new OPEN multilingual TTS model trained on 2M+ hours of data, by FishAudio fishaudio/openaudio-s1-mini ‚ú® Supports 14 languages ‚ú® 50+ emotions &amp; tones ‚ú® RLHF-optimized ‚ú® Special effects: laughing, crying, shouting, etc. See translation</description><pubDate>Fri, 06 Jun 2025 09:26:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/854108171347548</guid></item><item><title>If you didn't yet, you should read the technical report for SmolVLA, published yesterday by the Hugging Face robotics team!</title><link>https://huggingface.co/posts/m-ric/239772998134713</link><description>If you didn't yet, you should read the technical report for SmolVLA, published yesterday by the Hugging Face robotics team! ‚û°Ô∏è Amongst other ideas, it introduces "Async inference" to boost their robot actions. Robots have a problem: performing the actions takes time (Unlike agents where action executions are near-instant!) Most often, robots wait until they've finished performing actions to start thinking about hte next steps. This is a huge latency cost! So the team decided to have the PolicyServer (aka the"thinking" part) restart early : instead of waiting for the n observations they just sent to be completed, they gather the observations after k &lt; n steps, and start preparing the next actions based on that while the steps are running until n, to directly send their next steps. ‚û°Ô∏è This boosted robot throughput by ~30%! (nearly 2√ó tasks per time window). gg @ cadene and team! üëè Report here: SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics (2506.01844)...</description><pubDate>Fri, 06 Jun 2025 09:26:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/239772998134713</guid></item><item><title>Agents &amp; MCP Hackathon Day 2</title><link>https://huggingface.co/posts/azettl/543079037838951</link><description>Agents &amp; MCP Hackathon Day 2 Again, a short night, but here are some updates from my Hackathon projects before starting night #3. I managed to get the first version of both submissions (custom Gradio component and MCP server) online! You can check the roundtable MCP where multiple AIs discuss your question and try to reach consensus: azettl/consilium_mcp . The Gradio component is here: azettl/gradio_consilium_roundtable . I placed my API keys in the env variables, so you can test without needing your own keys, but I will remove them soon as I did not find a limit setting in Sambanova. Still, you can check them by adding your own keys in the config tab. Looking forward to your feedback, there are still many days I can and will improve this. See translation</description><pubDate>Fri, 06 Jun 2025 09:26:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/azettl/543079037838951</guid></item></channel></rss>