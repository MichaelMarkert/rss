<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board ‚Äî including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. ü§óüî•</title><link>https://huggingface.co/posts/prithivMLmods/844227545389355</link><description>The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board ‚Äî including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. ü§óüî• ‚ö° Space / App: prithivMLmods/Qwen3-VL-HF-Demo The model‚Äôs demo supports a wide range of tasks, including; Image Inference, Video Inference, PDF Inference, Image Captioning (VLA), GIF Inference. ‚ö° Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Thanks for granting the blazing-fast Zero GPU access, @ merve üôè ‚ö° Other Pages &gt; Github: https://github.com/prithivsakthiur/qwen3-vl-hf-demo &gt; Multimodal VLMs July'25 : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 &gt; VL caption ‚Äî &lt; Sep 15 ‚Äô25 : prithivMLmods/vl-caption-sep-15-25-68c7f6d737985c63c13e2391 &gt; Multimodal...</description><pubDate>Mon, 13 Oct 2025 09:28:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/844227545389355</guid></item><item><title>Ovi is Local Version of VEO 3 &amp; SORA 2 - The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer on Windows even with a 6GB GPUs - Full Tutorial for Windows, RunPod and Massed Compute - Gradio App &gt;</title><link>https://huggingface.co/posts/MonsterMMORPG/350754844731753</link><description>Ovi is Local Version of VEO 3 &amp; SORA 2 - The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer on Windows even with a 6GB GPUs - Full Tutorial for Windows, RunPod and Massed Compute - Gradio App &gt; https://youtu.be/T00VmkMQRPQ Tutorial : https://youtu.be/T00VmkMQRPQ Forget waiting lists and expensive APIs. The era of closed-off, corporate-controlled AI video generation is soon over. This is Ovi : The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer‚Äîeven with a 6GB GPU! This isn't just a demo; it's a full, step-by-step revolution. Tutorial Info In this ultimate A-Z guide, I'll show you EVERYTHING you need to know to install and master this Sora 2 and VEO3 like AI. We'll go from zero to generating incredible talking videos from text or a single image. üî• In This Tutorial, You Will Learn To: üéì Master the Ultimate SORA 2 and VEO 3...</description><pubDate>Mon, 13 Oct 2025 09:28:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/350754844731753</guid></item><item><title>Arm will be @ PyTorch Conference, Join Us!</title><link>https://huggingface.co/posts/sondhiArm/422203128655094</link><description>Arm will be @ PyTorch Conference, Join Us! Join us on site October 22-23 to see how Arm empowers developers to build and deploy AI applications with ease using PyTorch and ExecuTorch. Learn about the latest AI technologies from Arm and our ecosystem while expanding your professional network alongside like-minded AI engineers. Learn more here: https://huggingface.co/blog/Arm/arm-at-pytorch-conference See translation</description><pubDate>Mon, 13 Oct 2025 09:28:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sondhiArm/422203128655094</guid></item><item><title>üöÄ No invite. No watermark. Just pure AI magic.</title><link>https://huggingface.co/posts/Ethank01/902315574816069</link><description>üöÄ No invite. No watermark. Just pure AI magic. Experience Sora 2 on iMini ‚Äî free for members üëâ https://imini.com/ See translation</description><pubDate>Mon, 13 Oct 2025 09:28:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ethank01/902315574816069</guid></item><item><title>Benchmarking xLLM and Specialized Language Models: New Approach &amp; Results</title><link>https://huggingface.co/posts/vincentg64/231735241059410</link><description>Benchmarking xLLM and Specialized Language Models: New Approach &amp; Results https://mltblog.com/4nzaKUb Standard benchmarking techniques using LLM as a judge have strong limitations. First it creates a circular loop and reflects the flaws present in the AI judges. Then, the perceived quality depends on the end user: an enterprise LLM appeals to professionals and business people, while a generic one appeals to laymen. The two have almost opposite criteria to assess the value. Finally, benchmarking metrics currently in use fail to capture many of the unique features of specialized LLMs, such as exhaustivity, or the quality of the relevancy and trustworthiness scores attached to each element in the response. In fact, besides xLLM, very few if any LLMs display such scores to the user. I now discuss these points, as well as the choice of test prompts, and preliminary results about xLLM, compared to others. -- Structured output vs standard response -- A peculiarity of xLLM is that if offers...</description><pubDate>Mon, 13 Oct 2025 09:28:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/vincentg64/231735241059410</guid></item><item><title>Super nice intro to fine-tuning with TRL, just dropped by</title><link>https://huggingface.co/posts/sergiopaniego/617301570898525</link><description>Super nice intro to fine-tuning with TRL, just dropped by @ google (runs free on Colab)! They use SFT + QLoRA to fine-tune the tiny Gemma 3 270M model for emoji generation Here‚Äôs what the fine-tuned model generates for the prompt: ‚ÄúI'm learning to tweet‚Äù ‚Üí üê¶üó£üíª Colab: https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb Try it out: google/emoji-gemma Learn more: https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/ See translation</description><pubDate>Mon, 13 Oct 2025 09:28:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/617301570898525</guid></item><item><title>STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! üö®</title><link>https://huggingface.co/posts/m-ric/175050207181959</link><description>STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! üö® A lone scientist just proposed Tiny Recursive Model (TRM), and it is literally the most impressive model that I've seen this year. ‚û°Ô∏è Tiny Recursive Model is 7M parameters ‚û°Ô∏è On ARC-AGI, it beats flagship models like Gemini-2.5-pro Consider how wild this is: Gemini-2.5-pro must be over 10,000x bigger and had 1,000 as many authors üòÇ (Alexia is alone on the paper) What's this sorcery? In short: it's a very tiny Transformers, but it loops over itself at two different frequencies, updating two latent variables: one for the proposed answer and one for the reasoning. @ AlexiaJM started from the paper Hierarchical Reasoning Model, published a few months ago, that already showed breakthrough improvement on AGI for its small size (27M) Hierarchical Reasoning Model had introduced one main feature: üîé Deep supervision In their model, one part (here one layer) would run at high frequency, and...</description><pubDate>Mon, 13 Oct 2025 09:28:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/175050207181959</guid></item><item><title>9 Powerful AI Video Generation Tools</title><link>https://huggingface.co/posts/Kseniase/543365627154110</link><description>9 Powerful AI Video Generation Tools Since Sora 2 is on fire these weeks, reminding us what high-quality video generation should look like, we decided you really need this list of video generation tools ‚Äì great alternatives or complements to it. 1. Sora 2 ‚Üí https://openai.com/sora/ It needs no introduction, but this OpenAI‚Äôs text-to-video model produces short, ultra-realistic clips across styles (cinematic, photorealistic, animated, etc.) with synced audio 2. Google Veo 3 (Gemini Video Generation) ‚Üí https://aistudio.google.com/models/veo-3 Part of Gemini AI. Generates 8-second high-fidelity videos from text or images with native sound: background soundtracks and realistic voices with near-perfect lip sync 3. Runway (Gen-4 by Runway ML) ‚Üí https://runwayml.com/ Text, image, or video-to-video generation with advanced editing like changing lighting, weather, camera angles or replacing objects. Popular in AI filmmaking 4. Pika Labs ‚Üí https://pollo.ai/m/pika-ai Provides creative, often...</description><pubDate>Mon, 13 Oct 2025 09:28:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/543365627154110</guid></item><item><title>How to compress long code context? üìö</title><link>https://huggingface.co/posts/YerbaPage/639392890035292</link><description>How to compress long code context? üìö Check out our LongCodeZip! Paper just got accepted to ASE 2025. üî• Code &amp; Demo: https://github.com/YerbaPage/LongCodeZip Paper: LongCodeZip: Compress Long Context for Code Language Models (2510.00446) See translation</description><pubDate>Mon, 13 Oct 2025 09:28:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/639392890035292</guid></item><item><title>The future is arriving too fast not to use programmatic discovery and replication.</title><link>https://huggingface.co/posts/salma-remyx/845418366065483</link><description>The future is arriving too fast not to use programmatic discovery and replication. Search arXiv ‚Üí Execute in 30 seconds with pre-built Docker environments Check out our latest integration with AG2 to accelerate your discovery loop. As easy as: from remyxai.client.search import SearchClient from autogen.coding import RemyxCodeExecutor # Search by topic papers = SearchClient().search( "data synthesis strategies" , has_docker= True , # Only papers with pre-built environments limit= 10 ) executor = RemyxCodeExecutor(arxiv_id=papers[ 0 ].arxiv_id) remyx_executor.explore( goal= "Run a test with my model remyxai/SpaceThinker-Qwen2.5VL-3B" , interactive= False # Automated exploration ) Tutorial: https://github.com/ag2ai/ag2/blob/4c6954e3959fe672980191f264e30d451bc23554/notebook/agentchat_remyx_executor.ipynb PR: https://github.com/ag2ai/ag2/pull/2141 See translation</description><pubDate>Mon, 13 Oct 2025 09:28:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/salma-remyx/845418366065483</guid></item></channel></rss>