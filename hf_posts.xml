<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Inspired by the heroes of day zero quants (</title><link>https://huggingface.co/posts/marksverdhei/460500590246249</link><description>Inspired by the heroes of day zero quants ( @ TheBloke @ danielhanchen @ shimmyshimmer @ bartowski ), I decided to join the race by releasing the first FP8 quant of glm-4.7-flash! Not as easy as i expected, but I'm happy i was still able to have it working within a few hours after the original model was released! Interested in feedback if anyone wants to try it out! marksverdhei/GLM-4.7-Flash-FP8 Note: If my PR to vLLM isn't merged yet you might have to use my fork. Cheers! ü§ó See translation</description><pubDate>Thu, 22 Jan 2026 09:37:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/marksverdhei/460500590246249</guid></item><item><title>Run GLM-4.7-Flash locally on your device with 24GB RAM!üî•</title><link>https://huggingface.co/posts/danielhanchen/143027024579647</link><description>Run GLM-4.7-Flash locally on your device with 24GB RAM!üî• It's the best performing 30B model on SWE-Bench and GPQA. With 200K context, it excels at coding, agents, chat &amp; reasoning. GGUF: unsloth/GLM-4.7-Flash-GGUF Guide: https://unsloth.ai/docs/models/glm-4.7-flash See translation</description><pubDate>Thu, 22 Jan 2026 09:37:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/143027024579647</guid></item><item><title>So, Koreans are also doing great progress behind Chinese,</title><link>https://huggingface.co/posts/Ujjwal-Tyagi/869541950904709</link><description>So, Koreans are also doing great progress behind Chinese, Their two open source ai models that are actually good in coding. upstage/Solar-Open-100B skt/A.X-K1 See translation</description><pubDate>Thu, 22 Jan 2026 09:37:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ujjwal-Tyagi/869541950904709</guid></item><item><title>Google published the paper. I shipped the code. üöÄ</title><link>https://huggingface.co/posts/hassenhamdi/338157395556750</link><description>Google published the paper. I shipped the code. üöÄ DeepMind just released PACEvolve (Progress-Aware Consistent Evolution), a massive overhaul of the AlphaEvolve framework. It solves the critical issues of "Context Pollution" and "Mode Collapse" that have historically crippled evolutionary coding agents. But there was no public implementation. So I built one. Introducing OpenPACEvolve: A fully open-source, production-grade implementation of the PACEvolve framework. üõ† I engineered this framework solo, but I wasn't working alone. I orchestrated a custom coding agents powered by Claude Opus 4.5 as Engineer and Gemini Pro 3 Preview ensuring fiedelity and quallty. By leveraging these SOTA models, I was able to translate complex theoretical research into functional, modular Python architecture in record time. This is what the future of AI engineering looks like: Human architectural oversight + AI velocity. üß† What OpenPACEvolve Solves: Unlike standard agents that get "stuck" in loops, this...</description><pubDate>Thu, 22 Jan 2026 09:37:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hassenhamdi/338157395556750</guid></item><item><title>Our engineer Alan from</title><link>https://huggingface.co/posts/branikita/663180639810394</link><description>Our engineer Alan from https://robonine.com/ (Educational Robotics) integrated Feetech STS3250 and STS3215 servo motors into the prototype and completed the first test run of a 6-DOF semi-SCARA manipulator. During motion, the structure demonstrates high stiffness with no visible backlash or mechanical play. The kinematic chain remains stable throughout the test trajectory, confirming the rigidity of the mechanical design and joint assembly. The next stage includes full assembly with all actuators operating in backlash compensation mode, followed by quantitative measurement of positioning accuracy and repeatability. See translation</description><pubDate>Thu, 22 Jan 2026 09:37:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/663180639810394</guid></item><item><title>GLM-4.7-Flash is fast, good and cheap.</title><link>https://huggingface.co/posts/mitkox/833172754531021</link><description>GLM-4.7-Flash is fast, good and cheap. 3,074 tokens/sec peak at 200k tokens context window on my desktop PC. Works with Claude Code and opencode for hours. No errors, drop-in replacement of the Anthropic cloud AI. MIT licensed, open weights, free for commercial use and modifications. Supports speculative decoding using MTP, which is highly effective in mitigating latency. Great for on device AI coding as AWQ 4bit at 18.5 GB. Hybrid inference on a single consumer GPU + CPU RAM. See translation</description><pubDate>Thu, 22 Jan 2026 09:37:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/833172754531021</guid></item><item><title>Check out Orpheus Karaoke! Turn any MIDI into a unique Karaoke MIDI!</title><link>https://huggingface.co/posts/projectlosangeles/732365874551092</link><description>Check out Orpheus Karaoke! Turn any MIDI into a unique Karaoke MIDI! projectlosangeles/Orpheus-Karaoke See translation</description><pubDate>Thu, 22 Jan 2026 09:37:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/projectlosangeles/732365874551092</guid></item><item><title>üèõÔ∏è Google Code Archive Dataset -</title><link>https://huggingface.co/posts/nyuuzyou/331224318760046</link><description>üèõÔ∏è Google Code Archive Dataset - nyuuzyou/google-code-archive Expanding beyond the modern code series, this release presents a massive historical snapshot from the Google Code Archive. This dataset captures the open-source landscape from 2006 to 2016, offering a unique time capsule of software development patterns during the era before GitHub's dominance. Key Stats: - 65,825,565 files from 488,618 repositories - 47 GB compressed Parquet storage - 454 programming languages (Heavily featuring Java, PHP, and C++) - Extensive quality filtering (excluding vendor code and build artifacts) - Rich historical metadata: original repo names, file paths, and era-specific licenses This is one of those releases that I'm most interested in getting feedback on. Would you like to see more old code datasets? See translation</description><pubDate>Thu, 22 Jan 2026 09:37:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nyuuzyou/331224318760046</guid></item><item><title>DeepSeek R1 dropped one year ago üê≥ and a lot has changed.</title><link>https://huggingface.co/posts/AdinaY/119145219843817</link><description>DeepSeek R1 dropped one year ago üê≥ and a lot has changed. With @ irenesolaiman , we‚Äôre launching a blog series about how that moment reshaped AI + open source in 2025, starting with strategic shifts and the explosion of new open models in China! https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment See translation</description><pubDate>Thu, 22 Jan 2026 09:37:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/119145219843817</guid></item><item><title>Two things to know right before starting:</title><link>https://huggingface.co/posts/paulpham157/197160445199465</link><description>Two things to know right before starting: - Learn Git. Git is a great versioning tool, even when working alone. It's also essential when working in a team. Don't make excuses that you only do DL and can't do software development. üòû Don't create files like: main_backup_1.py main_backup_2.py main_backup_3.py anymore... (It sounds ridiculous, but I've actually seen some people do that... weren't students.) - Try to keep everything stable. Imagine encountering errors during a demo. Keep the code clean so it runs smoothly and is maintainable. Always minimize DevOps steps to ensure quick reboot (this can be covered by some platforms; thanks to Hugging Face for making it easy and providing a basic infrastructure that most people can access almost for free). ü§§ See translation</description><pubDate>Thu, 22 Jan 2026 09:37:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/paulpham157/197160445199465</guid></item></channel></rss>