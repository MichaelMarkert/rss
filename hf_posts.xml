<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Run OpenAI's new gpt-oss models locally with Unsloth GGUFs! 🔥🦥</title><link>https://huggingface.co/posts/danielhanchen/446160279272944</link><description>Run OpenAI's new gpt-oss models locally with Unsloth GGUFs! 🔥🦥 20b GGUF: unsloth/gpt-oss-20b-GGUF 120b GGUF: unsloth/gpt-oss-120b-GGUF Model will run on 14GB RAM for 20b and 66GB for 120b. See translation</description><pubDate>Thu, 07 Aug 2025 17:26:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/446160279272944</guid></item><item><title>Finaly OpenAI is open to share open-source models after GPT2-2019.</title><link>https://huggingface.co/posts/ImranzamanML/667361724381561</link><description>Finaly OpenAI is open to share open-source models after GPT2-2019. gpt-oss-120b gpt-oss-20b openai/gpt-oss-120b #AI #GPT #LLM #Openai See translation</description><pubDate>Thu, 07 Aug 2025 17:26:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ImranzamanML/667361724381561</guid></item><item><title>😎 I just published Sentence Transformers v5.1.0, and it's a big one. 2x-3x speedups of SparseEncoder models via ONNX and/or OpenVINO backends, easier distillation data preparation with hard negatives mining, and more:</title><link>https://huggingface.co/posts/tomaarsen/619466658423382</link><description>😎 I just published Sentence Transformers v5.1.0, and it's a big one. 2x-3x speedups of SparseEncoder models via ONNX and/or OpenVINO backends, easier distillation data preparation with hard negatives mining, and more: 1️⃣ Faster ONNX and OpenVINO backends for SparseEncoder models Usage is as simple as backend="onnx" or backend="openvino" when initializing a SparseEncoder to get started, but I also included utility functions for optimization, dynamic quantization, and static quantization, plus benchmarks. 2️⃣ New n-tuple-scores output format from mine_hard_negatives This new output format is immediately compatible with the MarginMSELoss and SparseMarginMSELoss for training SentenceTransformer, CrossEncoder, and SparseEncoder losses. 3️⃣ Gathering across devices When doing multi-GPU training using a loss that has in-batch negatives (e.g. MultipleNegativesRankingLoss), you can now use gather_across_devices=True to load in-batch negatives from the other devices too! Essentially a free...</description><pubDate>Thu, 07 Aug 2025 17:26:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tomaarsen/619466658423382</guid></item><item><title>I'm building a mmo-ish RPG with LLM agents that can (hopefully) complete player tasks, as an experiment. I've started documenting my progress here:</title><link>https://huggingface.co/posts/neph1/202844482773668</link><description>I'm building a mmo-ish RPG with LLM agents that can (hopefully) complete player tasks, as an experiment. I've started documenting my progress here: https://huggingface.co/blog/neph1/rpg-llm-agents Let me know if you want to see more of it. See translation</description><pubDate>Thu, 07 Aug 2025 17:26:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/neph1/202844482773668</guid></item><item><title>🚀 GPT-OSS 120B &amp; 20B - Use Both Models in One Space!</title><link>https://huggingface.co/posts/openfree/275314685023370</link><description>🚀 GPT-OSS 120B &amp; 20B - Use Both Models in One Space! openfree/OpenAI-gpt-oss VIDraft/gpt-oss-RAG 🎯 Two Models, One Space! GPT-OSS hit #1 on HF just 2 hours after release! 🏆 Now you can use both models conveniently in a single space. 📋 Model Selection Made Easy! Just pick from the dropdown ✅ ├── GPT-OSS-120B (Complex tasks) └── GPT-OSS-20B (Quick chats) 💫 How to Use (Takes 30 seconds!) Sign in → With your HF account 🔐 Select model → Choose what you need 📌 Apply → Click! ⚡ Start chatting → That's it! 💬 🌈 Perfect For: 120B → Deep analysis, professional work 🧠 20B → Fast responses, casual conversations ⚡ No installation needed - just use it in your browser! 🌐 ✨ Special Features 🎨 Beautiful gradient UI 🌙 Dark mode support 🔄 Real-time model switching 🆓 Completely free! 👉 Try it now! It's really that simple! #GPT-OSS #HuggingFace #FreeAI #EasyToUse See translation</description><pubDate>Thu, 07 Aug 2025 17:26:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/275314685023370</guid></item><item><title>Well, it took just 2 hours for</title><link>https://huggingface.co/posts/fdaudens/331415397781817</link><description>Well, it took just 2 hours for openai/gpt-oss-120b to hit #1 on Hugging Face. Don’t remember seeing anything rise that fast! See translation</description><pubDate>Thu, 07 Aug 2025 17:26:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/331415397781817</guid></item><item><title>Qwen Image – The Latest Image Generation Model🔥</title><link>https://huggingface.co/posts/prithivMLmods/372876915549424</link><description>Qwen Image – The Latest Image Generation Model🔥 Below are some samples generated using the Qwen Image Diffusion Model. Qwen-Image, a 20B MMDiT model for next-generation text-to-image generation, preserves typographic details, layout coherence, and contextual harmony with stunning accuracy. It is especially strong at creating stunning graphic posters with native text. The model is now open-source. [ 𝚀𝚠𝚎𝚗-𝙸𝚖𝚊𝚐𝚎 : Qwen/Qwen-Image ] ⤷ Try the Qwen Image demo here: prithivMLmods/Qwen-Image-Diffusion ⤷ Qwen-Image Technical Report : Qwen-Image Technical Report (2508.02324) ⤷ Qwen Image [GitHub] : https://github.com/QwenLM/Qwen-Image Even more impressively, it demonstrates a strong ability to understand images. The model supports a wide range of vision-related tasks such as object detection, semantic segmentation, depth and edge (Canny) estimation, novel view synthesis, and image super-resolution. While each task is technically distinct, they can all be viewed as advanced forms of...</description><pubDate>Thu, 07 Aug 2025 17:26:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/372876915549424</guid></item><item><title>Announcing Artificial Analysis Long Context Reasoning (AA-LCR), a new benchmark to evaluate long context performance through testing reasoning capabilities across multiple long documents (~100k tokens)</title><link>https://huggingface.co/posts/georgewritescode/981174566402338</link><description>Announcing Artificial Analysis Long Context Reasoning (AA-LCR), a new benchmark to evaluate long context performance through testing reasoning capabilities across multiple long documents (~100k tokens) The focus of AA-LCR is to replicate real knowledge work and reasoning tasks, testing capability critical to modern AI applications spanning document analysis, codebase understanding, and complex multi-step workflows. AA-LCR is 100 hard text-based questions that require reasoning across multiple real-world documents that represent ~100k input tokens. Questions are designed so answers cannot be directly found but must be reasoned from multiple information sources, with human testing verifying that each question requires genuine inference rather than retrieval. Key takeaways: ➤ Today’s leading models achieve ~70% accuracy: the top three places go to OpenAI o3 (69%), xAI Grok 4 (68%) and Qwen3 235B 2507 Thinking (67%) ➤👀 We also already have gpt-oss results! 120B performs close to o4-mini...</description><pubDate>Thu, 07 Aug 2025 17:26:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/georgewritescode/981174566402338</guid></item><item><title>OpenAI's open models are out! 💃</title><link>https://huggingface.co/posts/sergiopaniego/625485045816019</link><description>OpenAI's open models are out! 💃 Try: https://www.gpt-oss.com/ Learn: https://huggingface.co/blog/welcome-openai-gpt-oss See translation</description><pubDate>Thu, 07 Aug 2025 17:26:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/625485045816019</guid></item><item><title>I run Claude Code with Qwen3 Coder Flash locally on my MacBook Air. It works offline, zero cloud, zero internet, zero EU AI Act anxiety. No limit with all tokens on the house.</title><link>https://huggingface.co/posts/mitkox/378542221866585</link><description>I run Claude Code with Qwen3 Coder Flash locally on my MacBook Air. It works offline, zero cloud, zero internet, zero EU AI Act anxiety. No limit with all tokens on the house. It’s not great, not terrible- adequate performance for an on device AI agent chewing through code on a 1.24 kg laptop. I wrote an interpreter to broker peace between Claude Code and my local AI runtime. Make sure you own your AI. AI in the cloud is not aligned with you; it’s aligned with the company that owns it. See translation</description><pubDate>Thu, 07 Aug 2025 17:26:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mitkox/378542221866585</guid></item></channel></rss>