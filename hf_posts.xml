<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! ğŸš¨</title><link>https://huggingface.co/posts/m-ric/175050207181959</link><description>STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! ğŸš¨ A lone scientist just proposed Tiny Recursive Model (TRM), and it is literally the most impressive model that I've seen this year. â¡ï¸ Tiny Recursive Model is 7M parameters â¡ï¸ On ARC-AGI, it beats flagship models like Gemini-2.5-pro Consider how wild this is: Gemini-2.5-pro must be over 10,000x bigger and had 1,000 as many authors ğŸ˜‚ (Alexia is alone on the paper) What's this sorcery? In short: it's a very tiny Transformers, but it loops over itself at two different frequencies, updating two latent variables: one for the proposed answer and one for the reasoning. @ AlexiaJM started from the paper Hierarchical Reasoning Model, published a few months ago, that already showed breakthrough improvement on AGI for its small size (27M) Hierarchical Reasoning Model had introduced one main feature: ğŸ” Deep supervision In their model, one part (here one layer) would run at high frequency, and...</description><pubDate>Thu, 09 Oct 2025 17:20:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/175050207181959</guid></item><item><title>ğŸ¤– What if building your own robot arm costs less than Â£220?</title><link>https://huggingface.co/posts/hba123/315319549896319</link><description>ğŸ¤– What if building your own robot arm costs less than Â£220? For years, robotics has been locked behind high prices and complex systems. So we decided to change that. Today, weâ€™re open-sourcing Ark-Bot â€” a fully 3D-printed, 6-DOF robot arm that works seamlessly with our Python robotics library, Ark. And yesâ€¦ Itâ€™s only Â£215.86 to build. ğŸ§ ArkBot Specs ğŸ§  1ï¸âƒ£ Reach: 1 meter 2ï¸âƒ£ Weight: 2.6 kg 3ï¸âƒ£ Payload: 1.8 kg ğŸ’ª 4ï¸âƒ£ DOF: 6 5ï¸âƒ£ Input Voltage: DC 12V ğŸ¤ŸFully 3D-printable &amp; open-source ğŸ¤ŸIntegrated with Ark â€” no ROS required ğŸ“¹ Weâ€™ve also released a video showing the full assembly process â€” because robotics should be something everyone can learn, build, and improve on. ğŸ‘©â€ğŸ“ With Ark-Bot, anyone â€” from students to AI researchers â€” can experiment with embodied AI, robot learning, and control algorithms on real hardware, affordably. If you could control a 1-meter robot arm from your laptop for under Â£220â€¦ ğŸ‘‰ What would you build first? ğŸ”—https://github.com/Robotics-Ark/ark_bot ğŸ¥...</description><pubDate>Thu, 09 Oct 2025 17:20:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/315319549896319</guid></item><item><title>Hi, I just published research paper that's introducing my Reactive Transformer (RxT) architecture. I would be grateful if you could check it and upvote on HuggingFace Daily Papers -</title><link>https://huggingface.co/posts/AdamF92/854077189361039</link><description>Hi, I just published research paper that's introducing my Reactive Transformer (RxT) architecture. I would be grateful if you could check it and upvote on HuggingFace Daily Papers - Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models (2510.03561) Architecture is based on stateful real-time processing with innovational asynchronous memory update. Instead of reprocessing all the conversation history for each message, it's processing only single query with all the context moved to dedicated memory layers. Memory is updated after generating the answer, so it's not influencing latency - in tests, time to first token was almost the same as generating a single token. It has also better quality/accuracy in multi-turn dialogue than the same size stateless decoder-only model. Initial experiments were small scale (12M to 160M params models trained on simple synthetic datasets), but just now I'm starting training of bigger 270M params model on...</description><pubDate>Thu, 09 Oct 2025 17:20:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdamF92/854077189361039</guid></item><item><title>LiquidAI/LFM2-8B-A1B</title><link>https://huggingface.co/posts/mlabonne/852622328581891</link><description>LiquidAI/LFM2-8B-A1B just dropped! 8.3B params with only 1.5B active/token ğŸš€ &gt; Quality â‰ˆ 3â€“4B dense, yet faster than Qwen3-1.7B &gt; MoE designed to run on phones/laptops (llama.cpp / vLLM) &gt; Pre-trained on 12T tokens â†’ strong math/code/IF See translation</description><pubDate>Thu, 09 Oct 2025 17:20:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/852622328581891</guid></item><item><title>Have built the new Image Studio with the Gemini Image Gen models for the following multiple tasks:</title><link>https://huggingface.co/posts/prithivMLmods/886559259732378</link><description>Have built the new Image Studio with the Gemini Image Gen models for the following multiple tasks: imagen-4.0-fast-generate-001 model for Image Generation (Text-to-Image) and Multi-Image Editing (Image-to-Image), and Draw-to-Image powered by gemini-2.5-flash-image (aka Nano Banana). â­ Gemini-Image-Studio: prithivMLmods/Gemini-Image-Studio (Latest) ğŸ¤ Old-App: prithivMLmods/Nano-Banana-AIO ğŸ¥Š GitHub: https://github.com/prithivsakthiur/gemini-image-studio-hf To proceed, you need to add your Gemini API key. Your API key is stored only for the duration of your session and will be lost when you reload or exit the page. It will not be shared or exposed anywhere. See translation</description><pubDate>Thu, 09 Oct 2025 17:20:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/886559259732378</guid></item><item><title>We've just forked LBM to reproduce the LBM eraser results</title><link>https://huggingface.co/posts/piercus/787328298619334</link><description>We've just forked LBM to reproduce the LBM eraser results Our fork : https://github.com/finegrain-ai/LBM LBM paper: LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) LBM relighting demo : jasperai/LBM_relighting See translation</description><pubDate>Thu, 09 Oct 2025 17:20:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/piercus/787328298619334</guid></item><item><title>MLX port of BDH (Baby Dragon Hatchling) is up!</title><link>https://huggingface.co/posts/Severian/804699132791352</link><description>MLX port of BDH (Baby Dragon Hatchling) is up! Iâ€™ve ported the BDH ( https://github.com/pathwaycom/bdh ) model to MLX for Apple Silicon. Itâ€™s a faithful conversion of the PyTorch version: same math, same architecture (byte-level vocab, shared weights across layers, ReLU sparsity, RoPE attention with Q=K), with MLX-friendly APIs and a detailed README explaining the few API-level differences and why results are equivalent. Code, docs, and training script are ready to use. You may need to adjust the training script a bit to fit your own custom dataset. Only tested on M4 so far, but should work perfect for any M1/M2/M3 users out there. Iâ€™m currently training this MLX build on my Internal Knowledge Map (IKM) dataset Severian/Internal-Knowledge-Map Trainingâ€™s underway; expect a day or so before I publish weights. When itâ€™s done, Iâ€™ll upload the checkpoint to Hugging Face for anyone to test. Repo: https://github.com/severian42/BDH-MLX HF model (coming soon): Severian/BDH-MLX If you try it...</description><pubDate>Thu, 09 Oct 2025 17:20:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Severian/804699132791352</guid></item><item><title>At the close of the National HolidayğŸ‡¨ğŸ‡³, Antgroup drops a new SoTA model.</title><link>https://huggingface.co/posts/AdinaY/255762505303069</link><description>At the close of the National HolidayğŸ‡¨ğŸ‡³, Antgroup drops a new SoTA model. Ling-1T ğŸ”¥ the trillion-parameter flagship of the Ling 2.0 series. inclusionAI/Ling-1T âœ¨1T total / 50B active params per token âœ¨20T+ reasoning-dense tokens (Evo-CoT) âœ¨128K context via YaRN âœ¨FP8 training: 15%+ faster, same precision as BF16 âœ¨Hybrid Syntax-Function-Aesthetics reward for front-end &amp; visual generation See translation</description><pubDate>Thu, 09 Oct 2025 17:20:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/255762505303069</guid></item><item><title>I found it excellent and very well done.</title><link>https://huggingface.co/posts/SelmaNajih001/229668569857091</link><description>I found it excellent and very well done. One of the best explanations of embedding I've ever read. Well done, @ hesamation ! Had to share this: hesamation/primer-llm-embedding See translation</description><pubDate>Thu, 09 Oct 2025 17:20:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/SelmaNajih001/229668569857091</guid></item><item><title>Online training methods (e.g., GRPO) require real-time generation, a compute- and memory-heavy bottleneck.</title><link>https://huggingface.co/posts/sergiopaniego/573302448191586</link><description>Online training methods (e.g., GRPO) require real-time generation, a compute- and memory-heavy bottleneck. TRL has built-in vLLM support and in this new recipe, we show how to leverage it for efficient online training. Run on Colab âš¡, scale to multi-GPU/multi-node! ğŸ§‘â€ğŸ³ recipe: https://huggingface.co/learn/cookbook/grpo_vllm_online_training See translation</description><pubDate>Thu, 09 Oct 2025 17:20:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/573302448191586</guid></item></channel></rss>