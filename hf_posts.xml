<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>New family of 1B models just dropped!</title><link>https://huggingface.co/posts/mlabonne/713929804806596</link><description>New family of 1B models just dropped! &gt; LiquidAI/LFM2.5-1.2B-Base : 10T ‚Üí 28T tokens &gt; LiquidAI/LFM2.5-1.2B-Instruct : new large-scale multi-stage RL &gt; LiquidAI/LFM2.5-1.2B-JP : our most polite model &gt; LiquidAI/LFM2.5-VL-1.6B : multi-image multilingual &gt; LiquidAI/LFM2.5-Audio-1.5B : 8x times faster, no quality loss Super proud of this release ü§ó See translation</description><pubDate>Thu, 08 Jan 2026 13:43:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/713929804806596</guid></item><item><title>New Book: No-Blackbox, Secure, Efficient AI and LLM Solutions</title><link>https://huggingface.co/posts/vincentg64/731522720644742</link><description>New Book: No-Blackbox, Secure, Efficient AI and LLM Solutions https://mltblog.com/4aRwvM5 Large language models and modern AI is often presented as technology that needs deep neural networks (DNNs) with billions of Blackbox parameters, expensive and time consuming training, along with GPU farms, yet prone to hallucinations. This book presents alternatives that rely on explainable AI, featuring new algorithms based on radically different technology with trustworthy, auditable, fast, accurate, secure, replicable Enterprise AI. Most of the material is proprietary and made from scratch, showcasing the culmination of decades of research away from standard models to establish a new framework in machine learning and AI technology. I discuss an efficient DNN architecture based on a new type of universal functions in chapter 4, with DNN distillation and protection via watermarking in chapter 5. Then, in chapter 6, I discuss non-DNN alternatives that yield exact interpolation on the training...</description><pubDate>Thu, 08 Jan 2026 13:43:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/vincentg64/731522720644742</guid></item><item><title>üéâ OpenMed 2025 Year in Review: 6 Months of Open Medical AI</title><link>https://huggingface.co/posts/MaziyarPanahi/255552518498714</link><description>üéâ OpenMed 2025 Year in Review: 6 Months of Open Medical AI I'm thrilled to share what the OpenMed community has accomplished since our July 2025 launch! üìä The Numbers 29,700,000 downloads Thank you! üôè - 481 total models (475 medical NER models + 6 fine-tuned LLMs) - 475 medical NER models in [OpenMed]( OpenMed ) organization - 6 fine-tuned LLMs in [openmed-community]( openmed-community ) - 551,800 PyPI downloads of the [openmed package]( https://pypi.org/project/openmed/ ) - 707 followers on HuggingFace (you!) - 97 GitHub stars on the [toolkit repo]( https://github.com/maziyarpanahi/openmed ) üèÜ Top Models by Downloads 1. [OpenMed-NER-PharmaDetect-SuperClinical-434M]( OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-434M ) ‚Äî 147,305 downloads 2. [OpenMed-NER-ChemicalDetect-ElectraMed-33M]( OpenMed/OpenMed-NER-ChemicalDetect-ElectraMed-33M ) ‚Äî 126,785 downloads 3. [OpenMed-NER-BloodCancerDetect-TinyMed-65M]( OpenMed/OpenMed-NER-BloodCancerDetect-TinyMed-65M ) ‚Äî 126,465 downloads üî¨ Model...</description><pubDate>Thu, 08 Jan 2026 13:43:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MaziyarPanahi/255552518498714</guid></item><item><title>We can't build more private AI if we can't measure privacy intelligence.</title><link>https://huggingface.co/posts/MikeDoes/265606187790986</link><description>We can't build more private AI if we can't measure privacy intelligence. That's why we're highlighting the Priv-IQ benchmark, a new, solution-oriented framework for evaluating LLMs on eight key privacy competencies, from visual privacy to knowledge of privacy law. The direct connection to our work is clear: the researchers relied on samples from the Ai4Privacy dataset to build out questions for Privacy Risk Assessment and Multilingual Entity Recognition. This is the power of open-source collaboration. We provide the data building blocks, and researchers construct powerful new evaluation tools on top of them. It's a win-win for the entire ecosystem when we can all benefit from transparent, data-driven benchmarks that help push for better, safer AI. Kudos to Sakib Shahriar and Rozita A. Dara for this important contribution. Read the paper to see the results: https://www.proquest.com/docview/3170854914?pq-origsite=gscholar&amp;fromopenview=true&amp;sourcetype=Scholarly%20Journals #OpenSource...</description><pubDate>Thu, 08 Jan 2026 13:43:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MikeDoes/265606187790986</guid></item><item><title>üéâ Exciting News ‚Äî NVIDIA Cosmos is celebrating its 1st birthday and has hit 5 MILLION downloads! üéâ</title><link>https://huggingface.co/posts/tsungyi/951918573083601</link><description>üéâ Exciting News ‚Äî NVIDIA Cosmos is celebrating its 1st birthday and has hit 5 MILLION downloads! üéâ In just one year, the Cosmos ecosystem has grown rapidly: üß† Cosmos Reason and Cosmos Predict have surpassed 2 MILLION downloads each on @ HuggingFace , topping physical AI leaderboards üîÑ Cosmos Transfer is enabling adaptation across domains and tasks üîÆ Cosmos Cookbook is the go-to hub for recipes from developers and partners like Uber and IntBot. Thank you to our amazing developer community for making this possible. Here's to pushing the boundaries of world foundation models together! üßëüèª‚Äçüç≥Read the Cosmos Cookbook: https://nvda.ws/4qevli8 üìö Explore Models &amp; Datasets: https://huggingface.co/collections/nvidia/nvidia-cosmos-2 See translation</description><pubDate>Thu, 08 Jan 2026 13:43:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tsungyi/951918573083601</guid></item><item><title>Assertions are  sanity checks for your API.</title><link>https://huggingface.co/posts/dhruv3006/531790411219507</link><description>Assertions are sanity checks for your API. In Voiden, assertions are reusable blocks . How it works : 1. Type /assertion-block 2. Run the request Ctrl + Enter ) 3. Check the Response Panel to see if it passed or broke Quick confidence. Zero guesswork. Download Voiden here : https://voiden.md/ See translation</description><pubDate>Thu, 08 Jan 2026 13:43:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/531790411219507</guid></item><item><title>Open-source parallel gripper for SO-ARM100/101 released!</title><link>https://huggingface.co/posts/branikita/472070918537855</link><description>Open-source parallel gripper for SO-ARM100/101 released! We‚Äôve published the full open-source parallel gripper design for SO-ARM100 and SO-ARM101. Includes: - Mechanical specs and documentation - Step-by-step assembly guide - Full BOM with sourcing links - STL/CAD files for FDM printing (tested on Bambu Lab A1 mini, Prusa MINI+, ‚â•180√ó180 mm bed) The gripper supports an interchangeable camera holder compatible with common research cameras: - IMX335 (USB RGB, 5 MP) - GC2093 (USB RGB, 2 MP) - Orbbec Gemini 2 (RGB-D) - Intel RealSense D405 (RGB-D, close-range) - Intel RealSense D435 / D435i (RGB-D, general purpose) - Intel RealSense D455 (RGB-D, long-range) ~30‚Äì45 min assembly, fully 3D-printable, ready to integrate with SO-ARM. Github repo: https://github.com/roboninecom/SO-ARM100-101-Parallel-Gripper See translation</description><pubDate>Thu, 08 Jan 2026 13:43:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/472070918537855</guid></item><item><title>Skill Reflect: A Concept for Automated AI Skill Mastery</title><link>https://huggingface.co/posts/mindchain/258347414806989</link><description>Skill Reflect: A Concept for Automated AI Skill Mastery Let‚Äôs be real for a second: most of us are using AI all wrong. We send a prompt, get a "meh" answer, and then spend twenty minutes fixing it ourselves. That‚Äôs not a workflow; that‚Äôs just a digital chore. I wanted to see if I could push Claude further‚Äîto see if I could build a system that actually learns and refines itself. That‚Äôs how the Claude-Reflect-System (Skill Reflect) was born. But here‚Äôs the thing: this isn‚Äôt some polished, final product. It‚Äôs a concept. It‚Äôs a blueprint. I‚Äôve built the foundation of a recursive reflection loop that forces the AI to step back, look at its work, and act as its own harshest critic. It identifies the "skill delta"‚Äîthe gap between "okay" and "mastery"‚Äîand closes it. This logic isn't just for Claude; you can grab this architecture and drop it right into codex-cli, terminal agents, or whatever stack you're building. I‚Äôm a big believer in the law of causality. Action, reaction. Cause and...</description><pubDate>Thu, 08 Jan 2026 13:43:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mindchain/258347414806989</guid></item><item><title>Is it better to show a model too many images once (Diversity), or extract as much information from a small set of images?</title><link>https://huggingface.co/posts/Akhil-Theerthala/396893563842558</link><description>Is it better to show a model too many images once (Diversity), or extract as much information from a small set of images? I have always wanted to do an ablation study on this and recently I got the chance to do exactly that. Why? In applied domains like robotics, manufacturing, or banking, we rarely have the luxury of internet-scale diverse image datasets. We are often "Data Poor" in terms of diversity but "Data Rich" in depth. The takeaway? Density is efficient for facts but dangerous for reasoning (logical collapse) if you don't have larger scale data. More details: https://huggingface.co/blog/Akhil-Theerthala/diversity-density-for-vision-language-models See translation</description><pubDate>Thu, 08 Jan 2026 13:43:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Akhil-Theerthala/396893563842558</guid></item><item><title>Big news from CES ‚Äî Cosmos Reason 2 is here ‚Äî our most advanced reasoning vision-language model for physical AI, now topping the Physical AI Bench leaderboardüèÜ</title><link>https://huggingface.co/posts/tsungyi/839679910683551</link><description>Big news from CES ‚Äî Cosmos Reason 2 is here ‚Äî our most advanced reasoning vision-language model for physical AI, now topping the Physical AI Bench leaderboardüèÜ shi-labs/physical-ai-bench-leaderboard What‚Äôs new: - Enhanced physical reasoning &amp; spatio-temporal understanding - Flexible deployment with 2B &amp; 8B model sizes - Long-context understanding (up to 256K tokens) - Object detection with 2D/3D point localizations and trajectory data - New Cosmos Cookbook Recipes for faster onboarding Read the full blog üìñ https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning Download Cosmos Reason 2 üëâ nvidia/Cosmos-Reason2-8B On top of Cosmos Reason 2, we also rolled out other new updates, including: - Cosmos Predict 2.5 ‚Äì Unified Text2World/Image2World/Video2World model for higher-quality synthetic video worlds - Cosmos Transfer 2.5-2B ‚Äì Lightweight, high-fidelity world-to-world translation with stronger physics alignment - NVIDIA GR00T N1.6 ‚Äì Open robot foundation...</description><pubDate>Thu, 08 Jan 2026 13:43:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tsungyi/839679910683551</guid></item></channel></rss>