<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>KittenTTS Nano â€” Tiny, Expressive, Practical</title><link>https://huggingface.co/posts/Javedalam/795050655622219</link><description>KittenTTS Nano â€” Tiny, Expressive, Practical KittenTTS Nano is a lightweight, CPU-only text-to-speech model designed to prove that natural, expressive voices donâ€™t require massive cloud stacks or GPUs. At roughly ~15M parameters, it runs fast on modest hardware, supports multiple expressive voices, and exposes simple controls for pacing and tone. This makes it ideal for edge devices, demos, and anyone who wants full control over TTS without latency, lock-in, or infrastructure overhead. Try it here Javedalam/KittenTTS The model page KittenML/kitten-tts-nano-0.2 See translation</description><pubDate>Sat, 31 Jan 2026 05:41:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Javedalam/795050655622219</guid></item><item><title>Daggr UI version of the Qwen3-TTS demo.ğŸ”¥</title><link>https://huggingface.co/posts/prithivMLmods/617850685706562</link><description>Daggr UI version of the Qwen3-TTS demo.ğŸ”¥ (custom voice, voice design, qwen3-asr and voice cloning) nodes. No remote spaces used for API inference; all functions run in-app fn . Powered by t4-m and built with daggr@0.5.2 and gradio@6. ğŸ‘‰Demo: prithivMLmods/Qwen3-TTS-Daggr-UI â­Github: https://github.com/PRITHIVSAKTHIUR/Qwen3-TTS-Daggr-UI See translation</description><pubDate>Sat, 31 Jan 2026 05:41:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/617850685706562</guid></item><item><title>Just built my entire AI Engineer portfolio by pasting 2 links (GitHub and LinkedIn)  into</title><link>https://huggingface.co/posts/RakshitAralimatti/297622038982343</link><description>Just built my entire AI Engineer portfolio by pasting 2 links (GitHub and LinkedIn) into moonshotai Kimi 2.5. That's it. That's the workflow. Zero coding. Zero iteration. Zero "make the button bigger." See for yourself: https://rakshit2020.github.io/rakshitaralimatti.github.io/ The model: âœ… Scraped my GitHub repos automatically âœ… Pulled my experience from LinkedIn âœ… Designed an Aurora Glass theme âœ… Mapped every skill to projects âœ… Added animations I'd never code myself See translation</description><pubDate>Sat, 31 Jan 2026 05:41:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/RakshitAralimatti/297622038982343</guid></item><item><title>You can now run Kimi K2.5 locally! ğŸ”¥</title><link>https://huggingface.co/posts/danielhanchen/602916263790271</link><description>You can now run Kimi K2.5 locally! ğŸ”¥ We shrank the 1T model to 240GB (-60%) via Dynamic 1-bit. Get &gt;40 tok/s on 242GB or 622GB VRAM/RAM for near full precision. GGUF: unsloth/Kimi-K2.5-GGUF Guide: https://unsloth.ai/docs/models/kimi-k2.5 See translation</description><pubDate>Sat, 31 Jan 2026 05:41:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/602916263790271</guid></item><item><title>Introducing Rain-v2: Democratizing LLM training on gaming GPUs! âš¡</title><link>https://huggingface.co/posts/raincandy-u/348219893520522</link><description>Introducing Rain-v2: Democratizing LLM training on gaming GPUs! âš¡ â€‹Following Rain-100M, weâ€™re scaling up. Rain-v2 features a larger training dataset. Weâ€™ve published a comprehensive blog covering the end-to-end journeyâ€”from raw data collection to rigorous evaluation and safety testing. â€‹HF Repo: ğŸ¤— raincandy-u/Rain-v2 â€‹Blog: ğŸ“š https://angelkawaii.xyz/2026/01/29/rain-v2/ â€‹Special thanks to the open-source community and the SmolLM2 team for their foundational work! ğŸš€ HuggingFaceTB SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model (2502.02737) See translation</description><pubDate>Sat, 31 Jan 2026 05:41:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/raincandy-u/348219893520522</guid></item><item><title>Hey Hugging Face!</title><link>https://huggingface.co/posts/unmodeled-tyler/896019753498554</link><description>Hey Hugging Face! Type 2 in Project Enneagram just came out: vanta-research/PE-Type-2-Alma-4B PE-Type-2-Alma-4B is the second release in Project Enneagram, where I'm finetuning each of the 9 Enneagram types onto Gemma 3 4B Type 2-Alma is designed to exhibit the "helper" profile: - Empathetic Support: Emotional attunement - managing bad days, anxiety, grief, rejection, or feeling unseen - Interpersonal Connections: Relationship building - making friends, listening, conflict, reciprocity, apologies - Generous Guidance: Going above and beyond - cover letters, meal prep, gardening, wedding speeches, etc - Identity: Alma's name, tone, and conversational style Type 3 soon! See translation</description><pubDate>Sat, 31 Jan 2026 05:41:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/896019753498554</guid></item><item><title>Was tinkering with a Daggr node generator script earlier today (</title><link>https://huggingface.co/posts/Csplk/934898193054995</link><description>Was tinkering with a Daggr node generator script earlier today ( Csplk/DaggrGenerator )and started on a GUI for it for folks who are not comfy with writing code and like a GUI instead for something to motivate working on some Daggr stuff. *Will have time later to keep working on it so donâ€™t hesitate to comment with bugs or issues found if trying it out.* Csplk/DaggrGenerator Thanks @ merve @ ysharma @ abidlabs and team daggr for making daggr :) See translation</description><pubDate>Sat, 31 Jan 2026 05:41:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Csplk/934898193054995</guid></item><item><title>ğŸš€ Geilim-1B-Instruct â€” Implicit Deep Reasoning, Zero Verbosity</title><link>https://huggingface.co/posts/OzTianlu/906408758517388</link><description>ğŸš€ Geilim-1B-Instruct â€” Implicit Deep Reasoning, Zero Verbosity NoesisLab/Geilim-1B-Instruct https://huggingface.co/collections/NoesisLab/geilim-large-language-models No &lt;think&gt; tags. No long CoT. Reasoning happens inside the hidden states, not in the output. Whatâ€™s different ğŸ§  Implicit reasoning: deep causal reasoning without exposing chains ğŸ•¸ï¸ ASPP (Adjacency-Structured Parallel Propagation): parent-only causal graph, O(n) message passing ğŸŒŠ Ï€-flow: internal probability-space refinement instead of token-level deliberation âš–ï¸ Hybrid gating: learns when to use structure vs attention Why it matters Lower latency &amp; token cost Cleaner, production-ready outputs CoT-level reasoning depth without verbosity tax Built on Llama-3.2-1B-Instruct, trained for math, logic, and commonsense. Designed for small-model reasoning at the edge. #ImplicitReasoning #SmallLLM #EfficientAI #ReasoningModels #ASPP #PiFlow See translation</description><pubDate>Sat, 31 Jan 2026 05:41:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/OzTianlu/906408758517388</guid></item><item><title>âœ¨ Research-Papers (various topics across AI/LLM research areas)</title><link>https://huggingface.co/posts/tegridydev/468392803773601</link><description>âœ¨ Research-Papers (various topics across AI/LLM research areas) tegridydev/research-papers Currently building out the foundation topics and raw .pdf research paper files Will be processing and cleaning up and converting into high quality training datasets Check it out, give it a like and leave a comment below or join community discussion and suggest what fields and research topics you want to see included! See translation</description><pubDate>Sat, 31 Jan 2026 05:41:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/tegridydev/468392803773601</guid></item><item><title>ğŸ’¥</title><link>https://huggingface.co/posts/alvarobartt/259854920222577</link><description>ğŸ’¥ hf-mem v0.4.1 now also estimates KV cache memory requirements for any context length and batch size with the --experimental flag! uvx hf-mem --model-id ... --experimental will automatically pull the required information from the Hugging Face Hub to include the KV cache estimation, when applicable. ğŸ’¡ Alternatively, you can also set the --max-model-len , --batch-size and --kv-cache-dtype arguments (Ã  la vLLM) manually if preferred. See translation</description><pubDate>Sat, 31 Jan 2026 05:41:51 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/alvarobartt/259854920222577</guid></item></channel></rss>