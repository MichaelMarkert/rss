<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>2025: The Year of Agents.</title><link>https://huggingface.co/posts/daavoo/629869039060445</link><description>2025: The Year of Agents. 2026: The Year of Local Agents? Relying on cloud-hosted LLMs is often overkill. While frontier models still lead in complex coding, local models are now more than capable of handling many agentic workflowsâ€”with zero latency and total privacy. To help bridge the gap between local inference and usable agents, Iâ€™m releasing agent.cpp: https://github.com/mozilla-ai/agent.cpp It provides minimal, high-performance building blocks for agents in C++, built directly around the awesome llama.cpp ecosystem. Stop sending your data to a remote API. Start building and running agents on your own hardware. See translation</description><pubDate>Mon, 22 Dec 2025 05:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/daavoo/629869039060445</guid></item><item><title>Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... ğŸ¤—ğŸ”¥</title><link>https://huggingface.co/posts/prithivMLmods/787095126804028</link><description>Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... ğŸ¤—ğŸ”¥ âœ¨ TRELLIS.2-Text-to-3D [Demo]: prithivMLmods/TRELLIS.2-Text-to-3D âœ¨ Multimodal Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations âœ¨ Github: https://github.com/PRITHIVSAKTHIUR/TRELLIS.2-Text-to-3D To know more about it, visit the app page or the respective model page! See translation</description><pubDate>Mon, 22 Dec 2025 05:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/787095126804028</guid></item><item><title>PawMatchAI â€” Smarter, Safer, and More Thoughtful Recommendations ğŸ•âœ¨</title><link>https://huggingface.co/posts/DawnC/213826857024975</link><description>PawMatchAI â€” Smarter, Safer, and More Thoughtful Recommendations ğŸ•âœ¨ ğŸ¾ Recommendation system update â€” deeper reasoning, safer decisions Over the past weeks, user feedback led me to rethink how PawMatchAI handles description-based breed recommendations. Instead of only matching surface-level preferences, the system now implements a multi-dimensional semantic reasoning architecture that emphasizes real-life compatibility and risk awareness. Key technical improvements: - SBERT-powered semantic understanding with dynamic weight allocation across six constraint dimensions (space, activity, noise, grooming, experience, family) - Hierarchical constraint management distinguishing critical safety constraints from flexible preferences, with progressive relaxation when needed -Multi-head scoring system combining semantic matching (15%), lifestyle compatibility (70%), constraint adherence (10%), and confidence calibration (5%) -Intelligent risk filtering that applies graduated penalties (-10% to...</description><pubDate>Mon, 22 Dec 2025 05:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/213826857024975</guid></item><item><title>Introducing demos for new SOTA models from AI2: SAGE-MM (Smart Any-Horizon Agents for Long-Video Reasoning) and Molmo-2, an open vision-language model that supports multi-image (QA and pointing) and video (QA, pointing, and tracking). The respective demo-related collections are listed below. ğŸƒğŸ”¥</title><link>https://huggingface.co/posts/prithivMLmods/761836377624422</link><description>Introducing demos for new SOTA models from AI2: SAGE-MM (Smart Any-Horizon Agents for Long-Video Reasoning) and Molmo-2, an open vision-language model that supports multi-image (QA and pointing) and video (QA, pointing, and tracking). The respective demo-related collections are listed below. ğŸƒğŸ”¥ âœ¨ SAGE-MM [Video-Reasoning]: prithivMLmods/SAGE-MM-Video-Reasoning âœ¨ Molmo2 [Demo]: prithivMLmods/Molmo2-HF-Demo ğŸƒ GitHub[SAGE-MM]: https://github.com/PRITHIVSAKTHIUR/SAGE-MM-Video-Reasoning ğŸƒ GitHub[Molmo2]: https://github.com/PRITHIVSAKTHIUR/Molmo2-HF-Demo ğŸƒ Multimodal Implementations: https://huggingface.co/collections/prithivMLmods/multimodal-implementations To know more about it, visit the app page or the respective model page! See translation</description><pubDate>Mon, 22 Dec 2025 05:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/761836377624422</guid></item><item><title>æˆ‘å³å°†è¾¾åˆ°å…¬å…±å­˜å‚¨ç©ºé—´ä¸Šé™ã€‚æˆ‘å‘ç°æˆ‘çš„ä»“åº“ John1604/Kimi-K2-Thinking-q6K-gguf æ²¡æœ‰è·å¾—è¶³å¤Ÿçš„ä¸‹è½½é‡ï¼Œå‡ ä¹å ç”¨äº† 1T å­˜å‚¨ç©ºé—´ã€‚å°½ç®¡æˆ‘å–œçˆ± Kimi K2 çš„æ€è€ƒæ–¹å¼ï¼Œä½†å¯èƒ½ä¸å¾—ä¸åˆ é™¤è¿™ä¸ªæ¨¡å‹ã€‚å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªçœŸæ­£çš„å¼€æº 1T LLMï¼Œä¸ä»»ä½•å‰æ²¿çš„ LLM æ¨¡å‹ç›¸åª²ç¾ã€‚åœ¨ AI ç«äº‰ä¸­ï¼Œç¾å›½æœ‰å››å®¶å…¬å¸æ‹¥æœ‰1T+æ¨¡å‹ï¼šxAI,  OpenAI, è°·æ­Œå’ŒAnthropologieã€‚ä¸­å›½ä¹Ÿæœ‰å››å®¶å…¬å¸æ‹¥æœ‰1T+æ¨¡å‹ï¼šé˜¿é‡Œå·´å·´, Kimi, DeepSeekå’ŒGLMã€‚ç›®å‰åŒæ–¹åŠ¿å‡åŠ›æ•Œã€‚</title><link>https://huggingface.co/posts/John1604/712509372180068</link><description>æˆ‘å³å°†è¾¾åˆ°å…¬å…±å­˜å‚¨ç©ºé—´ä¸Šé™ã€‚æˆ‘å‘ç°æˆ‘çš„ä»“åº“ John1604/Kimi-K2-Thinking-q6K-gguf æ²¡æœ‰è·å¾—è¶³å¤Ÿçš„ä¸‹è½½é‡ï¼Œå‡ ä¹å ç”¨äº† 1T å­˜å‚¨ç©ºé—´ã€‚å°½ç®¡æˆ‘å–œçˆ± Kimi K2 çš„æ€è€ƒæ–¹å¼ï¼Œä½†å¯èƒ½ä¸å¾—ä¸åˆ é™¤è¿™ä¸ªæ¨¡å‹ã€‚å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªçœŸæ­£çš„å¼€æº 1T LLMï¼Œä¸ä»»ä½•å‰æ²¿çš„ LLM æ¨¡å‹ç›¸åª²ç¾ã€‚åœ¨ AI ç«äº‰ä¸­ï¼Œç¾å›½æœ‰å››å®¶å…¬å¸æ‹¥æœ‰1T+æ¨¡å‹ï¼šxAI, OpenAI, è°·æ­Œå’ŒAnthropologieã€‚ä¸­å›½ä¹Ÿæœ‰å››å®¶å…¬å¸æ‹¥æœ‰1T+æ¨¡å‹ï¼šé˜¿é‡Œå·´å·´, Kimi, DeepSeekå’ŒGLMã€‚ç›®å‰åŒæ–¹åŠ¿å‡åŠ›æ•Œã€‚ I'm about to reach my public storage limit. I've discovered that my repository John1604/Kimi-K2-Thinking-q6K-gguf isn't getting enough downloads and is nearly consuming 1TB of storage. While I love Kimi K2's way of thinking, I have to delete this model because it's a true open-source 1TB LLM, comparable to any cutting-edge LLM model. In the AI â€‹â€‹race, four US companies have 1TB+ models: xAI, OpenAI, Google, and Anthropic. China also has four companies with 1TB+ models: Alibaba, Kimi, DeepSeek, and GLM. Currently, the two sides are evenly matched. Only American team and Chinese team have LLM with 1T+ parameters. Let's cheer for them to reach AGI in next 5 to 10 years. Maybe a 64T chinese model will do it -- Human and cat brain...</description><pubDate>Mon, 22 Dec 2025 05:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/John1604/712509372180068</guid></item><item><title>Introducing PTS Visualizer - an interactive tool for exploring how language models reason!</title><link>https://huggingface.co/posts/codelion/327583349449116</link><description>Introducing PTS Visualizer - an interactive tool for exploring how language models reason! Visualize pivotal tokens, thought anchors, and reasoning circuits. See which tokens and sentences significantly impact success probability, explore embedding clusters, and trace reasoning step-by-step. Try it: codelion/pts-visualizer Explore PTS datasets: - Qwen3-0.6B: codelion/Qwen3-0.6B-pts - DeepSeek-R1: codelion/DeepSeek-R1-Distill-Qwen-1.5B-pts Or upload your own JSONL files! GitHub: https://github.com/codelion/pts See translation</description><pubDate>Mon, 22 Dec 2025 05:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/327583349449116</guid></item><item><title>From Prompt Engineering to Context Engineering: Main Design Patterns</title><link>https://huggingface.co/posts/Kseniase/746586334382009</link><description>From Prompt Engineering to Context Engineering: Main Design Patterns Earlier on, we relied on clever prompt wording, but now structured, complete context matters more than just magic phrasing. The next year is going to be a year of context engineering which expands beyond prompt engineering. The two complement each other: prompt engineering shapes how we ask, while context engineering shapes what the model knows, sees, and can do. To keep things clear, here are the main techniques and design patterns in both areas, with some useful resources for further exploration: â–ªï¸ 9 Prompt Engineering Techniques (configuring input text) 1. Zero-shot prompting â€“ giving a single instruction without examples. Relies entirely on pretrained knowledge. 2. Few-shot prompting â€“ adding inputâ€“output examples to encourage model to show the desired behavior. âŸ¶ https://arxiv.org/abs/2005.14165 3. Role prompting â€“ assigning a persona or role (e.g. "You are a senior researcher," "Say it as a specialist in...</description><pubDate>Mon, 22 Dec 2025 05:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/746586334382009</guid></item><item><title>Wan 2.2 Complete Training Tutorial - Text to Image, Text to Video, Image to Video, Windows &amp; Cloud :</title><link>https://huggingface.co/posts/MonsterMMORPG/210519935872315</link><description>Wan 2.2 Complete Training Tutorial - Text to Image, Text to Video, Image to Video, Windows &amp; Cloud : https://youtu.be/ocEkhAsPOs4 Wan 2.2 training is now so easy. I have done over 64 different unique Wan 2.2 trainings to prepare the very best working training configurations for you. The configurations are fully working locally with as low as 6 GB GPUs. So you will be able to train your awesome Wan 2.2 image or video generation LoRAs on your Windows computer with easiness. Moreover, I have shown how to train on cloud platforms RunPod and Massed Compute so even if you have no GPU or you want faster training, you can train on cloud for very cheap prices fully privately. Full step by step tutorial : https://youtu.be/ocEkhAsPOs4 â±ï¸ Video Chapters: 0:00 Introduction to Wan 2.2 Training &amp; Capabilities 0:56 Installing &amp; Updating Musubi Tuner Locally 2:20 Explanation of Optimized Presets &amp; Research Logic 4:00 Differences Between T2I, T2V, and I2V Configs 5:36 Extracting Files &amp; Running...</description><pubDate>Mon, 22 Dec 2025 05:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/210519935872315</guid></item><item><title>ğŸ”¥Check out Project Los Angeles new SOTA searchable MIDI dataset! ğŸ”¥</title><link>https://huggingface.co/posts/projectlosangeles/440962027896970</link><description>ğŸ”¥Check out Project Los Angeles new SOTA searchable MIDI dataset! ğŸ”¥ projectlosangeles/Discover-MIDI-Dataset The dataset features over 6.74M+ unique searchable MIDIs and is tailored for MIDI music discovery and symbolic music AI! If you like the dataset, pleaseâ¤ï¸ Sincerely, Alex Project Los Angeles Tegridy Code 2025 See translation</description><pubDate>Mon, 22 Dec 2025 05:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/projectlosangeles/440962027896970</guid></item><item><title>NEW MODEL:</title><link>https://huggingface.co/posts/unmodeled-tyler/429910399522120</link><description>NEW MODEL: vanta-research/scout-8b VANTA Research is excited to share our new model, Scout-8B! This iteration of Scout is based on the RNJ-1 Instruct architecture from Essential AI, and not only improves but expands on the capabilities from vanta-research/scout-4b Scout is specifically designed for: Tactical Intelligence Analysis - Systematic problem decomposition - Structured reconnaissance approach - Data-driven assessment methodology Operational Planning - Multi-phase operation planning - Risk assessment and mitigation - Resource allocation guidance Technical Assessment - Architecture evaluation and analysis - Performance optimization recommendations - Security perimeter assessment This model is great for anyone that works in security, IT, DevOps, or anyone looking for a unique, but functional AI collaborator. Check it out! See translation</description><pubDate>Mon, 22 Dec 2025 05:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/429910399522120</guid></item></channel></rss>