<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ğŸš€ FLUX Workflow Canvas</title><link>https://huggingface.co/posts/ginipick/141662077994282</link><description>ğŸš€ FLUX Workflow Canvas Welcome to Workflow Canvas, your ultimate AI-driven platform for crafting stunning design concepts and intricate workflow diagrams that empower your business! ğŸ¤–âœ¨ ginigen/Workflow-Canvas Features Product Design ğŸ› ï¸ Transform your ideas into reality with sleek, industrial product designs that blend modern aesthetics with advanced technology. Mindmap ğŸ§  Generate vibrant, educational mind maps that outline your strategies and processes in a clear, visually engaging layout. Mockup ğŸ“± Quickly prototype intuitive app interfaces and web designs using clean, hand-drawn wireframes that capture your vision. Infographic ğŸ“Š Build polished, data-rich infographics that communicate complex corporate metrics and trends with style and clarity. Diagram ğŸ“ˆ Illustrate comprehensive, end-to-end business workflowsâ€”from market analysis to implementationâ€”with detailed and organized diagrams. Flowchart ğŸ”„ Design easy-to-follow, hand-drawn style flowcharts that map out your operational...</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/141662077994282</guid></item><item><title>ğŸ¯ Perplexity drops their FIRST open-weight model on Hugging Face: A decensored DeepSeek-R1 with full reasoning capabilities. Tested on 1000+ examples for unbiased responses.</title><link>https://huggingface.co/posts/fdaudens/121352437859372</link><description>ğŸ¯ Perplexity drops their FIRST open-weight model on Hugging Face: A decensored DeepSeek-R1 with full reasoning capabilities. Tested on 1000+ examples for unbiased responses. Check it out: perplexity-ai/r1-1776 Blog post: https://perplexity.ai/hub/blog/open-sourcing-r1-1776 See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/121352437859372</guid></item><item><title>Dino: The Minimalist Multipurpose Chat System ğŸŒ </title><link>https://huggingface.co/posts/prithivMLmods/874083632338295</link><description>Dino: The Minimalist Multipurpose Chat System ğŸŒ  Agent-Dino : prithivMLmods/Agent-Dino By default, it performs the following tasks: {Text-to-Text Generation}, {Image-Text-Text Generation} @image : Generates an image using Stable Diffusion xL. @3d : Generates a 3D mesh. @web : Web search agents. @rAgent : Initiates a reasoning chain using Llama mode for coding explanations. @tts1-â™€ , @tts2-â™‚ : Voice generation (Female and Male voices). See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/874083632338295</guid></item><item><title>We crossed 1B+ tokens routed to inference providers partners on HF, that we released just a few days ago.</title><link>https://huggingface.co/posts/clem/679572962523651</link><description>We crossed 1B+ tokens routed to inference providers partners on HF, that we released just a few days ago. Just getting started of course but early users seem to like it &amp; always happy to be able to partner with cool startups in the ecosystem. Have you been using any integration and how can we make it better? https://huggingface.co/blog/inference-providers See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/679572962523651</guid></item><item><title>ğŸš€ StepFuné˜¶è·ƒæ˜Ÿè¾° is making BIG open moves!</title><link>https://huggingface.co/posts/AdinaY/709023807759284</link><description>ğŸš€ StepFuné˜¶è·ƒæ˜Ÿè¾° is making BIG open moves! Last year, their GOT-OCR 2.0 took the community by storm ğŸ”¥but many didnâ€™t know they were also building some amazing models. Now, theyâ€™ve just dropped something huge on the hub! ğŸ“º Step-Video-T2V: a 30B bilingual open video model that generates 204 frames (8-10s) at 540P resolution with high information density &amp; consistency. stepfun-ai/stepvideo-t2v ğŸ”Š Step-Audio-TTS-3B : a TTS trained with the LLM-Chat paradigm on a large synthetic dataset, capable of generating RAP &amp; Humming stepfun-ai/step-audio-67b33accf45735bb21131b0b See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/709023807759284</guid></item><item><title>Finally here it is: a faster, custom, scalable GRPO trainer for smaller models with &lt; 500M params, can train on 8gb ram cpu,  also supports gpu for sanity sake (includes support for vllm + flash attention). Using smolLM2-135M/360M-instructs as ref &amp; base models. Experience your own â€œahaâ€ moment ğŸ³ on 8gb ram.</title><link>https://huggingface.co/posts/Jaward/905904518817417</link><description>Finally here it is: a faster, custom, scalable GRPO trainer for smaller models with &lt; 500M params, can train on 8gb ram cpu, also supports gpu for sanity sake (includes support for vllm + flash attention). Using smolLM2-135M/360M-instructs as ref &amp; base models. Experience your own â€œahaâ€ moment ğŸ³ on 8gb ram. Code: https://github.com/Jaykef/ai-algorithms/blob/main/smollm2_360M_135M_grpo_gsm8k.ipynb See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/905904518817417</guid></item><item><title>This dataset was collected in roughly 4 hours using the Rapidata Python API, showcasing how quickly large-scale annotations can be performed with the right tooling!</title><link>https://huggingface.co/posts/jasoncorkill/578148904408624</link><description>This dataset was collected in roughly 4 hours using the Rapidata Python API, showcasing how quickly large-scale annotations can be performed with the right tooling! All that at less than the cost of a single hour of a typical ML engineer in Zurich! The new dataset of ~22,000 human annotations evaluating AI-generated videos based on different dimensions, such as Prompt-Video Alignment, Word for Word Prompt Alignment, Style, Speed of Time flow and Quality of Physics. Rapidata/text-2-video-Rich-Human-Feedback See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/578148904408624</guid></item><item><title>AGENTS + FINETUNING! This week Hugging Face learn has a whole pathway on finetuning for agentic applications. You can follow these two courses to get knowledge on levelling up your agent game beyond prompts:</title><link>https://huggingface.co/posts/burtenshaw/189514834246661</link><description>AGENTS + FINETUNING! This week Hugging Face learn has a whole pathway on finetuning for agentic applications. You can follow these two courses to get knowledge on levelling up your agent game beyond prompts: 1ï¸âƒ£ New Supervised Fine-tuning unit in the NLP Course https://huggingface.co/learn/nlp-course/en/chapter11/1 2ï¸âƒ£New Finetuning for agents bonus module in the Agents Course https://huggingface.co/learn/agents-course/bonus-unit1/introduction Fine-tuning will squeeze everything out of your model for how youâ€™re using it, more than any prompt. See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/burtenshaw/189514834246661</guid></item><item><title>Inference-time scaling meets Flux.1-Dev (and others) ğŸ”¥</title><link>https://huggingface.co/posts/sayakpaul/418493639663017</link><description>Inference-time scaling meets Flux.1-Dev (and others) ğŸ”¥ Presenting a simple re-implementation of "Inference-time scaling diffusion models beyond denoising steps" by Ma et al. I did the simplest random search strategy, but results can potentially be improved with better-guided search methods. Supports Gemini 2 Flash &amp; Qwen2.5 as verifiers for "LLMGrading" ğŸ¤— The steps are simple: For each round: 1&gt; Starting by sampling 2 starting noises with different seeds. 2&gt; Score the generations w.r.t a metric. 3&gt; Obtain the best generation from the current round. If you have more compute budget, go to the next search round. Scale the noise pool ( 2 ** search_round ) and repeat 1 - 3. This constitutes the random search method as done in the paper by Google DeepMind. Code, more results, and a bunch of other stuff are in the repository. Check it out here: https://github.com/sayakpaul/tt-scale-flux/ ğŸ¤— See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sayakpaul/418493639663017</guid></item><item><title>ğŸ“¢ For those who start to work with LLM streaming in web, here is a minimalistic example in JS for accessing server hosted by FastAPI via REST:</title><link>https://huggingface.co/posts/nicolay-r/892116616544214</link><description>ğŸ“¢ For those who start to work with LLM streaming in web, here is a minimalistic example in JS for accessing server hosted by FastAPI via REST: https://gist.github.com/nicolay-r/840425749cf6d3e397da3d329e894d59 The code above is a revised verison for accessing Replicate API posted earlier https://huggingface.co/posts/nicolay-r/390307941200307 The key difference from Replicate API: - using only POST for passing a body with parameters and fetching the reader. See translation</description><pubDate>Wed, 19 Feb 2025 17:18:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/nicolay-r/892116616544214</guid></item></channel></rss>