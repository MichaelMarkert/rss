<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! 🚨</title><link>https://huggingface.co/posts/m-ric/175050207181959</link><description>STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! 🚨 A lone scientist just proposed Tiny Recursive Model (TRM), and it is literally the most impressive model that I've seen this year. ➡️ Tiny Recursive Model is 7M parameters ➡️ On ARC-AGI, it beats flagship models like Gemini-2.5-pro Consider how wild this is: Gemini-2.5-pro must be over 10,000x bigger and had 1,000 as many authors 😂 (Alexia is alone on the paper) What's this sorcery? In short: it's a very tiny Transformers, but it loops over itself at two different frequencies, updating two latent variables: one for the proposed answer and one for the reasoning. @ AlexiaJM started from the paper Hierarchical Reasoning Model, published a few months ago, that already showed breakthrough improvement on AGI for its small size (27M) Hierarchical Reasoning Model had introduced one main feature: 🔎 Deep supervision In their model, one part (here one layer) would run at high frequency, and...</description><pubDate>Sat, 11 Oct 2025 09:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/m-ric/175050207181959</guid></item><item><title>At the close of the National Holiday🇨🇳, Antgroup drops a new SoTA model.</title><link>https://huggingface.co/posts/AdinaY/255762505303069</link><description>At the close of the National Holiday🇨🇳, Antgroup drops a new SoTA model. Ling-1T 🔥 the trillion-parameter flagship of the Ling 2.0 series. inclusionAI/Ling-1T ✨1T total / 50B active params per token ✨20T+ reasoning-dense tokens (Evo-CoT) ✨128K context via YaRN ✨FP8 training: 15%+ faster, same precision as BF16 ✨Hybrid Syntax-Function-Aesthetics reward for front-end &amp; visual generation See translation</description><pubDate>Sat, 11 Oct 2025 09:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/255762505303069</guid></item><item><title>🤖 What if building your own robot arm costs less than £220?</title><link>https://huggingface.co/posts/hba123/315319549896319</link><description>🤖 What if building your own robot arm costs less than £220? For years, robotics has been locked behind high prices and complex systems. So we decided to change that. Today, we’re open-sourcing Ark-Bot — a fully 3D-printed, 6-DOF robot arm that works seamlessly with our Python robotics library, Ark. And yes… It’s only £215.86 to build. 🧠ArkBot Specs 🧠 1️⃣ Reach: 1 meter 2️⃣ Weight: 2.6 kg 3️⃣ Payload: 1.8 kg 💪 4️⃣ DOF: 6 5️⃣ Input Voltage: DC 12V 🤟Fully 3D-printable &amp; open-source 🤟Integrated with Ark — no ROS required 📹 We’ve also released a video showing the full assembly process — because robotics should be something everyone can learn, build, and improve on. 👩‍🎓 With Ark-Bot, anyone — from students to AI researchers — can experiment with embodied AI, robot learning, and control algorithms on real hardware, affordably. If you could control a 1-meter robot arm from your laptop for under £220… 👉 What would you build first? 🔗https://github.com/Robotics-Ark/ark_bot 🎥...</description><pubDate>Sat, 11 Oct 2025 09:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/315319549896319</guid></item><item><title>MLX port of BDH (Baby Dragon Hatchling) is up!</title><link>https://huggingface.co/posts/Severian/804699132791352</link><description>MLX port of BDH (Baby Dragon Hatchling) is up! I’ve ported the BDH ( https://github.com/pathwaycom/bdh ) model to MLX for Apple Silicon. It’s a faithful conversion of the PyTorch version: same math, same architecture (byte-level vocab, shared weights across layers, ReLU sparsity, RoPE attention with Q=K), with MLX-friendly APIs and a detailed README explaining the few API-level differences and why results are equivalent. Code, docs, and training script are ready to use. You may need to adjust the training script a bit to fit your own custom dataset. Only tested on M4 so far, but should work perfect for any M1/M2/M3 users out there. I’m currently training this MLX build on my Internal Knowledge Map (IKM) dataset Severian/Internal-Knowledge-Map Training’s underway; expect a day or so before I publish weights. When it’s done, I’ll upload the checkpoint to Hugging Face for anyone to test. Repo: https://github.com/severian42/BDH-MLX HF model (coming soon): Severian/BDH-MLX If you try it...</description><pubDate>Sat, 11 Oct 2025 09:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Severian/804699132791352</guid></item><item><title>No invitation code needed — create AI videos with one click!</title><link>https://huggingface.co/posts/Ethank01/441451500060564</link><description>No invitation code needed — create AI videos with one click! Experience Sora 2, Veo 3, and Wan 2.2 all in one place on iMini. 👉 Try it here: https://imini.com/ See translation</description><pubDate>Sat, 11 Oct 2025 09:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ethank01/441451500060564</guid></item><item><title>🌎 AI ethics and sustainability are two sides of the same coin.</title><link>https://huggingface.co/posts/giadap/452837154929545</link><description>🌎 AI ethics and sustainability are two sides of the same coin. In our new blog post with Dr. Sasha Luccioni, we argue that separating them (as is too often the case) means missing the bigger picture of how AI systems impact both people and the planet. Ethical and sustainable AI development can’t be pursued in isolation. The same choices that affect who benefits or is harmed by AI systems also determine how much energy and resources they consume. We explore how two key concepts, evaluation and transparency, can serve as bridges between these domains: 📊 Evaluation, by moving beyond accuracy or performance metrics to include environmental and social costs, as we’ve done with tools like the AI Energy Score. 🔍 Transparency, by enabling reproducibility, accountability, and environmental reporting through open tools like the Environmental Transparency Space. AI systems mirror our priorities. If we separate ethics from sustainability, we risk building technologies that are efficient but...</description><pubDate>Sat, 11 Oct 2025 09:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/452837154929545</guid></item><item><title>We've just forked LBM to reproduce the LBM eraser results</title><link>https://huggingface.co/posts/piercus/787328298619334</link><description>We've just forked LBM to reproduce the LBM eraser results Our fork : https://github.com/finegrain-ai/LBM LBM paper: LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) LBM relighting demo : jasperai/LBM_relighting See translation</description><pubDate>Sat, 11 Oct 2025 09:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/piercus/787328298619334</guid></item><item><title>LiquidAI/LFM2-8B-A1B</title><link>https://huggingface.co/posts/mlabonne/852622328581891</link><description>LiquidAI/LFM2-8B-A1B just dropped! 8.3B params with only 1.5B active/token 🚀 &gt; Quality ≈ 3–4B dense, yet faster than Qwen3-1.7B &gt; MoE designed to run on phones/laptops (llama.cpp / vLLM) &gt; Pre-trained on 12T tokens → strong math/code/IF See translation</description><pubDate>Sat, 11 Oct 2025 09:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mlabonne/852622328581891</guid></item><item><title>🎉 NEW RELEASES: Cosmos Predict 2.5 and Transfer 2.5</title><link>https://huggingface.co/posts/jwgu/145606397801534</link><description>🎉 NEW RELEASES: Cosmos Predict 2.5 and Transfer 2.5 Cosmos Predict 2.5: - Combines Text2World, Image2World, and Video2World - Multimodal, future-state video prediction Cosmos Transfer 2.5: - High-fidelity multicontrol world simulations - Inputs: RGB, depth, segmentation—blended seamlessly These updates boost development of autonomous vehicles, robotics, and video analytics. Don’t miss Jensen Huang’s keynote at NVIDIA GTC Washington, D.C. on 10/28 to hear the latest in physical AI. 📺 Watch live: https://nvda.ws/4pUjF4x 🔗 Try Predict 2.5: https://nvda.ws/4otReZZ 🔗 Try Transfer 2.5: https://nvda.ws/46GEx7T See translation</description><pubDate>Sat, 11 Oct 2025 09:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jwgu/145606397801534</guid></item><item><title>New advanced AI cheat sheet / article.</title><link>https://huggingface.co/posts/dimentox/989959783700082</link><description>New advanced AI cheat sheet / article. https://huggingface.co/blog/dimentox/ai-cheet-sheet See translation</description><pubDate>Sat, 11 Oct 2025 09:20:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dimentox/989959783700082</guid></item></channel></rss>