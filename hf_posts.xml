<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Today, we're unveiling two new open-source AI robots! HopeJR for $3,000 &amp; Reachy Mini for $300 ü§ñü§ñü§ñ</title><link>https://huggingface.co/posts/clem/522668354429256</link><description>Today, we're unveiling two new open-source AI robots! HopeJR for $3,000 &amp; Reachy Mini for $300 ü§ñü§ñü§ñ Let's go open-source AI robotics! See translation</description><pubDate>Fri, 30 May 2025 05:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/clem/522668354429256</guid></item><item><title>Cua : Docker for computer-use agents</title><link>https://huggingface.co/posts/dhruv3006/675063918098240</link><description>Cua : Docker for computer-use agents Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers. Github : https://github.com/trycua See translation</description><pubDate>Fri, 30 May 2025 05:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/dhruv3006/675063918098240</guid></item><item><title>VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration</title><link>https://huggingface.co/posts/DawnC/538322807718464</link><description>VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration I'm excited to share significant improvements to VisionScout that substantially enhance accuracy and analytical capabilities. ‚≠êÔ∏è Key Enhancements - CLIP Zero-Shot Landmark Detection: The system now identifies famous landmarks and architectural features without requiring specific training data, expanding scene understanding beyond generic object detection. - Places365 Environmental Classification: Integration of MIT's Places365 model provides robust scene baseline classification across 365 categories, significantly improving lighting analysis accuracy and overall scene identification precision. - Enhanced Multi-Modal Fusion: Advanced algorithms now dynamically combine insights from YOLOv8, CLIP, and Places365 to optimize accuracy across diverse scenarios. - Refined LLM Narratives: Llama 3.2 integration continues to transform analytical data into fluent, contextually rich descriptions while maintaining...</description><pubDate>Fri, 30 May 2025 05:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/538322807718464</guid></item><item><title>I am so happy  to share to all that I‚Äôve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I‚Äôm doing my best to keep up. Excited for what‚Äôs ahead!</title><link>https://huggingface.co/posts/lukmanaj/495766537273785</link><description>I am so happy to share to all that I‚Äôve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I‚Äôm doing my best to keep up. Excited for what‚Äôs ahead! See translation</description><pubDate>Fri, 30 May 2025 05:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/lukmanaj/495766537273785</guid></item><item><title>I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book.</title><link>https://huggingface.co/posts/hesamation/260011784391977</link><description>I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book. It gives an overview, then goes into detail for each stage, even providing best practices. It‚Äôs 115 pages on arxiv, definitely worth a read. Check it out: https://arxiv.org/abs/2408.13296 See translation</description><pubDate>Fri, 30 May 2025 05:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/260011784391977</guid></item><item><title>üéµ Dream come true for content creators! TIGER AI can extract voice, effects &amp; music from ANY audio file ü§Ø</title><link>https://huggingface.co/posts/fdaudens/323840314242853</link><description>üéµ Dream come true for content creators! TIGER AI can extract voice, effects &amp; music from ANY audio file ü§Ø This lightweight model uses frequency band-split technology to separate speech like magic. Kudos to @ fffiloni for the amazing demo! fffiloni/TIGER-audio-extraction See translation</description><pubDate>Fri, 30 May 2025 05:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/323840314242853</guid></item><item><title>üî• New benchmark &amp; dataset for Subject-to-Video generation</title><link>https://huggingface.co/posts/AdinaY/228294422537840</link><description>üî• New benchmark &amp; dataset for Subject-to-Video generation OPENS2V-NEXUS by Pekin University ‚ú® Fine-grained evaluation for subject consistency BestWishYsh/OpenS2V-Eval ‚ú® 5M-scale dataset: BestWishYsh/OpenS2V-5M ‚ú® New metrics ‚Äì automatic scores for identity, realism, and text match See translation</description><pubDate>Fri, 30 May 2025 05:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/228294422537840</guid></item><item><title>introducing: VLM vibe eval ü™≠</title><link>https://huggingface.co/posts/merve/470654136703534</link><description>introducing: VLM vibe eval ü™≠ visionLMsftw/VLMVibeEval vision LMs are saturated over benchmarks, so we built vibe eval üí¨ &gt; compare different models with refreshed in-the-wild examples in different categories ü§† &gt; submit your favorite model for eval no numbers -- just vibes! See translation</description><pubDate>Fri, 30 May 2025 05:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/470654136703534</guid></item><item><title>deepseek-ai/DeepSeek-R1-0528</title><link>https://huggingface.co/posts/AtAndDev/639250895656011</link><description>deepseek-ai/DeepSeek-R1-0528 This is the end See translation</description><pubDate>Fri, 30 May 2025 05:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AtAndDev/639250895656011</guid></item><item><title>üëè Congrats</title><link>https://huggingface.co/posts/jeffboudier/548880163054097</link><description>üëè Congrats @ jinanz adding TimesFM times series forecasting to Transformers! Learn how to use TimesFM in this blog post by the Nutanix team: https://huggingface.co/blog/Nutanix/introducing-timesfm-for-time-series-forcasting See translation</description><pubDate>Fri, 30 May 2025 05:23:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jeffboudier/548880163054097</guid></item></channel></rss>