<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>LTX-2 Camera-Control LoRA demo with dolly-in/out and dolly-left/right is now available on Hugging Face, paired with ltx-2-19b-distilled-lora for fast inference. It also includes dynamic GPU duration adjustments for long video generations. Click the related Space links below.</title><link>https://huggingface.co/posts/prithivMLmods/771186692524026</link><description>LTX-2 Camera-Control LoRA demo with dolly-in/out and dolly-left/right is now available on Hugging Face, paired with ltx-2-19b-distilled-lora for fast inference. It also includes dynamic GPU duration adjustments for long video generations. Click the related Space links below. ü§óTry it now on : prithivMLmods/LTX-2-LoRAs-Camera-Control-Dolly ‚≠êGithub: https://github.com/PRITHIVSAKTHIUR/LTX-2-LoRAs-Camera-Control-Dolly üïπÔ∏èCollection: https://huggingface.co/collections/prithivMLmods/image-generation-apps-collection To learn more, visit the app page or the respective model pages. See translation</description><pubDate>Mon, 12 Jan 2026 13:46:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/771186692524026</guid></item><item><title>For more better details and analysis, you can read the article here:</title><link>https://huggingface.co/posts/Ujjwal-Tyagi/924115096887729</link><description>For more better details and analysis, you can read the article here: https://huggingface.co/blog/Ujjwal-Tyagi/steering-not-censoring , We are sleepwalking into a crisis. I am deeply concerned about AI model safety right now because, as the community rushes to roll out increasingly powerful open-source models, we are completely neglecting the most critical aspect: safety. It seems that nobody is seriously thinking about the potential consequences of unregulated model outputs or the necessity of robust guardrails. We are essentially planting the seeds for our own destruction if we prioritize raw performance over security. This negligence is terrifyingly evident when you look at the current landscape. Take Qwen Image 2512, for example; while it delivers undeniably strong performance, it has incredibly weak guardrails that make it dangerous to deploy. In stark contrast, Z Image might not get as much hype for its power, but it has much better safety guardrails than Qwen Image 2512. It is...</description><pubDate>Mon, 12 Jan 2026 13:46:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Ujjwal-Tyagi/924115096887729</guid></item><item><title>Our engineer Alan from</title><link>https://huggingface.co/posts/branikita/526284345540053</link><description>Our engineer Alan from https://robonine.com team has assembled the mechanical frame of our 6-DoF manipulator prototype - without servo motors for now. At this stage we are evaluating how easy the structure is to assemble, checking for any mechanical play, and validating the kinematics. Good news: the structure feels solid and Alan reports no detectable backlash so far. See translation</description><pubDate>Mon, 12 Jan 2026 13:46:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/branikita/526284345540053</guid></item><item><title>VividFlow: AI Image Enhancement &amp; Video Generation üé¨üé®</title><link>https://huggingface.co/posts/DawnC/872062429240283</link><description>VividFlow: AI Image Enhancement &amp; Video Generation üé¨üé® Bring your images to life with cinematic motion AND create stunning AI backgrounds! VividFlow combines professional-grade video generation with intelligent background replacement in one streamlined platform. üé≠ Dual Creative Powers Transform any static image into high-quality dynamic videos with smooth, natural motion ranging from 0.5 to 5 seconds. Choose from curated motion templates across 8 categories designed for portraits, products, landscapes, and artistic content. Create photorealistic backgrounds by selecting from 24 professionally crafted scene presets spanning studios, natural environments, urban settings, and artistic atmospheres...etc. ‚ö° Optimized Performance Video generation currently completes in 4-5 minutes with active optimization underway to dramatically reduce processing time. Background replacement finishes in 30-40 seconds after initial loading. The independent dual-tab design ensures smooth workflow without...</description><pubDate>Mon, 12 Jan 2026 13:46:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/872062429240283</guid></item><item><title>NEW MODEL:</title><link>https://huggingface.co/posts/unmodeled-tyler/815553899913449</link><description>NEW MODEL: vanta-research/mox-8b Hey everyone! I changed up my approach with this one a bit. Mox was designed with the following characteristics: - self coherence - direct opinions - epistemic confidence - grounded meta-awareness - reasoned refusals I've been thinking a lot about what "helpfulness" means lately. Commonly in AI, that looks like fulfilling user requests as closely as possible as long as the request isn't unsafe. But I wanted to know what it was like to build a model that might be helpful in the same way a human would be. For example, if you ask Mox to write a 10 page paper on the cultural significance of staplers, Mox will probably refuse, tell you that wouldn't be useful or helpful to ANYBODY and recommend a different, but more useful approach. Mox is still very much a work in progress, but I think that this is a good starting point! I'm already generating more datasets to add more elements to Mox's persona in future versions, which you should see on the hub soon!...</description><pubDate>Mon, 12 Jan 2026 13:46:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/unmodeled-tyler/815553899913449</guid></item><item><title>Claude Code Self &amp; Continual Learning</title><link>https://huggingface.co/posts/mindchain/535995505101203</link><description>Claude Code Self &amp; Continual Learning Hey everyone! üëã 30 GitHub Stars in 4 Days - Thank You! I'm really grateful for the positive response to the Claude Reflect System. In just 4 days, 30 developers have shown interest by starring the project. Thank you so much! What Is Claude Reflect? Correct once, never again. Claude Reflect helps Claude Code remember your corrections and preferences across sessions. Instead of repeating the same feedback, the system learns and applies it automatically. Main Features: üß† Learning System - Detects corrections and preferences from conversations - Stores them permanently in skill files - Applies learnings in future sessions üîí Safety First - Automatic backups before changes - YAML validation - Git version control ‚ö° Two Modes - Manual: Run /reflect when you want - Auto: Reflects automatically at session end How It Works If you correct Claude to use pytest instead of unittest, this preference gets saved. Next time, Claude will remember and use pytest...</description><pubDate>Mon, 12 Jan 2026 13:46:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mindchain/535995505101203</guid></item><item><title>ü•É Distilling Tiny Embeddings. We're happy to build on the BERT Hash Series of models with this new set of fixed dimensional tiny embeddings models.</title><link>https://huggingface.co/posts/davidmezzetti/443029997908966</link><description>ü•É Distilling Tiny Embeddings. We're happy to build on the BERT Hash Series of models with this new set of fixed dimensional tiny embeddings models. Ranging from 244K parameters to 970K and 50 dimensions to 128 dimensions these tiny models pack quite a punch. Use cases include on-device semantic search, similarity comparisons, LLM chunking and Retrieval Augmented Generation (RAG). The advantage is that data never needs to leave the device while still having solid performance. https://huggingface.co/blog/NeuML/bert-hash-embeddings See translation</description><pubDate>Mon, 12 Jan 2026 13:46:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/davidmezzetti/443029997908966</guid></item><item><title>NVFP4 With CUDA 13 Full Tutorial, 100%+ Speed Gain + Quality Comparison &amp; New Cheap Cloud SimplePod</title><link>https://huggingface.co/posts/MonsterMMORPG/671443723641634</link><description>NVFP4 With CUDA 13 Full Tutorial, 100%+ Speed Gain + Quality Comparison &amp; New Cheap Cloud SimplePod Full tutorial: https://www.youtube.com/watch?v=yOj9PYq3XYM Finally NVFP4 models has arrived to ComfyUI thus SwarmUI with CUDA 13. NVFP4 models are literally 100%+ faster with minimal impact on quality. I have done grid quality comparison to show you the difference on FLUX 2, Z Image Turbo and FLUX 1 of NVFP4 versions. To make CUDA 13 work, I have compiled Flash Attention, Sage Attention &amp; xFormers for both Windows and Linux with all of the CUDA archs to support literally all GPUs starting from GTX 1650 series, RTX 2000, 3000, 4000, 5000 series and more. In this full tutorial, I will show you how to upgrade your ComfyUI and thus SwarmUI to use latest CUDA 13 with latest libraries and Torch 2.9.1. Moreover, our compiled libraries such as Sage Attention works with all models on all GPUs without generating black images or videos such as Qwen Image or Wan 2.2 models. Hopefully LTX 2...</description><pubDate>Mon, 12 Jan 2026 13:46:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/671443723641634</guid></item><item><title>‚úÖ New Article: *Designing, Safeguarding, and Evaluating Learning Companions* (v0.1)</title><link>https://huggingface.co/posts/kanaria007/479061057773185</link><description>‚úÖ New Article: *Designing, Safeguarding, and Evaluating Learning Companions* (v0.1) Title: üõ°Ô∏è Designing, Safeguarding, and Evaluating SI-Core Learning Companions üîó https://huggingface.co/blog/kanaria007/designing-safeguarding-and-evaluating --- Summary: Most ‚ÄúAI tutoring‚Äù talks about prompts, content, and engagement graphs. But real learning companions‚Äîespecially for children / ND learners‚Äîfail in quieter ways: *the system ‚Äúworks‚Äù while stress rises, agency drops, or fairness erodes.* This article is a practical playbook for building SI-Core‚Äìwrapped learning companions that are *goal-aware (GCS surfaces), safety-bounded (ETH guardrails), and honestly evaluated (PoC ‚Üí real-world studies)*‚Äîwithout collapsing everything into a single score. &gt; Mastery is important, but not the only axis. &gt; *Wellbeing, autonomy, and fairness must be first-class.* --- Why It Matters: ‚Ä¢ Replaces ‚Äúone number‚Äù optimization with *goal surfaces* (and explicit anti-goals) ‚Ä¢ Treats *child/ND safety* as a runtime...</description><pubDate>Mon, 12 Jan 2026 13:46:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/479061057773185</guid></item><item><title>We (DataMuncher-Labs) just made a new dataset for inducing mathematical and analytical reasoning in LLMs.</title><link>https://huggingface.co/posts/Reality123b/421924644414133</link><description>We (DataMuncher-Labs) just made a new dataset for inducing mathematical and analytical reasoning in LLMs. DataMuncher-Labs/UltraMath-Reasoning-Small See translation</description><pubDate>Mon, 12 Jan 2026 13:46:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Reality123b/421924644414133</guid></item></channel></rss>