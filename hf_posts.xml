<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>How to compress long code context? üìö</title><link>https://huggingface.co/posts/YerbaPage/558970453952386</link><description>How to compress long code context? üìö Check out our LongCodeZip! Paper just got accepted to ASE 2025. üî• Code: https://github.com/YerbaPage/LongCodeZip Paper: LongCodeZip: Compress Long Context for Code Language Models (2510.00446) See translation</description><pubDate>Fri, 03 Oct 2025 09:22:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YerbaPage/558970453952386</guid></item><item><title>GLM-4.6 is hereüöÄ</title><link>https://huggingface.co/posts/AdinaY/926192442043020</link><description>GLM-4.6 is hereüöÄ zai-org/GLM-4.6 ‚ú® 200K context window ‚ú® Superior coding &amp; polished UI generation ‚ú® Stronger reasoning &amp; tool use ‚ú® More capable agents &amp; agent frameworks See translation</description><pubDate>Fri, 03 Oct 2025 09:22:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/AdinaY/926192442043020</guid></item><item><title>One of the hardest challenges in AI safety is finding the right balance: how do we protect people from harm without undermining their agency? This tension is especially visible in conversational systems, where safeguards can sometimes feel more paternalistic than supportive.</title><link>https://huggingface.co/posts/giadap/999915316832908</link><description>One of the hardest challenges in AI safety is finding the right balance: how do we protect people from harm without undermining their agency? This tension is especially visible in conversational systems, where safeguards can sometimes feel more paternalistic than supportive. In my latest piece for Hugging Face, I argue that open source and community-driven approaches offer a promising (though not exclusive) way forward. ‚ú® Transparency can make safety mechanisms into learning opportunities. ‚ú® Collaboration with diverse communities makes safeguards more relevant across contexts. ‚ú® Iteration in the open lets protections evolve rather than freeze into rigid, one-size-fits-all rules. Of course, this isn‚Äôt a silver bullet. Top-down safety measures will still be necessary in some cases. But if we only rely on corporate control, we risk building systems that are safe at the expense of trust and autonomy. Read the blog post here: https://huggingface.co/blog/giadap/preserving-agency See...</description><pubDate>Fri, 03 Oct 2025 09:22:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/giadap/999915316832908</guid></item><item><title>üöÄ Qwen3-Omni for Marketing: A Game-Changer</title><link>https://huggingface.co/posts/Sri-Vigneshwar-DJ/891594547382960</link><description>üöÄ Qwen3-Omni for Marketing: A Game-Changer Just wanted to share something exciting I've been exploring‚ÄîQwen3-Omni and how it's transforming marketing workflows. What makes it special? At Hawky.ai we are started experimenting with Qwen3 recently for Analysis and Optimization. Unlike traditional tools that look at text, images, or audio separately, Qwen3-Omni analyzes everything together. It handles 119 languages, processes 40-minute audio sequences, and understands both images and videos‚Äîall at once. The cool part? It's 2-3x faster than similar models thanks to its MoE architecture. Real applications I'm seeing: Ad Analysis: It scores video ads by combining visual elements, audio tone, and text‚Äîgiving 25% better CTR predictions than single-mode tools. Campaign Localization: Drop in one ad, get 10 localized versions with native voiceovers in under a minute. Perfect for testing across markets. Market Research: Feed it competitor content, podcasts, or UGC videos. It extracts actionable...</description><pubDate>Fri, 03 Oct 2025 09:22:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Sri-Vigneshwar-DJ/891594547382960</guid></item><item><title>Introducing</title><link>https://huggingface.co/posts/SelmaNajih001/721687692996128</link><description>Introducing SelmaNajih001/StockPredictionExplanation , built with GRPO and RAG: -GRPO trains the model to predict and explain stock direction. -RAG grounds explanations in historical financial news and central bank speeches. Together, they create a system that forecasts stock movements and shows the reasoning behind them. Full article: Explainable Financial Predictions ‚Äî https://huggingface.co/blog/SelmaNajih001/explainable-financial-predictions Try it here: StockPredictionExplanation Space ‚Äî SelmaNajih001/StockPredictionExplanation See translation</description><pubDate>Fri, 03 Oct 2025 09:22:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/SelmaNajih001/721687692996128</guid></item><item><title>üöÄ Big news from XenArcAI!</title><link>https://huggingface.co/posts/Parveshiiii/228189451590505</link><description>üöÄ Big news from XenArcAI! We‚Äôve just released our new dataset: **Bhagwat‚ÄëGita‚ÄëInfinity** üå∏üìñ ‚ú® What‚Äôs inside: - Verse‚Äëaligned Sanskrit, Hindi, and English - Clean, structured, and ready for ML/AI projects - Perfect for research, education, and open‚Äësource exploration üîó Hugging Face: XenArcAI/Bhagwat-Gita-Infinity Let‚Äôs bring timeless wisdom into modern AI together üôå See translation</description><pubDate>Fri, 03 Oct 2025 09:22:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Parveshiiii/228189451590505</guid></item><item><title>Hello everyone,</title><link>https://huggingface.co/posts/andywu-kby/790599686035068</link><description>Hello everyone, I hope you‚Äôre doing well. We‚Äôre currently developing a chatbot that can analyze and forecast sales directly from Excel files. Do you think this would be useful? Miragic-AI/Miragic-Sales-Pilot Please share your feedback by üëç or üëé this post. Best regards, See translation</description><pubDate>Fri, 03 Oct 2025 09:22:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/andywu-kby/790599686035068</guid></item><item><title>Hey, amazing, awesome people of the beautiful internet üòçü•∞</title><link>https://huggingface.co/posts/hba123/508032894003486</link><description>Hey, amazing, awesome people of the beautiful internet üòçü•∞ Distillation has been (from my point of view) a main driving factor for the success of hashtag#LLMs - like distilling the knowledge of an amazing big model (say hashtag#DeepSeekv3, or hashtag#GeminiAI) into yours. Probably, you have done it with minimising a KL divergence, and it somehow worked. Well, not that well, right? 1Ô∏è‚É£ Your model tends to memorise! 2Ô∏è‚É£ Your model might get the right answer, but its reasoning might be flawed. To fix those problems, we rethink distillation and process a new approach! A method that is based on constrained RL that comes with nice theoretical guarantees and excellent performance! Check it out: Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective (2509.22921) Let us do distillation right! Please upvote if you find it useful! See translation</description><pubDate>Fri, 03 Oct 2025 09:22:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hba123/508032894003486</guid></item><item><title>Try the Hugging Face Space demo for</title><link>https://huggingface.co/posts/prithivMLmods/704561076669428</link><description>Try the Hugging Face Space demo for Logics-MLLM/Logics-Parsing , the latest multimodal VLM from the Logics Team at Alibaba Group. It enables end-to-end document parsing with precise content extraction in markdown format, and it also generates a clean HTML representation of the document while preserving its logical structure. ü§óüî• Additionally, I‚Äôve integrated one of my recent works ‚Äî prithivMLmods/Gliese-OCR-7B-Post1.0 ‚Äî which also excels at document comprehension. ‚≠ê Space / App : prithivMLmods/Logics-Parsing-VLM üìÑ Technical Report by the Logics Team, Alibaba Group : Logics-Parsing Technical Report (2509.19760) ‚ö° Collections : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Other Pages: ‚ûî Multimodal VLMs - July'25 : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 ‚ûî Multimodal VLMs - Aug'25 : prithivMLmods/multimodal-vlms-aug25-68a56aac39fe8084f3c168bd ‚ûî VL caption ‚Äî &lt; Sep 15 ‚Äô25 : prithivMLmods/vl-caption-sep-15-25-68c7f6d737985c63c13e2391 . . ....</description><pubDate>Fri, 03 Oct 2025 09:22:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/704561076669428</guid></item><item><title>Want to deploy open models using vLLM as the inference engine?</title><link>https://huggingface.co/posts/sergiopaniego/392040363386800</link><description>Want to deploy open models using vLLM as the inference engine? We just released a step-by-step guide on how to do it with @ huggingface Inference Endpoints, now available in the vLLM docs. let the gpus go brrr https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints.html See translation</description><pubDate>Fri, 03 Oct 2025 09:22:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/392040363386800</guid></item></channel></rss>