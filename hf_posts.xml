<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>fav open-source multimodal reasoning model just got an update üî•</title><link>https://huggingface.co/posts/merve/432717492221522</link><description>fav open-source multimodal reasoning model just got an update üî• moonshotai/Kimi-VL-A3B-Thinking-2506 has &gt; smarter with less tokens, small size (only 3B active params!!!) &gt; better accuracy &gt; video reasoning &gt; higher resolution ü§Ø Read their blog https://huggingface.co/blog/moonshotai/kimi-vl-a3b-thinking-2506 See translation</description><pubDate>Tue, 24 Jun 2025 13:37:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/432717492221522</guid></item><item><title>Release picks of the past week is here!  Find more models, datasets, Spaces here</title><link>https://huggingface.co/posts/merve/128480916969769</link><description>Release picks of the past week is here! Find more models, datasets, Spaces here merve/june-20-releases-68594824d1f4dfa61aee3433 üñºÔ∏è VLMs/OCR &gt; moonshotai/Kimi-VL-A3B-Thinking-2506 is a powerful reasoning vision LM, 3B active params, smarter with less tokens, supports long documents, videos üëè (OS) &gt; nanonets/Nanonets-OCR-s is 3.75B params OCR model based on Qwen2.5VL-3B-Instruct (OS) üí¨ LLMs &gt; moonshotai/Kimi-Dev-72B is a strong coding model based on Qwen2.5-72B (OS) &gt; Mistral released mistralai/Mistral-Small-3.2-24B-Instruct-2506 , an update to their former model with better function calling &amp; instruction following (OS) üó£Ô∏è Audio &gt; Google released google/magenta-realtime , real time music generation &amp; audio synthesis (cc-by-4) &gt; kyutai released new speech-to-text models that come in 1B &amp; 2B ( kyutai/stt-1b-en_fr , stt-2b-en_fr) with 0.5s and 2.5s delay 3D &gt; Tencent released tencent/Hunyuan3D-2.1 an image-to-3D model (see below) See translation</description><pubDate>Tue, 24 Jun 2025 13:37:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/128480916969769</guid></item><item><title>üí´ Next-Level On-Device AI Showdown</title><link>https://huggingface.co/posts/yeonseok-zeticai/882369268235109</link><description>üí´ Next-Level On-Device AI Showdown ü™Ω See It to Believe It, How QWEN4b works at On-device environment without expensive GPU Cloud server? We‚Äôve crafted a side-by-side demo video showcasing both Jan-Nano and QWEN 4B in action‚Äîno more wondering which model reigns supreme. Click play, compare their speeds, accuracy, and memory footprints, and decide which one fits your needs best! üëã Why You Can‚Äôt Miss This We are actively creating runnable sLLM environments for On-device AI. You can just build On-device AI apps within few hours. Including Jan-Nano, QWEN4b, there are several sLLM models ready to be used on your AI application!. ü§ë Please feel free to use, because it is free to use!. Ready to Compare? Watch now, draw your own conclusions, and let us know which model you‚Äôd deploy in your next edge-AI project! üåçüí° #OnDeviceAI #EdgeAI #AIShowdown #MLOptimization #DemoVideo #AIComparison See translation</description><pubDate>Tue, 24 Jun 2025 13:37:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/yeonseok-zeticai/882369268235109</guid></item><item><title>10 Techniques for Boosting LLM Reasoning in 2025</title><link>https://huggingface.co/posts/Kseniase/930317125764918</link><description>10 Techniques for Boosting LLM Reasoning in 2025 Everyone‚Äôs chasing top reasoning, but sometimes it's still the bottleneck for many real-world tasks. This week, let's spotlight some powerful techniques that have shown promise in helping LLMs achieve more consistent logic, planning, and depth: 1. Retrieval-Augmented CoT Chaining (RAG+CoT) -&gt; CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models (2504.13534) Combines Chain-of-Thought prompting with retrieval augmentation at intermediate steps. Relevant documents are fetched after each reasoning subgoal, updating context dynamically. Great for open-domain QA, math, logic and multi-hop fact-checking 2. Tool-use by example injection -&gt; Self-Training Large Language Models for Tool-Use Without Demonstrations (2502.05867) Injects few-shot tool interaction examples during training to implicitly teach calling patterns. Helps in plug-and-play tool use without training new...</description><pubDate>Tue, 24 Jun 2025 13:37:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/930317125764918</guid></item><item><title>Hello everyone, happy to share with you my experimentation of a Deep Research Assistant, using 7 agents and a quality assurance pipeline :</title><link>https://huggingface.co/posts/mallocode200/873699618032778</link><description>Hello everyone, happy to share with you my experimentation of a Deep Research Assistant, using 7 agents and a quality assurance pipeline : ü§ñ What makes this special: ‚úÖ Agent-Based Architecture - 7 specialised AI agents working together: - Planner Agent - Strategic search planning - Search Agent - Multi-source web research - Writer Agent - Comprehensive report generation - Evaluator Agent - Automatic quality assessment - Optimiser Agent - Iterative improvement when needed - Email Agent - Professional report delivery - Clarifier Agent - Interactive query refinement ‚úÖ Quality Assurance Pipeline - Every report is scored (1-10) and automatically improved if it scores below 7/10 ‚úÖ Multiple Research Modes - From quick queries to deep, clarification-driven analysis ‚úÖ Production-Ready - Deployed on Hugging Face Spaces with comprehensive documentation üîß Technical Stack: - Frontend: Gradio with theme-adaptive UI - Backend: OpenAI Agents framework - Integration: SendGrid for email delivery -...</description><pubDate>Tue, 24 Jun 2025 13:37:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/mallocode200/873699618032778</guid></item><item><title>Awesome intro to LLM course "Language Modeling from Scratch" by stanford. love the aesthetics behind the lecture notes, notes-in-code genius ideaüëç</title><link>https://huggingface.co/posts/Jaward/445538723467397</link><description>Awesome intro to LLM course "Language Modeling from Scratch" by stanford. love the aesthetics behind the lecture notes, notes-in-code genius ideaüëç Course site: https://stanford-cs336.github.io/spring2025/ Repo: https://github.com/stanford-cs336/spring2025-lectures Videos: https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_ See translation</description><pubDate>Tue, 24 Jun 2025 13:37:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/445538723467397</guid></item><item><title>Was going to post this on /r/LocalLLaMa, but apparently it's without moderation at this time :')</title><link>https://huggingface.co/posts/bartowski/460622149989234</link><description>Was going to post this on /r/LocalLLaMa, but apparently it's without moderation at this time :') bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF Was able to use previous mistral chat templates, some hints from Qwen templates, and Claude to piece together a seemingly working chat template, tested it with llama.cpp server and got perfect results, though lmstudio still seems to be struggling for some reason (don't know how to specify a jinja file there) Outlined the details of the script and results in my llama.cpp PR to add the jinja template: https://github.com/ggml-org/llama.cpp/pull/14349 Start server with a command like this: ./llama-server -m /models/mistralai_Mistral-Small -3 .2 -24 B-Instruct -2506 -Q4_K_M.gguf --jinja --chat-template-file /models/Mistral-Small -3 .2 -24 B-Instruct -2506 .jinja and it should be perfect! Hoping it'll work for ALL tools if lmstudio gets an update or something, not just llama.cpp, but very happy to see it works flawlessly in llama.cpp...</description><pubDate>Tue, 24 Jun 2025 13:37:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/bartowski/460622149989234</guid></item><item><title>Updated the docscopeOCR-7B-050425-exp with the DREX-062225-exp, with improved preciseness in table structure and line spacing in the markdown used on the document page. And though this is still an experimental one, it's expected to perform well in the defined DREX use cases [ Document Retrieval and Extraction eXpert ‚Äì experimental ocr ]. üíª</title><link>https://huggingface.co/posts/prithivMLmods/359250136885875</link><description>Updated the docscopeOCR-7B-050425-exp with the DREX-062225-exp, with improved preciseness in table structure and line spacing in the markdown used on the document page. And though this is still an experimental one, it's expected to perform well in the defined DREX use cases [ Document Retrieval and Extraction eXpert ‚Äì experimental ocr ]. üíª ‚§∑ Model : prithivMLmods/DREX-062225-exp ‚§∑ Demo : prithivMLmods/Doc-VLMs ‚§∑ Collection : prithivMLmods/doc-vl-685839064a863e1cd23be3f1 ‚§∑ Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 ‚§∑ Git : https://github.com/PRITHIVSAKTHIUR/DREX.git . . . To know more about it, visit the model card of the respective model. !! See translation</description><pubDate>Tue, 24 Jun 2025 13:37:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/359250136885875</guid></item><item><title># The future trends of Explainable AI in 2024</title><link>https://huggingface.co/posts/ghostai1/349800616277230</link><description># The future trends of Explainable AI in 2024 The world of artificial intelligence (AI) is constantly evolving, with new advancements and applications emerging every day. One trend that has captured the attention of many is Explainable AI. As the name suggests, this revolutionary technology aims to provide a clear, understandable explanation for the decisions and actions taken by AI systems. In the future, Explainable AI is expected to become even more sophisticated, with advanced algorithms and techniques being developed to better interpret and analyze the vast amounts of data generated by AI systems. This will not only make AI systems more reliable and trustworthy, but it will also help to demystify the world of AI, making it more accessible to a wider audience. As the demand for AI solutions grows, the need for Explainable AI will become increasingly important. Businesses, governments, and individuals will require clear, concise explanations for the AI systems they are using,...</description><pubDate>Tue, 24 Jun 2025 13:37:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ghostai1/349800616277230</guid></item><item><title>Hi all!</title><link>https://huggingface.co/posts/CultriX/162887385956065</link><description>Hi all! I was hoping somebody would be willing to check out this thought experiment of mine with the aim to reduce tokens in inter-agent communications. How It Works: 1. You provide a task in natural language (NL) 2. NL-to-CCL Agent: Converts your request into a structured Compressed Communication Language (CCL) task. 3. Inter-agent communication occurs in CCL 4. CCL is translated back to NL before being presented to the user. I have a notebook with an example that claims to achieve these results: --- Token Usage Summary --- Total NL Tokens (User Input &amp; Final Output): 364 Total CCL Tokens (for NL/CCL Conversions): 159 Total CCL Tokens (Internal Agent Communication): 194 Overall token savings on NL-to-CCL conversion portions: 56.32% ------------------------ When asking Gemini it concludes: "Yes, the methods used in this notebook are sensible. The multi-agent architecture is logical, and the introduction of a Compressed Communication Language (CCL) is a clever and practical solution...</description><pubDate>Tue, 24 Jun 2025 13:37:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/CultriX/162887385956065</guid></item></channel></rss>