<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Can AI models trained solely on 100% synthetic data achieve top-tier accuracy in real-world object detection?</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/399941870774449</link><description>Can AI models trained solely on 100% synthetic data achieve top-tier accuracy in real-world object detection? ğŸ‘‰ Sergio Sanz, PhD just proved it while winning Duality AIâ€™s Synthetic-to-Real Object Detection Challenge using Falcon-generated imagery. His model achieved perfect real-world detection accuracy without a single real image in the training loop. In this blog, Dr. Sanz walks us through his method, which includes the design and training of an advanced pipeline to achieve 100% detection accuracy. His full technical breakdown covers: ğŸ“ Synthetic-only training ğŸ“ Data augmentation with an ensemble learning approach for better generalization ğŸ“ Custom occlusion generation ğŸ“ A Faster R-CNN model fine-tuned with Falcon generated data ğŸ“ And much more! The results speak for themselves! ğŸ“– Read the blog here: https://www.duality.ai/blog/leveraging-synthetic-data-for-real-world-object-detection Congratulations Sergio! We can't wait to see what you do next. ğŸ”” Ready to take on the next...</description><pubDate>Sun, 15 Jun 2025 09:23:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/399941870774449</guid></item><item><title>this repo is gold! a collection of LLM apps with multi-agents, MCP, RAG and so much more.</title><link>https://huggingface.co/posts/hesamation/842061188959684</link><description>this repo is gold! a collection of LLM apps with multi-agents, MCP, RAG and so much more. the best way to learn is by building, and this repo provides the blueprint. Repo: https://github.com/Shubhamsaboo/awesome-llm-apps See translation</description><pubDate>Sun, 15 Jun 2025 09:23:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hesamation/842061188959684</guid></item><item><title>New diffusion model for text-to-image and video-to-world generation: Cosmos Predict-2 ğŸ‘½</title><link>https://huggingface.co/posts/a-r-r-o-w/709852031491261</link><description>New diffusion model for text-to-image and video-to-world generation: Cosmos Predict-2 ğŸ‘½ Model collection: nvidia/cosmos-predict2-68028efc052239369a0f2959 Diffusers support: https://github.com/huggingface/diffusers/pull/11695 Documentation: https://huggingface.co/docs/diffusers/main/en/api/pipelines/cosmos These are results with the 2B param model. Imagine what you could do with the 14B version! Go check it out now! See translation</description><pubDate>Sun, 15 Jun 2025 09:23:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/a-r-r-o-w/709852031491261</guid></item><item><title>New Research: Theoretical Foundations for In-Context Learning in Transformers</title><link>https://huggingface.co/posts/codelion/378799954783125</link><description>New Research: Theoretical Foundations for In-Context Learning in Transformers I'm excited to share our latest theoretical work that formally proves an interesting property of large language models: base transformer models can approximate fine-tuned capabilities using only inference-time techniques like in-context learning. The core question we investigated: Can specialized behaviors typically acquired through expensive supervised fine-tuning be elicited from base models without any parameter updates? Our theoretical contribution: We provide a formal proof, grounded in the Turing completeness of transformers, showing that this is indeed possible under certain assumptions. The work establishes mathematical bounds on the minimal dataset sizes needed for approximation. Key theoretical results: - For text generation tasks: O(mV/ÎµÂ²) examples suffice (where m = number of contexts, V = vocabulary size, Îµ = error tolerance) - For linear classification: O(d/Îµ) examples (where d = input...</description><pubDate>Sun, 15 Jun 2025 09:23:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/codelion/378799954783125</guid></item><item><title>Recently, I've been focusing my learning on the following topics:</title><link>https://huggingface.co/posts/a-r-r-o-w/231008365980283</link><description>Recently, I've been focusing my learning on the following topics: - Pytorch internals, specifically the inductor system (roughly ~1 month of experience) - Triton internals (~8 moe) - CUDA (~3 moe) - Understanding fusion patterns in compilers and how to improve them (~1 moe) - Parallelism strategies for large scale inference optimization (~6-7 moe) I thought it would be nice to document it somewhere for no particular reason. Maybe someone will find it useful? It's also because I want to get into the habit of writing, but had no motivation to do so. Maybe writing short informal posts will help build the habit. Since I don't have a personal site, and don't plan to create one in the near future, I think HF posts are best suited for short and informal documentation to share my little discoveries and learnings. If you're interested, strap in! First post in this series will be on basic study of Pytorch's float32 matmuls and their Triton implementation (nothing much, just the tutorial...</description><pubDate>Sun, 15 Jun 2025 09:23:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/a-r-r-o-w/231008365980283</guid></item><item><title>âš¡ FusionX Enhanced Wan 2.1 I2V (14B) ğŸ¬</title><link>https://huggingface.co/posts/seawolf2357/480409853177984</link><description>âš¡ FusionX Enhanced Wan 2.1 I2V (14B) ğŸ¬ ğŸš€ Revolutionary Image-to-Video Generation Model Generate cinematic-quality videos in just 8 steps! Heartsync/WAN2-1-fast-T2V-FusioniX âœ¨ Key Features ğŸ¯ Ultra-Fast Generation: Premium quality in just 8-10 steps ğŸ¬ Cinematic Quality: Smooth motion with detailed textures ğŸ”¥ FusionX Technology: Enhanced with CausVid + MPS Rewards LoRA ğŸ“ Optimized Resolution: 576Ã—1024 default settings âš¡ 50% Speed Boost: Faster rendering compared to base models ğŸ› ï¸ Technical Stack Base Model: Wan2.1 I2V 14B Enhancement Technologies: ğŸ”— CausVid LoRA (1.0 strength) - Motion modeling ğŸ”— MPS Rewards LoRA (0.7 strength) - Detail optimization Scheduler: UniPC Multistep (flow_shift=8.0) Auto Prompt Enhancement: Automatic cinematic keyword injection ğŸ¨ How to Use Upload Image - Select your starting image Enter Prompt - Describe desired motion and style Adjust Settings - 8 steps, 2-5 seconds recommended Generate - Complete in just minutes! ğŸ’¡ Optimization Tips âœ… Recommended Settings:...</description><pubDate>Sun, 15 Jun 2025 09:23:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/seawolf2357/480409853177984</guid></item><item><title>Say hallo to GermaNER ğŸ’ªâ€“ a lightweight, high-accuracy NER model for German texts, powered by XLM-RoBERTa + LoRA adapters!</title><link>https://huggingface.co/posts/zamal/577847741789622</link><description>Say hallo to GermaNER ğŸ’ªâ€“ a lightweight, high-accuracy NER model for German texts, powered by XLM-RoBERTa + LoRA adapters! âš¡ Fast, efficient, and open-source â€“ perfect for tagging names, places &amp; orgs in real-world German data. Try it now on Hugging Face ğŸ‘‰ fau/GermaNER See translation</description><pubDate>Sun, 15 Jun 2025 09:23:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/zamal/577847741789622</guid></item><item><title>ğŸ¤— I'm leading 'Openfree AI', Korea's most prominent AI open-source community. First and foremost, I'd like to express my deepest gratitude for Hugging Face's continuous support and efforts. ğŸ’™</title><link>https://huggingface.co/posts/openfree/428786122279500</link><description>ğŸ¤— I'm leading 'Openfree AI', Korea's most prominent AI open-source community. First and foremost, I'd like to express my deepest gratitude for Hugging Face's continuous support and efforts. ğŸ’™ Our Openfree AI collaborates with various AI communities across Korea, contributing to knowledge sharing and ecosystem development. ğŸ¤ I've been actively promoting the critical importance of Hugging Face as Korea's AI infrastructure backbone, engaging with senior government officials, National Assembly members, university leaders, and media executives to emphasize how Hugging Face represents Korea's AI future at a national policy level. I consider myself a 'voluntary Korean ambassador for Hugging Face'. ğŸ‡°ğŸ‡·âœ¨ Let me share our community's achievements on the Hugging Face platform over the past year: ğŸ¯ ğŸš€ Published hundreds of models and spaces ğŸ‘¥ Surpassed 10 million cumulative visitors ğŸ“ˆ Achieved 1.7 million Monthly Active Users (MAU) ğŸ¨ Generated over 1 million images/videos per month These...</description><pubDate>Sun, 15 Jun 2025 09:23:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/428786122279500</guid></item><item><title>Did you know how simple it was to get started with your own custom compiler backend with</title><link>https://huggingface.co/posts/a-r-r-o-w/723587414820562</link><description>Did you know how simple it was to get started with your own custom compiler backend with torch.compile ? What's stopping you from writing your own compiler? import torch from torch._functorch.partitioners import draw_graph def compiler ( fx_module: torch.fx.GraphModule, _ ): draw_graph(fx_module, f"compile.dot" ) return fx_module.forward def capture ( model, *inputs ): compiled_model = torch. compile (model, backend=compiler) y = compiled_model(*inputs) y. sum ().backward() class MLP (torch.nn.Module): def __init__ ( self ): super ().__init__() self.linear_1 = torch.nn.Linear( 16 , 32 ) self.linear_2 = torch.nn.Linear( 32 , 16 ) def forward ( self, x ): x = self.linear_1(x) x = torch.nn.functional.silu(x) x = self.linear_2(x) return x if __name__ == '__main__' : model = MLP() model.to( "mps" ) x = torch.randn( 4 , 16 , device= "mps" , dtype=torch.float32) capture(model, x) -------------- Part of https://huggingface.co/posts/a-r-r-o-w/231008365980283 See translation</description><pubDate>Sun, 15 Jun 2025 09:23:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/a-r-r-o-w/723587414820562</guid></item><item><title>ğŸ¬ VEO3 Directors - All-in-One AI Video Creation Suite</title><link>https://huggingface.co/posts/ginipick/718905723783644</link><description>ğŸ¬ VEO3 Directors - All-in-One AI Video Creation Suite ğŸš€ What is VEO3 Directors? VEO3 Directors is a revolutionary end-to-end AI video creation platform that transforms your ideas into cinematic reality. From story conception to final video with synchronized audio - all in one seamless workflow! ğŸ”— Try It Now ginigen/VEO3-Directors ginigen/VEO3-Free ginigen/VEO3-Free-mirror âœ¨ Key Features ğŸ“ Story Seed Generator ğŸ² Instantly generate creative story ideas across multiple genres ğŸŒ Bilingual support (English/Korean) ğŸ­ Rich categories: Genre, Setting, Characters, and more ğŸ¥ AI Script &amp; Prompt Crafting ğŸ’¬ Powered by Friendli API for Hollywood-quality prompts ğŸ¤– AI Director writes detailed cinematography instructions ğŸ¬ Professional elements: camera movements, lighting, VFX ğŸ¬ Video + Audio Generation ğŸ¨ Wan2.1-T2V-14B for stunning visual quality âš¡ NAG 4-step inference - 10x faster generation ğŸµ MMAudio auto-generates matching soundscapes ğŸ›ï¸ Full control over resolution, duration, and style...</description><pubDate>Sun, 15 Jun 2025 09:23:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/718905723783644</guid></item></channel></rss>