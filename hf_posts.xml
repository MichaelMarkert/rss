<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>I have concluded first 8 traininings of Qwen Image LoRA - we are not at the level of FLUX yet and next 8 trainings starting hopefully - 2656x2656px image generated with 8 steps Fast Qwen LoRA + myself trained LoRA :</title><link>https://huggingface.co/posts/MonsterMMORPG/683040638338113</link><description>I have concluded first 8 traininings of Qwen Image LoRA - we are not at the level of FLUX yet and next 8 trainings starting hopefully - 2656x2656px image generated with 8 steps Fast Qwen LoRA + myself trained LoRA : Grid test results shared here along with App installer : https://www.patreon.com/posts/137551634 See translation</description><pubDate>Thu, 04 Sep 2025 05:20:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/683040638338113</guid></item><item><title>large AI labs have dropped so many open models last week ğŸ”¥ don't miss out on them</title><link>https://huggingface.co/posts/merve/771481819901416</link><description>large AI labs have dropped so many open models last week ğŸ”¥ don't miss out on them â†’ Apple released on-device vision LMs apple/fastvlm-68ac97b9cd5cacefdd04872e &amp; apple/mobileclip2-68ac947dcb035c54bcd20c47 â†’ OpenGVLab released InternVL3.5, 32 new vision LMs with one based on gpt-oss! (OS) OpenGVLab/internvl35-68ac87bd52ebe953485927fb â†’ MSFT released a killer small TTS model (OS) microsoft/VibeVoice-1.5B find more herehttps://huggingface.co/collections/merve/august-29-releases-68b5a3754cfb8abf59e2b486 See translation</description><pubDate>Thu, 04 Sep 2025 05:20:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/771481819901416</guid></item><item><title>Supercharge Appleâ€™s Shortcuts using Cloudflare Workers and Gemini within minutes (and for free, up to 1,500 requests per day) â˜ï¸âœ¨</title><link>https://huggingface.co/posts/louisbrulenaudet/591445663705551</link><description>Supercharge Appleâ€™s Shortcuts using Cloudflare Workers and Gemini within minutes (and for free, up to 1,500 requests per day) â˜ï¸âœ¨ Hello everyone, last week, while experimenting for fun, I created an API that allows you to easily access AI models (in this case, Google's) from the Shortcut app in order to analyze data from my apps and make the most of it thanks to the generative capabilities of advanced models. It costs me nothing, and I think it might be good to share it so that others can build on it. In README.md, you will find everything you need to get started and put your own microservice into production, which you can call from the appâ€™s HTTP request features. You will simply be asked to have a free Cloudflare account and an API key obtained from Google's AI Studio. Feel free to take a look and get back to me if you encounter any problems during deployment. Here is the GitHub repo where you can find all the source code and run it on your own:...</description><pubDate>Thu, 04 Sep 2025 05:20:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/louisbrulenaudet/591445663705551</guid></item><item><title>FastVLMs by Apple are the talk of the week for edge device VLMs and also for consumer-grade VLMs on the Hub. They have some impressive demos available on the Hub for live captioning and inference tasks. Meanwhile, Iâ€™m still exploring one of the coolest edge-device multimodal releasesâ€”Liquid AIâ€™s LFM2-VL (450M and 1.6B). Iâ€™ve also made a live camera video inference demo, which is capable of running on Colabâ€™s free-tier T4 GPU.</title><link>https://huggingface.co/posts/prithivMLmods/632863448558657</link><description>FastVLMs by Apple are the talk of the week for edge device VLMs and also for consumer-grade VLMs on the Hub. They have some impressive demos available on the Hub for live captioning and inference tasks. Meanwhile, Iâ€™m still exploring one of the coolest edge-device multimodal releasesâ€”Liquid AIâ€™s LFM2-VL (450M and 1.6B). Iâ€™ve also made a live camera video inference demo, which is capable of running on Colabâ€™s free-tier T4 GPU. ğŸ¤—Live Captioning Notebooks: â  LiquidAI LFM2 VL 1.6B Live Cam: https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-Notebooks/blob/main/LiquidAI-LFM2-VL-Live-Cam/LiquidAI_LFM2_VL_1_6B_Live_Cam.ipynb â  LiquidAI LFM2 VL 450M Live Cam: https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-Notebooks/blob/main/LiquidAI-LFM2-VL-Live-Cam/LiquidAI_LFM2_VL_450M_Live_Cam.ipynb âœ¨I also made a demo for the FastVLM Live Captioning Notebook. â  FastVLM 0.5B Live Cam: https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-Notebooks/blob/main/Apple-FastVLM-0.5B-Live-...</description><pubDate>Thu, 04 Sep 2025 05:20:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prithivMLmods/632863448558657</guid></item><item><title>PawMatchAI â€” Now with SBERT-Powered Recommendations! ğŸ¶âœ¨</title><link>https://huggingface.co/posts/DawnC/381537695345047</link><description>PawMatchAI â€” Now with SBERT-Powered Recommendations! ğŸ¶âœ¨ â­ï¸ NEW: Description-based recommendations are here! Just type in your lifestyle or preferences (e.g. â€œI live in an apartment and want a quiet dogâ€), and PawMatchAI uses SBERT semantic embeddings to understand your needs and suggest compatible breeds. What can PawMatchAI do today? ğŸ“¸ Upload a photo to identify your dog from 124 breeds with detailed info. âš–ï¸ Compare two breeds side-by-side, from grooming needs to health insights. ğŸ“Š Visualize breed traits with radar and comparison charts. ğŸ¨ Try Style Transfer to turn your dogâ€™s photo into anime, watercolor, cyberpunk, and more. Whatâ€™s next? ğŸ¯ More fine-tuned recommendations. ğŸ“± Mobile-friendly deployment. ğŸ¾ Expansion to additional species. My goal: To make breed discovery not only accurate but also interactive and fun â€” combining computer vision, semantic understanding, and creativity to help people find their perfect companion. ğŸ‘‰ Try it here: DawnC/PawMatchAI If you enjoy...</description><pubDate>Thu, 04 Sep 2025 05:20:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/381537695345047</guid></item><item><title>We released a new competition!</title><link>https://huggingface.co/posts/DualityAI-RebekahBogdanoff/955693162356423</link><description>We released a new competition! https://www.kaggle.com/competitions/grocery-items-multi-class-object-detection/overview Join to: ğŸ’¡ Learn from others ğŸ¤” Develop your Sim2Real skills using simulation ğŸ“¸ Generate custom data on the cloud to improve your model âœ¨ and more! See translation</description><pubDate>Thu, 04 Sep 2025 05:20:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DualityAI-RebekahBogdanoff/955693162356423</guid></item><item><title>ğŸš€ğŸš€ğŸš€ The largest ever dataset of co-folded 3D protein-ligand structures just dropped on HF!!</title><link>https://huggingface.co/posts/cgeorgiaw/327580602040047</link><description>ğŸš€ğŸš€ğŸš€ The largest ever dataset of co-folded 3D protein-ligand structures just dropped on HF!! Meet SAIR (Structurally Augmented ICâ‚…â‚€ Repository): 5M+ AI-generated complexes with experimentally measured drug potency data from SandboxAQ. ğŸš€ğŸš€ğŸš€ Check it out and explore here: SandboxAQ/SAIR See translation</description><pubDate>Thu, 04 Sep 2025 05:20:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/cgeorgiaw/327580602040047</guid></item><item><title>Super excited to announce that our research team at Hugging Face will be doing an AMA on reddit r/LocalLLaMA.</title><link>https://huggingface.co/posts/eliebak/258004860353376</link><description>Super excited to announce that our research team at Hugging Face will be doing an AMA on reddit r/LocalLLaMA. Come ask any questions to the team behind SmolLM, FineWeb and more! And who knows, maybe thereâ€™ll be a shiny new release to talk about? Thursday 4th September, 8AM-11AM PST ğŸ¤— science See translation</description><pubDate>Thu, 04 Sep 2025 05:20:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/eliebak/258004860353376</guid></item><item><title>Releasing the Jupyter Agent Dataset! ğŸš€</title><link>https://huggingface.co/posts/hannayukhymenko/737771230390400</link><description>Releasing the Jupyter Agent Dataset! ğŸš€ Built from 7 TB of real Kaggle datasets + 20k notebooks, creating real code exec traces using Qwen3-Coder and E2B. Training on this data dramatically improves the ability to execute code and analyze data. We ( @ baptistecolle @ hannayukhymenko @ lvwerra ) have created a novel synthetic data generation pipeline with efficient scaffolding, which gives a big performance boost after training your coding agentğŸ”¥With the help of real Kaggle notebooks and datasets we generate synthetic notebooks which aim to analyze datasets and answer factual questions about them more efficiently. We simulate a real code execution environment by prompting LLMs or with the help of E2B sandboxes. We have built a dataset of 50k+ high-quality LLM-generated notebooks which can help your agent become better at performing data analysis and question answering. Link: data-agents/jupyter-agent-dataset See translation</description><pubDate>Thu, 04 Sep 2025 05:20:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hannayukhymenko/737771230390400</guid></item><item><title>Introducing ShortiFoley ğŸµ â€” an AI tool that transforms short videos into realistic Foley audio.</title><link>https://huggingface.co/posts/Bils/120204910303830</link><description>Introducing ShortiFoley ğŸµ â€” an AI tool that transforms short videos into realistic Foley audio. Built on Tencentâ€™s HunyuanVideo-Foley with SigLIP2 + CLAP, and designed for media automation pipelines like n8n âœ… Generate Foley from video âœ… Autosave results with metadata âœ… MCP endpoints for workflows Bils/ShortiFoley See translation</description><pubDate>Thu, 04 Sep 2025 05:20:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Bils/120204910303830</guid></item></channel></rss>