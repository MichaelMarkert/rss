<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>6 Comprehensive Resources on AI Coding</title><link>https://huggingface.co/posts/Kseniase/300455492795256</link><description>6 Comprehensive Resources on AI Coding AI coding is moving fast, and itâ€™s getting harder to tell what actually works. Agents, workflows, context management and many other aspects are reshaping how software gets built. Weâ€™ve collected a set of resources to help you understand how AI coding is evolving today and what building strategies work best: 1. AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities (2508.11126) Provides a clear taxonomy, compares agent architectures, and exposes practical gaps in tools, benchmarks, and reliability that AI coding agents now struggle with 2. Does AI-Assisted Coding Deliver? A Difference-in-Differences Study of Cursor's Impact on Software Projects (2511.04427) This survey from Carnegie Mellon University shows causal evidence that LLM agent assistants deliver short-term productivity gains but have lasting quality costs that can slow development over time 3. A Survey of Vibe Coding with Large Language Models (2510.12399) Turns...</description><pubDate>Mon, 15 Dec 2025 13:44:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Kseniase/300455492795256</guid></item><item><title>Check out your 2025 Hugging Face Wrapped, a small experimental recap</title><link>https://huggingface.co/posts/daqc/540565360726745</link><description>Check out your 2025 Hugging Face Wrapped, a small experimental recap hf-wrapped/2025 See translation</description><pubDate>Mon, 15 Dec 2025 13:44:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/daqc/540565360726745</guid></item><item><title>I wasted days on a GPU node on a bug that shouldn't exist</title><link>https://huggingface.co/posts/martinsu/305383997158992</link><description>I wasted days on a GPU node on a bug that shouldn't exist So I was fine-tuning TildeOPEN-30B and the outputs were... weird. Token ID 179 (&lt;0x00&gt;) kept appearing between almost every token pair. Took me a bit to figure out what was going on. Turns out I used the fast tokenizer for training, but the model was trained on the slow one. Silent failure. Well... long story shortâ€”TGI uses (forces) the fast tokenizer, no questions asked. And you'll have agile's kryptonite: silent failure. If the model was trained on slow, it's a silent disaster. I got curious and wrote a quick script to check how common this is. Ran it on 6,014 LLM HF models overnight. Roughly 10% of HF model downloads have mismatched tokenizers. Not all mismatches are catastrophic, but some are brutal â€” like chat template markers inflating from 1 token to 3, silently wrecking context windows and causing model act weird. This wasn't rigorous research, but the drift is real. And the worst part? 968 models(out of 500+...</description><pubDate>Mon, 15 Dec 2025 13:44:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/martinsu/305383997158992</guid></item><item><title>ğŸš€ Introducing VideoCoF: Unified Video Editing with a Temporal Reasoner (Chain-of-Frames)!</title><link>https://huggingface.co/posts/sanaka87/963485970840656</link><description>ğŸš€ Introducing VideoCoF: Unified Video Editing with a Temporal Reasoner (Chain-of-Frames)! Weâ€™re excited to introduce VideoCoF, a unified framework for instruction-based video editing that enables temporal reasoning and ~4Ã— video length extrapolation, trained with only 50k video pairs. ğŸ”¥ ğŸ” What makes VideoCoF different? ğŸ§  Chain-of-Frames reasoning , mimic human thinking process like Seeing â†’ Reasoning â†’ Editing to apply edits accurately over time without external masks, ensuring physically plausible results. ğŸ“ˆ Strong length generalization â€” trained on 33-frame clips, yet supports multi-shot editing and long-video extrapolation (~4Ã—). ğŸ¯ Unified fine-grained editing â€” Object Removal, Addition, Swap, and Local Style Transfer, with instance-level &amp; part-level, spatial-aware control. âš¡ Fast inference update ğŸš€ H100: ~20s / video with 4-step inference, making high-quality video editing far more practical for real-world use. ğŸ”— Links ğŸ“„ Paper: https://arxiv.org/abs/2512.07469 ğŸ’» Code:...</description><pubDate>Mon, 15 Dec 2025 13:44:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sanaka87/963485970840656</guid></item><item><title>How POTUS Completely Broke My Flash 2.5-Based Guardrail</title><link>https://huggingface.co/posts/martinsu/283521322948177</link><description>https://huggingface.co/blog/martinsu/potus-broke-my-pipeline How POTUS Completely Broke My Flash 2.5-Based Guardrail Did quite a bit of deep research on this one, since it IMHO matters. At first I used this story to amuse fellow MLOps guys, but then I went deeper and was surprised. To those who don't want to read too much, in plain English: when you give the model a high-stakes statement that clashes with what it "knows" about the world, it gets more brittle. Sometimes to a point of being unusable. Or an even shorter version: do not clash with the model's given worldviewâ€”it will degrade to some extent. And in practice, it means that in lower-resource languages like Latvian and Finnish (and probably others), Flash 2.5 is an unreliable guardrail model when something clashes with the model's general "worldview". However, I'm sure this degradation applies to other languages and models as well to varying extents. In one totally normal week of MLOps, my news summarization pipeline started...</description><pubDate>Mon, 15 Dec 2025 13:44:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/martinsu/283521322948177</guid></item><item><title>ğŸš€ Introducing VideoCoF: Unified Video Editing with a Temporal Reasoner (Chain-of-Frames)!</title><link>https://huggingface.co/posts/XiangpengYang/572526773544518</link><description>ğŸš€ Introducing VideoCoF: Unified Video Editing with a Temporal Reasoner (Chain-of-Frames)! Weâ€™re excited to introduce VideoCoF, a unified framework for instruction-based video editing that enables temporal reasoning and ~4Ã— video length extrapolation, trained with only 50k video pairs. ğŸ”¥ ğŸ” What makes VideoCoF different? ğŸ§  Chain-of-Frames reasoning , mimic human thinking process like Seeing â†’ Reasoning â†’ Editing to apply edits accurately over time without external masks, ensuring physically plausible results. ğŸ“ˆ Strong length generalization â€” trained on 33-frame clips, yet supports multi-shot editing and long-video extrapolation (~4Ã—). ğŸ¯ Unified fine-grained editing â€” Object Removal, Addition, Swap, and Local Style Transfer, with instance-level &amp; part-level, spatial-aware control. âš¡ Fast inference update ğŸš€ H100: ~20s / video with 4-step inference, making high-quality video editing far more practical for real-world use. ğŸ”— Links ğŸ“„ Paper: https://arxiv.org/abs/2512.07469 ğŸ’» Code:...</description><pubDate>Mon, 15 Dec 2025 13:44:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/XiangpengYang/572526773544518</guid></item><item><title>I just released LayaCodec, a highly efficient neural audio tokenizer/codec for TTS models, far better than most previous audio tokenizers.</title><link>https://huggingface.co/posts/YatharthS/731858391215619</link><description>I just released LayaCodec, a highly efficient neural audio tokenizer/codec for TTS models, far better than most previous audio tokenizers. ğŸ¤¯ Next-gen TTS models that use this could achieve several 100s of times real-time speed while producing clearer audio!! ğŸ¤¯ GitHub repo: https://github.com/ysharma3501/LayaCodec Model: YatharthS/LayaCodec See translation</description><pubDate>Mon, 15 Dec 2025 13:44:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/YatharthS/731858391215619</guid></item><item><title>Today's winner is Ling 1T with a score of 38!</title><link>https://huggingface.co/posts/etemiz/601408246654891</link><description>Today's winner is Ling 1T with a score of 38! Btw AHA2 is in the works, with more domains, better comparison LLMs and questions, overall better signal. See translation</description><pubDate>Mon, 15 Dec 2025 13:44:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/etemiz/601408246654891</guid></item><item><title>ğŸ„ last talk of the year about open AI and HF today at Universidad Rey Juan Carlos for undergrad students</title><link>https://huggingface.co/posts/sergiopaniego/621181656886485</link><description>ğŸ„ last talk of the year about open AI and HF today at Universidad Rey Juan Carlos for undergrad students always a pleasure to be back at my alma mater ğŸ… slides: https://github.com/sergiopaniego/talks See translation</description><pubDate>Mon, 15 Dec 2025 13:44:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/sergiopaniego/621181656886485</guid></item><item><title>Language Dexterity Benchmark</title><link>https://huggingface.co/posts/prabhatkr/593169299364014</link><description>Language Dexterity Benchmark I am working on a new benchmark to establish human language dexterity. My hypothesis is that certain language allow for more accurate dexterous behaviour - Pointed, unambigous, and confusion-free references of parts of speech in small and large contexts. There are certain languages with high degree of accurate grammar like Sanskrit, Esperanto, and Turkish. I am native Sanskrit speaker. I have plans to establish this benchmark and test this hypothesis across 100 langauges. I have created 25 task prompts for text, image, video and robotics manipulation. We can test langauges across multiple popular models. Here is the github link: https://github.com/ParamTatva-org/Linguistic-Dexterity-Benchmark See translation</description><pubDate>Mon, 15 Dec 2025 13:44:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/prabhatkr/593169299364014</guid></item></channel></rss>