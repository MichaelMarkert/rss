<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>🚀 AI Blog Generator with Streamlit: The Ultimate Guide!</title><link>https://huggingface.co/posts/ginipick/721764114758575</link><description>🚀 AI Blog Generator with Streamlit: The Ultimate Guide! ginigen/blogger Hello there! Today I'm excited to introduce you to a powerful AI blog creation tool called Ginigen Blog. This amazing app automatically generates high-quality blog content using Streamlit and the latest ChatGPT 4.1 API. And the best part? It's completely free to use! 👩‍💻✨ 🧠 What Makes Ginigen Blog Special Ginigen Blog is not just a simple text generator! It offers these exceptional features: Multiple Blog Templates: SEO-optimized, tutorials, reviews, and more Web Search Integration: Creates accurate content based on the latest information File Upload Analysis: Automatically analyzes TXT, CSV, and PDF files to incorporate into blogs Automatic Image Generation: Creates images that match your blog topic Various Output Formats: Download in Markdown, HTML, and more Latest GPT-4.1 Model: Cutting-edge AI technology for higher quality blog creation Completely Free Service: Access high-quality content generation without...</description><pubDate>Fri, 25 Apr 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ginipick/721764114758575</guid></item><item><title>30 seconds hard test on FramePack - [0] a man talking , [5] a man crying , [10] a man smiling , [15] a man frowning , [20] a man sleepy , [25] a man going crazy - i think result is excellent when we consider how hard this test is - Generated with SECourses FramePack App V40</title><link>https://huggingface.co/posts/MonsterMMORPG/129160122230232</link><description>30 seconds hard test on FramePack - [0] a man talking , [5] a man crying , [10] a man smiling , [15] a man frowning , [20] a man sleepy , [25] a man going crazy - i think result is excellent when we consider how hard this test is - Generated with SECourses FramePack App V40 App link and 1-click installers for Windows, RunPod and Massed Compute here : https://www.patreon.com/posts/126855226 I got the prompt using idea from this pull request : https://github.com/lllyasviel/FramePack/pull/218/files Not exactly same implementation but i think pretty accurate when considering that it is a 30 second 30 fps video at 840p resolution See translation</description><pubDate>Fri, 25 Apr 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/129160122230232</guid></item><item><title>@thomwolf</title><link>https://huggingface.co/posts/fdaudens/720993710312130</link><description>@ thomwolf and @ m-ric teaming up as the perfect instructor duo for DeepLearning.ai’s new course: Building Code Agents with Hugging Face smolagents! https://www.deeplearning.ai/short-courses/building-code-agents-with-hugging-face-smolagents/ See translation</description><pubDate>Fri, 25 Apr 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/720993710312130</guid></item><item><title>Think AGI is just around the corner? Not so fast.</title><link>https://huggingface.co/posts/shekkizh/738330695730383</link><description>Think AGI is just around the corner? Not so fast. When OpenAI released its Computer-Using Agent (CUA) API, I happened to be playing Wordle 🧩 and thought, why not see how the model handles it? Spoiler: Wordle turned out to be a surprisingly effective benchmark. So Romain Cosentino Ph.D. and I dug in and analyzed the results of several hundred runs. 🔑 Takeaways 1️⃣ Even the best computer-using models struggle with simple, context-dependent tasks. 2️⃣ Visual perception and reasoning remain major hurdles for multimodal agents. 3️⃣ Real-world use cases reveal significant gaps between hype and reality. Perception accuracy drops to near zero by the last turn 📉 🔗 Read our arxiv article for more details https://www.arxiv.org/abs/2504.15434 See translation</description><pubDate>Fri, 25 Apr 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/shekkizh/738330695730383</guid></item><item><title>I've recently attended a panel on AI applications. The panelists are managers/directors of Fortune 500 companies. These people make things happen and own results, so their stories and pain points are fresh.</title><link>https://huggingface.co/posts/onekq/992154552707771</link><description>I've recently attended a panel on AI applications. The panelists are managers/directors of Fortune 500 companies. These people make things happen and own results, so their stories and pain points are fresh. (1) Models are used EVERYWHERE, customer facing and internal support, etc. (2) A successful application must improve one of the following: revenue (💵💵), cost (💵💵), CSAT (still 💵💵) (3) They proactively search on 🤗HF🤗 for models and use them. Open source models (especially small ones) can flexibly fit into their existing workflows/infras, which enable them to deliver, and fast. (4) The main barrier for adoption is license. A director told me they picked a model and finetuned it, then learned they would have to share enhancements. As a result, they dropped this model and the million dollar impact went to another model. So to fellow model builders: (1) celebrate that our work is useful and generate lots of values (2) make your license permissive if you want maximum impact See...</description><pubDate>Fri, 25 Apr 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/onekq/992154552707771</guid></item><item><title>DIA TTS is just amazing - please share your funniest gens (here is mine) 😂</title><link>https://huggingface.co/posts/victor/207569978315745</link><description>DIA TTS is just amazing - please share your funniest gens (here is mine) 😂 nari-labs/Dia-1.6B See translation</description><pubDate>Fri, 25 Apr 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/victor/207569978315745</guid></item><item><title>New reasoning algo just dropped: Adaptive Parallel Reasoning</title><link>https://huggingface.co/posts/Jaward/857345172218118</link><description>New reasoning algo just dropped: Adaptive Parallel Reasoning “we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations.” Paper: https://arxiv.org/pdf/2504.15466 Code: https://github.com/Parallel-Reasoning/APR See translation</description><pubDate>Fri, 25 Apr 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/Jaward/857345172218118</guid></item><item><title>@SmallDoge</title><link>https://huggingface.co/posts/JingzeShi/736642176876033</link><description>@ SmallDoge SmallTalks( SmallDoge/SmallTalks ) is a synthetic dataset designed for supervised fine-tuning of language models. The dataset covers a variety of conversational content, including daily conversations, tool usage, Python programming, encyclopedia Q&amp;A, exam problem-solving, logical reasoning, and more. Each task is provided in both English and Chinese versions. See translation</description><pubDate>Fri, 25 Apr 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/JingzeShi/736642176876033</guid></item><item><title>New foundation model on image and video captioning just dropped by NVIDIA AI 🔥</title><link>https://huggingface.co/posts/merve/338904780470677</link><description>New foundation model on image and video captioning just dropped by NVIDIA AI 🔥 Describe Anything Model (DAM) is a 3B vision language model to generate detailed captions with localized references 😮 The team released the models, the dataset, a new benchmark and a demo 🤩 nvidia/describe-anything-680825bb8f5e41ff0785834c Most of the vision LMs focus on image as a whole, lacking localized references in captions, and not taking in visual prompts (points, boxes, drawings around objects) DAM addresses this on two levels: new vision backbone that takes in focal crops and the image itself, and a large scale dataset 👀 They generate a dataset by extending existing segmentation and referring expression generation datasets like REFCOCO, by passing in the images and classes to VLMs and generating captions. Lastly, they also release a new benchmark again with self-supervision, they use an LLM to evaluate the detailed captions focusing on localization 👏 See translation</description><pubDate>Fri, 25 Apr 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merve/338904780470677</guid></item><item><title>🚀 We are delighted to announce MamayLM, a new state-of-the-art efficient Ukrainian LLM!</title><link>https://huggingface.co/posts/hannayukhymenko/367157502110648</link><description>🚀 We are delighted to announce MamayLM, a new state-of-the-art efficient Ukrainian LLM! 📈 MamayLM surpasses similar-sized models in both English and Ukrainian, while matching or overtaking up to 10x larger models. 📊 MamayLM is a 9B model that can run on a single GPU, enabling cost-efficient AI autonomy and adoption across sectors in Ukraine such as education, legal, healthcare, public services and others (e.g., by specializing it to particular use cases). MalayLM is also attractive for organizations wishing to preserve data privacy as it s efficiency allows it to run on a local machine. 🧠 MamayLM is trained on high-quality Ukrainian data and understands Ukrainian language, culture, and history. It is built on top of Google’s Gemma 2 9B model, but uses a number of new advances stemming from INSAIT’s experience in creating BgGPT, a Bulgarian LLM we released last year, now adopted nationwide and profiled several times by Google as a worldwide success case. 🤝 MamayLM is developed in a...</description><pubDate>Fri, 25 Apr 2025 17:19:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/hannayukhymenko/367157502110648</guid></item></channel></rss>