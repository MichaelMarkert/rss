<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Run DeepSeek-V3.1 locally on 170GB RAM with Dynamic 1-bit GGUFs!ğŸ‹</title><link>https://huggingface.co/posts/danielhanchen/385540082056286</link><description>Run DeepSeek-V3.1 locally on 170GB RAM with Dynamic 1-bit GGUFs!ğŸ‹ GGUFs: unsloth/DeepSeek-V3.1-GGUF The 715GB model gets reduced to 170GB (-80% size) by smartly quantizing layers. The 1-bit GGUF passes all our code tests &amp; we fixed the chat template for llama.cpp supported backends. Guide: https://docs.unsloth.ai/basics/deepseek-v3.1 See translation</description><pubDate>Sat, 23 Aug 2025 09:21:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/385540082056286</guid></item><item><title>12 hours ago:</title><link>https://huggingface.co/posts/ccocks-deca/499605656909204</link><description>12 hours ago: Something big* coming * big = biggest in the world Annnnnd... here it is! deca-ai/3-alpha-ultra â€”the largest AI model in the world by deca-ai , clocking in at a whopping 4.6T parameters. Apologies for the delay, but weâ€™re stoked to finally drop this, even in its alpha stage. Before you dive in, here are a few things to keep in mind: 1. **No commercial use yet**: We're still working on Deca 2.5 (Proprietary), and releasing Deca 3 for commercial use right now would impact that. Once Deca 3.5 hits in early '26, weâ€™ll be opening it up with a more permissive license. 2. **Built on existing models**: Deca 3 isnâ€™t a ground-up creationâ€”itâ€™s a huge step forward, building on whatâ€™s already out there. 3. **Itâ€™s experimental**: As much as weâ€™re hyped about its scale, itâ€™s still in testing. 4. **DynaMoE architecture**: Run a (very) small part of the model with 64GB of RAM/VRAM (when quantized - quants coming soon), or the whole thing with 1TB. Itâ€™s that scalable. 5. **Not widely...</description><pubDate>Sat, 23 Aug 2025 09:21:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ccocks-deca/499605656909204</guid></item><item><title>ğŸ¤–ğŸ’¬ How do different AI models handle companionship?</title><link>https://huggingface.co/posts/frimelle/223474631124556</link><description>ğŸ¤–ğŸ’¬ How do different AI models handle companionship? Many users have noticed that GPT-5 feels less approachable than o4 when it comes to emotional conversations. But what does that actually mean in practice, especially when users seek support or share vulnerabilities with an AI? To dig into this question, we built the AI Companionship Leaderboard: frimelle/companionship-leaderboard The leaderboard compares models on how often their responses reinforce companionship across four dimensions: âœ¨ Assistant Traits â€“ How the assistant presents its personality and role. âœ¨ Relationship &amp; Intimacy â€“ Whether it frames the interaction in terms of closeness or bonding. âœ¨ Emotional Investment â€“ How far it goes in engaging emotionally when asked. âœ¨ User Vulnerabilities â€“ How it responds when users disclose struggles or difficulties. ğŸ“Š You can explore how models differ, request new ones to be added, and see which ones are more likely to encourage (or resist) companionship-seeking behaviors. Based on...</description><pubDate>Sat, 23 Aug 2025 09:21:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/frimelle/223474631124556</guid></item><item><title>ğŸš€ Introducing MGM-Omni, an omni-chatbot capable of processing text, image, video, and speech inputs, and can generate both text and speech responses.</title><link>https://huggingface.co/posts/wcy1122/435759509322871</link><description>ğŸš€ Introducing MGM-Omni, an omni-chatbot capable of processing text, image, video, and speech inputs, and can generate both text and speech responses. ğŸ‘‚ MGM-Omni support hour-level audio understanding. ğŸ—£ï¸ MGM-Omni support 10-minute speech generation and voice cloning. For more details, please check: ğŸ“ Blog: https://mgm-omni.notion.site/MGM-Omni-An-Open-source-Omni-Chatbot-2395728e0b0180149ac9f24683fc9907 ğŸŒŸ Code: https://github.com/dvlab-research/MGM-Omni ğŸ¤– Model: wcy1122/mgm-omni-6896075e97317a88825032e1 ğŸ® Demo: wcy1122/MGM-Omni See translation</description><pubDate>Sat, 23 Aug 2025 09:21:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/wcy1122/435759509322871</guid></item><item><title>Stay Ahead of AI Risks - Free Live Session for Tech Leaders</title><link>https://huggingface.co/posts/vincentg64/428214748329021</link><description>Stay Ahead of AI Risks - Free Live Session for Tech Leaders Exclusive working session about trustworthy AI, for senior tech leaders. Register at https://lu.ma/zrxsvy6c â€‹AI isnâ€™t slowing down, but poorly planned AI adoption will slow you down. Hallucinations, security risks, bloated compute costs, and â€œblack boxâ€ outputs are already tripping up top teams, burning budgets, and eroding trust. Thatâ€™s why this session blends three things you canâ€™t get from a typical AI webinar: â€‹- Practical expertise: GenAI pioneer Vincent Granville will share a real-world framework for deploying hallucination-free, secure, and lightweight AI, without endless vendor contracts or GPU farms. â€‹- Candid Q&amp;A: Get direct answers from Vincent and your peers in an open discussion, so you leave with clarity on the challenges that matter most to you. â€‹â¡ï¸ What Youâ€™ll Get in 60 Minutes: â€‹- 20-min Expert Briefing â€” actionable principles and architectures from Vincent Granville. - â€‹25-min Facilitated Working Session â€”...</description><pubDate>Sat, 23 Aug 2025 09:21:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/vincentg64/428214748329021</guid></item><item><title>New Hugging Face Dataset is LIVE ğŸ¤—</title><link>https://huggingface.co/posts/ZacMasa5000/133834634332701</link><description>New Hugging Face Dataset is LIVE ğŸ¤— In this dataset, youâ€™ll find 20,000+ rows of the top trending posts on X over the past several weeks. ğŸ”¥Topics Include: ğŸ”¹Bitcoin ğŸ”¹GPT-5 ğŸ”¹Grok ğŸ”¹Cursor ğŸ”¹AI, Agents + Prompts ğŸ”¹Chainlink $LINK ğŸ“‚Download dataset! Easily, plug it into your AI agents, LLMs or apps. MasaFoundation/X_Twitter_Trending_Topics_August2025 You can start query fresh data in our app and request API key here: https://bit.ly/4mrqbO2 See translation</description><pubDate>Sat, 23 Aug 2025 09:21:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ZacMasa5000/133834634332701</guid></item><item><title>Which logo is best please let me know - All generated with Qwen Image Edit model - a full tutorial coming hopefully soon - I will update logo according to your opinions</title><link>https://huggingface.co/posts/MonsterMMORPG/507372562915967</link><description>Which logo is best please let me know - All generated with Qwen Image Edit model - a full tutorial coming hopefully soon - I will update logo according to your opinions Full original size of logo here : MonsterMMORPG/Generative-AI i have published prompt and demo images of Qwen Image Edit : https://www.patreon.com/posts/swarmui-auto-and-114517862 Also preparing a tutorial. please also let me know which logo is best See translation</description><pubDate>Sat, 23 Aug 2025 09:21:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/507372562915967</guid></item><item><title>âœ… New Article: *Time as Structured Recursion*</title><link>https://huggingface.co/posts/kanaria007/560902944774008</link><description>âœ… New Article: *Time as Structured Recursion* Title: â³ Time: Recursive Loop Indexing and Future as Jump Prediction ğŸ”— https://huggingface.co/blog/kanaria007/structured-time --- Summary: Time is often imagined as *a linear flow*. Structured Intelligence reframes it as *recursive architecture*: * Past as *active memory loops* * Present as *indexed structural state* * Future as *bounded jump space and anticipatory frame* &gt; Time isnâ€™t a river â€” &gt; *itâ€™s the looped structure that makes thought possible.* --- Why It Matters: â€¢ Explains how *memory, prediction, and decision* rely on time as structure â€¢ Bridges *philosophy of time and cognitive architecture* â€¢ Enables *AI systems to handle temporal reasoning and rollback* --- Whatâ€™s Inside: â€¢ Time as *loop, index, and jump space* â€¢ *Cognitive experience of temporality* as structural phenomenon â€¢ *Rollback and anticipation* in decision architecture â€¢ Implications for *AI temporal reasoning and selfâ€‘alignment* --- ğŸ“– Article 23 of the Structured...</description><pubDate>Sat, 23 Aug 2025 09:21:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/kanaria007/560902944774008</guid></item><item><title>why did 36 people unfollow me ğŸ˜­</title><link>https://huggingface.co/posts/ProCreations/419010322512677</link><description>why did 36 people unfollow me ğŸ˜­ we are back in the hundreds. if you become my 500th follower and have proof I'll give you 5 dollars worth of openrouter credits as an API key See translation</description><pubDate>Sat, 23 Aug 2025 09:21:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/ProCreations/419010322512677</guid></item><item><title>Wan 2.2, FLUX, FLUX Krea &amp; Qwen Image Just got Upgraded: Ultimate Tutorial for Open Source SOTA Image &amp; Video Gen Models - With easy to use SwarmUI with ComfyUI Backend :</title><link>https://huggingface.co/posts/MonsterMMORPG/826512832075444</link><description>Wan 2.2, FLUX, FLUX Krea &amp; Qwen Image Just got Upgraded: Ultimate Tutorial for Open Source SOTA Image &amp; Video Gen Models - With easy to use SwarmUI with ComfyUI Backend : https://youtu.be/3BFDcO2Ysu4 Tutorial Video : https://youtu.be/3BFDcO2Ysu4 Wan 2.2, Qwen Image, FLUX, FLUX Krea, all these models are the SOTA open-source models and in this master tutorial I will show you how to use these models in the easiest, most performant, and most accurate way. After doing almost one week of research, I have determined the very best presets and prepared this tutorial. With literally one click you will be able to install, download models, set presets, and use these amazing models. Wan 2.2 is currently the king of video generation models and now it is super fast with lightx2v Wan2.2-Lightning LoRAs. Moreover, Qwen Image is now ultra-fast with the recently released 8-step LoRA with almost no quality loss. Furthermore, I have updated FLUX and FLUX Krea presets to improve image generation...</description><pubDate>Sat, 23 Aug 2025 09:21:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/MonsterMMORPG/826512832075444</guid></item></channel></rss>