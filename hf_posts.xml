<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Posts</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the Hugginface trending posts.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>You can now run Llama 4 on your own local device! ğŸ¦™</title><link>https://huggingface.co/posts/danielhanchen/859959880164586</link><description>You can now run Llama 4 on your own local device! ğŸ¦™ Run our Dynamic 1.78-bit and 2.71-bit Llama 4 GGUFs: unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF You can run them on llama.cpp and other inference engines. See our guide here: https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4 See translation</description><pubDate>Wed, 09 Apr 2025 17:19:48 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/danielhanchen/859959880164586</guid></item><item><title>I read the 456-page AI Index report so you don't have to (kidding). The wild part? While AI gets ridiculously more accessible, the power gap is actually widening:</title><link>https://huggingface.co/posts/fdaudens/650208950263848</link><description>I read the 456-page AI Index report so you don't have to (kidding). The wild part? While AI gets ridiculously more accessible, the power gap is actually widening: 1ï¸âƒ£ The democratization of AI capabilities is accelerating rapidly: - The gap between open and closed models is basically closed: difference in benchmarks like MMLU and HumanEval shrunk to just 1.7% in 2024 - The cost to run GPT-3.5-level performance dropped 280x in 2 years - Model size is shrinking while maintaining performance - Phi-3-mini hitting 60%+ MMLU at fraction of parameters of early models like PaLM 2ï¸âƒ£ But we're seeing concerning divides deepening: - Geographic: US private investment ($109B) dwarfs everyone else - 12x China's $9.3B - Research concentration: US and China dominate highly-cited papers (50 and 34 respectively in 2023), while next closest is only 7 - Gender: Major gaps in AI skill penetration rates - US shows 2.39 vs 1.71 male/female ratio The tech is getting more accessible but the benefits aren't...</description><pubDate>Wed, 09 Apr 2025 17:19:48 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fdaudens/650208950263848</guid></item><item><title>Qwen 3 can launch very soon. ğŸ‘€</title><link>https://huggingface.co/posts/merterbak/235850739835485</link><description>Qwen 3 can launch very soon. ğŸ‘€ https://github.com/ggml-org/llama.cpp/pull/12828 See translation</description><pubDate>Wed, 09 Apr 2025 17:19:48 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/merterbak/235850739835485</guid></item><item><title>AI agents are transforming how we interact with technology, but how sustainable are they? ğŸŒ</title><link>https://huggingface.co/posts/BrigitteTousi/559995441481207</link><description>AI agents are transforming how we interact with technology, but how sustainable are they? ğŸŒ Design choices â€” like model size and structure â€” can massively impact energy use and cost. âš¡ğŸ’° The key takeaway: smaller, task-specific models can be far more efficient than large, general-purpose ones. ğŸ”‘ Open-source models offer greater transparency, allowing us to track energy consumption and make more informed decisions on deployment. ğŸŒ± Open-source = more efficient, eco-friendly, and accountable AI. Read our latest, led by @ sasha with assists from myself + @ yjernite ğŸ¤— https://huggingface.co/blog/sasha/ai-agent-sustainability See translation</description><pubDate>Wed, 09 Apr 2025 17:19:48 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/BrigitteTousi/559995441481207</guid></item><item><title>ğŸ”¥ Yesterday was a fire day!</title><link>https://huggingface.co/posts/jasoncorkill/726469711226418</link><description>ğŸ”¥ Yesterday was a fire day! We dropped two brand-new datasets capturing Human Preferences for text-to-video and text-to-image generations powered by our own crowdsourcing tool! Whether you're working on model evaluation, alignment, or fine-tuning, this is for you. 1. Text-to-Video Dataset (Pika 2.2 model): Rapidata/text-2-video-human-preferences-pika2.2 2. Text-to-Image Dataset (Reve-AI Halfmoon): Rapidata/Reve-AI-Halfmoon_t2i_human_preference Letâ€™s train AI on AI-generated content with humans in the loop. Letâ€™s make generative models that actually get us. See translation</description><pubDate>Wed, 09 Apr 2025 17:19:48 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jasoncorkill/726469711226418</guid></item><item><title>ğŸ‰ GitHub selected the ultralytics computer vision project, known for its YOLOv8/YOLO11 real-time SOTA computer vision models, as one of the top 5 open-source projects for first-time contributors in 2024!</title><link>https://huggingface.co/posts/fcakyon/248454580146320</link><description>ğŸ‰ GitHub selected the ultralytics computer vision project, known for its YOLOv8/YOLO11 real-time SOTA computer vision models, as one of the top 5 open-source projects for first-time contributors in 2024! Link to the project: https://github.com/ultralytics/ultralytics Link to the full GitHub 2024 recap report: https://github.blog/news-insights/octoverse/octoverse-2024/ See translation</description><pubDate>Wed, 09 Apr 2025 17:19:48 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/fcakyon/248454580146320</guid></item><item><title>ğŸš€ Metaâ€™s Llama 4 Models Now on Hugging Face!</title><link>https://huggingface.co/posts/luigi12345/868237953690252</link><description>ğŸš€ Metaâ€™s Llama 4 Models Now on Hugging Face! Meta has released Llama 4 Scout and Llama 4 Maverick, now available on Hugging Face: â€¢ Llama 4 Scout: 17B active parameters, 16-expert Mixture of Experts (MoE) architecture, 10M token context window, fits on a single H100 GPU. ï¿¼ â€¢ Llama 4 Maverick: 17B active parameters, 128-expert MoE architecture, 1M token context window, optimized for DGX H100 systems. ï¿¼ ğŸ”¥ Key Features: â€¢ Native Multimodality: Seamlessly processes text and images. ï¿¼ â€¢ Extended Context Window: Up to 10 million tokens for handling extensive inputs. â€¢ Multilingual Support: Trained on 200 languages, with fine-tuning support for 12, including Arabic, Spanish, and German. ï¿¼ ğŸ› ï¸ Access and Integration: â€¢ Model Checkpoints: Available under the meta-llama organization on the Hugging Face Hub. â€¢ Transformers Compatibility: Fully supported in transformers v4.51.0 for easy loading and fine-tuning. â€¢ Efficient Deployment: Supports tensor-parallelism and automatic device mapping....</description><pubDate>Wed, 09 Apr 2025 17:19:48 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/luigi12345/868237953690252</guid></item><item><title>New in PawMatchAIğŸ¾ : Turn Your Dog Photos into Art!</title><link>https://huggingface.co/posts/DawnC/553378321840890</link><description>New in PawMatchAIğŸ¾ : Turn Your Dog Photos into Art! Iâ€™m excited to introduce a brand-new creative feature â€” Dog Style Transfer is now live on PawMatchAI! Just upload your dogâ€™s photo and transform it into 5 artistic styles: ğŸŒ¸ Japanese Anime ğŸ“š Classic Cartoon ğŸ–¼ï¸ Oil Painting ğŸ¨ Watercolor ğŸŒ† Cyberpunk All powered by Stable Diffusion and enhanced with smart prompt tuning to preserve your dogâ€™s unique traits and breed identity , so the artwork stays true to your furry friend. Whether you're creating a custom portrait or just having fun, this feature brings your pet photos to life in completely new ways. And hereâ€™s a little secret: although itâ€™s designed with dogs in mind, it actually works on any photo â€” cats, plush toys, even humans. Feel free to experiment! Results may not always be perfectly accurate, sometimes your photo might come back looking a little different, or even beyond your imagination. But thatâ€™s part of the fun! Itâ€™s all about creative surprises and letting the AI do its...</description><pubDate>Wed, 09 Apr 2025 17:19:48 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/DawnC/553378321840890</guid></item><item><title>ğŸš€ Llama-4 Model-Based Agentic AI System Released!</title><link>https://huggingface.co/posts/openfree/652290136793730</link><description>ğŸš€ Llama-4 Model-Based Agentic AI System Released! ğŸ”¥ Introducing the Latest Llama-4 Models Hello AI enthusiasts! Today we're excited to introduce our free API service powered by the cutting-edge Llama-4-Maverick-17B and Llama-4-Scout-17B models! These state-of-the-art models will upgrade your AI experience with remarkable stability and speed. Link1: openfree/Llama-4-Maverick-17B-Research Link2: openfree/Llama-4-Scout-17B-Research ğŸ§  The Innovation of Agentic AI: Deep Research Feature The standout feature of our service is the revolutionary "Deep Research" functionality! This innovative Agentic AI system includes: ğŸ” Optimized Keyword Extraction: LLM automatically generates the most effective keywords for searches ğŸŒ Real-time Web Search: Collects the latest information through the SerpHouse API ğŸ“Š Intelligent Information Analysis: Precise analysis utilizing the LLM's reasoning capabilities based on collected information ğŸ“ Contextualized Response Generation: Provides accurate answers...</description><pubDate>Wed, 09 Apr 2025 17:19:48 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/openfree/652290136793730</guid></item><item><title>What does it mean when models share the same bytes?</title><link>https://huggingface.co/posts/jsulz/855747629260036</link><description>What does it mean when models share the same bytes? We've investigated some quants and have seen that a considerable portion of quantizations of the same model share the same bytes and can be deduplicated to save considerable upload time for quantizers on the Hub. This space where we crack open a repo from @ bartowski shows we can get significant dedupe xet-team/quantization-dedup You can get a sense of why by reading this write-up: https://github.com/bartowski1182/llm-knowledge/blob/main/quantization/quantization.md But what about finetuned models? Since going into production the xet-team has migrated hundreds of repositories on the Hub to our storage layer, including classic "pre-Hub" open-source models like FacebookAI/xlm-roberta-large (XLM-R) from FacebookAI XLM-R, introduced in 2019, set new benchmarks for multilingual NLP by learning shared representations across 100 languages. It was then fine-tuned on English, Spanish, Dutch, and German, generating language-specific...</description><pubDate>Wed, 09 Apr 2025 17:19:48 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/posts/jsulz/855747629260036</guid></item></channel></rss>