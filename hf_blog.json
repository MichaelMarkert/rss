{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Blog", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_blog.json", "items": [{"id": "https://huggingface.co/blog/async-robot-inference", "image": "https://huggingface.co/blog/assets/async_inference/thumbnail_async_blog.png", "title": "Asynchronous Robot Inference: Decoupling Action Prediction and Execution", "content_text": "Back to Articles Asynchronous Robot Inference: Decoupling Action Prediction and Execution Published July 10, 2025 Update on GitHub Upvote 27 +21 Francesco Capuano fracapuano Follow Steven Palma imstevenpmwork Follow Michel Aractingi aractingi Follow Mustafa Shukor mshukor Follow Dana Aubakirova danaaubakirova Follow Adil Zouitine AdilZtn Follow Simon Alibert aliberts Follow Table of Contents Getting started Async inference: a deep dive 1. Why sequential inference falls short 2. Asynchronous inference, in a nutshell 3. System Architecture Robot Client Policy Server 4. Analyzing async inference 5. Using async in your setup Conclusions TL;DR Robotic policies are increasingly bulky, and predict chunks of future actions rather than a single next action. This results in the robot being idle while awaiting new actions to perform, introducing noticeable lags at execution, and lacking responsiveness. Asynchronous inference tightens the control loop, removing lags at runtime and resulting in...", "url": "https://huggingface.co/blog/async-robot-inference", "date_published": "2025-07-10T00:00:00"}, {"id": "https://huggingface.co/blog/screenenv", "image": "https://huggingface.co/blog/assets/screenenv/screenenv.png", "title": "ScreenEnv: Deploy your full stack Desktop Agent", "content_text": "Back to Articles ScreenEnv: Deploy your full stack Desktop Agent Published July 10, 2025 Update on GitHub Upvote 31 +25 Amir Mahla A-Mahla Follow Aymeric Roucher m-ric Follow What is ScreenEnv? Why ScreenEnv? \ud83c\udfaf One-Line Setup Two Integration Approaches Option 1: Direct Sandbox API Option 2: MCP Server Integration \u2728 Create a Desktop Agent with screenenv and smolagents 1. Choose Your Model 2. Define Your Custom Desktop Agent 3. Run the Agent on a Desktop Task Get Started Today What's Next? TL;DR : ScreenEnv is a powerful Python library that lets you create isolated Ubuntu desktop environments in Docker containers for testing and deploying GUI Agents (aka Computer Use agents). With built-in support for the Model Context Protocol (MCP), it's never been easier to deploy desktop agents that can see, click, and interact with real applications. What is ScreenEnv? Imagine you need to automate desktop tasks, test GUI applications, or build an AI agent that can interact with software. This...", "url": "https://huggingface.co/blog/screenenv", "date_published": "2025-07-10T00:00:00"}, {"id": "https://huggingface.co/blog/building-hf-mcp", "image": "https://huggingface.co/blog/assets/building-hf-mcp/building-hf-mcp.png", "title": "Building the Hugging Face MCP Server", "content_text": "Back to Articles Building the Hugging Face MCP Server Published July 10, 2025 Update on GitHub Upvote 35 +29 shaun smith evalstate Follow Julien Chaumond julien-c Follow Eliott Coyac coyotte508 Follow Abubakar Abid abidlabs Follow Introduction Design Choices Remote Servers Production Deployment Conclusion TL;DR: The Hugging Face Official MCP Server offers unique customization options for AI Assistants accessing the Hub, along with access to thousands of AI applications through one simple URL. We used MCPs \"Streamable HTTP\" transport for deployment, and examine in detail the trade-offs that Server Developers have. We've learned many things about building a useful MCP server in the last month - we'll describe our journey here. Introduction The Model Context Protocol (MCP) is fulfilling its promise of being the standard to connect AI Assistants to the outside world. At Hugging Face, providing access to the Hub via MCP is an obvious choice, and this article shares our experience...", "url": "https://huggingface.co/blog/building-hf-mcp", "date_published": "2025-07-10T00:00:00"}, {"id": "https://huggingface.co/blog/reachy-mini", "image": "https://huggingface.co/blog/assets/reachy-mini/thumbnail.jpg", "title": "Reachy Mini - The Open-Source Robot for Today's and Tomorrow's AI Builders", "content_text": "Back to Articles Reachy Mini \u2013 The Open-Source Robot for Today's and Tomorrow's AI Builders Published July 9, 2025 Update on GitHub Upvote 531 +525 Thomas Wolf thomwolf Follow Matthieu Lapeyre matthieu-lapeyre Follow pollen-robotics \ud83d\udd29 Robot technical info \ud83e\udde0 Built for Exploring, Playing, Learning & Sharing \ud83e\udd1d Designed for Human-Robot Interaction \ud83c\udf0d Open, Modular & Community-Powered Tiny price, small size, huge possibilities. Code, learn, share with AI builders of all ages, all around the globe. Order the lite version Order the wireless version $299 (+ taxes + shipping) $449 (+ taxes + shipping) Reachy Mini is an expressive, open-source robot designed for human-robot interaction, creative coding, and AI experimentation. Fully programmable in Python (and soon JavaScript, Scratch) and priced from $299 , it's your gateway into robotics AI: fun, customizable, and ready to be part of your next coding project. Whether you're an AI developer, hacker, researcher, teacher, robot enthusiast, or...", "url": "https://huggingface.co/blog/reachy-mini", "date_published": "2025-07-09T00:00:00"}, {"id": "https://huggingface.co/blog/mi300kernels", "image": "https://huggingface.co/blog/assets/mi300kernels/thumbnail.png", "title": "Creating custom kernels for the AMD MI300", "content_text": "Back to Articles Creating custom kernels for the AMD MI300 Published July 9, 2025 Update on GitHub Upvote 31 +25 R\u00e9mi Ouazan Reboul ror Follow seungrok jung seungrokj Follow amd AMD Kernels How to use these kernels The hf-rocm-kernels repo Integration in VLLM A quick introduction to the MI300X Threads Warps Compute units XCDs The entire GPU (MI300X) Day 0 performance analysis RMS norm kernel Optimization: memory-related Results SwiGLU kernel Optimization: compute-related Results Skinny GEMM kernel Optimization: split-K Optimization: removing padding Optimization: warp specialization and asynchronous execution Results Conclusion Introduction More than a billion per day: that\u2019s a low estimate of how many requests ChatGPT handles daily, a number which is unlikely to go down soon. For each request and each generated token, we run an inference of a multi-billion parameters model. This is why model optimization is paramount at each and every level: when one deals with these kinds of...", "url": "https://huggingface.co/blog/mi300kernels", "date_published": "2025-07-09T00:00:00"}, {"id": "https://huggingface.co/blog/gradio-mcp-servers", "image": "https://huggingface.co/blog/assets/upskill-llms-with-gradio-mcp/UpskillThumbnail.png", "title": "Upskill your LLMs with Gradio MCP Servers", "content_text": "Back to Articles Upskill your LLMs with Gradio MCP Servers Published July 9, 2025 Update on GitHub Upvote 14 +8 Freddy Boulton freddyaboulton Follow A Brief Intro To MCP Hugging Face Spaces: The MCP App Store An Example: An LLM that can edit images Conclusion Upskill your LLMs With Gradio MCP Servers Have you ever wanted your favorite Large Language Model (LLM) to do more than just answer questions? What if it could edit images for you, browse the web, or organize your email inbox? Well, now it can! In this blog post, I'll show you: What the MCP protocol is and how it works similarly to the smartphone apps we're all used to, but for LLMs. How you can find thousands of MCP servers via the \"MCP App Store.\" How to add one of these servers to your favorite LLM of choice to grant it a new ability. We'll work through an example using Flux.1 Kontext[dev] which edits images from plain text instructions. A Brief Intro To MCP The Model Context Protocol (MCP) is an open standard that enables...", "url": "https://huggingface.co/blog/gradio-mcp-servers", "date_published": "2025-07-09T00:00:00"}, {"id": "https://huggingface.co/blog/smollm3", "image": "https://huggingface.co/blog/assets/smollm3/image.png", "title": "SmolLM3: smol, multilingual, long-context reasoner", "content_text": "Back to Articles SmolLM3: smol, multilingual, long-context reasoner Published July 8, 2025 Update on GitHub Upvote 500 +494 Elie Bakouch eliebak Follow Carlos Miguel Pati\u00f1o cmpatino Follow Anton Lozhkov anton-l Follow Edward Beeching edbeeching Follow Aymeric Roucher m-ric Follow Nouamane Tazi nouamanetazi Follow Aksel Joonas Reedi akseljoonas Follow Guilherme Penedo guipenedo Follow Hynek Kydlicek hynky Follow Cl\u00e9mentine Fourrier clefourrier Follow Nathan Habib SaylorTwift Follow Kashif Rasul kashif Follow Quentin Gallou\u00e9dec qgallouedec Follow Hugo Larcher hlarcher Follow Mathieu Morlon glutamatt Follow Joshua Xenova Follow Vaibhav Srivastav reach-vb Follow Xuan-Son Nguyen ngxson Follow Colin Raffel craffel Follow Lewis Tunstall lewtun Follow Loubna Ben Allal loubnabnl Follow Leandro von Werra lvwerra Follow Thomas Wolf thomwolf Follow Architecture and training details Data mixture and training stages Long Context extension Reasoning Mid-training Building the Chat Template...", "url": "https://huggingface.co/blog/smollm3", "date_published": "2025-07-08T00:00:00"}, {"id": "https://huggingface.co/blog/infrastructure-alerting", "image": "https://huggingface.co/blog/assets/infrastructure-alerting/thumbnail.jpg", "title": "Three Mighty Alerts Supporting Hugging Face\u2019s Production Infrastructure", "content_text": "Back to Articles Three Mighty Alerts Supporting Hugging Face\u2019s Production Infrastructure Published July 8, 2025 Update on GitHub Upvote 6 Jeremy Udit jcudit Follow High NAT Gateway Throughput Hub Request Logs Archival Success Rate Kubernetes API Request Errors and Rate Limiting Bonus Alert: New Cluster Sending Zero Metrics Wrapping Up The Infrastructure team at Hugging Face is excited to share a behind-the-scenes look at the inner workings of Hugging Face's production infrastructure, which we\u2019ve had the privilege of helping to build and maintain. Our team's dedication to designing and implementing a robust monitoring and alerting system has been instrumental in ensuring the stability and scalability of our platforms. We\u2019re constantly reminded of the impact that our alerts have on our ability to identify and respond to potential issues before they become major incidents. In this blog post, we\u2019ll dive into the details of three mighty alerts that play their unique role in supporting...", "url": "https://huggingface.co/blog/infrastructure-alerting", "date_published": "2025-07-08T00:00:00"}, {"id": "https://huggingface.co/blog/mmdp", "image": "https://huggingface.co/blog/assets/mmdp/thumbnail.png", "title": "Efficient MultiModal Data Pipeline", "content_text": "Back to Articles Efficient MultiModal Data Pipeline Published July 8, 2025 Update on GitHub Upvote 45 +39 Aritra Roy Gosthipaty ariG23498 Follow Luis Wiedmann lusxvr Follow Andres Marafioti andito Follow Sergio Paniego sergiopaniego Follow Pedro Cuenca pcuenq Follow [Stage 0] Preparation [Stage 1] Visualising the Dataset [Stage 2] Naive Padding [Stage 3] Constrained Padding [Stage 4]: Packing Smarter with Knapsacks Switching to an Iterable Dataset Producer-Consumer Magic Greedy Packing Bin-Packing for Tighter Fits [Stage 5] Knapsacks for Multimodal Data Conclusion You've got everything ready - data, model, a beefy GPU setup. You hit \"run\" and... wait. And wait some more. Your GPUs are barely breaking a sweat while your wallet's getting lighter by the hour. Sound familiar? We've been there. After some detective work on our nanoVLM project, we discovered the real culprit wasn't our model or hardware, it was our data pipeline being incredibly wasteful. Here's what we found: Idle GPUs :...", "url": "https://huggingface.co/blog/mmdp", "date_published": "2025-07-08T00:00:00"}, {"id": "https://huggingface.co/blog/train-sparse-encoder", "image": "https://huggingface.co/blog/assets/train-sentence-transformers/st-hf-thumbnail.png", "title": "Training and Finetuning Sparse Embedding Models with Sentence Transformers v5", "content_text": "Back to Articles Training and Finetuning Sparse Embedding Models with Sentence Transformers v5 Published July 1, 2025 Update on GitHub Upvote 88 +82 Tom Aarsen tomaarsen Follow Arthur BRESNU arthurbresnu Follow Table of Contents What are Sparse Embedding models? Query and Document Expansion Why Use Sparse Embedding Models? Why Finetune? Training Components Model Splade Inference-free Splade Contrastive Sparse Representation (CSR) Architecture Picker Guide Dataset Data on the Hugging Face Hub Local Data (CSV, JSON, Parquet, Arrow, SQL) Local Data that requires pre-processing Dataset Format Loss Function Training Arguments Evaluator SparseNanoBEIREvaluator SparseEmbeddingSimilarityEvaluator with STSb SparseTripletEvaluator with AllNLI Trainer Callbacks Multi-Dataset Training Evaluation Training Tips Vector Database Integration Qdrant Integration Example Prerequisites: Additional Resources Training Examples Documentation Sentence Transformers is a Python library for using and training...", "url": "https://huggingface.co/blog/train-sparse-encoder", "date_published": "2025-07-01T00:00:00"}, {"id": "https://huggingface.co/blog/gemma3n", "image": "https://huggingface.co/blog/assets/gemma3n/thumbnail.png", "title": "Gemma 3n fully available in the open-source ecosystem!", "content_text": "Back to Articles Gemma 3n fully available in the open-source ecosystem! Published June 26, 2025 Update on GitHub Upvote 106 +100 Aritra Roy Gosthipaty ariG23498 Follow Pedro Cuenca pcuenq Follow Sergio Paniego sergiopaniego Follow Vaibhav Srivastav reach-vb Follow Christopher Fleetwood FL33TW00D-HF Follow Joshua Xenova Follow Steven Zheng Steveeeeeeen Follow Kashif Rasul kashif Follow Models released today Details of the models Architecture Highlights Performance & Benchmarks: Demo Space Inference with transformers Inference with pipeline Detailed inference with transformers Inference with MLX Inference with llama.cpp Inference with Transformers.js and ONNXRuntime Fine Tune in a Free Google Colab Hugging Face Gemma Recipes Conclusion Gemma 3n was announced as a preview during Google I/O. The on-device community got really excited, because this is a model designed from the ground up to run locally on your hardware. On top of that, it\u2019s natively multimodal , supporting image, text,...", "url": "https://huggingface.co/blog/gemma3n", "date_published": "2025-06-26T00:00:00"}, {"id": "https://huggingface.co/blog/transformers-backend-sglang", "image": "https://huggingface.co/blog/assets/196_transformers_backend_sglang/thumbnail.jpg", "title": "Transformers backend integration in SGLang", "content_text": "Back to Articles Transformers backend integration in SGLang Published June 23, 2025 Update on GitHub Upvote 46 +40 Yineng Zhang zhyncs Follow sgl-project Ke Bao ispobock Follow sgl-project Lianmin lmzheng Follow sgl-project Jin Pan JinnP Follow guest Marc Sun marcsun13 Follow Transformers SGLang Usage Example Hugging Face transformers library is the standard for working with state-of-the-art models \u2014 from experimenting with cutting-edge research to fine-tuning on custom data. Its simplicity, flexibility, and expansive model zoo make it a powerful tool for rapid development. But once you're ready to move from notebooks to production, inference performance becomes mission-critical. That\u2019s where SGLang comes in. Designed for high-throughput, low-latency inference, SGLang now offers seamless integration with transformers as a backend. This means you can pair the flexibility of transformers with the raw performance of SGLang. Let\u2019s dive into what this integration enables and how you can...", "url": "https://huggingface.co/blog/transformers-backend-sglang", "date_published": "2025-06-23T00:00:00"}, {"id": "https://huggingface.co/blog/flux-qlora", "image": "https://huggingface.co/blog/assets/flux-qlora/thumbnail.png", "title": "(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware", "content_text": "Back to Articles (LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware Published June 19, 2025 Update on GitHub Upvote 75 +69 Derek Liu derekl35 Follow Marc Sun marcsun13 Follow Sayak Paul sayakpaul Follow merve merve Follow Linoy Tsaban linoyts Follow Table of Contents Dataset FLUX Architecture QLoRA Fine-tuning FLUX.1-dev with Diffusers Key Optimization Techniques Pre-computing Text Embeddings (CLIP/T5) How to use it Setup & Results Model Comparison: Base vs. QLoRA Fine-tuned (fp16) FP8 Fine-tuning with TorchAO Inference with Trained LoRA Adapters Option 1: Loading LoRA Adapters Option 2: Merging LoRA into Base Model Running on Google Colab Conclusion Share your creations on the Hub! In our previous post, Exploring Quantization Backends in Diffusers , we dived into how various quantization techniques can shrink diffusion models like FLUX.1-dev, making them significantly more accessible for inference without drastically compromising performance. We saw how bitsandbytes , torchao , and...", "url": "https://huggingface.co/blog/flux-qlora", "date_published": "2025-06-19T00:00:00"}, {"id": "https://huggingface.co/blog/inference-providers-groq", "image": "https://huggingface.co/blog/assets/inference-providers/welcome-groq.jpg", "title": "Groq on Hugging Face Inference Providers \ud83d\udd25", "content_text": "Back to Articles Groq on Hugging Face Inference Providers \ud83d\udd25 Published June 16, 2025 Update on GitHub Upvote 39 +33 Ben Ankiel benank-groq Follow Groq Hatice Ozen hozen Follow Groq C\u00e9lina Hanouti celinah Follow Lucain Pouget Wauplin Follow Simon Brandeis sbrandeis Follow How it works In the website UI From the client SDKs Billing Feedback and next steps We're thrilled to share that Groq is now a supported Inference Provider on the Hugging Face Hub! Groq joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub\u2019s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers. Groq supports a wide variety of text and conversational models, including the latest open-source models such as Meta's Llama 4 , Qwen's QWQ-32B , and many more. At the heart of Groq's technology is the Language Processing Unit...", "url": "https://huggingface.co/blog/inference-providers-groq", "date_published": "2025-06-16T00:00:00"}, {"id": "https://huggingface.co/blog/hello-hf-kernels", "image": "https://huggingface.co/blog/assets/hello-hf-kernels/kernel-hub-five-mins-short-21.png", "title": "Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub", "content_text": "Back to Articles \ud83c\udfce\ufe0f Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub Published June 12, 2025 Update on GitHub Upvote 113 +107 David Holtz drbh Follow Dani\u00ebl de Kok danieldk Follow Nicolas Patry Narsil Follow Pedro Cuenca pcuenq Follow Simon Pagezy pagezyhf Follow merve merve Follow Vaibhav Srivastav reach-vb Follow 1. What is the Kernel Hub? Benefits of the Kernel Hub: 2. How to Use the Kernel Hub (Basic Example) What's happening here? 3. Add a Kernel to a Simple Model 4. Benchmarking the Performance Impact 5. Real World Use Cases Get Started and Next Steps! Conclusion Boost your model performance with pre-optimized kernels, easily loaded from the Hub. Today, we'll explore an exciting development from Hugging Face: the Kernel Hub ! As ML practitioners, we know that maximizing performance often involves diving deep into optimized code, custom CUDA kernels, or complex build systems. The Kernel Hub simplifies this process dramatically! Below is a short example of how...", "url": "https://huggingface.co/blog/hello-hf-kernels", "date_published": "2025-06-12T00:00:00"}]}