{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Blog", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_blog.json", "items": [{"id": "https://huggingface.co/blog/intel-gaudi-backend-for-tgi", "image": "https://huggingface.co/blog/assets/intel-gaudi-backend-for-tgi/tgi-gaudi-thumbnail.png", "title": "Accelerating LLM Inference with TGI on Intel Gaudi", "content_text": "Back to Articles \ud83d\ude80 Accelerating LLM Inference with TGI on Intel Gaudi Published March 28, 2025 Update on GitHub Upvote 8 +2 baptistecolle Baptiste Colle regisss R\u00e9gis Pierrard IlyasMoutawwakil Ilyas Moutawwakil echarlaix Ella Charlaix kding1 Ke Ding Intel \u2728 What's New? \ud83c\udf1f Why This Matters \ud83d\udea6 Getting Started with TGI on Gaudi \ud83c\udf89 Top features \ud83d\udcaa Getting Involved We're excited to announce the native integration of Intel Gaudi hardware support directly into Text Generation Inference (TGI), our production-ready serving solution for Large Language Models (LLMs). This integration brings the power of Intel's specialized AI accelerators to our high-performance inference stack, enabling more deployment options for the open-source AI community \ud83c\udf89 \u2728 What's New? We've fully integrated Gaudi support into TGI's main codebase in PR #3091 . Previously, we maintained a separate fork for Gaudi devices at tgi-gaudi . This was cumbersome for users and prevented us from supporting the latest TGI features at...", "url": "https://huggingface.co/blog/intel-gaudi-backend-for-tgi", "date_published": "2025-03-28T00:00:00"}, {"id": "https://huggingface.co/blog/train-reranker", "image": "https://huggingface.co/blog/assets/train-sentence-transformers/st-hf-thumbnail.png", "title": "Training and Finetuning Reranker Models with Sentence Transformers v4", "content_text": "Back to Articles Training and Finetuning Reranker Models with Sentence Transformers v4 Published March 26, 2025 Update on GitHub Upvote 67 +61 tomaarsen Tom Aarsen Table of Contents What are Reranker models? Why Finetune? Training Components Dataset Data on the Hugging Face Hub Local Data (CSV, JSON, Parquet, Arrow, SQL) Local Data that requires pre-processing Dataset Format Hard Negatives Mining Loss Function Training Arguments Evaluator CrossEncoderCorrelationEvaluator with STSb CrossEncoderRerankingEvaluator with GooAQ mined negatives Trainer Callbacks Multi-Dataset Training Training Tips Evaluation Additional Resources Training Examples Documentation Sentence Transformers is a Python library for using and training embedding and reranker models for a wide range of applications, such as retrieval augmented generation, semantic search, semantic textual similarity, paraphrase mining, and more. Its v4.0 update introduces a new training approach for rerankers, also known as cross-...", "url": "https://huggingface.co/blog/train-reranker", "date_published": "2025-03-26T00:00:00"}, {"id": "https://huggingface.co/blog/gradio-dataframe-upgrade", "image": "https://huggingface.co/blog/assets/gradio-dataframe-update/thumbnail.png", "title": "Introducing Gradio's new Dataframe!", "content_text": "Back to Articles Introducing Gradio's new Dataframe! Published March 24, 2025 Update on GitHub Upvote 18 +12 hmb hannah abidlabs Abubakar Abid What\u2019s next? Try it yourself! Gradio\u2019s gr.Dataframe component is one of our most popular components, we've seen it used in a variety of awesome apps, like leaderboards, dashboards, and interactive visualisations. Although we hadn't made any changes to the dataframe in quite some time, our backlog of issues had been growing, and some improvements had been in demand for a while. Well \u2014 we\u2019re now super excited to release a host of new updates to Gradio\u2019s dataframe component. Over the last 6 weeks, we\u2019ve closed over 70 dataframe issues - including bugs, improvements and enhancements. 1. Multi-Cell Selection You can select multiple cells at once! Copy or delete values across your selection with ease. 2. Row Numbers & Column Pinning Add row number columns and keep critical columns in view while navigating wide datasets using the pinned_columns...", "url": "https://huggingface.co/blog/gradio-dataframe-upgrade", "date_published": "2025-03-24T00:00:00"}, {"id": "https://huggingface.co/blog/endpoint-analytics", "image": "https://huggingface.co/blog/assets/endpoint-analytics/thumbnail.png", "title": "The New and Fresh analytics in Inference Endpoints", "content_text": "Back to Articles Analytics is important Published March 21, 2025 Update on GitHub Upvote 17 +11 erikkaum Erik Kaunism\u00e4ki beurkinger Thibault Goehringer rtrm Remy co42 Corentin Regal michellehbn Michelle Habonneau Analytics and metrics are the cornerstone of understanding what's happening with your deployment. Are your Inference Endpoints overloaded? How many requests are they handling? Having well-visualized, relevant metrics displayed in real-time is crucial for monitoring and debugging. We realized that our analytics dashboard needed a refresh. Since we debug a lot of endpoints ourselves, we\u2019ve felt the same pain as our users. That\u2019s why we sat down to plan and make several improvements to provide a better experience for you. What\u2019s New? \u23f0 Real-Time Metrics: Data now updates in real-time, ensuring you get an accurate and up-to-the-second view of your endpoint\u2019s performance. Whether you\u2019re monitoring request latency, response times, or error rates, you can now see the events as...", "url": "https://huggingface.co/blog/endpoint-analytics", "date_published": "2025-03-21T00:00:00"}, {"id": "https://huggingface.co/blog/olympic-coder-lmstudio", "image": "https://huggingface.co/blog/assets/olympic-coder-lmstudio/banner_correct.png", "title": "Open R1: How to use OlympicCoder locally for coding?", "content_text": "Back to Articles Open R1: How to use OlympicCoder locally for coding Published March 20, 2025 Update on GitHub Upvote 52 +46 burtenshaw ben burtenshaw reach-vb Vaibhav Srivastav lewtun Lewis Tunstall edbeeching Edward Beeching yagilb Yagil Burowski lmstudio-ai Everyone\u2019s been using Claude and OpenAI as coding assistants for the last few years, but there\u2019s less appeal if you look at the developments coming out of open source projects like Open R1 . If we look at the evaluation on LiveCodeBench below, we can see that the 7B parameter variant outperforms Claude 3.7 Sonnet and GPT-4o. These models are the daily driver of many engineers in applications like Cursor and VSCode. Evals are great and all, but I want to get my hands dirty and feel the commits! This blog post focuses on how you can integrate these models in your IDE now. We will set up OlympicCoder 7B, the smaller of the two OlympicCoder variants, and we\u2019ll use a quantized variant for optimum local inference. Here\u2019s the stack...", "url": "https://huggingface.co/blog/olympic-coder-lmstudio", "date_published": "2025-03-20T00:00:00"}, {"id": "https://huggingface.co/blog/ai-action-wh-2025", "image": "https://huggingface.co/blog/assets/151_policy_ntia_rfc/us_policy_thumbnail.png", "title": "AI Policy: \ud83e\udd17 Response to the White House AI Action Plan RFI", "content_text": "Back to Articles AI Policy @\ud83e\udd17: Response to the White House AI Action Plan RFI Published March 19, 2025 Update on GitHub Upvote 21 +15 yjernite Yacine Jernite evijit Avijit Ghosh irenesolaiman Irene Solaiman Context: Don't Sleep on (Strongly) Open Models' Capabilities Recommendation 1: Recognize Open Source and Open Science as Fundamental to AI Success Recommendation 2: Prioritize Efficiency and Reliability to Unlock Broad Innovation Recommendation 3: Secure AI through Open, Traceable, and Transparent Systems On March 14, we submitted Hugging Face's response to the White House Office of Science and Technology Policy's request for information on the White House AI Action Plan . We took this opportunity to (re-)assert the fundamental role that open AI systems and open science play in enabling the technology to be more performant and efficient, broadly and reliably adopted, and meeting the highest standards of security. This blog post provides a summary of our response, the full text is...", "url": "https://huggingface.co/blog/ai-action-wh-2025", "date_published": "2025-03-19T00:00:00"}, {"id": "https://huggingface.co/blog/nvidia-physical-ai", "image": "https://huggingface.co/blog/assets/nvidia-physical-ai/thumbnail.png", "title": "NVIDIA's GTC 2025 Announcement for Physical AI Developers: New Open Models and Datasets", "content_text": "Back to Articles NVIDIA's GTC 2025 Announcement for Physical AI Developers: New Open Models and Datasets Published March 18, 2025 Update on GitHub Upvote 30 +24 mingyuliutw Ming-Yu Liu nvidia hannamao Hanzi Mao nvidia jwgu Jinwei Gu nvidia PranjaliJoshi Pranjali Joshi nvidia asawareeb Asawaree guest NVIDIA Isaac GR00T N1 used in object manipulation. New World Foundation Model - Cosmos Transfer How it works Open Physical AI Dataset Purpose Built Model for Humanoids - NVIDIA Isaac GR00T N1 Path Forward At its annual GTC conference, NVIDIA has unveiled a trio of groundbreaking open-source releases aimed at accelerating physical AI development. Release of a new suite of world foundation models (WFMs) with multicontrols called Cosmos Transfer , a highly curated Physical AI Dataset , and the first open model for general humanoid reasoning called NVIDIA Isaac GR00T N1 - represent a significant leap forward in physical AI technology, offering developers powerful tools and resources to...", "url": "https://huggingface.co/blog/nvidia-physical-ai", "date_published": "2025-03-18T00:00:00"}, {"id": "https://huggingface.co/blog/xet-on-the-hub", "image": "https://huggingface.co/blog/assets/xet-on-the-hub/thumbnail.png", "title": "Xet is on the Hub", "content_text": "Back to Articles Xet is on the Hub Published March 18, 2025 Update on GitHub Upvote 34 +28 assafvayner Assaf Vayner xet-team brianronan Brian Ronan xet-team seanses Di Xiao xet-team jgodlewski Joseph Godlewski xet-team sirahd Sam Horradarn xet-team jsulz Jared Sulzdorf xet-team Want to skip the details and get straight to faster uploads and downloads with bigger files than ever before? The Xet Difference Migration Day Post-Migration Challenges Download Overhead from Block Format Pod Load Imbalance Migration Takeaways Ready. Xet. Go. Click here to read about joining the Xet waitlist (or head over to join immediately ). Over the past few weeks, Hugging Face\u2019s Xet Team took a major step forward by migrating the first Model and Dataset repositories off LFS and to Xet storage . This marks one of many steps to fulfill Hugging Face\u2019s vision for the Hub by empowering AI builders to build, iterate, and collaborate more effectively on massive models and datasets. If you're interested in...", "url": "https://huggingface.co/blog/xet-on-the-hub", "date_published": "2025-03-18T00:00:00"}, {"id": "https://huggingface.co/blog/gemma3", "image": "https://huggingface.co/blog/assets/gemma3/thumbnail.png", "title": "Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM", "content_text": "Back to Articles Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM Published March 12, 2025 Update on GitHub Upvote 353 +347 ariG23498 Aritra Roy Gosthipaty merve Merve Noyan pcuenq Pedro Cuenca reach-vb Vaibhav Srivastav TL;DR What is Gemma 3? Technical Enhancements in Gemma 3 Longer Context Length Multimodality Multilinguality Gemma 3 evaluation Inference with \ud83e\udd17 transformers On Device & Low Resource Devices MLX Llama.cpp Deploy on Hugging Face Endpoints Acknowledgements TL;DR Today Google releases Gemma 3 , a new iteration of their Gemma family of models. The models range from 1B to 27B parameters, have a context window up to 128k tokens, can accept images and text, and support 140+ languages. Try out Gemma 3 now \ud83d\udc49\ud83c\udffb Gemma 3 Space Gemma 2 Gemma 3 Size Variants 2B 9B 27B 1B 4B 12B 27B Context Window Length 8k 32k (1B) 128k (4B, 12B, 27B) Multimodality (Images and Text) \u274c \u274c (1B) \u2705 (4B, 12B, 27B) Multilingual Support \u2013 English (1B) +140 languages (4B,...", "url": "https://huggingface.co/blog/gemma3", "date_published": "2025-03-12T00:00:00"}, {"id": "https://huggingface.co/blog/lerobot-goes-to-driving-school", "image": "https://huggingface.co/blog/assets/193_l2d/lerobot-driver.gif", "title": "LeRobot goes to driving school: World\u2019s largest open-source self-driving dataset", "content_text": "Back to Articles LeRobot goes to driving school Published March 11, 2025 Update on GitHub Upvote 69 +63 sandhawalia Harsimrat Sandhawalia yaak-ai cadene Remi Cadene OpenStreetMap Multimodal search LeRobot LeRobot driver Appendix A.1 Route tasks A.2 LLM prompts A.2 Data collection hardware State-of-the art Vision Language Models and Large Language Models are trained on open-source image-text corpora sourced from the internet, which spearheaded the recent acceleration of open-source AI. Despite these breakthroughs, the adoption of end-to-end AI within the robotics and automotive community remains low, primarily due to a lack of high quality, large scale multimodal datasets like OXE . To unlock the potential for robotics AI, Yaak teamed up with the LeRobot team at \ud83e\udd17 and is excited to announce Learning to Drive (L2D) to the robotics AI community. L2D is the world\u2019s largest multimodal dataset aimed at building an open-sourced spatial intelligence for the automotive domain with first...", "url": "https://huggingface.co/blog/lerobot-goes-to-driving-school", "date_published": "2025-03-11T00:00:00"}, {"id": "https://huggingface.co/blog/llm-inference-on-edge", "image": "https://huggingface.co/blog/assets/llm_inference_on_edge/thumbnail.png", "title": "LLM Inference on Edge: A Fun and Easy Guide to run LLMs via React Native on your Phone!", "content_text": "Back to Articles LLM Inference on Edge: A Fun and Easy Guide to run LLMs via React Native on your Phone! Published March 7, 2025 Update on GitHub Upvote 45 +39 medmekk Mohamed Mekkouri marcsun13 Marc Sun Why You Should Follow This Tutorial? 0. Choosing the Right Models Model Size Considerations GGUF Quantization Formats Recommended Models to Try Finding More Models 1. Setting Up Your Environment Tools You Need Virtual Device Setup 2. Create the App Project Structure 3. Running the Demo & Project Running the Demo Running the Project 4. App Implementation Installing Dependencies State Management Fetching available GGUF models from the Hub Model Download Implementation Model Loading and Initialization Chat Implementation The UI & Logic The other Functionnalities 5. How to Debug Chrome DevTools Debugging Common Debugging Tips 6. Additional Features we can add 7. Acknowledgments 8. Conclusion As LLMs continue to evolve, they are becoming smaller and smarter, enabling them to run directly...", "url": "https://huggingface.co/blog/llm-inference-on-edge", "date_published": "2025-03-07T00:00:00"}, {"id": "https://huggingface.co/blog/jfrog", "image": "https://huggingface.co/blog/assets/jfrog/thumbnail.png", "title": "Hugging Face and JFrog partner to make AI Security more transparent", "content_text": "Back to Articles Hugging Face and JFrog partner to make AI Security more transparent Published March 4, 2025 Update on GitHub Upvote 21 +15 mcpotato Luc Georges srmish-jfrog Shachar M jfrog Model security refresher Integration We are pleased to announce our partnership with JFrog , creators of the JFrog Software Supply Chain Platform, as part of our long-standing commitment to provide a safe and reliable platform for the ML community. We have decided to add JFrog's scanner to our platform to continue improving security on the Hugging Face Hub. JFrog's scanner brings new functionality to scanning, aimed at reducing false positives on the Hub. Indeed, what we currently observe is that model weights can contain code that is executed upon deserialization and sometimes at inference time, depending on the format. This code is oftentimes a non harmful practicality for the developer. As our picklescan scanner only performs pattern matching on module names, we cannot always confirm that...", "url": "https://huggingface.co/blog/jfrog", "date_published": "2025-03-04T00:00:00"}, {"id": "https://huggingface.co/blog/aya-vision", "image": "https://huggingface.co/blog/assets/aya-vision/thumbnail.png", "title": "A Deepdive into Aya Vision: Advancing the Frontier of Multilingual Multimodality", "content_text": "Back to Articles A Deepdive into Aya Vision: Advancing the Frontier of Multilingual Multimodality Published March 4, 2025 Update on GitHub Upvote 70 +64 saurabhdash Saurabh Dash CohereForAI olivernan Yiyang Nan guest ArashAhmadian Arash Ahmadian guest johndang-cohere John Dang guest Aya Vision Architecture and Training Training process Multimodal Data Enhancement and Expanding Language Coverage Multimodal Model Merging Scaling up to 32B Aya Vision Benchmark \u2013 a multilingual evaluation data Designed for real-world applications Getting Started with Aya Acknowledgments References With the release of the Aya Vision family , our new 8B and 32B parameter vision-language models (VLMs), we are addressing one of the biggest challenges in AI: bringing multilingual performance to multimodal models . Aya Vision is Cohere For AI 's latest open-weight multilingual and multimodal model family, designed to be a strong foundation for language and vision understanding across 23 languages . It builds...", "url": "https://huggingface.co/blog/aya-vision", "date_published": "2025-03-04T00:00:00"}, {"id": "https://huggingface.co/blog/smolagents-phoenix", "image": "https://huggingface.co/blog/assets/smolagents-phoenix/thumbnail.jpg", "title": "Trace & Evaluate your Agent with Arize Phoenix", "content_text": "Back to Articles Trace & Evaluate your Agent with Arize Phoenix Published February 28, 2025 Update on GitHub Upvote 35 +29 schavalii Sri Chavali arize-ai jgilhuly16 John Gilhuly arize-ai m-ric Aymeric Roucher Make An Agent Step 1: Install the Required Libraries Step 2: Import all the Essential Building Blocks Step 3: Set Up Our Base Models Step 4: Create the Tool-Calling Agent Step 5: Run the Agent Trace Your Agent Evaluate Your Agent Step 1: Install OpenAI Step 2: Retrieve Tool Execution Spans Step 3: Import Prompt Template Step 4: Run the Evaluation Step 5: Send Evaluation Results to Phoenix So, you\u2019ve built your agent. It takes in inputs and tools, processes them, and generates responses. Maybe it\u2019s making decisions, retrieving information, executing tasks autonomously, or all three. But now comes the big question \u2013 how effectively is it performing? And more importantly, how do you know? Building an agent is one thing; understanding its behavior is another. That\u2019s where tracing...", "url": "https://huggingface.co/blog/smolagents-phoenix", "date_published": "2025-02-28T00:00:00"}, {"id": "https://huggingface.co/blog/iisc-huggingface-collab", "image": "https://huggingface.co/blog/assets/iisc-huggingface-collab/thumbnail.png", "title": "HuggingFace, IISc partner to supercharge model building on India's diverse languages", "content_text": "Back to Articles HuggingFace, IISc partner to supercharge model building on India's diverse languages Published February 27, 2025 Update on GitHub Upvote 18 +12 prasantg Prasanta Kumar Ghosh guest nihar-artpark Nihar Desai ARTPARK-IISc Visruth-sanka Sanka ARTPARK-IISc SujithPulikodan Sujith Pulikodan ARTPARK-IISc Partnership About Vaani Dataset District wise language distribution Transcribed subset Utility of Vaani in the Age of LLMs What's next How You Can Contribute The Indian Institute of Science IISc and ARTPARK partner with Hugging Face to enable developers across the globe to access Vaani , India's most diverse open-source, multi-modal, multi-lingual dataset. Both organisations share a commitment to building inclusive, accessible, and state-of-the-art AI technologies that honor linguistic and cultural diversity. Partnership The partnership between Hugging Face and IISc/ARTPARK aims to increase the accessibility and improve usability of the Vaani dataset, encouraging the...", "url": "https://huggingface.co/blog/iisc-huggingface-collab", "date_published": "2025-02-27T00:00:00"}]}