{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Blog", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_blog.json", "items": [{"id": "https://huggingface.co/blog/fastrtc", "image": "https://huggingface.co/blog/assets/fastrtc/fastrtc_logo.jpg", "title": "FastRTC: The Real-Time Communication Library for Python", "content_text": "Back to Articles FastRTC: The Real-Time Communication Library for Python Published February 25, 2025 Update on GitHub Upvote 64 +58 freddyaboulton Freddy Boulton abidlabs Abubakar Abid Getting Started Leveling-Up: LLM Voice Chat Bonus: Call via Phone Next Steps In the last few months, many new real-time speech models have been released and entire companies have been founded around both open and closed source models. To name a few milestones: OpenAI and Google released their live multimodal APIs for ChatGPT and Gemini. OpenAI even went so far as to release a 1-800-ChatGPT phone number! Kyutai released Moshi , a fully open-source audio-to-audio LLM. Alibaba released Qwen2-Audio and Fixie.ai released Ultravox - two open-source LLMs that natively understand audio. ElevenLabs raised $180m in their Series C. Despite the explosion on the model and funding side, it's still difficult to build real-time AI applications that stream audio and video, especially in Python. ML engineers may not...", "url": "https://huggingface.co/blog/fastrtc", "date_published": "2025-02-25T00:00:00"}, {"id": "https://huggingface.co/blog/remote_vae", "image": "https://huggingface.co/blog/assets/remote_vae/thumbnail.png", "title": "Remote VAEs for decoding with HF endpoints \ud83e\udd17", "content_text": "Back to Articles Remote VAEs for decoding with Inference Endpoints \ud83e\udd17 Published February 24, 2025 Update on GitHub Upvote 26 +20 hlky hlky sayakpaul Sayak Paul Getting started Code Basic example Options Generation Queueing Available VAEs Advantages of using a remote VAE Provide feedback Steps: When operating with latent-space diffusion models for high-resolution image and video synthesis, the VAE decoder can consume quite a bit more memory. This makes it hard for the users to run these models on consumer GPUs without going through latency sacrifices and others alike. For example, with offloading, there is a device transfer overhead, causing delays in the overall inference latency. Tiling is another solution that lets us operate on so-called \u201ctiles\u201d of inputs. However, it can have a negative impact on the quality of the final image. Therefore, we want to pilot an idea with the community \u2014 delegating the decoding process to a remote endpoint. No data is stored or tracked, and code is...", "url": "https://huggingface.co/blog/remote_vae", "date_published": "2025-02-24T00:00:00"}, {"id": "https://huggingface.co/blog/siglip2", "image": "https://huggingface.co/blog/assets/siglip2/thumbnail.png", "title": "SigLIP 2: A better multilingual vision language encoder", "content_text": "Back to Articles SigLIP 2: A better multilingual vision language encoder Published February 21, 2025 Update on GitHub Upvote 92 +86 ariG23498 Aritra Roy Gosthipaty merve Merve Noyan qubvel-hf Pavel Iakubovskii TL;DR Introduction Add a decoder (it\u2019s that simple) Self-distillation with Global-Local loss and Masked Prediction Adapting to different resolutions Run inference with transformers Zero-shot Classification Encode images for downstream tasks Comparing SigLIP 1 with SigLIP 2 Using the encoder for VLMs Acknowledgements TL;DR Today Google releases a new and better family of multilingual vision-language encoders, SigLIP 2 . The authors have extended the training objective of SigLIP ( sigmoid loss ) with additional objectives for improved semantic understanding, localization, and dense features. Additional objectives (Source: https://huggingface.co/papers/2502.14786 ) SigLIP 2 models outperform the older SigLIP ones at all model scales in core capabilities, including zero-shot...", "url": "https://huggingface.co/blog/siglip2", "date_published": "2025-02-21T00:00:00"}, {"id": "https://huggingface.co/blog/smolvlm2", "image": "https://huggingface.co/blog/assets/smolvlm2/banner.png", "title": "SmolVLM2: Bringing Video Understanding to Every Device", "content_text": "Back to Articles SmolVLM2: Bringing Video Understanding to Every Device Published February 20, 2025 Update on GitHub Upvote 170 +164 orrzohar Orr Zohar Stanford mfarre Miquel Farr\u00e9 andito Andres Marafioti merve Merve Noyan pcuenq Pedro Cuenca cyrilzakka Cyril Xenova Joshua TL;DR: SmolVLM can now watch \ud83d\udcfa with even better visual understanding Table of Contents Technical Details SmolVLM2 2.2B: Our New Star Player for Vision and Video Going Even Smaller: Meet the 500M and 256M Video Models Suite of SmolVLM2 Demo applications Using SmolVLM2 with Transformers and MLX Transformers Inference with MLX Fine-tuning SmolVLM2 Read More TL;DR: SmolVLM can now watch \ud83d\udcfa with even better visual understanding SmolVLM2 represents a fundamental shift in how we think about video understanding - moving from massive models that require substantial computing resources to efficient models that can run anywhere. Our goal is simple: make video understanding accessible across all devices and use cases, from...", "url": "https://huggingface.co/blog/smolvlm2", "date_published": "2025-02-20T00:00:00"}, {"id": "https://huggingface.co/blog/paligemma2mix", "image": "https://huggingface.co/blog/assets/paligemma2/thumbnail.png", "title": "PaliGemma 2 Mix - New Instruction Vision Language Models by Google", "content_text": "Back to Articles PaliGemma 2 Mix - New Instruction Vision Language Models by Google Published February 19, 2025 Update on GitHub Upvote 59 +53 merve Merve Noyan ariG23498 Aritra Roy Gosthipaty andsteing Andreas P. Steiner google TL;DR Table of Contents PaliGemma 2 Mix Models Comparing PaliGemma 2 Mix Variants General Vision-Language Tasks Document Understanding Localization Tasks Text Recognition in Images Inference and Fine-tuning using Transformers Demo Read More Acknowledgments TL;DR Last December, Google released PaliGemma 2: a new family of pre-trained ( pt ) PaliGemma vision language models (VLMs) based on SigLIP and Gemma 2 . The models come in three different sizes (3B, 10B, 28B) and three different resolutions (224x224, 448x448, 896x896). Today, Google is releasing PaliGemma 2 mix : fine-tuned on a mix of vision language tasks, including OCR, long and short captioning and more. PaliGemma 2 pretrained ( pt ) variants are great vision language models to transfer on a given...", "url": "https://huggingface.co/blog/paligemma2mix", "date_published": "2025-02-19T00:00:00"}, {"id": "https://huggingface.co/blog/inference-providers-nebius-novita-hyperbolic", "image": "https://huggingface.co/blog/assets/inference-providers/second-batch-thumbnail.webp", "title": "Introducing Three New Serverless Inference Providers: Hyperbolic, Nebius AI Studio, and Novita \ud83d\udd25", "content_text": "Back to Articles Introducing Three New Serverless Inference Providers: Hyperbolic, Nebius AI Studio, and Novita \ud83d\udd25 Published February 18, 2025 Update on GitHub Upvote 89 +83 julien-c Julien Chaumond kramp Bertrand Chevrier reach-vb Vaibhav Srivastav sbrandeis Simon Brandeis albertworks Albert Abdulmanov nebius viktor-hu Viktor Hu novita cchevli Connor Chevli Hyperbolic How it works In the website UI From the client SDKs Billing Feedback and next steps We\u2019re thrilled to announce the addition of three more outstanding serverless Inference Providers to the Hugging Face Hub: Hyperbolic , Nebius AI Studio , and Novita . These providers join our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub\u2019s model pages. They\u2019re also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers. These partners join the ranks of our existing providers, including...", "url": "https://huggingface.co/blog/inference-providers-nebius-novita-hyperbolic", "date_published": "2025-02-18T00:00:00"}, {"id": "https://huggingface.co/blog/fireworks-ai", "image": "https://huggingface.co/blog/assets/inference-providers/welcome-fireworks.jpg", "title": "Welcome Fireworks.ai on the Hub \ud83c\udf86", "content_text": "Back to Articles Welcome Fireworks.ai on the Hub \ud83c\udf86 Published February 14, 2025 Update on GitHub Upvote 53 +47 teofeliu Teo Feliu fireworks-ai shaunak-fireworks Shaunak Godbole fireworks-ai julien-c Julien Chaumond How it works In the website UI From the client SDKs From HTTP calls Billing Following our recent announcement on Inference Providers on the Hub , we're thrilled to share that Fireworks.ai is now a supported Inference Provider on HF Hub! Fireworks.ai delivers blazing-fast serverless inference directly on model pages, as well as throughout the whole HF ecosystem of libraries and tools, making it easier than ever to run inference on your favorite models. Among others, starting now, you can run serverless inference to the following models via Fireworks.ai: deepseek-ai/DeepSeek-R1 deepseek-ai/DeepSeek-V3 mistralai/Mistral-Small-24B-Instruct-2501 Qwen/Qwen2.5-Coder-32B-Instruct meta-llama/Llama-3.2-90B-Vision-Instruct and many more, you can find the full list here . Light up...", "url": "https://huggingface.co/blog/fireworks-ai", "date_published": "2025-02-14T00:00:00"}, {"id": "https://huggingface.co/blog/math_verify_leaderboard", "image": "https://huggingface.co/blog/assets/math_verify_leaderboard/thumbnail.png", "title": "Fixing Open LLM Leaderboard with Math-Verify", "content_text": "Back to Articles Fixing Open LLM Leaderboard with Math-Verify Published February 14, 2025 Update on GitHub Upvote 26 +20 hynky Hynek Kydlicek alozowski Alina Lozovskaya SaylorTwift Nathan Habib clefourrier Cl\u00e9mentine Fourrier Why math evaluation on the Open LLM Leaderboard was broken Which model is the best at math? A complete reshuffling of cards thanks to fairer evaluations Impact of the change Model Family Changes Changes in the MATH-Hard Leaderboard Changes in the Leaderboard Wrapping Up 3 weeks ago, we showed how hard it is to correctly evaluate LLM performance on math problems, and introduced Math-Verify , a better solution to validate models on math (read more in the announcement )! Today, we\u2019re thrilled to share that we\u2019ve used Math-Verify to thoroughly re-evaluate all 3,751 models ever submitted to the Open LLM Leaderboard, for even fairer and more robust model comparisons! Why math evaluation on the Open LLM Leaderboard was broken The Open LLM Leaderboard is the most used...", "url": "https://huggingface.co/blog/math_verify_leaderboard", "date_published": "2025-02-14T00:00:00"}, {"id": "https://huggingface.co/blog/billion-classifications", "image": "https://huggingface.co/blog/assets/billion-classifications/billion-classifications-thumbnail.png", "title": "1 Billion Classifications", "content_text": "Back to Articles 1 Billion Classifications Published February 13, 2025 Update on GitHub Upvote 39 +33 derek-thomas Derek Thomas guest Approach Optimization Setup Load Testing Parameters K6 Orchestration Classification Introduction Experiment Results Embedding Introduction Experiment Results Vision Embedding Introduction Experiment Results Analysis Conclusion References Appendix Sanity Checks Cost Analysis Infinity Client Other Lessons Learned Future Improvements You\u2019ve optimized your model. Your pipeline is running smoothly. But now, your cloud bill has skyrocketed. Running 1B+ classifications or embeddings per day isn\u2019t just a technical challenge\u2014it\u2019s a financial one. How do you process at this scale without blowing your budget? Whether you're running large-scale document classification or bulk embedding pipelines for Retrieval-Augmented Generation (RAG), you need cost-efficient, high-throughput inference to make it feasible, and you get that from having a well optimized...", "url": "https://huggingface.co/blog/billion-classifications", "date_published": "2025-02-13T00:00:00"}, {"id": "https://huggingface.co/blog/from-chunks-to-blocks", "image": "https://huggingface.co/blog/assets/from-chunks-to-blocks/thumbnail.png", "title": "From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub", "content_text": "Back to Articles From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub Published February 12, 2025 Update on GitHub Upvote 49 +43 jsulz Jared Sulzdorf xet-team yuchenglow yuchenglow xet-team znation Zach Nation xet-team saba9 saba noorassa xet-team The Realities of Scaling Deduplication Design Principles for Deduplication at Scale Scaling Deduplication with Aggregation Aggregated Deduplication in Practice Content-defined chunking (CDC) plays a central role in enabling deduplication within a Xet-backed repository . The idea is straightforward: break each file\u2019s data into chunks, store only unique ones, reap the benefits. In practice, it's more complex. If we focused solely on maximizing deduplication, the design would call for the smallest possible chunk size. By doing that, we\u2019d create significant overheads for the infrastructure and the builders on the Hub. On Hugging Face's Xet team , we're bringing CDC from theory to production to deliver faster uploads and...", "url": "https://huggingface.co/blog/from-chunks-to-blocks", "date_published": "2025-02-12T00:00:00"}, {"id": "https://huggingface.co/blog/vid_ds_scripts", "image": "https://huggingface.co/blog/assets/vid_ds_scripts/thumbnail.png", "title": "Build awesome datasets for video generation", "content_text": "Back to Articles Build awesome datasets for video generation Published February 12, 2025 Update on GitHub Upvote 25 +19 hlky hlky sayakpaul Sayak Paul Tooling Stage 1 (Acquisition) Stage 2 (Pre-processing/filtering) Stage 3 (Processing) Filtering examples OCR/Caption Putting this tooling to use \ud83d\udc68\u200d\ud83c\udf73 Your Turn Tooling for image generation datasets is well established, with img2dataset being a fundamental tool used for large scale dataset preparation, and complemented with various community guides, scripts and UIs that cover smaller scale initiatives. Our ambition is to make tooling for video generation datasets equally established, by creating open video dataset scripts suited for small scale, and leveraging video2dataset for large scale use cases. \u201cIf I have seen further it is by standing on the shoulders of giants\u201d In this post, we provide an overview of the tooling we are developing to make it easy for the community to build their own datasets for fine-tuning video generation...", "url": "https://huggingface.co/blog/vid_ds_scripts", "date_published": "2025-02-12T00:00:00"}, {"id": "https://huggingface.co/blog/leaderboard-arabic-v2", "image": "https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_arabic.png", "title": "The Open Arabic LLM Leaderboard 2", "content_text": "Back to Articles The Open Arabic LLM Leaderboard 2 Published February 10, 2025 Update on GitHub Upvote 27 +21 alielfilali01 Ali El Filali 2A2I Manel-Hik Manel ALOUI OALL tarickMorty Tarique Husaain AI71ai amztheory Ahmed Alzubaidi tiiuae Basma-b Basma Boussaha tiiuae rcojocaru Ruxandra Cojocaru tiiuae HakimHacid Hakim Hacid tiiuae clefourrier Cl\u00e9mentine Fourrier Current status of Arabic LLMs leaderboards Impact of the previous leaderboard Why do we need a new leaderboard? What's new in this version? Results from v1 and v2 Conclusion and future work Acknowledgments Citations References Current status of Arabic LLMs leaderboards The growing availability of LLMs supporting Arabic, both as monolingual and multilingual models, prompted the community to create dedicated Arabic language leaderboards. Previously, Arabic-focused leaderboards were typically confined to narrow benchmarks introduced by specific authors, often as demos for their work. In these cases, the authors would set up...", "url": "https://huggingface.co/blog/leaderboard-arabic-v2", "date_published": "2025-02-10T00:00:00"}, {"id": "https://huggingface.co/blog/open-deep-research", "image": "https://huggingface.co/blog/assets/open-deep-research/thumbnail.png", "title": "Open-source DeepResearch \u2013 Freeing our search agents", "content_text": "Back to Articles Open-source DeepResearch \u2013 Freeing our search agents Published February 4, 2025 Update on GitHub Upvote 1100 +1094 m-ric Aymeric Roucher albertvillanova Albert Villanova del Moral merve Merve Noyan thomwolf Thomas Wolf clefourrier Cl\u00e9mentine Fourrier TLDR Table of Contents What are Agent frameworks and why they matter? The GAIA benchmark Building an open Deep Research Using a CodeAgent Making the right tools \ud83d\udee0\ufe0f Results \ud83c\udfc5 Community Reproductions Most important next steps TLDR Yesterday, OpenAI released Deep Research , a system that browses the web to summarize content and answer questions based on the summary. The system is impressive and blew our minds when we tried it for the first time. One of the main results in the blog post is a strong improvement of performances on the General AI Assistants benchmark (GAIA) , a benchmark we\u2019ve been playing with recently as well, where they successfully reached near 67% correct answers on 1-shot on average, and 47.6% on...", "url": "https://huggingface.co/blog/open-deep-research", "date_published": "2025-02-04T00:00:00"}, {"id": "https://huggingface.co/blog/pi0", "image": "https://huggingface.co/blog/assets/192_pi0/new_thumbnail_pi0.001.png", "title": "\u03c00 and \u03c00-FAST: Vision-Language-Action Models for General Robot Control", "content_text": "Back to Articles \u03c00 and \u03c00-FAST: Vision-Language-Action Models for General Robot Control Published February 4, 2025 Update on GitHub Upvote 109 +103 danaaubakirova Dana Aubakirova Molbap Pablo Montalvo mshukor Mustafa Shukor cadene Remi Cadene Introduction \ud83d\udd0d What is \u03c00? How to Use \u03c00 in LeRobot? Inference on \u03c00 pretrained model Fine-tuning the \u03c00 Pretrained Model What is the difference between VLMs and VLAs? Attention Mechanisms in Robotics Policies Key Idea \u26a1 Towards the Faster Attention in \u03c00 Handling 2D Attention Masks Can we use FlashAttention2? Using FlexAttention in PyTorch How to effectively represent Actions? \ud83d\ude80 What is \u03c00-FAST? Key Advantages of \u03c00-FAST: How does FAST work? How to use FAST tokenizer? What\u2019s Next for Generalist Robot Intelligence? Additional Resources References We have ported the first robotics foundation models to Hugging Face LeRobot ! Both \u03c00 and \u03c00-FAST , developed by Physical Intelligence, are now available in the LeRobot repository , bringing...", "url": "https://huggingface.co/blog/pi0", "date_published": "2025-02-04T00:00:00"}, {"id": "https://huggingface.co/blog/dabstep", "image": "https://huggingface.co/blog/assets/dabstep/thumbnail.png", "title": "DABStep: Data Agent Benchmark for Multi-step Reasoning", "content_text": "Back to Articles DABStep: Data Agent Benchmark for Multi-step Reasoning Published February 4, 2025 Update on GitHub Upvote 50 +44 eggie5 Alex Egg guest martinigoyanes Martin Iglesias Goyanes guest frisokingma Friso Kingma guest andreumora Andreu Mora guest lvwerra Leandro von Werra thomwolf Thomas Wolf Motivation Introducing DABstep What's inside the DABstep? Data Tasks Evaluations Real-time leaderboard Baselines Getting Started and Infra Future direction Related Works Language models are becoming increasingly capable and can solve tasks autonomously as agents. There are many exciting use cases, especially at the intersection of reasoning, code, and data. However, proper evaluation benchmarks on real-world problems are lacking and hinder progress in the field. To tackle this challenge, Adyen and Hugging Face built the Data Agent Benchmark for Multi-step Reasoning (DABstep) together. DABstep consists of over 450 data analysis tasks designed to evaluate the capabilities of state-of-...", "url": "https://huggingface.co/blog/dabstep", "date_published": "2025-02-04T00:00:00"}]}