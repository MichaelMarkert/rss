{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Blog", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_blog.json", "items": [{"id": "https://huggingface.co/blog/diffusers-quantization", "image": "https://huggingface.co/blog/assets/diffusers-quantization/thumbnail.png", "title": "Exploring Quantization Backends in Diffusers", "content_text": "Back to Articles Exploring Quantization Backends in Diffusers Published May 21, 2025 Update on GitHub Upvote 18 +12 derekl35 Derek Liu marcsun13 Marc Sun sayakpaul Sayak Paul Spot The Quantized Model Quantization Backends in Diffusers bitsandbytes (BnB) torchao Quanto GGUF FP8 Layerwise Casting ( enable_layerwise_casting ) Combining with More Memory Optimizations and torch.compile Ready to use quantized checkpoints Conclusion Large diffusion models like Flux (a flow-based text-to-image generation model) can create stunning images, but their size can be a hurdle, demanding significant memory and compute resources. Quantization offers a powerful solution, shrinking these models to make them more accessible without drastically compromising performance. But the big question always is: can you actually tell the difference in the final image? Before we dive into the technical details of how various quantization backends in Hugging Face Diffusers work, why not test your own perception?...", "url": "https://huggingface.co/blog/diffusers-quantization", "date_published": "2025-05-21T00:00:00"}, {"id": "https://huggingface.co/blog/nanovlm", "image": "https://huggingface.co/blog/assets/nanovlm/thumbnail.png", "title": "nanoVLM: The simplest repository to train your VLM in pure PyTorch", "content_text": "Back to Articles nanoVLM: The simplest repository to train your VLM in pure PyTorch Published May 21, 2025 Update on GitHub Upvote 63 +57 ariG23498 Aritra Roy Gosthipaty lusxvr Luis Wiedmann andito Andres Marafioti sergiopaniego Sergio Paniego merve Merve Noyan pcuenq Pedro Cuenca reach-vb Vaibhav Srivastav Table of contents: TL;DR What is a Vision Language Model? Working with the repository Architecture Train your own VLM Run inference on a pre-trained model Conclusion References nanoVLM is the simplest way to get started with training your very own Vision Language Model (VLM) using pure PyTorch. It is lightweight toolkit which allows you to launch a VLM training on a free tier colab notebook . We were inspired by Andrej Karpathy \u2019s nanoGPT , and provide a similar project for the vision domain. At its heart, nanoVLM is a toolkit that helps you build and train a model that can understand both images and text, and then generate text based on that. The beauty of nanoVLM lies in its...", "url": "https://huggingface.co/blog/nanovlm", "date_published": "2025-05-21T00:00:00"}, {"id": "https://huggingface.co/blog/azure-ai-foundry", "image": "https://huggingface.co/blog/assets/azure-ai-foundry/satya-hf-build-compressed.png", "title": "Microsoft and Hugging Face expand collaboration", "content_text": "Back to Articles Microsoft and Hugging Face expand collaboration to make open models easy to use on Azure Published May 19, 2025 Update on GitHub Upvote 16 +10 jeffboudier Jeff Boudier pagezyhf Simon Pagezy alvarobartt Alvaro Bartolome It\u2019s time to build - an expanded collaboration How to use Hugging Face in Azure AI Foundry More Hugging Face to come in Azure AI Foundry Today at the Microsoft Build conference, Satya Nadella announced an expanded collaboration with Hugging Face, to make its wide diversity of open models easy to deploy on Azure secure infrastructure. If you head over to Azure AI Foundry today, you will find a vastly expanded collection of 10,000+ Hugging Face models you can deploy in a couple clicks to power AI applications working with text, audio and images. And we\u2019re just getting started! It\u2019s time to build - an expanded collaboration 2 years ago, Microsoft and Hugging Face started a collaboration to make open models more easily accessible on Azure - back then the...", "url": "https://huggingface.co/blog/azure-ai-foundry", "date_published": "2025-05-19T00:00:00"}, {"id": "https://huggingface.co/blog/transformers-model-definition", "image": "https://huggingface.co/blog/assets/transformers-model-definition/transformers-thumbnail.png", "title": "The Transformers Library: standardizing model definitions", "content_text": "Back to Articles The Transformers Library: standardizing model definitions Published May 15, 2025 Update on GitHub Upvote 100 +94 lysandre Lysandre ArthurZ Arthur Zucker pcuenq Pedro Cuenca julien-c Julien Chaumond A model-definition library Striving for even simpler model contributions How does this affect you? What this means for you, as a model user What this means for you, as a model creator TLDR: Going forward, we're aiming for Transformers to be the pivot across frameworks: if a model architecture is supported by transformers, you can expect it to be supported in the rest of the ecosystem. Transformers was created in 2019, shortly following the release of the BERT Transformer model. Since then, we've continuously aimed to add state-of-the-art architectures, initially focused on NLP, then growing to Audio and computer vision. Today, transformers is the default library for LLMs and VLMs in the Python ecosystem. Transformers now supports 300+ model architectures, with an average...", "url": "https://huggingface.co/blog/transformers-model-definition", "date_published": "2025-05-15T00:00:00"}, {"id": "https://huggingface.co/blog/kaggle-integration", "image": "https://huggingface.co/blog/assets/kaggle-integration/thumbnail.png", "title": "Improving Hugging Face Model Access for Kaggle Users", "content_text": "Back to Articles Improving Hugging Face Model Access for Kaggle Users Published May 14, 2025 Update on GitHub Upvote 24 +18 roseberryv Vincent Roseberry kaggle megrisdal Meg Risdal kaggle julien-c Julien Chaumond pcuenq Pedro Cuenca reach-vb Vaibhav Srivastav How to get started How does this work with private and consent-gated Hugging Face models? What\u2019s next Kaggle and Hugging Face users are part of one AI community. That\u2019s why we\u2019re excited to announce our plans to bring our platforms and communities closer to better serve AI developers everywhere. Beginning today, Kaggle is launching an integration that enhances visibility and discoverability for Hugging Face models directly on Kaggle. How to get started You can navigate from Hugging Face models to Kaggle and vice versa. Start by visiting a Hugging Face model page like Qwen/Qwen3-1.7B . To use it in a Kaggle Notebook, you can click on \u201cUse this model\u201d and select \u201cKaggle\u201d to open up a Kaggle notebook with a pre-populated code...", "url": "https://huggingface.co/blog/kaggle-integration", "date_published": "2025-05-14T00:00:00"}, {"id": "https://huggingface.co/blog/fast-whisper-endpoints", "image": "https://huggingface.co/blog/assets/fast-whisper-endpoints/thumbnail.png", "title": "Blazingly fast whisper transcriptions with Inference Endpoints", "content_text": "Back to Articles Blazingly fast whisper transcriptions with Inference Endpoints Published May 13, 2025 Update on GitHub Upvote 62 +56 mfuntowicz Morgan Funtowicz freddyaboulton Freddy Boulton Steveeeeeeen Steven Zheng reach-vb Vaibhav Srivastav erikkaum Erik Kaunism\u00e4ki michellehbn Michelle Habonneau Inference Stack Benchmarks How to deploy Inference FastRTC Demo Today we are happy to introduce a new blazing fast OpenAI Whisper deployment option on Inference Endpoints . It provides up to 8x performance improvements compared to the previous version, and makes everyone one click away from deploying dedicated, powerful transcription models in a cost-effective way, leveraging the amazing work done by the AI community. Through this release, we would like to make Inference Endpoints more community-centric and allow anyone to come and contribute to create incredible inference deployments on the Hugging Face Platform. Along with the community, we would like to propose optimized deployments...", "url": "https://huggingface.co/blog/fast-whisper-endpoints", "date_published": "2025-05-13T00:00:00"}, {"id": "https://huggingface.co/blog/vlms-2025", "image": "https://huggingface.co/blog/assets/vlms2/vlms2.png", "title": "Vision Language Models (Better, Faster, Stronger)", "content_text": "Back to Articles Vision Language Models (Better, Faster, Stronger) Published May 12, 2025 Update on GitHub Upvote 359 +353 merve Merve Noyan sergiopaniego Sergio Paniego ariG23498 Aritra Roy Gosthipaty pcuenq Pedro Cuenca andito Andres Marafioti Motivation Table of Contents New model trends Any-to-any models Reasoning Models Smol yet Capable Models Mixture-of-Experts as Decoders Vision-Language-Action Models Specialized Capabilities Object Detection, Segmentation, Counting with Vision Language Models Multimodal Safety Models Multimodal RAG: retrievers, rerankers Multimodal Agents Video Language Models New Alignment Techniques for Vision Language Models New benchmarks MMT-Bench MMMU-Pro Useful Resources Motivation Vision Language Models (VLMs) are the talk of the town. In a previous blog post (from April 2024 ), we talked a lot about VLMs. A major chunk was about LLaVA , the first successful and easily reproducible open-source vision language model, along with tips on how to...", "url": "https://huggingface.co/blog/vlms-2025", "date_published": "2025-05-12T00:00:00"}, {"id": "https://huggingface.co/blog/lerobot-datasets", "image": "https://huggingface.co/blog/assets/195_lerobot_datasets/1.png", "title": "LeRobot Community Datasets: The \u201cImageNet\u201d of Robotics \u2014 When and How?", "content_text": "Back to Articles LeRobot Community Datasets: The \u201cImageNet\u201d of Robotics \u2014 When and How? Published May 11, 2025 Update on GitHub Upvote 46 +40 danaaubakirova Dana Aubakirova Beegbrain Alexandre Chapin guest mshukor Mustafa Shukor m1b Marina guest villekuosmanen Ville Kuosmanen guest cadene Remi Cadene pcuenq Pedro Cuenca Introduction From Models to Data: Shifting the Perspective Why does Robotics lack its ImageNet Moment? Building a LeRobot Community Scaling Responsibly Better data = Better models Challenges with Current Community Datasets 1. Incomplete or Inconsistent Task Annotations 2. Feature Mapping Inconsistencies 3. Low-Quality or Incomplete Episodes 4. Inconsistent Action/State Dimensions What Makes a Good Dataset? Image Quality Metadata & Recording Protocol Feature Naming Conventions Task Annotation How Can You Help? \ud83e\udded TL;DR \u2014 Why This Blogpost? In this post, we: Recognize the growing impact of community-contributed LeRobot datasets Highlight the current challenges in...", "url": "https://huggingface.co/blog/lerobot-datasets", "date_published": "2025-05-11T00:00:00"}, {"id": "https://huggingface.co/blog/gradio-mcp", "image": "https://huggingface.co/blog/assets/gradio-mcp/thumbnail.png", "title": "How to Build an MCP Server with Gradio", "content_text": "Back to Articles How to Build an MCP Server in 5 Lines of Python Published April 30, 2025 Update on GitHub Upvote 112 +106 abidlabs Abubakar Abid ysharma yuvraj sharma Why Build an MCP Server? Example: Counting Letters in a Word Key features of the Gradio <> MCP Integration Further Reading Gradio is a Python library used by more than 1 million developers each month to build interfaces for machine learning models. Beyond just creating UIs, Gradio also exposes API capabilities and \u2014 now! \u2014 Gradio apps can be launched Model Context Protocol (MCP) servers for LLMs. This means that your Gradio app, whether it's an image generator or a tax calculator or something else entirely, can be called as a tool by an LLM. This guide will show you how to use Gradio to build an MCP server in just a few lines of Python. Prerequisites If not already installed, please install Gradio with the MCP extra: pip install \"gradio[mcp]\" This will install the necessary dependencies, including the mcp package....", "url": "https://huggingface.co/blog/gradio-mcp", "date_published": "2025-04-30T00:00:00"}, {"id": "https://huggingface.co/blog/llama-guard-4", "image": "https://huggingface.co/blog/assets/llama-guard-4/thumbnail.png", "title": "Welcoming Llama Guard 4 on Hugging Face Hub", "content_text": "Back to Articles Welcoming Llama Guard 4 on Hugging Face Hub Published April 29, 2025 Update on GitHub Upvote 37 +31 merve Merve Noyan ariG23498 Aritra Roy Gosthipaty sergiopaniego Sergio Paniego pcuenq Pedro Cuenca Table-of-Contents What is Llama Guard 4? Model Details Llama Guard 4 Llama Prompt Guard 2 Getting Started using \ud83e\udd17 transformers Llama Prompt Guard 2 Useful Resources TL;DR: Today, Meta releases Llama Guard 4, a 12B dense (not a MoE!) multimodal safety model, and two new Llama Prompt Guard 2 models. This release comes with multiple open model checkpoints, along with an interactive notebook for you to get started easily \ud83e\udd17. Model checkpoints can be found in Llama 4 Collection . Table-of-Contents What is Llama Guard 4? Model Details Llama Guard 4 Llama Prompt Guard 2 Getting Started using \ud83e\udd17transformers Llama Guard 4 Llama Prompt Guard 2 Useful Resources What is Llama Guard 4? Vision and large language models deployed to production can be exploited to generate unsafe output...", "url": "https://huggingface.co/blog/llama-guard-4", "date_published": "2025-04-29T00:00:00"}, {"id": "https://huggingface.co/blog/qwen-3-chat-template-deep-dive", "image": "https://huggingface.co/blog/assets/qwen-3-chat-template-deep-dive/thumbnail.png", "title": "The 4 Things Qwen-3's Chat Template Teaches Us", "content_text": "Back to Articles The 4 Things Qwen-3\u2019s Chat Template Teaches Us Published April 30, 2025 Update on GitHub Upvote 43 +37 cfahlgren1 Caleb Fahlgren What is a Chat Template? 1. Reasoning doesn't have to be forced 2. Context Management Should be Dynamic Example 3. Tool Arguments Need Better Serialization 4. There's No Need for a Default System Prompt Conclusion What a boring Jinja snippet tells us about the new Qwen-3 model. The new Qwen-3 model by Qwen ships with a much more sophisticated chat template than its predecessors Qwen-2.5 and QwQ. By taking a look at the differences in the Jinja template, we can find interesting insights into the new model. Chat Templates Qwen-3 Chat Template Qwen-2.5 Chat Template Qwen-QwQ Chat Template What is a Chat Template? A chat template defines how conversations between users and models are structured and formatted. The template acts as a translator, converting a human-readable conversation: [ { role : \"user\" , content : \"Hi there!\" }, { role :...", "url": "https://huggingface.co/blog/qwen-3-chat-template-deep-dive", "date_published": "2025-04-30T00:00:00"}, {"id": "https://huggingface.co/blog/tiny-agents", "image": "https://huggingface.co/blog/assets/tiny-agents/thumbnail.jpg", "title": "Tiny Agents: a MCP-powered agent in 50 lines of code", "content_text": "Back to Articles Tiny Agents: an MCP-powered agent in 50 lines of code Published April 25, 2025 Update on GitHub Upvote 252 +246 julien-c Julien Chaumond How to run the complete demo Default model and provider Where does the code live The foundation for this: tool calling native support in LLMs. Implementing an MCP client on top of InferenceClient How to use the tools Our 50-lines-of-code Agent \ud83e\udd2f The complete while loop Next steps Over the past few weeks, I've been diving into MCP ( Model Context Protocol ) to understand what the hype around it was all about. My TL;DR is that it's fairly simple, but still quite powerful: MCP is a standard API to expose sets of Tools that can be hooked to LLMs. It is fairly simple to extend an Inference Client \u2013 at HF, we have two official client SDKs: @huggingface/inference in JS, and huggingface_hub in Python \u2013 to also act as a MCP client and hook the available tools from MCP servers into the LLM inference. But while doing that, came my second...", "url": "https://huggingface.co/blog/tiny-agents", "date_published": "2025-04-25T00:00:00"}, {"id": "https://huggingface.co/blog/autoround", "image": "https://huggingface.co/blog/assets/autoround/thumbnail.png", "title": "Introducing AutoRound: Intel\u2019s Advanced Quantization for LLMs and VLMs", "content_text": "Back to Articles What is AutoRound? Published April 29, 2025 Update on GitHub Upvote 29 +23 wenhuach wenhua cheng Intel Haihao Haihao Shen Intel weiweiz1 weiweiz1 Intel n1ck-guo Heng Guo Intel isaacmac Huang, Tai Intel kding1 Ke Ding Intel IlyasMoutawwakil Ilyas Moutawwakil marcsun13 Marc Sun medmekk Mohamed Mekkouri Superior Accuracy at Low Bit Widths 2. Broad Compatibility Models Devices Quantization Configurations Export Formats 3. Flexible/Efficient Quantization Installation Quantization and Serialization Command Line Usage AutoRound API Usage Inference CPU/Intel GPU/CUDA Convert GPTQ/AWQ to AutoRound As large language models (LLMs) and vision-language models (VLMs) continue to grow in size and complexity, deploying them efficiently becomes increasingly challenging. Quantization offers a solution by reducing model size and inference latency. Intel's AutoRound emerges as a cutting-edge quantization tool that balances accuracy, efficiency, and compatibility. AutoRound is a weight-...", "url": "https://huggingface.co/blog/autoround", "date_published": "2025-04-29T00:00:00"}, {"id": "https://huggingface.co/blog/why-gradio-stands-out", "image": "https://huggingface.co/blog/assets/why-gradio-stands-out/thumbnail.png", "title": "17 Reasons Why Gradio Isn't Just Another UI Library", "content_text": "Back to Articles 17 Reasons Why Gradio Isn't Just Another UI Library Published April 16, 2025 Update on GitHub Upvote 35 +29 ysharma yuvraj sharma abidlabs Abubakar Abid Introduction 1. Universal API Access 2. Interactive API Recorder for Development 3. Fast ML Apps with Server-Side Rendering 4. Automatic Queue Management for ML Tasks 5. High-Performance Streaming for Real-Time ML Outputs 6. Integrated Multi-Page Application Support 7. New Client-Side Function Execution With Groovy 8. A Comprehensive Theming System and Modern UI Components 9. Gradio's Dynamic Interfaces 10. Visual Interface Development with Gradio Sketch 11. Progressive Web App (PWA) Support 12. In-Browser Execution with Gradio Lite 13. Accelerated Development with AI-Assisted Tooling 14. Hassle-Free App Sharing 15. Enterprise-Grade Security and Production Readiness 16. Enhanced Dataframe Component 17. Deep Links for Sharing App States Conclusion Introduction \"Oh, Gradio? That's a Python library for building UIs,...", "url": "https://huggingface.co/blog/why-gradio-stands-out", "date_published": "2025-04-16T00:00:00"}, {"id": "https://huggingface.co/blog/inference-providers-cohere", "image": "https://huggingface.co/blog/assets/inference-providers-cohere/thumbnail.png", "title": "Cohere on Hugging Face Inference Providers \ud83d\udd25", "content_text": "Back to Articles Cohere on Hugging Face Inference Providers \ud83d\udd25 Published April 16, 2025 Update on GitHub Upvote 126 +120 reach-vb Vaibhav Srivastav burtenshaw ben burtenshaw merve Merve Noyan celinah C\u00e9lina Hanouti alexrs Alejandro Rodriguez CohereLabs julien-c Julien Chaumond sbrandeis Simon Brandeis Cohere Models CohereLabs/c4ai-command-a-03-2025 \ud83d\udd17 CohereLabs/aya-expanse-32b \ud83d\udd17 CohereLabs/c4ai-command-r7b-12-2024 \ud83d\udd17 CohereLabs/aya-vision-32b \ud83d\udd17 How it works In the website UI From the client SDKs From OpenAI client Tool Use with Cohere Models Billing We're thrilled to share that Cohere is now a supported Inference Provider on HF Hub! This also marks the first model creator to share and serve their models directly on the Hub. Cohere is committed to building and serving models purpose-built for enterprise use-cases. Their comprehensive suite of secure AI solutions, from cutting-edge Generative AI to powerful Embeddings and Ranking models, are designed to tackle real-world business...", "url": "https://huggingface.co/blog/inference-providers-cohere", "date_published": "2025-04-16T00:00:00"}]}