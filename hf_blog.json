{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Blog", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_blog.json", "items": [{"id": "https://huggingface.co/blog/billion-classifications", "image": "https://huggingface.co/blog/assets/billion-classifications/billion-classifications-thumbnail.png", "title": "1 Billion Classifications", "content_text": "Back to Articles 1 Billion Classifications Published February 13, 2025 Update on GitHub Upvote 4 derek-thomas Derek Thomas guest Approach Optimization Setup Load Testing Parameters K6 Orchestration Classification Introduction Experiment Results Embedding Introduction Experiment Results Vision Embedding Introduction Experiment Results Analysis Conclusion References Appendix Sanity Checks Cost Analysis Infinity Client Other Lessons Learned Future Improvements You\u2019ve optimized your model. Your pipeline is running smoothly. But now, your cloud bill has skyrocketed. Running 1B+ classifications or embeddings per day isn\u2019t just a technical challenge\u2014it\u2019s a financial one. How do you process at this scale without blowing your budget? Whether you're running large-scale document classification or bulk embedding pipelines for Retrieval-Augmented Generation (RAG), you need cost-efficient, high-throughput inference to make it feasible, and you get that from having a well optimized configuration....", "url": "https://huggingface.co/blog/billion-classifications", "date_published": "2025-02-13T00:00:00"}, {"id": "https://huggingface.co/blog/from-chunks-to-blocks", "image": "https://huggingface.co/blog/assets/from-chunks-to-blocks/thumbnail.png", "title": "From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub", "content_text": "Back to Articles From Chunks to Blocks: Accelerating Uploads and Downloads on the Hub Published February 12, 2025 Update on GitHub Upvote 18 +12 jsulz Jared Sulzdorf xet-team yuchenglow yuchenglow xet-team znation Zach Nation xet-team saba9 saba noorassa xet-team The Realities of Scaling Deduplication Design Principles for Deduplication at Scale Scaling Deduplication with Aggregation Aggregated Deduplication in Practice Content-defined chunking (CDC) plays a central role in enabling deduplication within a Xet-backed repository . The idea is straightforward: break each file\u2019s data into chunks, store only unique ones, reap the benefits. In practice, it's more complex. If we focused solely on maximizing deduplication, the design would call for the smallest possible chunk size. By doing that, we\u2019d create significant overheads for the infrastructure and the builders on the Hub. On Hugging Face's Xet team , we're bringing CDC from theory to production to deliver faster uploads and...", "url": "https://huggingface.co/blog/from-chunks-to-blocks", "date_published": "2025-02-12T00:00:00"}, {"id": "https://huggingface.co/blog/vid_ds_scripts", "image": "https://huggingface.co/blog/assets/vid_ds_scripts/thumbnail.png", "title": "Build awesome datasets for video generation", "content_text": "Back to Articles Build awesome datasets for video generation Published February 12, 2025 Update on GitHub Upvote 18 +12 hlky hlky sayakpaul Sayak Paul Tooling Stage 1 (Acquisition) Stage 2 (Pre-processing/filtering) Stage 3 (Processing) Filtering examples OCR/Caption Putting this tooling to use \ud83d\udc68\u200d\ud83c\udf73 Your Turn Tooling for image generation datasets is well established, with img2dataset being a fundamental tool used for large scale dataset preparation, and complemented with various community guides, scripts and UIs that cover smaller scale initiatives. Our ambition is to make tooling for video generation datasets equally established, by creating open video dataset scripts suited for small scale, and leveraging video2dataset for large scale use cases. \u201cIf I have seen further it is by standing on the shoulders of giants\u201d In this post, we provide an overview of the tooling we are developing to make it easy for the community to build their own datasets for fine-tuning video generation...", "url": "https://huggingface.co/blog/vid_ds_scripts", "date_published": "2025-02-12T00:00:00"}, {"id": "https://huggingface.co/blog/leaderboard-arabic-v2", "image": "https://huggingface.co/blog/assets/leaderboards-on-the-hub/thumbnail_arabic.png", "title": "The Open Arabic LLM Leaderboard 2", "content_text": "Back to Articles The Open Arabic LLM Leaderboard 2 Published February 10, 2025 Update on GitHub Upvote 20 +14 alielfilali01 Ali El Filali 2A2I Manel-Hik Manel ALOUI OALL tarickMorty Tarique Husaain AI71ai amztheory Ahmed Alzubaidi tiiuae Basma-b Basma Boussaha tiiuae rcojocaru Ruxandra Cojocaru tiiuae HakimHacid Hakim Hacid tiiuae clefourrier Cl\u00e9mentine Fourrier Current status of Arabic LLMs leaderboards Impact of the previous leaderboard Why do we need a new leaderboard? What's new in this version? Results from v1 and v2 Conclusion and future work Acknowledgments Citations References Current status of Arabic LLMs leaderboards The growing availability of LLMs supporting Arabic, both as monolingual and multilingual models, prompted the community to create dedicated Arabic language leaderboards. Previously, Arabic-focused leaderboards were typically confined to narrow benchmarks introduced by specific authors, often as demos for their work. In these cases, the authors would set up...", "url": "https://huggingface.co/blog/leaderboard-arabic-v2", "date_published": "2025-02-10T00:00:00"}, {"id": "https://huggingface.co/blog/open-deep-research", "image": "https://huggingface.co/blog/assets/open-deep-research/thumbnail.png", "title": "Open-source DeepResearch \u2013 Freeing our search agents", "content_text": "Back to Articles Open-source DeepResearch \u2013 Freeing our search agents Published February 4, 2025 Update on GitHub Upvote 953 +947 m-ric Aymeric Roucher albertvillanova Albert Villanova del Moral merve Merve Noyan thomwolf Thomas Wolf clefourrier Cl\u00e9mentine Fourrier TLDR Table of Contents What are Agent frameworks and why they matter? The GAIA benchmark Building an open Deep Research Using a CodeAgent Making the right tools \ud83d\udee0\ufe0f Results \ud83c\udfc5 Community Reproductions Most important next steps TLDR Yesterday, OpenAI released Deep Research , a system that browses the web to summarize content and answer questions based on the summary. The system is impressive and blew our minds when we tried it for the first time. One of the main results in the blog post is a strong improvement of performances on the General AI Assistants benchmark (GAIA) , a benchmark we\u2019ve been playing with recently as well, where they successfully reached near 67% correct answers on 1-shot on average, and 47.6% on...", "url": "https://huggingface.co/blog/open-deep-research", "date_published": "2025-02-04T00:00:00"}, {"id": "https://huggingface.co/blog/pi0", "image": "https://huggingface.co/blog/assets/192_pi0/new_thumbnail_pi0.001.png", "title": "\u03c00 and \u03c00-FAST: Vision-Language-Action Models for General Robot Control", "content_text": "Back to Articles \u03c00 and \u03c00-FAST: Vision-Language-Action Models for General Robot Control Published February 4, 2025 Update on GitHub Upvote 93 +87 danaaubakirova Dana Aubakirova Molbap Pablo Montalvo mshukor Mustafa Shukor cadene Remi Cadene Introduction \ud83d\udd0d What is \u03c00? How to Use \u03c00 in LeRobot? Inference on \u03c00 pretrained model Fine-tuning the \u03c00 Pretrained Model What is the difference between VLMs and VLAs? Attention Mechanisms in Robotics Policies Key Idea \u26a1 Towards the Faster Attention in \u03c00 Handling 2D Attention Masks Can we use FlashAttention2? Using FlexAttention in PyTorch How to effectively represent Actions? \ud83d\ude80 What is \u03c00-FAST? Key Advantages of \u03c00-FAST: How does FAST work? How to use FAST tokenizer? What\u2019s Next for Generalist Robot Intelligence? Additional Resources References We have ported the first robotics foundation models to Hugging Face LeRobot ! Both \u03c00 and \u03c00-FAST , developed by Physical Intelligence, are now available in the LeRobot repository , bringing generalist...", "url": "https://huggingface.co/blog/pi0", "date_published": "2025-02-04T00:00:00"}, {"id": "https://huggingface.co/blog/dabstep", "image": "https://huggingface.co/blog/assets/dabstep/thumbnail.png", "title": "DABStep: Data Agent Benchmark for Multi-step Reasoning", "content_text": "Back to Articles DABStep: Data Agent Benchmark for Multi-step Reasoning Published February 4, 2025 Update on GitHub Upvote 44 +38 eggie5 Alex Egg guest martinigoyanes Martin Iglesias Goyanes guest frisokingma Friso Kingma guest andreumora Andreu Mora guest lvwerra Leandro von Werra thomwolf Thomas Wolf Motivation Introducing DABstep What's inside the DABstep? Data Tasks Evaluations Real-time leaderboard Baselines Getting Started and Infra Future direction Related Works Language models are becoming increasingly capable and can solve tasks autonomously as agents. There are many exciting use cases, especially at the intersection of reasoning, code, and data. However, proper evaluation benchmarks on real-world problems are lacking and hinder progress in the field. To tackle this challenge, Adyen and Hugging Face built the Data Agent Benchmark for Multi-step Reasoning (DABstep) together. DABstep consists of over 450 data analysis tasks designed to evaluate the capabilities of state-of-...", "url": "https://huggingface.co/blog/dabstep", "date_published": "2025-02-04T00:00:00"}, {"id": "https://huggingface.co/blog/ai-art-newsletter-jan-25", "image": "https://huggingface.co/blog/assets/ai_art_newsletter_1/thumbnail.png", "title": "The AI tools for Art Newsletter - Issue 1", "content_text": "Back to Articles The AI tools for Art Newsletter Published January 31, 2025 Update on GitHub Upvote 51 +45 linoyts Linoy Tsaban multimodalart Apolin\u00e1rio from multimodal AI art First issue \ud83c\udf89 Table of Contents Major Releases of 2024 Image Generation Text-to-image generation Personalization & stylization Video Generation Audio Generation Creative Tools that Shined in 2024 What should we expect for AI & Art in 2025? Starting off strong - Open source releases of January 25 Announcing Our Newsletter \ud83d\uddde\ufe0f The AI space is moving so fast it\u2019s hard to believe that a year ago we still struggled to generate people with the correct amount of fingers \ud83d\ude02. The last couple of years have been pivotal for open source models and tools for artistic usage. AI tools for creative expression have never been more accessible, and we\u2019re only scratching the surface. Join us as we look back at the key milestones, tools, and breakthroughs in AI & Arts from 2024, and forward for what\u2019s to come in 2025 (spoiler \ud83d\udc40:...", "url": "https://huggingface.co/blog/ai-art-newsletter-jan-25", "date_published": "2025-01-31T00:00:00"}, {"id": "https://huggingface.co/blog/deepseek-r1-aws", "image": "https://huggingface.co/blog/assets/deepseek-r1-aws/thumbnail.png", "title": "How to deploy and fine-tune DeepSeek models on AWS", "content_text": "Back to Articles How to deploy and fine-tune DeepSeek models on AWS Published January 30, 2025 Update on GitHub Upvote 43 +37 pagezyhf Simon Pagezy jeffboudier Jeff Boudier dacorvo David Corvoysier What is DeepSeek-R1? Deploy DeepSeek R1 models Deploy on AWS with Hugging Face Inference Endpoints Deploy on Amazon Bedrock Marketplace Deploy on Amazon Sagemaker AI with Hugging Face LLM DLCs Deploy on EC2 Neuron with the Hugging Face Neuron Deep Learning AMI Fine-tune DeepSeek R1 models Fine tune on Amazon SageMaker AI with Hugging Face Training DLCs Fine tune on EC2 Neuron with the Hugging Face Neuron Deep Learning AMI A running document to showcase how to deploy and fine-tune DeepSeek R1 models with Hugging Face on AWS. What is DeepSeek-R1? If you\u2019ve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully. OpenAI\u2019s o1 model showed that when LLMs are trained to do the same\u2014by using more compute during inference\u2014they get...", "url": "https://huggingface.co/blog/deepseek-r1-aws", "date_published": "2025-01-30T00:00:00"}, {"id": "https://huggingface.co/blog/inference-providers", "image": "https://huggingface.co/blog/assets/inference-providers/thumbnail.png", "title": "Welcome to Inference Providers on the Hub \ud83d\udd25", "content_text": "Back to Articles Welcome to Inference Providers on the Hub \ud83d\udd25 Published January 28, 2025 Update on GitHub Upvote 335 +329 burkaygur Burkay Gur fal zeke Zeke Sikelianos replicate aton2006 Anton McGonnell sambanovasystems hassanelmghari Hassan El Mghari togethercomputer sbrandeis Simon Brandeis kramp Bertrand Chevrier julien-c Julien Chaumond How it works In the website UI From the client SDKs From HTTP calls Billing Feedback and next steps Today, we are launching the integration of four awesome serverless Inference Providers \u2013 fal, Replicate, Sambanova, Together AI \u2013 directly on the Hub\u2019s model pages. They are also seamlessly integrated into our client SDKs (for JS and Python), making it easier than ever to explore serverless inference of a wide variety of models that run on your favorite providers. We\u2019ve been hosting a serverless Inference API on the Hub for a long time (we launched the v1 in summer 2020 \u2013 wow, time flies \ud83e\udd2f). While this has enabled easy exploration and prototyping,...", "url": "https://huggingface.co/blog/inference-providers", "date_published": "2025-01-28T00:00:00"}, {"id": "https://huggingface.co/blog/open-r1", "image": "https://huggingface.co/blog/assets/open-r1/thumbnails.png", "title": "Open-R1: a fully open reproduction of DeepSeek-R1", "content_text": "Back to Articles Open-R1: a fully open reproduction of DeepSeek-R1 Published January 28, 2025 Update on GitHub Upvote 727 +721 eliebak Elie Bakouch lvwerra Leandro von Werra lewtun Lewis Tunstall What is DeepSeek-R1? How did they do it? Open-R1: the missing pieces What is DeepSeek-R1? If you\u2019ve ever struggled with a tough math problem, you know how useful it is to think a little longer and work through it carefully. OpenAI\u2019s o1 model showed that when LLMs are trained to do the same\u2014by using more compute during inference\u2014they get significantly better at solving reasoning tasks like mathematics, coding, and logic. However, the recipe behind OpenAI\u2019s reasoning models has been a well kept secret. That is, until last week, when DeepSeek released their DeepSeek-R1 model and promptly broke the internet (and the stock market! ). Besides performing as well or better than o1, the DeepSeek-R1 release was accompanied by a detailed tech report that outlined the key steps of their training...", "url": "https://huggingface.co/blog/open-r1", "date_published": "2025-01-28T00:00:00"}, {"id": "https://huggingface.co/blog/video_gen", "image": "https://huggingface.co/blog/assets/video_gen/thumbnail.png", "title": "State of open video generation models in Diffusers", "content_text": "Back to Articles State of open video generation models in Diffusers Published January 27, 2025 Update on GitHub Upvote 38 +32 sayakpaul Sayak Paul a-r-r-o-w Aryan V S dn6 Dhruv Nair Today\u2019s Video Generation Models and their Limitations Why is Video Generation Hard? Open Video Generation Models Video Generation with Diffusers Memory requirements Suite of optimizations Fine-tuning Looking ahead Resources OpenAI\u2019s Sora demo marked a striking advance in AI-generated video last year and gave us a glimpse of the potential capabilities of video generation models. The impact was immediate and since that demo, the video generation space has become increasingly competitive with major players and startups producing their own highly capable models such as Google\u2019s Veo2, Haliluo\u2019s Minimax, Runway\u2019s Gen3 Alpha, Kling, Pika, and Luma Lab\u2019s Dream Machine. Open-source has also had its own surge of video generation models with CogVideoX, Mochi-1, Hunyuan, Allegro, and LTX Video. Is the video...", "url": "https://huggingface.co/blog/video_gen", "date_published": "2025-01-27T00:00:00"}, {"id": "https://huggingface.co/blog/smolagents-can-see", "image": "https://huggingface.co/blog/assets/smolagents-can-see/thumbnail.png", "title": "We now support VLMs in smolagents!", "content_text": "Back to Articles We just gave sight to smolagents Published January 24, 2025 Update on GitHub Upvote 78 +72 m-ric Aymeric Roucher merve Merve Noyan albertvillanova Albert Villanova del Moral TL;DR Table of Contents Overview How we gave sight to smolagents How to create a Web browsing agent with vision Running the agent Next Steps You hypocrite, first take the log out of your own eye, and then you will see clearly to take the speck out of your brother's eye. Matthew 7, 3-5 TL;DR We have added vision support to smolagents, which unlocks the use of vision language models in agentic pipelines natively. Table of Contents Overview How we gave sight to smolagents How to create a Web browsing agent with vision Next Steps Overview In the agentic world, many capabilities are hidden behind a vision wall. A common example is web browsing: web pages feature rich visual content that you never fully recover by simply extracting their text, be it the relative position of objects, messages...", "url": "https://huggingface.co/blog/smolagents-can-see", "date_published": "2025-01-24T00:00:00"}, {"id": "https://huggingface.co/blog/smolervlm", "image": "https://huggingface.co/blog/assets/smolervlm/banner.png", "title": "SmolVLM Grows Smaller \u2013 Introducing the 250M & 500M Models!", "content_text": "Back to Articles SmolVLM Grows Smaller \u2013 Introducing the 250M & 500M Models! Published January 23, 2025 Update on GitHub Upvote 127 +121 andito Andres Marafioti mfarre Miquel Farr\u00e9 merve Merve Noyan TLDR Table of Contents Overview Why Go Smaller? Meet the 256M Parameter Giant A Step Up: 500M What Changed Since SmolVLM 2B? Smaller Multimodal Retrieval: ColSmolVLM 256M & 500M SmolDocling Using Smaller SmolVLM Next Steps TLDR We\u2019re excited to announce two new additions to the SmolVLM family: SmolVLM-256M and SmolVLM-500M. That\u2019s right\u2014256M parameters, making it the smallest Vision Language Model in the world! We built on everything we learned from SmolVLM 2B while focusing on efficiency, data mixtures, and new design trade-offs. We are excited to introduce a pair of models that preserve strong multimodal performance in a fraction of the footprint. This release comes with four checkpoints: two base models and two instruction fine-tuned models with sizes 256M and 500M parameters. These...", "url": "https://huggingface.co/blog/smolervlm", "date_published": "2025-01-23T00:00:00"}, {"id": "https://huggingface.co/blog/friendliai-partnership", "image": "https://huggingface.co/blog/assets/friendliai-partnership/thumbnail.png", "title": "Hugging Face and FriendliAI partner to supercharge model deployment on the Hub", "content_text": "Back to Articles Hugging Face and FriendliAI partner to supercharge model deployment on the Hub Published January 22, 2025 Update on GitHub Upvote 30 +24 ajshinfai Ahnjae Shin FriendliAI soominc Soomin Chun FriendliAI bgchun Byung-Gon Chun FriendliAI julien-c Julien Chaumond A Collaboration to Advance AI Innovation Simplifying Model Deployment Deploy models with NVIDIA H100 in Friendli Dedicated Endpoints Inference Open-Source Models with Friendli Serverless Endpoints What\u2019s Next FriendliAI\u2019s inference infrastructure is now integrated into the Hugging Face Hub as an option in the \u201cDeploy this model\u201d button, simplifying and accelerating generative AI model serving. A Collaboration to Advance AI Innovation Hugging Face empowers developers, researchers, and businesses to innovate in AI. Our common priority is building impactful partnerships that simplify workflows and provide cutting-edge tools for the AI community. Today, we are excited to announce a partnership between HF and...", "url": "https://huggingface.co/blog/friendliai-partnership", "date_published": "2025-01-22T00:00:00"}]}