<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Groq on Hugging Face Inference Providers üî•</title><link>https://huggingface.co/blog/inference-providers-groq</link><description>Back to Articles Groq on Hugging Face Inference Providers üî• Published June 16, 2025 Update on GitHub Upvote 13 +7 Ben Ankiel benank-groq Follow Groq Hatice Ozen hozen Follow Groq C√©lina Hanouti celinah Follow Lucain Pouget Wauplin Follow Simon Brandeis sbrandeis Follow How it works In the website UI From the client SDKs Billing Feedback and next steps We're thrilled to share that Groq is now a supported Inference Provider on the Hugging Face Hub! Groq joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub‚Äôs model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers. Groq supports a wide variety of text and conversational models, including the latest open-source models such as Meta's LLama 4 , Qwen's QWQ-32B , ad many more. At the heart of Groq's technology is the Language Processing Unit...</description><pubDate>Mon, 16 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/inference-providers-groq</guid></item><item><title>Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub</title><link>https://huggingface.co/blog/hello-hf-kernels</link><description>Back to Articles üèéÔ∏è Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub Published June 12, 2025 Update on GitHub Upvote 69 +63 David Holtz drbh Follow Dani√´l de Kok danieldk Follow Nicolas Patry Narsil Follow Pedro Cuenca pcuenq Follow Simon Pagezy pagezyhf Follow merve merve Follow Vaibhav Srivastav reach-vb Follow 1. What is the Kernel Hub? Benefits of the Kernel Hub: 2. How to Use the Kernel Hub (Basic Example) What's happening here? 3. Add a Kernel to a Simple Model 4. Benchmarking the Performance Impact 5. Real World Use Cases Get Started and Next Steps! Conclusion Boost your model performance with pre-optimized kernels, easily loaded from the Hub. Today, we'll explore an exciting development from Hugging Face: the Kernel Hub ! As ML practitioners, we know that maximizing performance often involves diving deep into optimized code, custom CUDA kernels, or complex build systems. The Kernel Hub simplifies this process dramatically! Below is a short example of how to...</description><pubDate>Thu, 12 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/hello-hf-kernels</guid></item><item><title>Featherless AI on Hugging Face Inference Providers üî•</title><link>https://huggingface.co/blog/inference-providers-featherless</link><description>Back to Articles Featherless AI on Hugging Face Inference Providers üî• Published June 12, 2025 Update on GitHub Upvote 37 +31 Wesley George wxgeorge Follow featherless-ai Poh Nean pohnean-recursal Follow featherless-ai Eugene Cheah picocreator Follow featherless-ai C√©lina Hanouti celinah Follow Lucain Pouget Wauplin Follow Simon Brandeis sbrandeis Follow How it works In the website UI From the client SDKs Billing Feedback and next steps We're thrilled to share that Featherless AI is now a supported Inference Provider on the Hugging Face Hub! Featherless AI joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub‚Äôs model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers. Featherless AI supports a wide variety of text and conversational models, including the latest open-source models from...</description><pubDate>Thu, 12 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/inference-providers-featherless</guid></item><item><title>Introducing Training Cluster as a Service - a new collaboration with NVIDIA</title><link>https://huggingface.co/blog/nvidia-training-cluster</link><description>Back to Articles Introducing Training Cluster as a Service - a new collaboration with NVIDIA Published June 11, 2025 Update on GitHub Upvote 20 +14 Jeff Boudier jeffboudier Follow Arjuna ark393 Follow Simon Pagezy pagezyhf Follow Making GPU Clusters Accessible How it works Clusters at Work Advancing Rare Genetic Disease Research with TIGEM Advancing AI for Mathematics with Numina Advancing Material Science with Mirror Physics Powering the Diversity of AI Research Enabling AI Builders with NVIDIA Today at GTC Paris, we are excited to announce Training Cluster as a Service in collaboration with NVIDIA, to make large GPU clusters more easily accessible for research organizations all over the world, so they can train the foundational models of tomorrow in every domain. Making GPU Clusters Accessible Many Gigawatt-size GPU supercluster projects are being built to train the next gen of AI models. This can make it seem that the compute gap between the ‚ÄúGPU poor‚Äù and the ‚ÄúGPU rich‚Äù is...</description><pubDate>Wed, 11 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/nvidia-training-cluster</guid></item><item><title>ScreenSuite - The most comprehensive evaluation suite for GUI Agents!</title><link>https://huggingface.co/blog/screensuite</link><description>Back to Articles ScreenSuite - The most comprehensive evaluation suite for GUI Agents! Published June 6, 2025 Update on GitHub Upvote 40 +34 Amir Mahla A-Mahla Follow Aymeric Roucher m-ric Follow Thomas Wolf thomwolf Follow Releasing ScreenSuite, the most comprehensive evaluation suite for GUI Agents! WTF is a GUI Agent? Introducing ScreenSuite ü•≥ Ranking leading VLMs on ScreenSuite üìä Start your custom evaluation in 30s ‚ö°Ô∏è Next steps üöÄ Releasing ScreenSuite, the most comprehensive evaluation suite for GUI Agents! TL;DR Over the past few weeks, we‚Äôve been working tirelessly on making GUI agents more open, accessible and easy to integrate. Along the way, we created the largest benchmarking suite for GUI agents performances üëâ let us introduce ScreenSuite . We are very excited to share it with you today: ScreenSuite is the most comprehensive and easiest way to evaluate Vision Language Models (VLMs)across many agentic capabilities! WTF is a GUI Agent? GUI Agents in action - courtesy of...</description><pubDate>Fri, 06 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/screensuite</guid></item><item><title>KV Cache from scratch in nanoVLM</title><link>https://huggingface.co/blog/kv-cache</link><description>Back to Articles KV Cache from scratch in nanoVLM Published June 4, 2025 Update on GitHub Upvote 69 +63 Aritra Roy Gosthipaty ariG23498 Follow Kashif Rasul kashif Follow Luis Wiedmann lusxvr Follow Andres Marafioti andito Follow Pedro Cuenca pcuenq Follow TL;DR Introduction Revisiting the Transformer Architecture Self-Attention Computation Where Redundancy Creeps In How KV Caching Fixes It KV Caching in nanoVLM: From Theory to Practice 1. Updating KV Cache in the Attention Block 2. Tracking Cache Across Layers 3. Prefill vs Decode in the Generation Loop Summary of Changes Summary: Why KV Caching Matters TL;DR We have implemented KV Caching from scratch in our nanoVLM repository (a small codebase to train your own Vision Language Model with pure PyTorch). This gave us a 38% speedup in generation. In this blog post we cover KV Caching and all our experiences while implementing it. The lessons learnt are general and can be applied to all autoregressive language model generations....</description><pubDate>Wed, 04 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/kv-cache</guid></item><item><title>SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data</title><link>https://huggingface.co/blog/smolvla</link><description>Back to Articles SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data Published June 3, 2025 Update on GitHub Upvote 141 +135 Dana Aubakirova danaaubakirova Follow Andres Marafioti andito Follow merve merve Follow Aritra Roy Gosthipaty ariG23498 Follow Francesco Capuano fracapuano Follow Loubna Ben Allal loubnabnl Follow Pedro Cuenca pcuenq Follow Mustafa Shukor mshukor Follow Remi Cadene cadene Follow üß≠TL;DR üìö Table of Contents Introduction Meet SmolVLA! üöÄ How to Use SmolVLA? Install Finetune the pretrained model Train from scratch Method Main Architecture Design Choices for Efficiency and Robustness Asynchronous Inference Community Datasets Improving Task Annotations Standardizing Camera Views Results Conclusion Call to Action: üß≠TL;DR Today, we introduce SmolVLA , a compact (450M), open-source Vision-Language-Action model for robotics that runs on consumer hardware. Pretrained only on compatibly licensed, open-source community-shared datasets under the...</description><pubDate>Tue, 03 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/smolvla</guid></item><item><title>No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL</title><link>https://huggingface.co/blog/vllm-colocate</link><description>Back to Articles No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL Published June 3, 2025 Update on GitHub Upvote 51 +45 Mert Toslali toslali-ibm Follow ibm-ai-platform Yu Chin Fabian Lim mirinflim Follow ibm-ai-platform Quentin Gallou√©dec qgallouedec Follow Ed Snible esnible Follow ibm-ai-platform Raghu Ganti rganti Follow ibm-ai-platform Mudhakar Srivatsa mudhakar Follow ibm-ai-platform üöÄ Introduction üß® The Problem üí° The Opportunity What It Enables üß© Design: From Separate Servers to Shared GPUs Server TRL Setup (Top Row) Co-located TRL Setup (Bottom Row) üõ†Ô∏è Implementation Notes üìä Showcase: Co-located vs. Plain TRL Performance Experiment 1: 1.5B Model ‚Äî Varying Batch Sizes Experiment 2: 1.5B Model ‚Äî Varying Tensor Parallelism (TP) Experiment 3: 7B Model ‚Äî Varying Batch Sizes Experiment 4: 7B Model ‚Äî Varying Tensor Parallelism (TP) üìä Scaling to 72B Model Sleep Mode in vLLM DeepSpeed Optimizations Accelerate Integration Experiment 5: Qwen2.5-Math-72B ‚Äî Throughput,...</description><pubDate>Tue, 03 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/vllm-colocate</guid></item><item><title>CodeAgents + Structure: A Better Way to Execute Actions</title><link>https://huggingface.co/blog/structured-codeagent</link><description>Back to Articles CodeAgents + Structure: A Better Way to Execute Actions Published May 28, 2025 Update on GitHub Upvote 54 +48 Aksel Joonas Reedi akseljoonas Follow Aymeric Roucher m-ric Follow ü§î The Evolution of Agent Actions ‚û°Ô∏è Adding Structured outputs to Code Agent üß™ Benchmark Results üí° Why Structure (Generally) Helps The Parsing Problem is Real The Structure Tax üöÄ When to Use Structured CodeAgents How to use with smolagents: Implementation Tips The Bigger Picture - What's Next? Today we're sharing research that bridges two powerful paradigms in AI agent design: the expressiveness of code-based actions and the reliability of structured generation. Our findings show that forcing CodeAgents to generate both thoughts and code in a structured JSON format can significantly outperform traditional approaches across multiple benchmarks. Figure 1: Accuracy comparison of three approaches: Structured CodeAgent (blue), CodeAgent (orange), and ToolCallingAgent (gray) on SmolBench (GAIA,...</description><pubDate>Wed, 28 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/structured-codeagent</guid></item><item><title>üêØ Liger GRPO meets TRL</title><link>https://huggingface.co/blog/liger-grpo</link><description>Back to Articles üêØ Liger GRPO meets TRL Published May 25, 2025 Update on GitHub Upvote 41 +35 Shivam Sahni shisahni Follow guest Kashif Rasul kashif Follow Salman Mohammadi smohammadi Follow axolotl-ai-co Shirin Yamani ShirinYamani Follow Yanning Chen m0m0chen Follow guest Liberty liberty4321 Follow guest Motivation How Liger Kernel slashes memory for GRPO Plug-and-Play integration with TRL Benchmarks Scaling further with FSDP and PEFT Scaling even further with vLLM Conclusion TL; DR Liger supercharges TRL ‚Äôs Group Relative Policy Optimization GRPO Trainer by slashing memory usage by 40% with zero drop in model quality. We also added support for FSDP and PEFT , making it easier than ever to scale GRPO across multiple GPUs. Motivation Fine-tuning language models using reinforcement learning (RL) is a crucial step in a model's training lifecycle for steering models towards desirable behaviours which are more complex than can be achieved through typical supervised fine-tuning. RL has...</description><pubDate>Sun, 25 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/liger-grpo</guid></item><item><title>Dell Enterprise Hub is all you need to build AI on premises</title><link>https://huggingface.co/blog/dell-ai-applications</link><description>Back to Articles Dell Enterprise Hub is all you need to build AI on premises Published May 23, 2025 Update on GitHub Upvote 19 +13 Jeff Boudier jeffboudier Follow Andrew Reed andrewrreed Follow Simon Pagezy pagezyhf Follow Alvaro Bartolome alvarobartt Follow Thibault Goehringer beurkinger Follow Florent Gbelidji florentgbelidji Follow Arjuna ark393 Follow Balachandran Rajendran balaatdell Follow DellTechnologies Models Ready for Action Introducing AI Applications Powered by NVIDIA, AMD and Intel On-Device Models for Dell AI PC Now with CLI and Python SDK Wrapping up This week at Dell Tech World, we announced the new version of Dell Enterprise Hub , with a complete suite of models and applications to easily build AI running on premises with Dell AI servers and AI PCs. Models Ready for Action If you go to the Dell Enterprise Hub today, you can find some of the most popular models, like Meta Llama 4 Maverick , DeepSeek R1 or Google Gemma 3 , available for deployment and training in a...</description><pubDate>Fri, 23 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/dell-ai-applications</guid></item><item><title>Tiny Agents in Python: a MCP-powered agent in ~70 lines of code</title><link>https://huggingface.co/blog/python-tiny-agents</link><description>Back to Articles Tiny Agents in Python: an MCP-powered agent in ~70 lines of code Published May 23, 2025 Update on GitHub Upvote 130 +124 C√©lina Hanouti celinah Follow Julien Chaumond julien-c Follow Lucain Pouget Wauplin Follow shaun smith evalstate Follow guest How to Run the Demo Agent Configuration LLMs Can Use Tools Building our Python MCP Client Using the Tools: Streaming and Processing 1. Prepare tools and calling the LLM 2. Executing tools Our Tiny Python Agent: It's (Almost) Just a Loop! 1. Initializing the Agent 2. The agent‚Äôs core: the Loop Next Steps Inspired by Tiny Agents in JS , we ported the idea to Python üêç and extended the huggingface_hub client SDK to act as a MCP Client so it can pull tools from MCP servers and pass them to the LLM during inference. MCP ( Model Context Protocol ) is an open protocol that standardizes how Large Language Models (LLMs) interact with external tools and APIs. Essentially, it removed the need to write custom integrations for each tool,...</description><pubDate>Fri, 23 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/python-tiny-agents</guid></item><item><title>Exploring Quantization Backends in Diffusers</title><link>https://huggingface.co/blog/diffusers-quantization</link><description>Back to Articles Exploring Quantization Backends in Diffusers Published May 21, 2025 Update on GitHub Upvote 33 +27 Derek Liu derekl35 Follow Marc Sun marcsun13 Follow Sayak Paul sayakpaul Follow Spot The Quantized Model Quantization Backends in Diffusers bitsandbytes (BnB) torchao Quanto GGUF FP8 Layerwise Casting ( enable_layerwise_casting ) Combining with More Memory Optimizations and torch.compile Ready to use quantized checkpoints Conclusion Large diffusion models like Flux (a flow-based text-to-image generation model) can create stunning images, but their size can be a hurdle, demanding significant memory and compute resources. Quantization offers a powerful solution, shrinking these models to make them more accessible without drastically compromising performance. But the big question always is: can you actually tell the difference in the final image? Before we dive into the technical details of how various quantization backends in Hugging Face Diffusers work, why not test...</description><pubDate>Wed, 21 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/diffusers-quantization</guid></item><item><title>nanoVLM: The simplest repository to train your VLM in pure PyTorch</title><link>https://huggingface.co/blog/nanovlm</link><description>Back to Articles nanoVLM: The simplest repository to train your VLM in pure PyTorch Published May 21, 2025 Update on GitHub Upvote 153 +147 Aritra Roy Gosthipaty ariG23498 Follow Luis Wiedmann lusxvr Follow Andres Marafioti andito Follow Sergio Paniego sergiopaniego Follow merve merve Follow Pedro Cuenca pcuenq Follow Vaibhav Srivastav reach-vb Follow Table of contents: TL;DR What is a Vision Language Model? Working with the repository Architecture Train your own VLM Run inference on a pre-trained model Conclusion References nanoVLM is the simplest way to get started with training your very own Vision Language Model (VLM) using pure PyTorch. It is lightweight toolkit which allows you to launch a VLM training on a free tier colab notebook . We were inspired by Andrej Karpathy ‚Äôs nanoGPT , and provide a similar project for the vision domain. At its heart, nanoVLM is a toolkit that helps you build and train a model that can understand both images and text, and then generate text based...</description><pubDate>Wed, 21 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/nanovlm</guid></item><item><title>Microsoft and Hugging Face expand collaboration</title><link>https://huggingface.co/blog/azure-ai-foundry</link><description>Back to Articles Microsoft and Hugging Face expand collaboration to make open models easy to use on Azure Published May 19, 2025 Update on GitHub Upvote 22 +16 Jeff Boudier jeffboudier Follow Simon Pagezy pagezyhf Follow Alvaro Bartolome alvarobartt Follow It‚Äôs time to build - an expanded collaboration How to use Hugging Face in Azure AI Foundry More Hugging Face to come in Azure AI Foundry Today at the Microsoft Build conference, Satya Nadella announced an expanded collaboration with Hugging Face, to make its wide diversity of open models easy to deploy on Azure secure infrastructure. If you head over to Azure AI Foundry today, you will find a vastly expanded collection of 10,000+ Hugging Face models you can deploy in a couple clicks to power AI applications working with text, audio and images. And we‚Äôre just getting started! It‚Äôs time to build - an expanded collaboration 2 years ago, Microsoft and Hugging Face started a collaboration to make open models more easily accessible on...</description><pubDate>Mon, 19 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/azure-ai-foundry</guid></item></channel></rss>