<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Hugging Face and Cloudflare Partner to Make Real-Time Speech and Video Seamless with FastRTC</title><link>https://huggingface.co/blog/fastrtc-cloudflare</link><description>Back to Articles Hugging Face and Cloudflare Partner to Make Real-Time Speech and Video Seamless with FastRTC Published April 9, 2025 Update on GitHub Upvote 14 +8 freddyaboulton Freddy Boulton Meeting a Gap in the Toolbox of AI Developers Free Access with Your Hugging Face Account Why This Matters for AI Developers Getting Started What's Next? We're excited to announce a new partnership between Cloudflare and Hugging Face that gives FastRTC developers instant access to enterprise-grade WebRTC infrastructure with a Hugging Face token. As a preview of what you can build with FastRTC and Cloudflare, check out this voice chat app built with Meta's new Llama 4 model! Meeting a Gap in the Toolbox of AI Developers As conversational AI becomes a core interface for tools, products, and services, real-time communication infrastructure is increasingly essential to support natural, multimodal interactions. Hugging Face built FastRTC to let AI developers build low-latency AI-powered audio and...</description><pubDate>Wed, 09 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/fastrtc-cloudflare</guid></item><item><title>Arabic Leaderboards: Introducing Arabic Instruction Following, Updating AraGen, and More</title><link>https://huggingface.co/blog/leaderboard-3c3h-aragen-ifeval</link><description>Back to Articles Arabic Leaderboards: Introducing Arabic Instruction Following, Updating AraGen, and More Published April 8, 2025 Update on GitHub Upvote 11 +5 alielfilali01 Ali El Filali inceptionai SarahAlBarri Sarah AlBarri inceptionai Arwa88 Abouelseoud inceptionai samta-kamboj samta kamboj inceptionai neha1710 Neha Sengupta inceptionai preslavnakov Preslav Nakov MBZUAI Arabic-Leaderboards Space Latest Updates in AraGen Leaderboard AraGen-03-25 Release Dynamic Evaluation and Ranking Analysis Instruction Following Leaderboard What is Instruction Following as a Benchmark? Dataset: Arabic IFEval Evaluation Methodology &amp; Metrics Results &amp; Analysis Upcoming Work At Inception, we have been working to enhance AI model evaluations within the Arabic language context. Previously, we introduced AraGen , one of the first generative Arabic leaderboards, serving as a benchmark for evaluating Arabic LLMs on generative tasks. As part of our ongoing efforts, we are excited to share the following...</description><pubDate>Tue, 08 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/leaderboard-3c3h-aragen-ifeval</guid></item><item><title>Welcome Llama 4 Maverick &amp; Scout on Hugging Face!</title><link>https://huggingface.co/blog/llama4-release</link><description>Back to Articles Welcome Llama 4 Maverick &amp; Scout on Hugging Face Published April 5, 2025 Update on GitHub Upvote 140 +134 burtenshaw ben burtenshaw reach-vb Vaibhav Srivastav pcuenq Pedro Cuenca clem Clem 🤗 rajatarya Rajat Arya xet-team jsulz Jared Sulzdorf xet-team lysandre Lysandre What is Llama 4? Features and Integrations on Hugging Face Context Length and Architecture Choices How to Use with Transformers Evaluation Scores Pre-trained models Instruction tuned models Acknowledgments References We are incredibly excited to welcome the next generation of large language models from Meta to the Hugging Face Hub: Llama 4 Maverick (~400B) and Llama 4 Scout (~109B)! 🤗 Both are Mixture of Experts (MoE) models with 17B active parameters. Released today, these powerful, natively multimodal models represent a significant leap forward. We've worked closely with Meta to ensure seamless integration into the Hugging Face ecosystem, including both transformers and TGI from day one. This is just...</description><pubDate>Sat, 05 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/llama4-release</guid></item><item><title>Journey to 1 Million Gradio Users!</title><link>https://huggingface.co/blog/gradio-1m</link><description>Back to Articles Journey to 1 Million Gradio Users! Published April 4, 2025 Update on GitHub Upvote 13 +7 abidlabs Abubakar Abid 5 years ago, we launched Gradio as a simple Python library to let researchers at Stanford easily demo computer vision models with a web interface. Today, Gradio is used by &gt;1 million developers each month to build and share AI web apps. This includes some of the most popular open-source projects of all time, like Automatic1111 , Oobabooga’s Text Generation WebUI , Dall-E Mini , and LLaMA-Factory . How did we get here? How did Gradio keep growing in the very crowded field of open-source Python libraries? I get this question a lot from folks who are building their own open-source libraries. This post distills some of the lessons that I have learned over the past few years: Invest in good primitives, not high-level abstractions Embed virality directly into your library Focus on a (growing) niche Your only roadmap should be rapid iteration Maximize ways users...</description><pubDate>Fri, 04 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gradio-1m</guid></item><item><title>The NLP Course is becoming the LLM Course!</title><link>https://huggingface.co/blog/llm-course</link><description>Back to Articles The NLP Course is becoming the LLM Course! Published April 3, 2025 Update on GitHub Upvote 66 +60 burtenshaw ben burtenshaw reach-vb Vaibhav Srivastav lewtun Lewis Tunstall fdaudens Florent Daudens pcuenq Pedro Cuenca tomaarsen Tom Aarsen coyotte508 Eliott Coyac mishig Mishig Davaadorj sergiopaniego Sergio Paniego julien-c Julien Chaumond What’s going to happen to the NLP course material? Will there be new chapters? Will there be interactive exercises and live sessions? What’s next? Education has always been at the heart of Hugging Face’s mission to democratize AI and we’re doubling down on that by giving hf.co/learn a big upgrade! Our NLP course has been a go-to resource for the open-source AI community for the past 3 years, and it’s now time for a refresh. We’re updating and expanding it to keep up with all the exciting stuff happening in AI (which is not easy when there are breakthroughs every week!) We felt the excitement during the experimental smol-course and...</description><pubDate>Thu, 03 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/llm-course</guid></item><item><title>How Hugging Face Scaled Secrets Management for AI Infrastructure</title><link>https://huggingface.co/blog/scaling-secrets-management</link><description>Back to Articles How Hugging Face Scaled Secrets Management for AI Infrastructure Published March 31, 2025 Update on GitHub Upvote 5 segudev Thomas Segura Infisical Background Implementation Kubernetes Integration Local Development Security and Access Management CI/CD and Infrastructure Integration Technical Outcomes &amp; Insights Conclusion Resources Hugging Face has become synonymous with advancing AI at scale. With over 4 million builders deploying models on the Hub, the rapid growth of the platform necessitated a rethinking of how sensitive configuration data —secrets— are managed. Last year, the engineering teams set out to improve the handling of their secrets and credentials. After evaluating tools like HashiCorp Vault, they ultimately chose Infisical . This case study details their migration to Infisical, explains how they integrated its powerful features, and highlights how it enabled engineers to work more efficiently and securely. Background As Hugging Face's infrastructure...</description><pubDate>Mon, 31 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/scaling-secrets-management</guid></item><item><title>Accelerating LLM Inference with TGI on Intel Gaudi</title><link>https://huggingface.co/blog/intel-gaudi-backend-for-tgi</link><description>Back to Articles 🚀 Accelerating LLM Inference with TGI on Intel Gaudi Published March 28, 2025 Update on GitHub Upvote 13 +7 baptistecolle Baptiste Colle regisss Régis Pierrard IlyasMoutawwakil Ilyas Moutawwakil echarlaix Ella Charlaix kding1 Ke Ding Intel ✨ What's New? 🌟 Why This Matters 🚦 Getting Started with TGI on Gaudi 🎉 Top features 💪 Getting Involved We're excited to announce the native integration of Intel Gaudi hardware support directly into Text Generation Inference (TGI), our production-ready serving solution for Large Language Models (LLMs). This integration brings the power of Intel's specialized AI accelerators to our high-performance inference stack, enabling more deployment options for the open-source AI community 🎉 ✨ What's New? We've fully integrated Gaudi support into TGI's main codebase in PR #3091 . Previously, we maintained a separate fork for Gaudi devices at tgi-gaudi . This was cumbersome for users and prevented us from supporting the latest TGI features at...</description><pubDate>Fri, 28 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/intel-gaudi-backend-for-tgi</guid></item><item><title>Training and Finetuning Reranker Models with Sentence Transformers v4</title><link>https://huggingface.co/blog/train-reranker</link><description>Back to Articles Training and Finetuning Reranker Models with Sentence Transformers v4 Published March 26, 2025 Update on GitHub Upvote 107 +101 tomaarsen Tom Aarsen Table of Contents What are Reranker models? Why Finetune? Training Components Dataset Data on the Hugging Face Hub Local Data (CSV, JSON, Parquet, Arrow, SQL) Local Data that requires pre-processing Dataset Format Hard Negatives Mining Loss Function Training Arguments Evaluator CrossEncoderCorrelationEvaluator with STSb CrossEncoderRerankingEvaluator with GooAQ mined negatives Trainer Callbacks Multi-Dataset Training Training Tips Evaluation Additional Resources Training Examples Documentation Sentence Transformers is a Python library for using and training embedding and reranker models for a wide range of applications, such as retrieval augmented generation, semantic search, semantic textual similarity, paraphrase mining, and more. Its v4.0 update introduces a new training approach for rerankers, also known as cross-...</description><pubDate>Wed, 26 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/train-reranker</guid></item><item><title>Introducing Gradio's new Dataframe!</title><link>https://huggingface.co/blog/gradio-dataframe-upgrade</link><description>Back to Articles Introducing Gradio's new Dataframe! Published March 24, 2025 Update on GitHub Upvote 22 +16 hmb hannah abidlabs Abubakar Abid What’s next? Try it yourself! Gradio’s gr.Dataframe component is one of our most popular components, we've seen it used in a variety of awesome apps, like leaderboards, dashboards, and interactive visualisations. Although we hadn't made any changes to the dataframe in quite some time, our backlog of issues had been growing, and some improvements had been in demand for a while. Well — we’re now super excited to release a host of new updates to Gradio’s dataframe component. Over the last 6 weeks, we’ve closed over 70 dataframe issues - including bugs, improvements and enhancements. 1. Multi-Cell Selection You can select multiple cells at once! Copy or delete values across your selection with ease. 2. Row Numbers &amp; Column Pinning Add row number columns and keep critical columns in view while navigating wide datasets using the pinned_columns...</description><pubDate>Mon, 24 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gradio-dataframe-upgrade</guid></item><item><title>The New and Fresh analytics in Inference Endpoints</title><link>https://huggingface.co/blog/endpoint-analytics</link><description>Back to Articles Analytics is important Published March 21, 2025 Update on GitHub Upvote 19 +13 erikkaum Erik Kaunismäki beurkinger Thibault Goehringer rtrm Remy co42 Corentin Regal michellehbn Michelle Habonneau Analytics and metrics are the cornerstone of understanding what's happening with your deployment. Are your Inference Endpoints overloaded? How many requests are they handling? Having well-visualized, relevant metrics displayed in real-time is crucial for monitoring and debugging. We realized that our analytics dashboard needed a refresh. Since we debug a lot of endpoints ourselves, we’ve felt the same pain as our users. That’s why we sat down to plan and make several improvements to provide a better experience for you. What’s New? ⏰ Real-Time Metrics: Data now updates in real-time, ensuring you get an accurate and up-to-the-second view of your endpoint’s performance. Whether you’re monitoring request latency, response times, or error rates, you can now see the events as...</description><pubDate>Fri, 21 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/endpoint-analytics</guid></item><item><title>Open R1: How to use OlympicCoder locally for coding?</title><link>https://huggingface.co/blog/olympic-coder-lmstudio</link><description>Back to Articles Open R1: How to use OlympicCoder locally for coding Published March 20, 2025 Update on GitHub Upvote 56 +50 burtenshaw ben burtenshaw reach-vb Vaibhav Srivastav lewtun Lewis Tunstall edbeeching Edward Beeching yagilb Yagil Burowski lmstudio-ai Everyone’s been using Claude and OpenAI as coding assistants for the last few years, but there’s less appeal if you look at the developments coming out of open source projects like Open R1 . If we look at the evaluation on LiveCodeBench below, we can see that the 7B parameter variant outperforms Claude 3.7 Sonnet and GPT-4o. These models are the daily driver of many engineers in applications like Cursor and VSCode. Evals are great and all, but I want to get my hands dirty and feel the commits! This blog post focuses on how you can integrate these models in your IDE now. We will set up OlympicCoder 7B, the smaller of the two OlympicCoder variants, and we’ll use a quantized variant for optimum local inference. Here’s the stack...</description><pubDate>Thu, 20 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/olympic-coder-lmstudio</guid></item><item><title>AI Policy: 🤗 Response to the White House AI Action Plan RFI</title><link>https://huggingface.co/blog/ai-action-wh-2025</link><description>Back to Articles AI Policy @🤗: Response to the White House AI Action Plan RFI Published March 19, 2025 Update on GitHub Upvote 24 +18 yjernite Yacine Jernite evijit Avijit Ghosh irenesolaiman Irene Solaiman Context: Don't Sleep on (Strongly) Open Models' Capabilities Recommendation 1: Recognize Open Source and Open Science as Fundamental to AI Success Recommendation 2: Prioritize Efficiency and Reliability to Unlock Broad Innovation Recommendation 3: Secure AI through Open, Traceable, and Transparent Systems On March 14, we submitted Hugging Face's response to the White House Office of Science and Technology Policy's request for information on the White House AI Action Plan . We took this opportunity to (re-)assert the fundamental role that open AI systems and open science play in enabling the technology to be more performant and efficient, broadly and reliably adopted, and meeting the highest standards of security. This blog post provides a summary of our response, the full text is...</description><pubDate>Wed, 19 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/ai-action-wh-2025</guid></item><item><title>NVIDIA's GTC 2025 Announcement for Physical AI Developers: New Open Models and Datasets</title><link>https://huggingface.co/blog/nvidia-physical-ai</link><description>Back to Articles NVIDIA's GTC 2025 Announcement for Physical AI Developers: New Open Models and Datasets Published March 18, 2025 Update on GitHub Upvote 33 +27 mingyuliutw Ming-Yu Liu nvidia hannamao Hanzi Mao nvidia jwgu Jinwei Gu nvidia PranjaliJoshi Pranjali Joshi nvidia asawareeb Asawaree guest NVIDIA Isaac GR00T N1 used in object manipulation. New World Foundation Model - Cosmos Transfer How it works Open Physical AI Dataset Purpose Built Model for Humanoids - NVIDIA Isaac GR00T N1 Path Forward At its annual GTC conference, NVIDIA has unveiled a trio of groundbreaking open-source releases aimed at accelerating physical AI development. Release of a new suite of world foundation models (WFMs) with multicontrols called Cosmos Transfer , a highly curated Physical AI Dataset , and the first open model for general humanoid reasoning called NVIDIA Isaac GR00T N1 - represent a significant leap forward in physical AI technology, offering developers powerful tools and resources to...</description><pubDate>Tue, 18 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/nvidia-physical-ai</guid></item><item><title>Xet is on the Hub</title><link>https://huggingface.co/blog/xet-on-the-hub</link><description>Back to Articles Xet is on the Hub Published March 18, 2025 Update on GitHub Upvote 46 +40 assafvayner Assaf Vayner xet-team brianronan Brian Ronan xet-team seanses Di Xiao xet-team jgodlewski Joseph Godlewski xet-team sirahd Sam Horradarn xet-team jsulz Jared Sulzdorf xet-team Want to skip the details and get straight to faster uploads and downloads with bigger files than ever before? The Xet Difference Migration Day Post-Migration Challenges Download Overhead from Block Format Pod Load Imbalance Migration Takeaways Ready. Xet. Go. Click here to read about joining the Xet waitlist (or head over to join immediately ). Over the past few weeks, Hugging Face’s Xet Team took a major step forward by migrating the first Model and Dataset repositories off LFS and to Xet storage . This marks one of many steps to fulfill Hugging Face’s vision for the Hub by empowering AI builders to build, iterate, and collaborate more effectively on massive models and datasets. If you're interested in...</description><pubDate>Tue, 18 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/xet-on-the-hub</guid></item><item><title>Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM</title><link>https://huggingface.co/blog/gemma3</link><description>Back to Articles Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM Published March 12, 2025 Update on GitHub Upvote 382 +376 ariG23498 Aritra Roy Gosthipaty merve Merve Noyan pcuenq Pedro Cuenca reach-vb Vaibhav Srivastav TL;DR What is Gemma 3? Technical Enhancements in Gemma 3 Longer Context Length Multimodality Multilinguality Gemma 3 evaluation Inference with 🤗 transformers On Device &amp; Low Resource Devices MLX Llama.cpp Deploy on Hugging Face Endpoints Acknowledgements TL;DR Today Google releases Gemma 3 , a new iteration of their Gemma family of models. The models range from 1B to 27B parameters, have a context window up to 128k tokens, can accept images and text, and support 140+ languages. Try out Gemma 3 now 👉🏻 Gemma 3 Space Gemma 2 Gemma 3 Size Variants 2B 9B 27B 1B 4B 12B 27B Context Window Length 8k 32k (1B) 128k (4B, 12B, 27B) Multimodality (Images and Text) ❌ ❌ (1B) ✅ (4B, 12B, 27B) Multilingual Support – English (1B) +140 languages (4B,...</description><pubDate>Wed, 12 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gemma3</guid></item></channel></rss>