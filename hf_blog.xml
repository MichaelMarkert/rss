<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Introducing Trackio: A Lightweight Experiment Tracking Library from Hugging Face</title><link>https://huggingface.co/blog/trackio</link><description>Back to Articles Introducing Trackio: A Lightweight Experiment Tracking Library from Hugging Face Published July 29, 2025 Update on GitHub Upvote 94 +88 Abubakar Abid abidlabs Follow Zach Nation znation Follow Nouamane Tazi nouamanetazi Follow Sasha Luccioni sasha Follow Quentin Gallou√©dec qgallouedec Follow Background Why We Switched to Trackio Using Trackio Installing Usage Visualizing Results Sharing with ü§ó Spaces Integrated with ü§ó Transformers and ü§ó Accelerate Design Principles Next Steps TL;DR: Trackio is a new, open-source, and free experiment tracking Python library that provides a local dashboard and seamless integration with Hugging Face Spaces for easy sharing and collaboration. Since trackio is a drop-in replacement for wandb , you can get started with the syntax you already know! Background If you have trained your own machine learning model, you know how important it is to be able to track metrics, parameters, and hyperparameters during training and visualize them...</description><pubDate>Tue, 29 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/trackio</guid></item><item><title>Say hello to `hf`: a faster, friendlier Hugging Face CLI ‚ú®</title><link>https://huggingface.co/blog/hf-cli</link><description>Back to Articles Say hello to `hf`: a faster, friendlier Hugging Face CLI ‚ú® Published July 25, 2025 Update on GitHub Upvote 57 +51 Lucain Pouget Wauplin Follow C√©lina Hanouti celinah Follow Julien Chaumond julien-c Follow Getting started üîÄ Migration One more thing... üí• hf jobs We are glad to announce a long-awaited quality-of-life improvement: the Hugging Face CLI has been officially renamed from huggingface-cli to hf ! So... why this change? Typing huggingface-cli constantly gets old fast. More importantly, the CLI‚Äôs command structure became messy as new features were added over time (upload, download, cache management, repo management, etc.). Renaming the CLI is a chance to reorganize commands into a clearer, more consistent format. We decided not to reinvent the wheel and instead follow a well-known CLI pattern: hf &lt;resource&gt; &lt;action&gt; . This predictable grammar makes the Hugging Face CLI more ergonomic and discoverable, while also setting the stage for upcoming features. Getting...</description><pubDate>Fri, 25 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/hf-cli</guid></item><item><title>Parquet Content-Defined Chunking</title><link>https://huggingface.co/blog/parquet-cdc</link><description>Back to Articles Parquet Content-Defined Chunking Published July 25, 2025 Update on GitHub Upvote 46 +40 Krisztian Szucs kszucs Follow Table of Contents Introduction Data Preparation Upload the table as a Parquet file to Hugging Face Hub Different Use Cases for Parquet Deduplication 1. Re-uploading Exact Copies of the Table 2. Adding and Removing Columns from the Table 3. Changing Column Types in the Table 4. Appending New Rows and Concatenating Tables 5. Inserting / Deleting Rows in the Table 6. Using Different Row-group Sizes 7. Using Varying File-Level Splits Using Parquet CDC feature with Pandas References Conclusion Reduce Parquet file upload and download times on Hugging Face Hub by leveraging the new Xet storage layer and Apache Arrow‚Äôs Parquet Content-Defined Chunking (CDC) feature enabling more efficient and scalable data workflows. TL;DR: Parquet Content-Defined Chunking (CDC) is now available in PyArrow and Pandas, enabling efficient deduplication of Parquet files on...</description><pubDate>Fri, 25 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/parquet-cdc</guid></item><item><title>TimeScope: How Long Can Your Video Large Multimodal Model Go?</title><link>https://huggingface.co/blog/timescope-video-lmm-benchmark</link><description>Back to Articles TimeScope: How Long Can Your Video Large Multimodal Model Go? Published July 23, 2025 Update on GitHub Upvote 30 +24 Orr Zohar orrzohar Follow Stanford Rui Li ruili0 Follow guest Andres Marafioti andito Follow huggingface Xiaohan Wang nicholswang Follow Stanford TL;DR Table of Contents Why TimeScope? Motivating a Better Benchmark for Video Benchmark Design 1. Localized Retrieval 2. Information Synthesis 3. Fine-Grained Temporal Perception Evaluations &amp; Leaderboard What did we learn? Conclusion ‚Äì Let‚Äôs Raise the Bar for Long-Video AI TL;DR TimeScope is an open-source benchmark designed to measure how well vision-language models understand long videos. By adding short ‚Äúneedle‚Äù clips into videos ranging from 1 minute to 8 hours, it evaluates three skills: localized retrieval, information synthesis, fine-grained temporal perception. Timescope reveals that many state-of-the-art models still struggle with true temporal comprehension. Table of Contents Why TimeScope?...</description><pubDate>Wed, 23 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/timescope-video-lmm-benchmark</guid></item><item><title>Fast LoRA inference for Flux with Diffusers and PEFT</title><link>https://huggingface.co/blog/lora-fast</link><description>Back to Articles Fast LoRA inference for Flux with Diffusers and PEFT Published July 23, 2025 Update on GitHub Upvote 35 +29 Sayak Paul sayakpaul Follow Benjamin Bossan BenjaminB Follow Technical details of hotswapping Resources LoRA adapters provide a great deal of customization for models of all shapes and sizes. When it comes to image generation, they can empower the models with different styles, different characters, and much more . Sometimes, they can also be leveraged to reduce inference latency . Hence, their importance is paramount, particularly when it comes to customizing and fine-tuning models. In this post, we take the Flux.1-Dev model for text-to-image generation because of its widespread popularity and adoption, and how to optimize its inference speed when using LoRAs (~2.3x). It has over 30k adapters trained with it ( as reported on the Hugging Face Hub platform). Therefore, its importance to the community is significant. Note that even though we demonstrate speedups...</description><pubDate>Wed, 23 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/lora-fast</guid></item><item><title>Arc Virtual Cell Challenge: A Primer</title><link>https://huggingface.co/blog/virtual-cell-challenge</link><description>Back to Articles Arc Virtual Cell Challenge: A Primer Published July 18, 2025 Update on GitHub Upvote 41 +35 Christopher Fleetwood FL33TW00D-HF Follow Abhinav Adduri abhinadduri Follow arcinstitute Training data Modelling the challenge State Transition Model (ST) State Embedding Model (SE) A little biological detour Back to the model Perturbation Discrimination Differential Expression Conclusion Arc Institute recently unveiled the Virtual Cell Challenge . Participants are required to train a model capable of predicting the effect of silencing a gene in a (partially) unseen cell type, a task they term context generalization . For ML engineers with little to no biology background, the jargon and required context can seem quite daunting. To encourage participation, we recapitulate the challenge in a form better suited to engineers from other disciplines. Goal Train a model to predict the effect on a cell of silencing a gene using CRISPR. Doing things in the world of atoms is expensive,...</description><pubDate>Fri, 18 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/virtual-cell-challenge</guid></item><item><title>Consilium: When Multiple LLMs Collaborate</title><link>https://huggingface.co/blog/consilium-multi-llm</link><description>Back to Articles Consilium: When Multiple LLMs Collaborate Published July 17, 2025 Update on GitHub Upvote 18 +12 Andreas azettl Follow guest From Concept to Architecture Building the Visual Foundation Session State Management Making LLMs Actually Discuss LLM Selection and Research Integration Discovering the Open Floor Protocol Lessons Learned and Future Implications Picture this: four AI experts sitting around a poker table, debating your toughest decisions in real-time. That's exactly what Consilium, the multi-LLM platform I built during the Gradio Agents &amp; MCP Hackathon , does. It lets AI models discuss complex questions and reach consensus through structured debate. The platform works both as a visual Gradio interface and as an MCP (Model Context Protocol) server that integrates directly with applications like Cline (Claude Desktop had issues as the timeout could not be adjusted). The core idea was always about LLMs reaching consensus through discussion; that's where the name...</description><pubDate>Thu, 17 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/consilium-multi-llm</guid></item><item><title>Back to The Future: Evaluating AI Agents on Predicting Future Events</title><link>https://huggingface.co/blog/futurebench</link><description>Back to Articles Back to The Future: Evaluating AI Agents on Predicting Future Events Published July 17, 2025 Update on GitHub Upvote 28 +22 Federico Bianchi vinid Follow guest Junlin Wang junlinw Follow togethercomputer Zain Hasan zainhasan Follow togethercomputer Shang Zhu shangzhu Follow guest Roy coolcat21 Follow togethercomputer Cl√©mentine Fourrier clefourrier Follow James ZOu jameszou Follow guest Future of AI Can Agents Predict Future Events? 1. News-Generated Questions: Finding Tomorrow's Headlines Today 2. Polymarket Integration: Leveraging Prediction Markets Example Questions Future Bench: Three Levels of Systematic Evaluation Predicting The Future: Agents and Initial Results Initial Results Interesting Action Patterns References Future of AI Most current AI benchmarks focus on answering questions about the past, either by testing models on existing knowledge (in a static manner, such as HLE or GPQA, or augmented, like BrowseComp or GAIA) or previously solved problems...</description><pubDate>Thu, 17 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/futurebench</guid></item><item><title>Five Big Improvements to Gradio MCP Servers</title><link>https://huggingface.co/blog/gradio-mcp-updates</link><description>Back to Articles Five Big Improvements to Gradio MCP Servers Published July 17, 2025 Update on GitHub Upvote 18 +12 Freddy Boulton freddyaboulton Follow Seamless Local File Support Real-time Progress Notifications Transform OpenAPI Specs to MCP in One Line Improvements to Authentication Modifying Tool Descriptions Conclusion Gradio is an open-source Python package for creating AI-powered web applications. Gradio is compliant with the MCP server protocol and powers thousands of MCP servers hosted on Hugging Face Spaces . The Gradio team is betting big on Gradio and Spaces being the best way to build and host AI-powered MCP servers. To that end, here are some of the big improvements we've added to Gradio MCP servers as of version 5.38.0 . Seamless Local File Support If you've tried to use a remote Gradio MCP server that takes a file as input (image, video, audio), you've probably encountered this error: This happens because the Gradio server is hosted on a different machine, meaning...</description><pubDate>Thu, 17 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gradio-mcp-updates</guid></item><item><title>Seq vs Seq: the Ettin Suite of Paired Encoders and Decoders</title><link>https://huggingface.co/blog/ettin</link><description>Back to Articles Ettin Suite: SoTA Paired Encoders and Decoders Published July 16, 2025 Update on GitHub Upvote 50 +44 Orion Weller orionweller Follow jhu-clsp K Ricci kdricci Follow jhu-clsp Marc Marone mmarone Follow jhu-clsp Antoine Chaffin NohTow Follow lightonai Dawn Lawrie dlawrie Follow jhu-clsp Ben Van Durme vandurme Follow jhu-clsp TL;DR Encoders vs Decoders: The Architecture Divide Training Recipe: Modern Techniques for Both Architectures Sizes Three-Phase Training Process Modern Architecture Components Data Sources and Quality Encoder Results: Beating ModernBERT Decoder Results: Beating Llama 3.2 and SmolLM2 Fair Fight: Encoders vs Decoders on Even Ground Architecture-Specific Advantages Persist Cross-Objective Training Falls Short Beyond Performance: Understanding Model Behavior Usage Examples Encoders Decoders Fine-tuning Examples Encoders Decoders Model Family and Links TL;DR What would happen if you took the ModernBERT recipe and applied it to a decoder-only model?...</description><pubDate>Wed, 16 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/ettin</guid></item><item><title>Migrating the Hub from Git LFS to Xet</title><link>https://huggingface.co/blog/migrating-the-hub-to-xet</link><description>Back to Articles Migrating the Hub from Git LFS to Xet Published July 15, 2025 Update on GitHub Upvote 23 +17 Jared Sulzdorf jsulz Follow xet-team Joseph Godlewski jgodlewski Follow xet-team Sam Horradarn sirahd Follow xet-team Bridges and backward compatibility Scaling migrations Zero friction, faster transfers Xet for everyone In January of this year, Hugging Face's Xet Team deployed a new storage backend, and shortly after shifted ~6% of Hub downloads through the infrastructure . This represented a significant milestone, but it was just the beginning. In 6 months, 500,000 repositories holding 20 PB joined the move to Xet as the Hub outgrows Git LFS and transitions to a storage system that scales with the workloads of AI builders. Today, more than 1 million people on the Hub are using Xet. In May, it became the default on the Hub for new users and organizations . With only a few dozen GitHub issues, forum threads, and Discord messages, this is perhaps the quietest migration of...</description><pubDate>Tue, 15 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/migrating-the-hub-to-xet</guid></item><item><title>Asynchronous Robot Inference: Decoupling Action Prediction and Execution</title><link>https://huggingface.co/blog/async-robot-inference</link><description>Back to Articles Asynchronous Robot Inference: Decoupling Action Prediction and Execution Published July 10, 2025 Update on GitHub Upvote 37 +31 Francesco Capuano fracapuano Follow Steven Palma imstevenpmwork Follow Michel Aractingi aractingi Follow Mustafa Shukor mshukor Follow Dana Aubakirova danaaubakirova Follow Adil Zouitine AdilZtn Follow Simon Alibert aliberts Follow Table of Contents Getting started Async inference: a deep dive 1. Why sequential inference falls short 2. Asynchronous inference, in a nutshell 3. System Architecture Robot Client Policy Server 4. Analyzing async inference 5. Using async in your setup Conclusions TL;DR Robotic policies are increasingly bulky, and predict chunks of future actions rather than a single next action. This results in the robot being idle while awaiting new actions to perform, introducing noticeable lags at execution, and lacking responsiveness. Asynchronous inference tightens the control loop, removing lags at runtime and resulting in...</description><pubDate>Thu, 10 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/async-robot-inference</guid></item><item><title>ScreenEnv: Deploy your full stack Desktop Agent</title><link>https://huggingface.co/blog/screenenv</link><description>Back to Articles ScreenEnv: Deploy your full stack Desktop Agent Published July 10, 2025 Update on GitHub Upvote 57 +51 Amir Mahla A-Mahla Follow Aymeric Roucher m-ric Follow What is ScreenEnv? Why ScreenEnv? üéØ One-Line Setup Two Integration Approaches Option 1: Direct Sandbox API Option 2: MCP Server Integration ‚ú® Create a Desktop Agent with screenenv and smolagents 1. Choose Your Model 2. Define Your Custom Desktop Agent 3. Run the Agent on a Desktop Task Get Started Today What's Next? TL;DR : ScreenEnv is a powerful Python library that lets you create isolated Ubuntu desktop environments in Docker containers for testing and deploying GUI Agents (aka Computer Use agents). With built-in support for the Model Context Protocol (MCP), it's never been easier to deploy desktop agents that can see, click, and interact with real applications. What is ScreenEnv? Imagine you need to automate desktop tasks, test GUI applications, or build an AI agent that can interact with software. This...</description><pubDate>Thu, 10 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/screenenv</guid></item><item><title>Building the Hugging Face MCP Server</title><link>https://huggingface.co/blog/building-hf-mcp</link><description>Back to Articles Building the Hugging Face MCP Server Published July 10, 2025 Update on GitHub Upvote 53 +47 shaun smith evalstate Follow Julien Chaumond julien-c Follow Eliott Coyac coyotte508 Follow Abubakar Abid abidlabs Follow Introduction Design Choices Remote Servers Production Deployment Conclusion TL;DR: The Hugging Face Official MCP Server offers unique customization options for AI Assistants accessing the Hub, along with access to thousands of AI applications through one simple URL. We used MCPs "Streamable HTTP" transport for deployment, and examine in detail the trade-offs that Server Developers have. We've learned many things about building a useful MCP server in the last month - we'll describe our journey here. Introduction The Model Context Protocol (MCP) is fulfilling its promise of being the standard to connect AI Assistants to the outside world. At Hugging Face, providing access to the Hub via MCP is an obvious choice, and this article shares our experience...</description><pubDate>Thu, 10 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/building-hf-mcp</guid></item><item><title>Reachy Mini - The Open-Source Robot for Today's and Tomorrow's AI Builders</title><link>https://huggingface.co/blog/reachy-mini</link><description>Back to Articles Reachy Mini ‚Äì The Open-Source Robot for Today's and Tomorrow's AI Builders Published July 9, 2025 Update on GitHub Upvote 617 +611 Thomas Wolf thomwolf Follow Matthieu Lapeyre matthieu-lapeyre Follow pollen-robotics üî© Robot technical info üß† Built for Exploring, Playing, Learning &amp; Sharing ü§ù Designed for Human-Robot Interaction üåç Open, Modular &amp; Community-Powered Tiny price, small size, huge possibilities. Code, learn, share with AI builders of all ages, all around the globe. Order the lite version Order the wireless version $299 (+ taxes + shipping) $449 (+ taxes + shipping) Reachy Mini is an expressive, open-source robot designed for human-robot interaction, creative coding, and AI experimentation. Fully programmable in Python (and soon JavaScript, Scratch) and priced from $299 , it's your gateway into robotics AI: fun, customizable, and ready to be part of your next coding project. Whether you're an AI developer, hacker, researcher, teacher, robot enthusiast, or...</description><pubDate>Wed, 09 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/reachy-mini</guid></item></channel></rss>