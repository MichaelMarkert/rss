<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>AprielGuard: A Guardrail for Safety and Adversarial Robustness in Modern LLM Systems</title><link>https://huggingface.co/blog/ServiceNow-AI/aprielguard</link><description></description><pubDate>Sat, 27 Dec 2025 09:25:44 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/ServiceNow-AI/aprielguard</guid></item><item><title>Tokenization in Transformers v5: Simpler, Clearer, and More Modular</title><link>https://huggingface.co/blog/tokenizers</link><description>Back to Articles Tokenization in Transformers v5: Simpler, Clearer, and More Modular Published December 18, 2025 Update on GitHub Upvote 75 +69 Ita Zaporozhets itazap Follow Aritra Roy Gosthipaty ariG23498 Follow Arthur Zucker ArthurZ Follow Sergio Paniego sergiopaniego Follow merve merve Follow Pedro Cuenca pcuenq Follow Table of Contents What is tokenization? The tokenization pipeline Tokenization algorithms Accessing tokenizers through transformers How do you bridge the gap between raw tokenization and model requirements? The tokenizer class hierarchy in transformers PreTrainedTokenizerBase defines the common interface for all tokenizers TokenizersBackend wraps the tokenizers library PythonBackend provides a pure-Python mixin SentencePieceBackend handles SentencePiece models AutoTokenizer automatically selects the correct tokenizer class v5 Separates Tokenizer Architecture from Trained Vocab The problem with v4: tokenizers were opaque and tightly coupled The v5 solution:...</description><pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/tokenizers</guid></item><item><title>The Open Evaluation Standard: Benchmarking NVIDIA Nemotron 3 Nano with NeMo Evaluator</title><link>https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe</link><description></description><pubDate>Sat, 27 Dec 2025 09:25:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe</guid></item><item><title>CUGA on Hugging Face: Democratizing Configurable AI Agents</title><link>https://huggingface.co/blog/ibm-research/cuga-on-hugging-face</link><description></description><pubDate>Sat, 27 Dec 2025 09:25:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/ibm-research/cuga-on-hugging-face</guid></item><item><title>New in llama.cpp: Model Management</title><link>https://huggingface.co/blog/ggml-org/model-management-in-llamacpp</link><description></description><pubDate>Sat, 27 Dec 2025 09:25:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/ggml-org/model-management-in-llamacpp</guid></item><item><title>Codex is Open Sourcing AI models</title><link>https://huggingface.co/blog/hf-skills-training-codex</link><description>Back to Articles Codex is Open Sourcing AI models Published December 11, 2025 Update on GitHub Upvote 48 +42 ben burtenshaw burtenshaw Follow shaun smith evalstate Follow GOAL: End-to-end Machine Learning experiments Setup and Install Install Codex Install the Hugging Face Skills Connect to Hugging Face Your first AI Experiment Instruct Codex to do an end-to-end fine-tuning experiment Updating the Training Report Dataset Validation Review Before Submitting Track Progress using the Training Report Use Your Model Hardware and Cost What's Next Resources Codex Hugging Face Skills Building on our work to get Claude Code to train open source models, we are now getting Codex to go further. We gave Codex access to the Hugging Face Skills repository, which contains skills for Machine Learning and AI tasks such as training or evaluating models. With HF skills, a coding agent can: Fine-tune and apply RL alignment on language models Review, explain, and act on live training metrics from Trackio...</description><pubDate>Thu, 11 Dec 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/hf-skills-training-codex</guid></item><item><title>Introducing swift-huggingface: The Complete Swift Client for Hugging Face</title><link>https://huggingface.co/blog/swift-huggingface</link><description>Back to Articles Introducing swift-huggingface: The Complete Swift Client for Hugging Face Published December 5, 2025 Update on GitHub Upvote 31 +25 Mattt mattt Follow The Problem Introducing swift-huggingface Flexible Authentication with TokenProvider OAuth for User-Facing Apps Reliable Downloads Shared Cache with Python Before and After Beyond Downloads What's Next Try It Out Resources Today, we're announcing swift-huggingface , a new Swift package that provides a complete client for the Hugging Face Hub. You can start using it today as a standalone package, and it will soon integrate into swift-transformers as a replacement for its current HubApi implementation. The Problem When we released swift-transformers 1.0 earlier this year, we heard loud and clear from the community: Downloads were slow and unreliable. Large model files (often several gigabytes) would fail partway through with no way to resume. Developers resorted to manually downloading models and bundling them with...</description><pubDate>Fri, 05 Dec 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/swift-huggingface</guid></item><item><title>DeepMath: A lightweight math reasoning Agent with smolagents</title><link>https://huggingface.co/blog/intel-deepmath</link><description>Back to Articles DeepMath: A lightweight math reasoning Agent with smolagents Published December 4, 2025 Update on GitHub Upvote 30 +24 Daniel Fleischer danf Follow Intel Moshe Berchansky mber Follow Intel Moshe Wasserblat moshew Follow Intel Why DeepMath? How It Works Training with GRPO Evaluation Why It Matters Conclusion Try It Yourself Citation Limitations &amp; Future Work References By Intel AI Software Group DeepMath is an aligned math reasoning agent built on Qwen3-4B Thinking and fine-tuned with GRPO (Group Relative Policy Optimization) . Instead of verbose text, the model emits tiny Python snippets for intermediate steps, runs them in a secure sandbox, and folds the results back into its reasoning, reducing errors and output length. The agent is implemented using the smolagents library . We evaluate DeepMath on four math datasets: MATH500 , AIME , HMMT , and HLE , and show that: ðŸ¤– The math agent alone reduces output lengths by up to 66%, while often improving accuracy. âš¡ GRPO...</description><pubDate>Thu, 04 Dec 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/intel-deepmath</guid></item><item><title>We Got Claude to Fine-Tune an Open Source LLM</title><link>https://huggingface.co/blog/hf-skills-training</link><description>Back to Articles We Got Claude to Fine-Tune an Open Source LLM Published December 4, 2025 Update on GitHub Upvote 535 +529 ben burtenshaw burtenshaw Follow shaun smith evalstate Follow Setup and Install Claude Code Codex Gemini CLI Connect to Hugging Face Your First Training Run Instruct the coding agent to fine tune Review Before Submitting Track Progress Use Your Model Training Methods Supervised Fine-Tuning (SFT) Direct Preference Optimization (DPO) Group Relative Policy Optimization (GRPO) Hardware and Cost Model Size to GPU Mapping Demo vs Production Dataset Validation Monitoring Training Converting to GGUF What's Next Resources We gave Claude the ability to fine-tune language models using a new tool called Hugging Face Skills . Not just write training scripts, but to actually submit jobs to cloud GPUs, monitor progress, and push finished models to the Hugging Face Hub. This tutorial shows you how it works and how to use it yourself. Claude Code can use "skills"â€”packaged...</description><pubDate>Thu, 04 Dec 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/hf-skills-training</guid></item><item><title>Transformers v5: Simple model definitions powering the AI ecosystem</title><link>https://huggingface.co/blog/transformers-v5</link><description>Back to Articles Transformers v5: Simple model definitions powering the AI ecosystem Published December 1, 2025 Update on GitHub Upvote 254 +248 Lysandre lysandre Follow Arthur Zucker ArthurZ Follow Cyril Vallez cyrilvallez Follow Vaibhav Srivastav reach-vb Follow Simplicity Model Additions Code Reduction Training Pre-training at scale Fine-tuning &amp; Post-training Inference Production &amp; Local Quantization Conclusion Transformers' version v4.0.0rc-1, the initial release candidate for version 4, was released on November 19th, 2020. Five years later, we now release v5.0.0rc-0. Today, as we launch v5, Transformers is installed more than 3 million times each day via pip - up from 20,000/day in v4 ðŸ¤¯. Altogether, it has now surpassed 1.2 billion installs ! The ecosystem has expanded from 40 model architectures in v4 to over 400 today , and the community has contributed more than 750,000 model checkpoints on the Hub compatible with Transformers, up from roughly 1,000 at the time of v4. This...</description><pubDate>Mon, 01 Dec 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/transformers-v5</guid></item><item><title>Diffusers welcomes FLUX-2</title><link>https://huggingface.co/blog/flux-2</link><description>Back to Articles Welcome FLUX.2 - BFLâ€™s new open image generation model ðŸ¤— Published November 25, 2025 Update on GitHub Upvote 165 +159 YiYi Xu YiYiXu Follow Daniel Gu dg845 Follow Sayak Paul sayakpaul Follow Alvaro Somoza OzzyGT Follow Dhruv Nair dn6 Follow Aritra Roy Gosthipaty ariG23498 Follow Linoy Tsaban linoyts Follow ApolinÃ¡rio from multimodal AI art multimodalart Follow FLUX.2: A Brief Introduction Text encoder DiT Misc Inference With Diffusers Installation and Authentication Regular Inference Resource-constrained Multiple images as reference Advanced Prompting LoRA fine-tuning Memory optimizations for fine-tuning Resources FLUX.2 is the recent series of image generation models from Black Forest Labs, preceded by the Flux.1 series. It is an entirely new model with a new architecture and pre-training done from scratch! In this post, we discuss the key changes introduced in FLUX.2, performing inference with it under various setups, and LoRA fine-tuning. ðŸš¨ FLUX.2 is not meant to...</description><pubDate>Tue, 25 Nov 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/flux-2</guid></item><item><title>Continuous batching from first principles</title><link>https://huggingface.co/blog/continuous_batching</link><description>Back to Articles Continuous batching Published November 25, 2025 Update on GitHub Upvote 285 +279 RÃ©mi Ouazan Reboul ror Follow Arthur Zucker ArthurZ Follow Luc Georges mcpotato Follow Attention KV-cache Chunked prefill Continuous batching Conclusion TL;DR: in this blog post, starting from attention mechanisms and KV caching, we derive continuous batching by optimizing for throughput. If you've ever used Qwen, Claude, or any other AI chatbot, you've probably noticed something: it takes a while for the first word of the response to appear, and then words appear one-by-one on your screen with (hopefully) a regular and fast-paced frequency. That's because at the heart of it, all LLMs are just fancy next token predictors. An LLM first processes your entire prompt to produce one new token. Then it keeps adding tokens one by one, each time reading everything that came before, until it decides generation is over. This generation process is computationally expensive: it requires passing the...</description><pubDate>Tue, 25 Nov 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/continuous_batching</guid></item><item><title>Building Deep Research: How we Achieved State of the Art</title><link>https://huggingface.co/blog/Tavily/tavily-deep-research</link><description></description><pubDate>Sat, 27 Dec 2025 09:25:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/Tavily/tavily-deep-research</guid></item><item><title>OVHcloud on Hugging Face Inference Providers ðŸ”¥</title><link>https://huggingface.co/blog/OVHcloud/inference-providers-ovhcloud</link><description></description><pubDate>Sat, 27 Dec 2025 09:25:50 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/OVHcloud/inference-providers-ovhcloud</guid></item></channel></rss>