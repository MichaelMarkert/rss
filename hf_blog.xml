<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>SmolLM3: smol, multilingual, long-context reasoner</title><link>https://huggingface.co/blog/smollm3</link><description>Back to Articles SmolLM3: smol, multilingual, long-context reasoner Published July 8, 2025 Update on GitHub Upvote 48 +42 Elie Bakouch eliebak Follow Carlos Miguel Patiño cmpatino Follow Anton Lozhkov anton-l Follow Edward Beeching edbeeching Follow Aymeric Roucher m-ric Follow Nouamane Tazi nouamanetazi Follow Aksel Joonas Reedi akseljoonas Follow Guilherme Penedo guipenedo Follow Hynek Kydlicek hynky Follow Clémentine Fourrier clefourrier Follow Nathan Habib SaylorTwift Follow Kashif Rasul kashif Follow Quentin Gallouédec qgallouedec Follow Hugo Larcher hlarcher Follow Mathieu Morlon glutamatt Follow Joshua Xenova Follow Vaibhav Srivastav reach-vb Follow Xuan-Son Nguyen ngxson Follow Colin Raffel craffel Follow Lewis Tunstall lewtun Follow Loubna Ben Allal loubnabnl Follow Leandro von Werra lvwerra Follow Thomas Wolf thomwolf Follow Architecture and training details Data mixture and training stages Long Context extension Reasoning Mid-training Building the Chat Template Supervised...</description><pubDate>Tue, 08 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/smollm3</guid></item><item><title>Three Mighty Alerts Supporting Hugging Face’s Production Infrastructure</title><link>https://huggingface.co/blog/infrastructure-alerting</link><description>Back to Articles Three Mighty Alerts Supporting Hugging Face’s Production Infrastructure Published July 8, 2025 Update on GitHub Upvote - Jeremy Udit jcudit Follow High NAT Gateway Throughput Hub Request Logs Archival Success Rate Kubernetes API Request Errors and Rate Limiting Bonus Alert: New Cluster Sending Zero Metrics Wrapping Up The Infrastructure team at Hugging Face is excited to share a behind-the-scenes look at the inner workings of Hugging Face's production infrastructure, which we’ve had the privilege of helping to build and maintain. Our team's dedication to designing and implementing a robust monitoring and alerting system has been instrumental in ensuring the stability and scalability of our platforms. We’re constantly reminded of the impact that our alerts have on our ability to identify and respond to potential issues before they become major incidents. In this blog post, we’ll dive into the details of three mighty alerts that play their unique role in supporting...</description><pubDate>Tue, 08 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/infrastructure-alerting</guid></item><item><title>Efficient MultiModal Data Pipeline</title><link>https://huggingface.co/blog/mmdp</link><description>Back to Articles Efficient MultiModal Data Pipeline Published July 8, 2025 Update on GitHub Upvote 17 +11 Aritra Roy Gosthipaty ariG23498 Follow Luis Wiedmann lusxvr Follow Andres Marafioti andito Follow Sergio Paniego sergiopaniego Follow Pedro Cuenca pcuenq Follow [Stage 0] Preparation [Stage 1] Visualising the Dataset [Stage 2] Naive Padding [Stage 3] Constrained Padding [Stage 4]: Packing Smarter with Knapsacks Switching to an Iterable Dataset Producer-Consumer Magic Greedy Packing Bin-Packing for Tighter Fits [Stage 5] Knapsacks for Multimodal Data Conclusion You've got everything ready - data, model, a beefy GPU setup. You hit "run" and... wait. And wait some more. Your GPUs are barely breaking a sweat while your wallet's getting lighter by the hour. Sound familiar? We've been there. After some detective work on our nanoVLM project, we discovered the real culprit wasn't our model or hardware, it was our data pipeline being incredibly wasteful. Here's what we found: Idle GPUs :...</description><pubDate>Tue, 08 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/mmdp</guid></item><item><title>Training and Finetuning Sparse Embedding Models with Sentence Transformers v5</title><link>https://huggingface.co/blog/train-sparse-encoder</link><description>Back to Articles Training and Finetuning Sparse Embedding Models with Sentence Transformers v5 Published July 1, 2025 Update on GitHub Upvote 84 +78 Tom Aarsen tomaarsen Follow Arthur BRESNU arthurbresnu Follow Table of Contents What are Sparse Embedding models? Query and Document Expansion Why Use Sparse Embedding Models? Why Finetune? Training Components Model Splade Inference-free Splade Contrastive Sparse Representation (CSR) Architecture Picker Guide Dataset Data on the Hugging Face Hub Local Data (CSV, JSON, Parquet, Arrow, SQL) Local Data that requires pre-processing Dataset Format Loss Function Training Arguments Evaluator SparseNanoBEIREvaluator SparseEmbeddingSimilarityEvaluator with STSb SparseTripletEvaluator with AllNLI Trainer Callbacks Multi-Dataset Training Evaluation Training Tips Vector Database Integration Qdrant Integration Example Prerequisites: Additional Resources Training Examples Documentation Sentence Transformers is a Python library for using and training...</description><pubDate>Tue, 01 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/train-sparse-encoder</guid></item><item><title>Gemma 3n fully available in the open-source ecosystem!</title><link>https://huggingface.co/blog/gemma3n</link><description>Back to Articles Gemma 3n fully available in the open-source ecosystem! Published June 26, 2025 Update on GitHub Upvote 105 +99 Aritra Roy Gosthipaty ariG23498 Follow Pedro Cuenca pcuenq Follow Sergio Paniego sergiopaniego Follow Vaibhav Srivastav reach-vb Follow Christopher Fleetwood FL33TW00D-HF Follow Joshua Xenova Follow Steven Zheng Steveeeeeeen Follow Kashif Rasul kashif Follow Models released today Details of the models Architecture Highlights Performance &amp; Benchmarks: Demo Space Inference with transformers Inference with pipeline Detailed inference with transformers Inference with MLX Inference with llama.cpp Inference with Transformers.js and ONNXRuntime Fine Tune in a Free Google Colab Hugging Face Gemma Recipes Conclusion Gemma 3n was announced as a preview during Google I/O. The on-device community got really excited, because this is a model designed from the ground up to run locally on your hardware. On top of that, it’s natively multimodal , supporting image, text,...</description><pubDate>Thu, 26 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gemma3n</guid></item><item><title>Transformers backend integration in SGLang</title><link>https://huggingface.co/blog/transformers-backend-sglang</link><description>Back to Articles Transformers backend integration in SGLang Published June 23, 2025 Update on GitHub Upvote 42 +36 Yineng Zhang zhyncs Follow sgl-project Ke Bao ispobock Follow sgl-project Lianmin lmzheng Follow sgl-project Jin Pan JinnP Follow guest Marc Sun marcsun13 Follow Transformers SGLang Usage Example Hugging Face transformers library is the standard for working with state-of-the-art models — from experimenting with cutting-edge research to fine-tuning on custom data. Its simplicity, flexibility, and expansive model zoo make it a powerful tool for rapid development. But once you're ready to move from notebooks to production, inference performance becomes mission-critical. That’s where SGLang comes in. Designed for high-throughput, low-latency inference, SGLang now offers seamless integration with transformers as a backend. This means you can pair the flexibility of transformers with the raw performance of SGLang. Let’s dive into what this integration enables and how you can...</description><pubDate>Mon, 23 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/transformers-backend-sglang</guid></item><item><title>(LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware</title><link>https://huggingface.co/blog/flux-qlora</link><description>Back to Articles (LoRA) Fine-Tuning FLUX.1-dev on Consumer Hardware Published June 19, 2025 Update on GitHub Upvote 73 +67 Derek Liu derekl35 Follow Marc Sun marcsun13 Follow Sayak Paul sayakpaul Follow merve merve Follow Linoy Tsaban linoyts Follow Table of Contents Dataset FLUX Architecture QLoRA Fine-tuning FLUX.1-dev with Diffusers Key Optimization Techniques Pre-computing Text Embeddings (CLIP/T5) How to use it Setup &amp; Results Model Comparison: Base vs. QLoRA Fine-tuned (fp16) FP8 Fine-tuning with TorchAO Inference with Trained LoRA Adapters Option 1: Loading LoRA Adapters Option 2: Merging LoRA into Base Model Running on Google Colab Conclusion Share your creations on the Hub! In our previous post, Exploring Quantization Backends in Diffusers , we dived into how various quantization techniques can shrink diffusion models like FLUX.1-dev, making them significantly more accessible for inference without drastically compromising performance. We saw how bitsandbytes , torchao , and...</description><pubDate>Thu, 19 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/flux-qlora</guid></item><item><title>Groq on Hugging Face Inference Providers 🔥</title><link>https://huggingface.co/blog/inference-providers-groq</link><description>Back to Articles Groq on Hugging Face Inference Providers 🔥 Published June 16, 2025 Update on GitHub Upvote 39 +33 Ben Ankiel benank-groq Follow Groq Hatice Ozen hozen Follow Groq Célina Hanouti celinah Follow Lucain Pouget Wauplin Follow Simon Brandeis sbrandeis Follow How it works In the website UI From the client SDKs Billing Feedback and next steps We're thrilled to share that Groq is now a supported Inference Provider on the Hugging Face Hub! Groq joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub’s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers. Groq supports a wide variety of text and conversational models, including the latest open-source models such as Meta's Llama 4 , Qwen's QWQ-32B , and many more. At the heart of Groq's technology is the Language Processing Unit...</description><pubDate>Mon, 16 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/inference-providers-groq</guid></item><item><title>Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub</title><link>https://huggingface.co/blog/hello-hf-kernels</link><description>Back to Articles 🏎️ Enhance Your Models in 5 Minutes with the Hugging Face Kernel Hub Published June 12, 2025 Update on GitHub Upvote 108 +102 David Holtz drbh Follow Daniël de Kok danieldk Follow Nicolas Patry Narsil Follow Pedro Cuenca pcuenq Follow Simon Pagezy pagezyhf Follow merve merve Follow Vaibhav Srivastav reach-vb Follow 1. What is the Kernel Hub? Benefits of the Kernel Hub: 2. How to Use the Kernel Hub (Basic Example) What's happening here? 3. Add a Kernel to a Simple Model 4. Benchmarking the Performance Impact 5. Real World Use Cases Get Started and Next Steps! Conclusion Boost your model performance with pre-optimized kernels, easily loaded from the Hub. Today, we'll explore an exciting development from Hugging Face: the Kernel Hub ! As ML practitioners, we know that maximizing performance often involves diving deep into optimized code, custom CUDA kernels, or complex build systems. The Kernel Hub simplifies this process dramatically! Below is a short example of how...</description><pubDate>Thu, 12 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/hello-hf-kernels</guid></item><item><title>Featherless AI on Hugging Face Inference Providers 🔥</title><link>https://huggingface.co/blog/inference-providers-featherless</link><description>Back to Articles Featherless AI on Hugging Face Inference Providers 🔥 Published June 12, 2025 Update on GitHub Upvote 43 +37 Wesley George wxgeorge Follow featherless-ai Poh Nean pohnean-recursal Follow featherless-ai Eugene Cheah picocreator Follow featherless-ai Célina Hanouti celinah Follow Lucain Pouget Wauplin Follow Simon Brandeis sbrandeis Follow How it works In the website UI From the client SDKs Billing Feedback and next steps We're thrilled to share that Featherless AI is now a supported Inference Provider on the Hugging Face Hub! Featherless AI joins our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub’s model pages. Inference Providers are also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers. Featherless AI supports a wide variety of text and conversational models, including the latest open-source models from...</description><pubDate>Thu, 12 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/inference-providers-featherless</guid></item><item><title>Introducing Training Cluster as a Service - a new collaboration with NVIDIA</title><link>https://huggingface.co/blog/nvidia-training-cluster</link><description>Back to Articles Introducing Training Cluster as a Service - a new collaboration with NVIDIA Published June 11, 2025 Update on GitHub Upvote 23 +17 Jeff Boudier jeffboudier Follow Arjuna ark393 Follow Simon Pagezy pagezyhf Follow Making GPU Clusters Accessible How it works Clusters at Work Advancing Rare Genetic Disease Research with TIGEM Advancing AI for Mathematics with Numina Advancing Material Science with Mirror Physics Powering the Diversity of AI Research Enabling AI Builders with NVIDIA Today at GTC Paris, we are excited to announce Training Cluster as a Service in collaboration with NVIDIA, to make large GPU clusters more easily accessible for research organizations all over the world, so they can train the foundational models of tomorrow in every domain. Making GPU Clusters Accessible Many Gigawatt-size GPU supercluster projects are being built to train the next gen of AI models. This can make it seem that the compute gap between the “GPU poor” and the “GPU rich” is...</description><pubDate>Wed, 11 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/nvidia-training-cluster</guid></item><item><title>ScreenSuite - The most comprehensive evaluation suite for GUI Agents!</title><link>https://huggingface.co/blog/screensuite</link><description>Back to Articles ScreenSuite - The most comprehensive evaluation suite for GUI Agents! Published June 6, 2025 Update on GitHub Upvote 49 +43 Amir Mahla A-Mahla Follow Aymeric Roucher m-ric Follow Thomas Wolf thomwolf Follow Releasing ScreenSuite, the most comprehensive evaluation suite for GUI Agents! WTF is a GUI Agent? Introducing ScreenSuite 🥳 Ranking leading VLMs on ScreenSuite 📊 Start your custom evaluation in 30s ⚡️ Next steps 🚀 Releasing ScreenSuite, the most comprehensive evaluation suite for GUI Agents! TL;DR Over the past few weeks, we’ve been working tirelessly on making GUI agents more open, accessible and easy to integrate. Along the way, we created the largest benchmarking suite for GUI agents performances 👉 let us introduce ScreenSuite . We are very excited to share it with you today: ScreenSuite is the most comprehensive and easiest way to evaluate Vision Language Models (VLMs)across many agentic capabilities! WTF is a GUI Agent? GUI Agents in action - courtesy of...</description><pubDate>Fri, 06 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/screensuite</guid></item><item><title>KV Cache from scratch in nanoVLM</title><link>https://huggingface.co/blog/kv-cache</link><description>Back to Articles KV Cache from scratch in nanoVLM Published June 4, 2025 Update on GitHub Upvote 82 +76 Aritra Roy Gosthipaty ariG23498 Follow Kashif Rasul kashif Follow Luis Wiedmann lusxvr Follow Andres Marafioti andito Follow Pedro Cuenca pcuenq Follow TL;DR Introduction Revisiting the Transformer Architecture Self-Attention Computation Where Redundancy Creeps In How KV Caching Fixes It KV Caching in nanoVLM: From Theory to Practice 1. Updating KV Cache in the Attention Block 2. Tracking Cache Across Layers 3. Prefill vs Decode in the Generation Loop Summary of Changes Summary: Why KV Caching Matters TL;DR We have implemented KV Caching from scratch in our nanoVLM repository (a small codebase to train your own Vision Language Model with pure PyTorch). This gave us a 38% speedup in generation. In this blog post we cover KV Caching and all our experiences while implementing it. The lessons learnt are general and can be applied to all autoregressive language model generations....</description><pubDate>Wed, 04 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/kv-cache</guid></item><item><title>SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data</title><link>https://huggingface.co/blog/smolvla</link><description>Back to Articles SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data Published June 3, 2025 Update on GitHub Upvote 183 +177 Dana Aubakirova danaaubakirova Follow Andres Marafioti andito Follow merve merve Follow Aritra Roy Gosthipaty ariG23498 Follow Francesco Capuano fracapuano Follow Loubna Ben Allal loubnabnl Follow Pedro Cuenca pcuenq Follow Mustafa Shukor mshukor Follow Remi Cadene cadene Follow 🧭TL;DR 📚 Table of Contents Introduction Meet SmolVLA! 🚀 How to Use SmolVLA? Install Finetune the pretrained model Train from scratch Method Main Architecture Design Choices for Efficiency and Robustness Asynchronous Inference Community Datasets Improving Task Annotations Standardizing Camera Views Results Conclusion Call to Action: 🧭TL;DR Today, we introduce SmolVLA , a compact (450M), open-source Vision-Language-Action model for robotics that runs on consumer hardware. Pretrained only on compatibly licensed, open-source community-shared datasets under the...</description><pubDate>Tue, 03 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/smolvla</guid></item><item><title>No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL</title><link>https://huggingface.co/blog/vllm-colocate</link><description>Back to Articles No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL Published June 3, 2025 Update on GitHub Upvote 66 +60 Mert Toslali toslali-ibm Follow ibm-ai-platform Yu Chin Fabian Lim mirinflim Follow ibm-ai-platform Quentin Gallouédec qgallouedec Follow Ed Snible esnible Follow ibm-ai-platform Raghu Ganti rganti Follow ibm-ai-platform Mudhakar Srivatsa mudhakar Follow ibm-ai-platform 🚀 Introduction 🧨 The Problem 💡 The Opportunity What It Enables 🧩 Design: From Separate Servers to Shared GPUs Server TRL Setup (Top Row) Co-located TRL Setup (Bottom Row) 🛠️ Implementation Notes 📊 Showcase: Co-located vs. Plain TRL Performance Experiment 1: 1.5B Model — Varying Batch Sizes Experiment 2: 1.5B Model — Varying Tensor Parallelism (TP) Experiment 3: 7B Model — Varying Batch Sizes Experiment 4: 7B Model — Varying Tensor Parallelism (TP) 📊 Scaling to 72B Model Sleep Mode in vLLM DeepSpeed Optimizations Accelerate Integration Experiment 5: Qwen2.5-Math-72B — Throughput,...</description><pubDate>Tue, 03 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/vllm-colocate</guid></item></channel></rss>