<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Welcome EmbeddingGemma, Google's new efficient embedding model</title><link>https://huggingface.co/blog/embeddinggemma</link><description>Back to Articles Welcome EmbeddingGemma, Google's new efficient embedding model Published September 4, 2025 Update on GitHub Upvote 157 +151 Tom Aarsen tomaarsen Follow Joshua Xenova Follow Alvaro Bartolome alvarobartt Follow Aritra Roy Gosthipaty ariG23498 Follow Pedro Cuenca pcuenq Follow Sergio Paniego sergiopaniego Follow TL;DR Table of Contents Introduction Architecture Evaluation Demo Usage Sentence Transformers LangChain LlamaIndex Haystack txtai Transformers.js Text Embeddings Inference ONNX Runtime Finetuning Full Finetuning Script Training Finetuned Evaluation Further Reading TL;DR Today, Google releases EmbeddingGemma , a state-of-the-art multilingual embedding model perfect for on-device use cases. Designed for speed and efficiency, the model features a compact size of 308M parameters and a 2K context window , unlocking new possibilities for mobile RAG pipelines, agents, and more. EmbeddingGemma is trained to support over 100 languages and is the highest-ranking text-...</description><pubDate>Thu, 04 Sep 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/embeddinggemma</guid></item><item><title>Make your ZeroGPU Spaces go brrr with PyTorch ahead-of-time compilation</title><link>https://huggingface.co/blog/zerogpu-aoti</link><description>Back to Articles Make your ZeroGPU Spaces go brrr with ahead-of-time compilation Published September 2, 2025 Update on GitHub Upvote 40 +34 Charles Bensimon cbensimon Follow Sayak Paul sayakpaul Follow Linoy Tsaban linoyts Follow Apolinário from multimodal AI art multimodalart Follow Table of Contents What is ZeroGPU PyTorch compilation Ahead-of-time compilation on ZeroGPU 1. Getting example inputs 2. Exporting the model 3. Compiling the exported model 4. Using the compiled model in the pipeline 5. Wrapping it all together Gotchas Quantization Dynamic shapes Multi-compile / shared weights FlashAttention-3 AoT compiled ZeroGPU Spaces demos Speedup comparison Featured AoTI Spaces Conclusion Resources ZeroGPU lets anyone spin up powerful Nvidia H200 hardware in Hugging Face Spaces without keeping a GPU locked for idle traffic. It’s efficient, flexible, and ideal for demos but it doesn’t always make full use of everything the GPU and CUDA stack can offer. Generating images or videos can...</description><pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/zerogpu-aoti</guid></item><item><title>Generate Images with Claude and Hugging Face</title><link>https://huggingface.co/blog/claude-and-mcp</link><description>Back to Articles Generate Images with Claude and Hugging Face Published August 19, 2025 Update on GitHub Upvote 31 +25 shaun smith evalstate Follow Introduction Natural Images with Flux.1 Krea Dev Qwen Image Conclusion TL;DR: It's easier than ever to generate detailed pictures with state-of-the-art AI models by connecting Claude to Hugging Face Spaces. This article describes how and why, and introduces recently launched models which excel at producing natural images or images that include text. Introduction Recent advances in image generation models have improved their ability to produce realistic outputs and incorporate high quality text. It's easier than ever to use these models by connecting them directly to Claude. The advantages of generating pictures this way are: The AI can assist in building detailed prompts that may improve the quality of generated images. The AI can "see" the generated images, then help iterate on designs and techniques to get perfect results. You can...</description><pubDate>Tue, 19 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/claude-and-mcp</guid></item><item><title>From Zero to GPU: A Guide to Building and Scaling Production-Ready CUDA Kernels</title><link>https://huggingface.co/blog/kernel-builder</link><description>Back to Articles From Zero to GPU: A Guide to Building and Scaling Production-Ready CUDA Kernels Published August 18, 2025 Update on GitHub Upvote 54 +48 David Holtz drbh Follow Daniël de Kok danieldk Follow What You’ll Learn Part 1: Anatomy of a Modern CUDA Kernel Step 1: Project Structure Step 2: The build.toml Manifest Step 3: The flake.nix Reproducibility File Step 4: Writing the CUDA Kernel Step 5: Registering a Native PyTorch Operator Step 6: Building the Kernel Step 7: Sharing with the World Step 8: Loading and Testing Your Custom Op Part 2: From One Kernel to Many: Solving Production Challenges Kernel Versions Pre-downloading Locked Kernels Creating Legacy Python Wheels Custom CUDA kernels give your models a serious performance edge, but building them for the real world can feel daunting. How do you move beyond a simple GPU function to create a robust, scalable system without getting bogged down by endless build times and dependency nightmares? We created the kernel-builder...</description><pubDate>Mon, 18 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/kernel-builder</guid></item><item><title>MCP for Research: How to Connect AI to Research Tools</title><link>https://huggingface.co/blog/mcp-for-research</link><description>Back to Articles MCP for Research: How to Connect AI to Research Tools Published August 18, 2025 Update on GitHub Upvote 44 +38 Dylan Ebert dylanebert Follow Research Discovery: Three Layers of Abstraction 1. Manual Research 2. Scripted Tools 3. MCP Integration Setup and Usage Quick Setup Learn More Academic research involves frequent research discovery : finding papers, code, related models and datasets. This typically means switching between platforms like arXiv , GitHub , and Hugging Face , manually piecing together connections. The Model Context Protocol (MCP) is a standard that allows agentic models to communicate with external tools and data sources. For research discovery, this means AI can use research tools through natural language requests, automating platform switching and cross-referencing. Research Discovery: Three Layers of Abstraction Much like software development, research discovery can be framed in terms of layers of abstraction. 1. Manual Research At the lowest...</description><pubDate>Mon, 18 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/mcp-for-research</guid></item><item><title>TextQuests: How Good are LLMs at Text-Based Video Games?</title><link>https://huggingface.co/blog/textquests</link><description>Back to Articles TextQuests: How Good are LLMs at Text-Based Video Games? Published August 12, 2025 Update on GitHub Upvote 31 +25 Long Phan justinphan3110 Follow cais Clémentine Fourrier clefourrier Follow TextQuests Evaluations Discussion Citations The rapid advancement of Large Language Models (LLMs) has enabled remarkable progress on established academic and industrial benchmarks. Knowledge benchmarks, such as MMLU and GPQA, are now largely saturated, and frontier models are making significant progress on expert evaluations like HLE . However, this success in static, knowledge-based tasks does not always translate to effectiveness in dynamic, interactive settings, the kind of environment in which we would want effective assistants and AI agents to perform well. Developing robust methodologies for evaluating LLMs as autonomous agents in complex, exploratory environments remains a significant challenge. Two core avenues exist to evaluate autonomous agents: either use real-world...</description><pubDate>Tue, 12 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/textquests</guid></item><item><title>Introducing AI Sheets: a tool to work with datasets using open AI models!</title><link>https://huggingface.co/blog/aisheets</link><description>Back to Articles Introducing AI Sheets: a tool to work with datasets using open AI models! Published August 8, 2025 Update on GitHub Upvote 79 +73 Daniel Vila dvilasuero Follow Ame Vi Ameeeee Follow Francisco Aranda frascuchon Follow Damián Pumar damianpumar Follow Leandro von Werra lvwerra Follow Thomas Wolf thomwolf Follow Useful links What is AI Sheets What can I use it for How to use it Getting started Working with your dataset Refining and expanding the dataset Exporting your final dataset to the Hub Running data generation scripts using HF Jobs Examples Vibe testing and comparing models Add categories to a Hub dataset Evaluate models with LLMs-as-Judge Next steps 🧭TL;DR Hugging Face AI Sheets is a new, open-source tool for building, enriching, and transforming datasets using AI models with no code . The tool can be deployed locally or on the Hub. It lets you use thousands of open models from the Hugging Face Hub via Inference Providers or local models, including gpt-oss from...</description><pubDate>Fri, 08 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/aisheets</guid></item><item><title>Accelerate ND-Parallel: A Guide to Efficient Multi-GPU Training</title><link>https://huggingface.co/blog/accelerate-nd-parallel</link><description>Back to Articles Accelerate ND-Parallel: A guide to Efficient Multi-GPU Training Published August 8, 2025 Update on GitHub Upvote 59 +53 Salman Mohammadi smohammadi Follow axolotl-ai-co Matej Sirovatka siro1 Follow wing lian winglian Follow axolotl-ai-co Marc Sun marcsun13 Follow Dan Saunders djsaunde Follow axolotl-ai-co Contents Data Parallelism Fully Sharded Data Parallelism Tensor Parallelism Context Parallelism ND Parallelisms Hybrid Sharded Data Parallelism Fully Sharded Data Parallelism + Tensor Parallelism Fully Sharded Data Parallelism + Context Parallelism Hybrid Sharded Data Parallelism + Tensor Parallelism Usage notes Training large models across multiple GPUs can be challenging due to the complexities of different parallelism strategies. In Accelerate, together with Axolotl , we have integrated a quick and easy way to use any combination of parallelism strategies in your training script! Here is how to add it to your training script: from transformers import...</description><pubDate>Fri, 08 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/accelerate-nd-parallel</guid></item><item><title>🇵🇭 FilBench - Can LLMs Understand and Generate Filipino?</title><link>https://huggingface.co/blog/filbench</link><description>Back to Articles 🇵🇭 FilBench - Can LLMs Understand and Generate Filipino? Published August 12, 2025 Update on GitHub Upvote 15 +9 Lj V. Miranda ljvmiranda921 Follow UD-Filipino Elyanah Aco acocodes Follow UD-Filipino Conner Manuel connermanuel Follow UD-Filipino Jan Christian Blaise Cruz jcblaise Follow UD-Filipino Jan Christian Blaise Cruz jcblaise Follow SEACrowd Joseph Imperial josephimperial Follow SEACrowd Daniel van Strien davanstrien Follow Nathan Habib SaylorTwift Follow Clémentine Fourrier clefourrier Follow FilBench What did we learn from FilBench? Finding #1: Although region-specific LLMs still lag behind GPT-4, collecting data to train these models is still a promising direction Finding #2: Filipino translation is still a difficult task for LLMs Finding #3: Open LLMs Remain a Cost-Effective Choice for Filipino Language Tasks Does your LLM work on Philippine Languages? Try it on FilBench! Acknowledgements Citation As large language models (LLMs) become increasingly...</description><pubDate>Tue, 12 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/filbench</guid></item><item><title>Vision Language Model Alignment in TRL ⚡️</title><link>https://huggingface.co/blog/trl-vlm-alignment</link><description>Back to Articles Vision Language Model Alignment in TRL ⚡️ Published August 7, 2025 Update on GitHub Upvote 78 +72 Sergio Paniego sergiopaniego Follow merve merve Follow Quentin Gallouédec qgallouedec Follow Kashif Rasul kashif Follow Aritra Roy Gosthipaty ariG23498 Follow Introduction Table of Contents Alignment for Vision Language Models Mixed Preference Optimization (MPO) Multimodal Group Relative Policy Optimization (GRPO) Group Sequence Policy Optimization (GSPO) Comparison Native Supervised Fine-tuning Support vLLM Integration in TRL Useful Resources Introduction Vision Language Models (VLMs) are getting stronger, but aligning them to human preferences still matters. In TRL, we already showed how to post-train VLMs with Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) . This time, we’re going further. tl;dr We have added two new multimodal alignment methods to TRL: Group Relative Policy Optimization (GRPO) , its variant Group Sequence Policy Optimization...</description><pubDate>Thu, 07 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/trl-vlm-alignment</guid></item><item><title>Welcome GPT OSS, the new open-source model family from OpenAI!</title><link>https://huggingface.co/blog/welcome-openai-gpt-oss</link><description>Back to Articles Welcome GPT OSS, the new open-source model family from OpenAI! Published August 5, 2025 Update on GitHub Upvote 489 +483 Vaibhav Srivastav reach-vb Follow Pedro Cuenca pcuenq Follow Lewis Tunstall lewtun Follow Clem 🤗 clem Follow Matthew Carrigan Rocketknight1 Follow Clémentine Fourrier clefourrier Follow Célina Hanouti celinah Follow Lucain Pouget Wauplin Follow Marc Sun marcsun13 Follow Simon Pagezy pagezyhf Follow Ákos Hadnagy ahadnagy Follow Joao Gante joaogante Follow Contents Overview of Capabilities and Architecture API access through Inference Providers Local Inference Using Transformers Llama.cpp vLLM transformers serve Fine-Tuning Deploy on Hugging Face Partners Azure Dell Evaluating the Model Chats and Chat Templates System and Developer Messages Tool Use With transformers Acknowledgements GPT OSS is a hugely anticipated open-weights release by OpenAI, designed for powerful reasoning, agentic tasks, and versatile developer use cases. It comprises two...</description><pubDate>Tue, 05 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/welcome-openai-gpt-oss</guid></item><item><title>Build an AI Shopping Assistant with Gradio MCP Servers</title><link>https://huggingface.co/blog/gradio-vton-mcp</link><description>Back to Articles Implementing MCP Servers in Python: An AI Shopping Assistant with Gradio Published July 31, 2025 Update on GitHub Upvote 55 +49 Freddy Boulton freddyaboulton Follow The Goal: Your Personal AI Stylist Building the Gradio MCP Server Configuring VS Code Putting It All Together Conclusion Python Developers, want to give your LLM superpowers? Gradio is the fastest way to do it! With Gradio's Model Context Protocol (MCP) integration, your LLM can plug directly into the thousands of AI models and Spaces hosted on the Hugging Face Hub . By pairing the general reasoning capabilities of LLMs with the specialized abilities of models found on Hugging Face, your LLM can go beyond simply answering text questions to actually solving problems in your daily life. For Python developers, Gradio makes implementing powerful MCP servers a breeze, offering features like: Automatic conversion of python functions into LLM tools: Each API endpoint in your Gradio app is automatically...</description><pubDate>Thu, 31 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gradio-vton-mcp</guid></item><item><title>Introducing Trackio: A Lightweight Experiment Tracking Library from Hugging Face</title><link>https://huggingface.co/blog/trackio</link><description>Back to Articles Introducing Trackio: A Lightweight Experiment Tracking Library from Hugging Face Published July 29, 2025 Update on GitHub Upvote 170 +164 Abubakar Abid abidlabs Follow Zach Nation znation Follow Nouamane Tazi nouamanetazi Follow Sasha Luccioni sasha Follow Quentin Gallouédec qgallouedec Follow Background Why We Switched to Trackio Using Trackio Installing Usage Visualizing Results Sharing with 🤗 Spaces Integrated with 🤗 Transformers and 🤗 Accelerate Design Principles Next Steps TL;DR: Trackio is a new, open-source, and free experiment tracking Python library that provides a local dashboard and seamless integration with Hugging Face Spaces for easy sharing and collaboration. Since trackio is a drop-in replacement for wandb , you can get started with the syntax you already know! Background If you have trained your own machine learning model, you know how important it is to be able to track metrics, parameters, and hyperparameters during training and visualize them...</description><pubDate>Tue, 29 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/trackio</guid></item><item><title>Say hello to `hf`: a faster, friendlier Hugging Face CLI ✨</title><link>https://huggingface.co/blog/hf-cli</link><description>Back to Articles Say hello to `hf`: a faster, friendlier Hugging Face CLI ✨ Published July 25, 2025 Update on GitHub Upvote 80 +74 Lucain Pouget Wauplin Follow Célina Hanouti celinah Follow Julien Chaumond julien-c Follow Getting started 🔀 Migration One more thing... 💥 hf jobs We are glad to announce a long-awaited quality-of-life improvement: the Hugging Face CLI has been officially renamed from huggingface-cli to hf ! So... why this change? Typing huggingface-cli constantly gets old fast. More importantly, the CLI’s command structure became messy as new features were added over time (upload, download, cache management, repo management, etc.). Renaming the CLI is a chance to reorganize commands into a clearer, more consistent format. We decided not to reinvent the wheel and instead follow a well-known CLI pattern: hf &lt;resource&gt; &lt;action&gt; . This predictable grammar makes the Hugging Face CLI more ergonomic and discoverable, while also setting the stage for upcoming features. Getting...</description><pubDate>Fri, 25 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/hf-cli</guid></item><item><title>Parquet Content-Defined Chunking</title><link>https://huggingface.co/blog/parquet-cdc</link><description>Back to Articles Parquet Content-Defined Chunking Published July 25, 2025 Update on GitHub Upvote 62 +56 Krisztian Szucs kszucs Follow Table of Contents Introduction Data Preparation Upload the table as a Parquet file to Hugging Face Hub Different Use Cases for Parquet Deduplication 1. Re-uploading Exact Copies of the Table 2. Adding and Removing Columns from the Table 3. Changing Column Types in the Table 4. Appending New Rows and Concatenating Tables 5. Inserting / Deleting Rows in the Table 6. Using Different Row-group Sizes 7. Using Varying File-Level Splits Using Parquet CDC feature with Pandas References Conclusion Reduce Parquet file upload and download times on Hugging Face Hub by leveraging the new Xet storage layer and Apache Arrow’s Parquet Content-Defined Chunking (CDC) feature enabling more efficient and scalable data workflows. TL;DR: Parquet Content-Defined Chunking (CDC) is now available in PyArrow and Pandas, enabling efficient deduplication of Parquet files on...</description><pubDate>Fri, 25 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/parquet-cdc</guid></item></channel></rss>