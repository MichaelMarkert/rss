<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM</title><link>https://huggingface.co/blog/gemma3</link><description>Back to Articles Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM Published March 12, 2025 Update on GitHub Upvote 269 +263 ariG23498 Aritra Roy Gosthipaty merve Merve Noyan pcuenq Pedro Cuenca reach-vb Vaibhav Srivastav TL;DR What is Gemma 3? Technical Enhancements in Gemma 3 Longer Context Length Multimodality Multilinguality Gemma 3 evaluation Inference with ü§ó transformers On Device &amp; Low Resource Devices MLX Llama.cpp Deploy on Hugging Face Endpoints Acknowledgements TL;DR Today Google releases Gemma 3 , a new iteration of their Gemma family of models. The models range from 1B to 27B parameters, have a context window up to 128k tokens, can accept images and text, and support 140+ languages. Try out Gemma 3 now üëâüèª Gemma 3 Space Gemma 2 Gemma 3 Size Variants 2B 9B 27B 1B 4B 12B 27B Context Window Length 8k 32k (1B) 128k (4B, 12B, 27B) Multimodality (Images and Text) ‚ùå ‚ùå (1B) ‚úÖ (4B, 12B, 27B) Multilingual Support ‚Äì English (1B) +140 languages (4B,...</description><pubDate>Wed, 12 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gemma3</guid></item><item><title>LeRobot goes to driving school: World‚Äôs largest open-source self-driving dataset</title><link>https://huggingface.co/blog/lerobot-goes-to-driving-school</link><description>Back to Articles LeRobot goes to driving school Published March 11, 2025 Update on GitHub Upvote 48 +42 sandhawalia Harsimrat Sandhawalia yaak-ai cadene Remi Cadene OpenStreetMap Multimodal search LeRobot LeRobot driver Appendix A.1 Route tasks A.2 LLM prompts A.2 Data collection hardware State-of-the art Vision Language Models and Large Language Models are trained on open-source image-text corpora sourced from the internet, which spearheaded the recent acceleration of open-source AI. Despite these breakthroughs, the adoption of end-to-end AI within the robotics and automotive community remains low, primarily due to a lack of high quality, large scale multimodal datasets like OXE . To unlock the potential for robotics AI, Yaak teamed up with the LeRobot team at ü§ó and is excited to announce Learning to Drive (L2D) to the robotics AI community. L2D is the world‚Äôs largest multimodal dataset aimed at building an open-sourced spatial intelligence for the automotive domain with first...</description><pubDate>Tue, 11 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/lerobot-goes-to-driving-school</guid></item><item><title>LLM Inference on Edge: A Fun and Easy Guide to run LLMs via React Native on your Phone!</title><link>https://huggingface.co/blog/llm-inference-on-edge</link><description>Back to Articles LLM Inference on Edge: A Fun and Easy Guide to run LLMs via React Native on your Phone! Published March 7, 2025 Update on GitHub Upvote 32 +26 medmekk Mohamed Mekkouri marcsun13 Marc Sun Why You Should Follow This Tutorial? 0. Choosing the Right Models Model Size Considerations GGUF Quantization Formats Recommended Models to Try Finding More Models 1. Setting Up Your Environment Tools You Need Virtual Device Setup 2. Create the App Project Structure 3. Running the Demo &amp; Project Running the Demo Running the Project 4. App Implementation Installing Dependencies State Management Fetching available GGUF models from the Hub Model Download Implementation Model Loading and Initialization Chat Implementation The UI &amp; Logic The other Functionnalities 5. How to Debug Chrome DevTools Debugging Common Debugging Tips 6. Additional Features we can add 7. Acknowledgments 8. Conclusion As LLMs continue to evolve, they are becoming smaller and smarter, enabling them to run directly...</description><pubDate>Fri, 07 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/llm-inference-on-edge</guid></item><item><title>Hugging Face and JFrog partner to make AI Security more transparent</title><link>https://huggingface.co/blog/jfrog</link><description>Back to Articles Hugging Face and JFrog partner to make AI Security more transparent Published March 4, 2025 Update on GitHub Upvote 20 +14 mcpotato Luc Georges srmish-jfrog Shachar M jfrog Model security refresher Integration We are pleased to announce our partnership with JFrog , creators of the JFrog Software Supply Chain Platform, as part of our long-standing commitment to provide a safe and reliable platform for the ML community. We have decided to add JFrog's scanner to our platform to continue improving security on the Hugging Face Hub. JFrog's scanner brings new functionality to scanning, aimed at reducing false positives on the Hub. Indeed, what we currently observe is that model weights can contain code that is executed upon deserialization and sometimes at inference time, depending on the format. This code is oftentimes a non harmful practicality for the developer. As our picklescan scanner only performs pattern matching on module names, we cannot always confirm that...</description><pubDate>Tue, 04 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/jfrog</guid></item><item><title>A Deepdive into Aya Vision: Advancing the Frontier of Multilingual Multimodality</title><link>https://huggingface.co/blog/aya-vision</link><description>Back to Articles A Deepdive into Aya Vision: Advancing the Frontier of Multilingual Multimodality Published March 4, 2025 Update on GitHub Upvote 66 +60 saurabhdash Saurabh Dash CohereForAI olivernan Yiyang Nan CohereForAI ArashAhmadian Arash Ahmadian CohereForAI johndang-cohere John Dang CohereForAI Aya Vision Architecture and Training Training process Multimodal Data Enhancement and Expanding Language Coverage Multimodal Model Merging Scaling up to 32B Aya Vision Benchmark ‚Äì a multilingual evaluation data Designed for real-world applications Getting Started with Aya Acknowledgments References With the release of the Aya Vision family , our new 8B and 32B parameter vision-language models (VLMs), we are addressing one of the biggest challenges in AI: bringing multilingual performance to multimodal models . Aya Vision is Cohere For AI 's latest open-weight multilingual and multimodal model family, designed to be a strong foundation for language and vision understanding across 23...</description><pubDate>Tue, 04 Mar 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/aya-vision</guid></item><item><title>Trace &amp; Evaluate your Agent with Arize Phoenix</title><link>https://huggingface.co/blog/smolagents-phoenix</link><description>Back to Articles Trace &amp; Evaluate your Agent with Arize Phoenix Published February 28, 2025 Update on GitHub Upvote 31 +25 schavalii Sri Chavali arize-ai jgilhuly16 John Gilhuly arize-ai m-ric Aymeric Roucher Make An Agent Step 1: Install the Required Libraries Step 2: Import all the Essential Building Blocks Step 3: Set Up Our Base Models Step 4: Create the Tool-Calling Agent Step 5: Run the Agent Trace Your Agent Evaluate Your Agent Step 1: Install OpenAI Step 2: Retrieve Tool Execution Spans Step 3: Import Prompt Template Step 4: Run the Evaluation Step 5: Send Evaluation Results to Phoenix So, you‚Äôve built your agent. It takes in inputs and tools, processes them, and generates responses. Maybe it‚Äôs making decisions, retrieving information, executing tasks autonomously, or all three. But now comes the big question ‚Äì how effectively is it performing? And more importantly, how do you know? Building an agent is one thing; understanding its behavior is another. That‚Äôs where tracing...</description><pubDate>Fri, 28 Feb 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/smolagents-phoenix</guid></item><item><title>HuggingFace, IISc partner to supercharge model building on India's diverse languages</title><link>https://huggingface.co/blog/iisc-huggingface-collab</link><description>Back to Articles HuggingFace, IISc partner to supercharge model building on India's diverse languages Published February 27, 2025 Update on GitHub Upvote 14 +8 prasantg Prasanta Kumar Ghosh guest nihar-artpark Nihar Desai ARTPARK-IISc Visruth-sanka Sanka ARTPARK-IISc SujithPulikodan Sujith Pulikodan ARTPARK-IISc Partnership About Vaani Dataset District wise language distribution Transcribed subset Utility of Vaani in the Age of LLMs What's next How You Can Contribute The Indian Institute of Science IISc and ARTPARK partner with Hugging Face to enable developers across the globe to access Vaani , India's most diverse open-source, multi-modal, multi-lingual dataset. Both organisations share a commitment to building inclusive, accessible, and state-of-the-art AI technologies that honor linguistic and cultural diversity. Partnership The partnership between Hugging Face and IISc/ARTPARK aims to increase the accessibility and improve usability of the Vaani dataset, encouraging the...</description><pubDate>Thu, 27 Feb 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/iisc-huggingface-collab</guid></item><item><title>FastRTC: The Real-Time Communication Library for Python</title><link>https://huggingface.co/blog/fastrtc</link><description>Back to Articles FastRTC: The Real-Time Communication Library for Python Published February 25, 2025 Update on GitHub Upvote 145 +139 freddyaboulton Freddy Boulton abidlabs Abubakar Abid Getting Started Leveling-Up: LLM Voice Chat Bonus: Call via Phone Next Steps In the last few months, many new real-time speech models have been released and entire companies have been founded around both open and closed source models. To name a few milestones: OpenAI and Google released their live multimodal APIs for ChatGPT and Gemini. OpenAI even went so far as to release a 1-800-ChatGPT phone number! Kyutai released Moshi , a fully open-source audio-to-audio LLM. Alibaba released Qwen2-Audio and Fixie.ai released Ultravox - two open-source LLMs that natively understand audio. ElevenLabs raised $180m in their Series C. Despite the explosion on the model and funding side, it's still difficult to build real-time AI applications that stream audio and video, especially in Python. ML engineers may not...</description><pubDate>Tue, 25 Feb 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/fastrtc</guid></item><item><title>Remote VAEs for decoding with HF endpoints ü§ó</title><link>https://huggingface.co/blog/remote_vae</link><description>Back to Articles Remote VAEs for decoding with Inference Endpoints ü§ó Published February 24, 2025 Update on GitHub Upvote 36 +30 hlky hlky sayakpaul Sayak Paul Getting started Code Basic example Generation Queueing Available VAEs Advantages of using a remote VAE Provide feedback Steps: When operating with latent-space diffusion models for high-resolution image and video synthesis, the VAE decoder can consume quite a bit more memory. This makes it hard for the users to run these models on consumer GPUs without going through latency sacrifices and others alike. For example, with offloading, there is a device transfer overhead, causing delays in the overall inference latency. Tiling is another solution that lets us operate on so-called ‚Äútiles‚Äù of inputs. However, it can have a negative impact on the quality of the final image. Therefore, we want to pilot an idea with the community ‚Äî delegating the decoding process to a remote endpoint. No data is stored or tracked, and code is open...</description><pubDate>Mon, 24 Feb 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/remote_vae</guid></item><item><title>SigLIP 2: A better multilingual vision language encoder</title><link>https://huggingface.co/blog/siglip2</link><description>Back to Articles SigLIP 2: A better multilingual vision language encoder Published February 21, 2025 Update on GitHub Upvote 136 +130 ariG23498 Aritra Roy Gosthipaty merve Merve Noyan qubvel-hf Pavel Iakubovskii TL;DR Introduction Add a decoder (it‚Äôs that simple) Self-distillation with Global-Local loss and Masked Prediction Adapting to different resolutions Run inference with transformers Zero-shot Classification Encode images for downstream tasks Comparing SigLIP 1 with SigLIP 2 Using the encoder for VLMs Acknowledgements TL;DR Today Google releases a new and better family of multilingual vision-language encoders, SigLIP 2 . The authors have extended the training objective of SigLIP ( sigmoid loss ) with additional objectives for improved semantic understanding, localization, and dense features. Additional objectives (Source: https://huggingface.co/papers/2502.14786 ) SigLIP 2 models outperform the older SigLIP ones at all model scales in core capabilities, including zero-shot...</description><pubDate>Fri, 21 Feb 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/siglip2</guid></item><item><title>SmolVLM2: Bringing Video Understanding to Every Device</title><link>https://huggingface.co/blog/smolvlm2</link><description>Back to Articles SmolVLM2: Bringing Video Understanding to Every Device Published February 20, 2025 Update on GitHub Upvote 207 +201 orrzohar Orr Zohar Stanford mfarre Miquel Farr√© andito Andres Marafioti merve Merve Noyan pcuenq Pedro Cuenca cyrilzakka Cyril Xenova Joshua TL;DR: SmolVLM can now watch üì∫ with even better visual understanding Table of Contents Technical Details SmolVLM2 2.2B: Our New Star Player for Vision and Video Going Even Smaller: Meet the 500M and 256M Video Models Suite of SmolVLM2 Demo applications Using SmolVLM2 with Transformers and MLX Transformers Inference with MLX Fine-tuning SmolVLM2 Citation information Read More TL;DR: SmolVLM can now watch üì∫ with even better visual understanding SmolVLM2 represents a fundamental shift in how we think about video understanding - moving from massive models that require substantial computing resources to efficient models that can run anywhere. Our goal is simple: make video understanding accessible across all devices...</description><pubDate>Thu, 20 Feb 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/smolvlm2</guid></item><item><title>PaliGemma 2 Mix - New Instruction Vision Language Models by Google</title><link>https://huggingface.co/blog/paligemma2mix</link><description>Back to Articles PaliGemma 2 Mix - New Instruction Vision Language Models by Google Published February 19, 2025 Update on GitHub Upvote 65 +59 merve Merve Noyan ariG23498 Aritra Roy Gosthipaty andsteing Andreas P. Steiner google TL;DR Table of Contents PaliGemma 2 Mix Models Comparing PaliGemma 2 Mix Variants General Vision-Language Tasks Document Understanding Localization Tasks Text Recognition in Images Inference and Fine-tuning using Transformers Demo Read More Acknowledgments TL;DR Last December, Google released PaliGemma 2: a new family of pre-trained ( pt ) PaliGemma vision language models (VLMs) based on SigLIP and Gemma 2 . The models come in three different sizes (3B, 10B, 28B) and three different resolutions (224x224, 448x448, 896x896). Today, Google is releasing PaliGemma 2 mix : fine-tuned on a mix of vision language tasks, including OCR, long and short captioning and more. PaliGemma 2 pretrained ( pt ) variants are great vision language models to transfer on a given...</description><pubDate>Wed, 19 Feb 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/paligemma2mix</guid></item><item><title>Introducing Three New Serverless Inference Providers: Hyperbolic, Nebius AI Studio, and Novita üî•</title><link>https://huggingface.co/blog/inference-providers-nebius-novita-hyperbolic</link><description>Back to Articles Introducing Three New Serverless Inference Providers: Hyperbolic, Nebius AI Studio, and Novita üî• Published February 18, 2025 Update on GitHub Upvote 93 +87 julien-c Julien Chaumond kramp Bertrand Chevrier reach-vb Vaibhav Srivastav sbrandeis Simon Brandeis albertworks Albert Abdulmanov nebius viktor-hu Viktor Hu novita cchevli Connor Chevli Hyperbolic How it works In the website UI From the client SDKs Billing Feedback and next steps We‚Äôre thrilled to announce the addition of three more outstanding serverless Inference Providers to the Hugging Face Hub: Hyperbolic , Nebius AI Studio , and Novita . These providers join our growing ecosystem, enhancing the breadth and capabilities of serverless inference directly on the Hub‚Äôs model pages. They‚Äôre also seamlessly integrated into our client SDKs (for both JS and Python), making it super easy to use a wide variety of models with your preferred providers. These partners join the ranks of our existing providers, including...</description><pubDate>Tue, 18 Feb 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/inference-providers-nebius-novita-hyperbolic</guid></item><item><title>Welcome Fireworks.ai on the Hub üéÜ</title><link>https://huggingface.co/blog/fireworks-ai</link><description>Back to Articles Welcome Fireworks.ai on the Hub üéÜ Published February 14, 2025 Update on GitHub Upvote 55 +49 teofeliu Teo Feliu fireworks-ai shaunak-fireworks Shaunak Godbole fireworks-ai julien-c Julien Chaumond How it works In the website UI From the client SDKs From HTTP calls Billing Following our recent announcement on Inference Providers on the Hub , we're thrilled to share that Fireworks.ai is now a supported Inference Provider on HF Hub! Fireworks.ai delivers blazing-fast serverless inference directly on model pages, as well as throughout the whole HF ecosystem of libraries and tools, making it easier than ever to run inference on your favorite models. Among others, starting now, you can run serverless inference to the following models via Fireworks.ai: deepseek-ai/DeepSeek-R1 deepseek-ai/DeepSeek-V3 mistralai/Mistral-Small-24B-Instruct-2501 Qwen/Qwen2.5-Coder-32B-Instruct meta-llama/Llama-3.2-90B-Vision-Instruct and many more, you can find the full list here . Light up...</description><pubDate>Fri, 14 Feb 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/fireworks-ai</guid></item><item><title>Fixing Open LLM Leaderboard with Math-Verify</title><link>https://huggingface.co/blog/math_verify_leaderboard</link><description>Back to Articles Fixing Open LLM Leaderboard with Math-Verify Published February 14, 2025 Update on GitHub Upvote 27 +21 hynky Hynek Kydlicek alozowski Alina Lozovskaya SaylorTwift Nathan Habib clefourrier Cl√©mentine Fourrier Why math evaluation on the Open LLM Leaderboard was broken Which model is the best at math? A complete reshuffling of cards thanks to fairer evaluations Impact of the change Model Family Changes Changes in the MATH-Hard Leaderboard Changes in the Leaderboard Wrapping Up 3 weeks ago, we showed how hard it is to correctly evaluate LLM performance on math problems, and introduced Math-Verify , a better solution to validate models on math (read more in the announcement )! Today, we‚Äôre thrilled to share that we‚Äôve used Math-Verify to thoroughly re-evaluate all 3,751 models ever submitted to the Open LLM Leaderboard, for even fairer and more robust model comparisons! Why math evaluation on the Open LLM Leaderboard was broken The Open LLM Leaderboard is the most used...</description><pubDate>Fri, 14 Feb 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/math_verify_leaderboard</guid></item></channel></rss>