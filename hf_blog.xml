<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>KV Cache from scratch in nanoVLM</title><link>https://huggingface.co/blog/kv-cache</link><description>Back to Articles KV Cache from scratch in nanoVLM Published June 4, 2025 Update on GitHub Upvote 32 +26 Aritra Roy Gosthipaty ariG23498 Follow Kashif Rasul kashif Follow Luis Wiedmann lusxvr Follow Andres Marafioti andito Follow Pedro Cuenca pcuenq Follow TL;DR Introduction Revisiting the Transformer Architecture Self-Attention Computation Where Redundancy Creeps In How KV Caching Fixes It KV Caching in nanoVLM: From Theory to Practice 1. Updating KV Cache in the Attention Block 2. Tracking Cache Across Layers 3. Prefill vs Decode in the Generation Loop Summary of Changes Summary: Why KV Caching Matters TL;DR We have implemented KV Caching from scratch in our nanoVLM repository (a small code base to train your own Vision Language Model with pure PyTorch). This gave us a 38% of speedup in generation. In this blog post we cover KV Caching and all our experiences while implementing it. The lessons learnt are general and can be applied to all autoregressive language model generations....</description><pubDate>Wed, 04 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/kv-cache</guid></item><item><title>SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data</title><link>https://huggingface.co/blog/smolvla</link><description>Back to Articles SmolVLA: Efficient Vision-Language-Action Model trained on Lerobot Community Data Published June 3, 2025 Update on GitHub Upvote 76 +70 Dana Aubakirova danaaubakirova Follow Andres Marafioti andito Follow Merve Noyan merve Follow Aritra Roy Gosthipaty ariG23498 Follow Francesco Capuano fracapuano Follow Loubna Ben Allal loubnabnl Follow Pedro Cuenca pcuenq Follow Mustafa Shukor mshukor Follow Remi Cadene cadene Follow üß≠TL;DR üìö Table of Contents Introduction Meet SmolVLA! üöÄ How to Use SmolVLA? Install Finetune the pretrained model Train from scratch Method Main Architecture Design Choices for Efficiency and Robustness Asynchronous Inference Community Datasets Improving Task Annotations Standardizing Camera Views Results Conclusion Call to Action: üß≠TL;DR Today, we introduce SmolVLA , a compact (450M), open-source Vision-Language-Action model for robotics that runs on consumer hardware. Pretrained only on compatibly licensed, open-source community-shared datasets under...</description><pubDate>Tue, 03 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/smolvla</guid></item><item><title>No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL</title><link>https://huggingface.co/blog/vllm-colocate</link><description>Back to Articles No GPU left behind: Unlocking Efficiency with Co-located vLLM in TRL Published June 3, 2025 Update on GitHub Upvote 29 +23 Mert Toslali toslali-ibm Follow Yu Chin Fabian Lim mirinflim Follow Quentin Gallou√©dec qgallouedec Follow Ed Snible esnible Follow Raghu Ganti rganti Follow Mudhakar Srivatsa mudhakar Follow üöÄ Introduction üß® The Problem üí° The Opportunity What It Enables üß© Design: From Separate Servers to Shared GPUs Server TRL Setup (Top Row) Co-located TRL Setup (Bottom Row) üõ†Ô∏è Implementation Notes üìä Showcase: Co-located vs. Plain TRL Performance Experiment 1: 1.5B Model ‚Äî Varying Batch Sizes Experiment 2: 1.5B Model ‚Äî Varying Tensor Parallelism (TP) Experiment 3: 7B Model ‚Äî Varying Batch Sizes Experiment 4: 7B Model ‚Äî Varying Tensor Parallelism (TP) üìä Scaling to 72B Model Sleep Mode in vLLM DeepSpeed Optimizations Accelerate Integration Experiment 5: Qwen2.5-Math-72B ‚Äî Throughput, Accuracy, and Benchmark Results üéì Challenges &amp; Lessons Learned &amp; next steps...</description><pubDate>Tue, 03 Jun 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/vllm-colocate</guid></item><item><title>CodeAgents + Structure: A Better Way to Execute Actions</title><link>https://huggingface.co/blog/structured-codeagent</link><description>Back to Articles CodeAgents + Structure: A Better Way to Execute Actions Published May 28, 2025 Update on GitHub Upvote 38 +32 Aksel Joonas Reedi akseljoonas Follow Aymeric Roucher m-ric Follow ü§î The Evolution of Agent Actions ‚û°Ô∏è Adding Structured outputs to Code Agent üß™ Benchmark Results üí° Why Structure (Generally) Helps The Parsing Problem is Real The Structure Tax üöÄ When to Use Structured CodeAgents How to use with smolagents: Implementation Tips The Bigger Picture - What's Next? Today we're sharing research that bridges two powerful paradigms in AI agent design: the expressiveness of code-based actions and the reliability of structured generation. Our findings show that forcing CodeAgents to generate both thoughts and code in a structured JSON format can significantly outperform traditional approaches across multiple benchmarks. Figure 1: Accuracy comparison of three approaches: Structured CodeAgent (blue), CodeAgent (orange), and ToolCallingAgent (gray) on SmolBench (GAIA,...</description><pubDate>Wed, 28 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/structured-codeagent</guid></item><item><title>üêØ Liger GRPO meets TRL</title><link>https://huggingface.co/blog/liger-grpo</link><description>Back to Articles üêØ Liger GRPO meets TRL Published May 25, 2025 Update on GitHub Upvote 35 +29 Shivam Sahni shisahni Follow Kashif Rasul kashif Follow Salman Mohammadi smohammadi Follow Shirin Yamani ShirinYamani Follow Yanning Chen m0m0chen Follow Liberty liberty4321 Follow Motivation How Liger Kernel slashes memory for GRPO Plug-and-Play integration with TRL Benchmarks Scaling further with FSDP and PEFT Scaling even further with vLLM Conclusion TL; DR Liger supercharges TRL ‚Äôs Group Relative Policy Optimization GRPO Trainer by slashing memory usage by 40% with zero drop in model quality. We also added support for FSDP and PEFT , making it easier than ever to scale GRPO across multiple GPUs. Motivation Fine-tuning language models using reinforcement learning (RL) is a crucial step in a model's training lifecycle for steering models towards desirable behaviours which are more complex than can be achieved through typical supervised fine-tuning. RL has traditionally been applied to...</description><pubDate>Sun, 25 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/liger-grpo</guid></item><item><title>Dell Enterprise Hub is all you need to build AI on premises</title><link>https://huggingface.co/blog/dell-ai-applications</link><description>Back to Articles Dell Enterprise Hub is all you need to build AI on premises Published May 23, 2025 Update on GitHub Upvote 18 +12 Jeff Boudier jeffboudier Follow Andrew Reed andrewrreed Follow Simon Pagezy pagezyhf Follow Alvaro Bartolome alvarobartt Follow Thibault Goehringer beurkinger Follow Florent Gbelidji florentgbelidji Follow Arjuna ark393 Follow Balachandran Rajendran balaatdell Follow Models Ready for Action Introducing AI Applications Powered by NVIDIA, AMD and Intel On-Device Models for Dell AI PC Now with CLI and Python SDK Wrapping up This week at Dell Tech World, we announced the new version of Dell Enterprise Hub , with a complete suite of models and applications to easily build AI running on premises with Dell AI servers and AI PCs. Models Ready for Action If you go to the Dell Enterprise Hub today, you can find some of the most popular models, like Meta Llama 4 Maverick , DeepSeek R1 or Google Gemma 3 , available for deployment and training in a few clicks. But...</description><pubDate>Fri, 23 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/dell-ai-applications</guid></item><item><title>Tiny Agents in Python: a MCP-powered agent in ~70 lines of code</title><link>https://huggingface.co/blog/python-tiny-agents</link><description>Back to Articles Tiny Agents in Python: an MCP-powered agent in ~70 lines of code Published May 23, 2025 Update on GitHub Upvote 119 +113 C√©lina Hanouti celinah Follow Julien Chaumond julien-c Follow Lucain Pouget Wauplin Follow shaun smith evalstate Follow How to Run the Demo Agent Configuration LLMs Can Use Tools Building our Python MCP Client Using the Tools: Streaming and Processing 1. Prepare tools and calling the LLM 2. Executing tools Our Tiny Python Agent: It's (Almost) Just a Loop! 1. Initializing the Agent 2. The agent‚Äôs core: the Loop Next Steps Inspired by Tiny Agents in JS , we ported the idea to Python üêç and extended the huggingface_hub client SDK to act as a MCP Client so it can pull tools from MCP servers and pass them to the LLM during inference. MCP ( Model Context Protocol ) is an open protocol that standardizes how Large Language Models (LLMs) interact with external tools and APIs. Essentially, it removed the need to write custom integrations for each tool,...</description><pubDate>Fri, 23 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/python-tiny-agents</guid></item><item><title>Exploring Quantization Backends in Diffusers</title><link>https://huggingface.co/blog/diffusers-quantization</link><description>Back to Articles Exploring Quantization Backends in Diffusers Published May 21, 2025 Update on GitHub Upvote 31 +25 Derek Liu derekl35 Follow Marc Sun marcsun13 Follow Sayak Paul sayakpaul Follow Spot The Quantized Model Quantization Backends in Diffusers bitsandbytes (BnB) torchao Quanto GGUF FP8 Layerwise Casting ( enable_layerwise_casting ) Combining with More Memory Optimizations and torch.compile Ready to use quantized checkpoints Conclusion Large diffusion models like Flux (a flow-based text-to-image generation model) can create stunning images, but their size can be a hurdle, demanding significant memory and compute resources. Quantization offers a powerful solution, shrinking these models to make them more accessible without drastically compromising performance. But the big question always is: can you actually tell the difference in the final image? Before we dive into the technical details of how various quantization backends in Hugging Face Diffusers work, why not test...</description><pubDate>Wed, 21 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/diffusers-quantization</guid></item><item><title>nanoVLM: The simplest repository to train your VLM in pure PyTorch</title><link>https://huggingface.co/blog/nanovlm</link><description>Back to Articles nanoVLM: The simplest repository to train your VLM in pure PyTorch Published May 21, 2025 Update on GitHub Upvote 133 +127 Aritra Roy Gosthipaty ariG23498 Follow Luis Wiedmann lusxvr Follow Andres Marafioti andito Follow Sergio Paniego sergiopaniego Follow Merve Noyan merve Follow Pedro Cuenca pcuenq Follow Vaibhav Srivastav reach-vb Follow Table of contents: TL;DR What is a Vision Language Model? Working with the repository Architecture Train your own VLM Run inference on a pre-trained model Conclusion References nanoVLM is the simplest way to get started with training your very own Vision Language Model (VLM) using pure PyTorch. It is lightweight toolkit which allows you to launch a VLM training on a free tier colab notebook . We were inspired by Andrej Karpathy ‚Äôs nanoGPT , and provide a similar project for the vision domain. At its heart, nanoVLM is a toolkit that helps you build and train a model that can understand both images and text, and then generate text...</description><pubDate>Wed, 21 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/nanovlm</guid></item><item><title>Microsoft and Hugging Face expand collaboration</title><link>https://huggingface.co/blog/azure-ai-foundry</link><description>Back to Articles Microsoft and Hugging Face expand collaboration to make open models easy to use on Azure Published May 19, 2025 Update on GitHub Upvote 20 +14 Jeff Boudier jeffboudier Follow Simon Pagezy pagezyhf Follow Alvaro Bartolome alvarobartt Follow It‚Äôs time to build - an expanded collaboration How to use Hugging Face in Azure AI Foundry More Hugging Face to come in Azure AI Foundry Today at the Microsoft Build conference, Satya Nadella announced an expanded collaboration with Hugging Face, to make its wide diversity of open models easy to deploy on Azure secure infrastructure. If you head over to Azure AI Foundry today, you will find a vastly expanded collection of 10,000+ Hugging Face models you can deploy in a couple clicks to power AI applications working with text, audio and images. And we‚Äôre just getting started! It‚Äôs time to build - an expanded collaboration 2 years ago, Microsoft and Hugging Face started a collaboration to make open models more easily accessible on...</description><pubDate>Mon, 19 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/azure-ai-foundry</guid></item><item><title>The Transformers Library: standardizing model definitions</title><link>https://huggingface.co/blog/transformers-model-definition</link><description>Back to Articles The Transformers Library: standardizing model definitions Published May 15, 2025 Update on GitHub Upvote 110 +104 Lysandre lysandre Follow Arthur Zucker ArthurZ Follow Pedro Cuenca pcuenq Follow Julien Chaumond julien-c Follow A model-definition library Striving for even simpler model contributions How does this affect you? What this means for you, as a model user What this means for you, as a model creator TLDR: Going forward, we're aiming for Transformers to be the pivot across frameworks: if a model architecture is supported by transformers, you can expect it to be supported in the rest of the ecosystem. Transformers was created in 2019, shortly following the release of the BERT Transformer model. Since then, we've continuously aimed to add state-of-the-art architectures, initially focused on NLP, then growing to Audio and computer vision. Today, transformers is the default library for LLMs and VLMs in the Python ecosystem. Transformers now supports 300+ model...</description><pubDate>Thu, 15 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/transformers-model-definition</guid></item><item><title>Improving Hugging Face Model Access for Kaggle Users</title><link>https://huggingface.co/blog/kaggle-integration</link><description>Back to Articles Improving Hugging Face Model Access for Kaggle Users Published May 14, 2025 Update on GitHub Upvote 27 +21 Vincent Roseberry roseberryv Follow Meg Risdal megrisdal Follow Julien Chaumond julien-c Follow Pedro Cuenca pcuenq Follow Vaibhav Srivastav reach-vb Follow How to get started How does this work with private and consent-gated Hugging Face models? What‚Äôs next Kaggle and Hugging Face users are part of one AI community. That‚Äôs why we‚Äôre excited to announce our plans to bring our platforms and communities closer to better serve AI developers everywhere. Beginning today, Kaggle is launching an integration that enhances visibility and discoverability for Hugging Face models directly on Kaggle. How to get started You can navigate from Hugging Face models to Kaggle and vice versa. Start by visiting a Hugging Face model page like Qwen/Qwen3-1.7B . To use it in a Kaggle Notebook, you can click on ‚ÄúUse this model‚Äù and select ‚ÄúKaggle‚Äù to open up a Kaggle notebook with a...</description><pubDate>Wed, 14 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/kaggle-integration</guid></item><item><title>Blazingly fast whisper transcriptions with Inference Endpoints</title><link>https://huggingface.co/blog/fast-whisper-endpoints</link><description>Back to Articles Blazingly fast whisper transcriptions with Inference Endpoints Published May 13, 2025 Update on GitHub Upvote 67 +61 Morgan Funtowicz mfuntowicz Follow Freddy Boulton freddyaboulton Follow Steven Zheng Steveeeeeeen Follow Vaibhav Srivastav reach-vb Follow Erik Kaunism√§ki erikkaum Follow Michelle Habonneau michellehbn Follow Inference Stack Benchmarks How to deploy Inference FastRTC Demo Today we are happy to introduce a new blazing fast OpenAI Whisper deployment option on Inference Endpoints . It provides up to 8x performance improvements compared to the previous version, and makes everyone one click away from deploying dedicated, powerful transcription models in a cost-effective way, leveraging the amazing work done by the AI community. Through this release, we would like to make Inference Endpoints more community-centric and allow anyone to come and contribute to create incredible inference deployments on the Hugging Face Platform. Along with the community, we...</description><pubDate>Tue, 13 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/fast-whisper-endpoints</guid></item><item><title>Vision Language Models (Better, Faster, Stronger)</title><link>https://huggingface.co/blog/vlms-2025</link><description>Back to Articles Vision Language Models (Better, Faster, Stronger) Published May 12, 2025 Update on GitHub Upvote 411 +405 Merve Noyan merve Follow Sergio Paniego sergiopaniego Follow Aritra Roy Gosthipaty ariG23498 Follow Pedro Cuenca pcuenq Follow Andres Marafioti andito Follow Motivation Table of Contents New model trends Any-to-any models Reasoning Models Smol yet Capable Models Mixture-of-Experts as Decoders Vision-Language-Action Models Specialized Capabilities Object Detection, Segmentation, Counting with Vision Language Models Multimodal Safety Models Multimodal RAG: retrievers, rerankers Multimodal Agents Video Language Models New Alignment Techniques for Vision Language Models New benchmarks MMT-Bench MMMU-Pro Useful Resources Motivation Vision Language Models (VLMs) are the talk of the town. In a previous blog post (from April 2024 ), we talked a lot about VLMs. A major chunk was about LLaVA , the first successful and easily reproducible open-source vision language model,...</description><pubDate>Mon, 12 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/vlms-2025</guid></item><item><title>LeRobot Community Datasets: The ‚ÄúImageNet‚Äù of Robotics ‚Äî When and How?</title><link>https://huggingface.co/blog/lerobot-datasets</link><description>Back to Articles LeRobot Community Datasets: The ‚ÄúImageNet‚Äù of Robotics ‚Äî When and How? Published May 11, 2025 Update on GitHub Upvote 56 +50 Dana Aubakirova danaaubakirova Follow Alexandre Chapin Beegbrain Follow Mustafa Shukor mshukor Follow Marina m1b Follow Ville Kuosmanen villekuosmanen Follow Remi Cadene cadene Follow Pedro Cuenca pcuenq Follow Introduction From Models to Data: Shifting the Perspective Why does Robotics lack its ImageNet Moment? Building a LeRobot Community Scaling Responsibly Better data = Better models Challenges with Current Community Datasets 1. Incomplete or Inconsistent Task Annotations 2. Feature Mapping Inconsistencies 3. Low-Quality or Incomplete Episodes 4. Inconsistent Action/State Dimensions What Makes a Good Dataset? Image Quality Metadata &amp; Recording Protocol Feature Naming Conventions Task Annotation How Can You Help? üß≠ TL;DR ‚Äî Why This Blogpost? In this post, we: Recognize the growing impact of community-contributed LeRobot datasets Highlight...</description><pubDate>Sun, 11 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/lerobot-datasets</guid></item></channel></rss>