<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Introducing NVIDIA Cosmos Policy for Advanced Robot Control</title><link>https://huggingface.co/blog/nvidia/cosmos-policy-for-robot-control</link><description></description><pubDate>Sun, 01 Feb 2026 17:29:07 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/nvidia/cosmos-policy-for-robot-control</guid></item><item><title>Introducing Daggr: Chain apps programmatically, inspect visually</title><link>https://huggingface.co/blog/daggr</link><description>Back to Articles Introducing Daggr: Chain apps programmatically, inspect visually Published January 29, 2026 Update on GitHub Upvote 69 +63 merve merve Follow yuvraj sharma ysharma Follow Abubakar Abid abidlabs Follow hysts hysts Follow Pedro Cuenca pcuenq Follow Table of Contents Background Getting Started Node Types Sharing Your Workflows End-to-End Example with Different Nodes Next Steps TL;DR: Daggr is a new, open-source Python library for building AI workflows that connect Gradio apps, ML models, and custom functions. It automatically generates a visual canvas where you can inspect intermediate outputs, rerun individual steps, and manage state for complex pipelines, all in a few lines of Python code! Table of Contents Background Getting Started Sharing Your Workflows End-to-End Example with Different Nodes Next Steps Background If you've built AI applications that combine multiple models or processing steps, you know the pain: chaining API calls, debugging pipelines, and losing...</description><pubDate>Thu, 29 Jan 2026 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/daggr</guid></item><item><title>We Got Claude to Build CUDA Kernels and teach open models!</title><link>https://huggingface.co/blog/upskill</link><description>Back to Articles We got Claude to teach open models how to write CUDA kernels! Published January 28, 2026 Update on GitHub Upvote 105 +99 ben burtenshaw burtenshaw Follow shaun smith evalstate Follow merve merve Follow Pedro Cuenca pcuenq Follow What are agent skills? 1. Get the teacher (Claude Opus 4.5) to build a kernel 2. Make an agent skill from the trace 3. Take your skill to an open source, smaller, or cheaper model Deep dive tutorial into building kernels with agent skills Setup and Install Skill Generation Generate the Skill Evaluate on a Different Model How the evaluation in upskill works What's Next Resources The best thing about agent skills is upskilling your agents on hard problems. There are two ways to look at that: You can take Opus 4.5 or other SOTA models and tackle the hardest problems out there. You can take models that run on your laptop and upskill them to harder problems. In this blog post, we’ll show you how to take on the latter. This blog post walks through...</description><pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/upskill</guid></item><item><title>Architectural Choices in China's Open-Source AI Ecosystem: Building Beyond DeepSeek</title><link>https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2</link><description></description><pubDate>Sun, 01 Feb 2026 17:29:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment-blog-2</guid></item><item><title>Alyah ⭐️: Toward Robust Evaluation of Emirati Dialect Capabilities in Arabic LLMs</title><link>https://huggingface.co/blog/tiiuae/emirati-benchmarks</link><description></description><pubDate>Sun, 01 Feb 2026 17:29:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/tiiuae/emirati-benchmarks</guid></item><item><title>Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective</title><link>https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl</link><description></description><pubDate>Sun, 01 Feb 2026 17:29:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl</guid></item><item><title>AssetOpsBench: Bridging the Gap Between AI Agent Benchmarks and Industrial Reality</title><link>https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face</link><description></description><pubDate>Sun, 01 Feb 2026 17:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/ibm-research/assetopsbench-playground-on-hugging-face</guid></item><item><title>One Year Since the “DeepSeek Moment”</title><link>https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment</link><description></description><pubDate>Sun, 01 Feb 2026 17:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment</guid></item><item><title>Differential Transformer V2</title><link>https://huggingface.co/blog/microsoft/diff-attn-v2</link><description></description><pubDate>Sun, 01 Feb 2026 17:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/microsoft/diff-attn-v2</guid></item><item><title>Introducing Waypoint-1: Real-time interactive video diffusion from Overworld</title><link>https://huggingface.co/blog/waypoint-1</link><description>Back to Articles Waypoint-1: Real-time Interactive Video Diffusion from Overworld Published January 20, 2026 Update on GitHub Upvote 33 +27 Andrew Lapp lapp0 Follow guest Louis Castricato LouisCastricato Follow guest Scott Fox ScottieFox Follow guest Shahbuland Matiana shahbuland Follow guest David Rossi xAesthetics Follow guest Waypoint-1 Weights on the Hub Try Out The Model What is Waypoint-1? How was it trained? The Inference Library: WorldEngine Build with World Engine Stay in Touch Waypoint-1 Weights on the Hub Waypoint-1-Small Waypoint-1-Medium (Coming Soon!) Try Out The Model Overworld Stream: https://overworld.stream What is Waypoint-1? Waypoint-1 is Overworld’s real-time-interactive video diffusion model, controllable and prompted via text, mouse, and keyboard. You can give the model some frames, run the model, and have it create a world you can step into and interact with. The backbone of the model is a frame-causal rectified flow transformer trained on 10,000 hours of...</description><pubDate>Tue, 20 Jan 2026 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/waypoint-1</guid></item><item><title>Open Responses: What you need to know</title><link>https://huggingface.co/blog/open-responses</link><description>Back to Articles Open Responses: What you need to know Published January 15, 2026 Update on GitHub Upvote 101 +95 shaun smith evalstate Follow ben burtenshaw burtenshaw Follow merve merve Follow Pedro Cuenca pcuenq Follow What is Open Responses? What do we need to know to build with Open Responses? Client Requests to Open Responses Changes for Inference Clients and Providers Open Responses for Routing Tools Sub Agent Loops Next Steps Open Responses is a new and open inference standard. Initiated by OpenAI, built by the open source AI community, and backed by the Hugging Face ecosystem, Open Responses is based on the Responses API and is designed for the future of Agents. In this blog post, we’ll look at how Open Responses works and why the open source community should use Open Responses. The era of the chatbot is long gone, and agents dominate inference workloads. Developers are shifting toward autonomous systems that reason, plan, and act over long-time horizons. Despite this...</description><pubDate>Thu, 15 Jan 2026 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/open-responses</guid></item><item><title>NVIDIA Cosmos Reason 2 Brings Advanced Reasoning To Physical AI</title><link>https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning</link><description></description><pubDate>Sun, 01 Feb 2026 17:29:09 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning</guid></item><item><title>Introducing Falcon-H1-Arabic: Pushing the Boundaries of Arabic Language AI with Hybrid Architecture</title><link>https://huggingface.co/blog/tiiuae/falcon-h1-arabic</link><description></description><pubDate>Sun, 01 Feb 2026 17:29:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/tiiuae/falcon-h1-arabic</guid></item><item><title>NVIDIA brings agents to life with DGX Spark and Reachy Mini</title><link>https://huggingface.co/blog/nvidia-reachy-mini</link><description>Back to Articles NVIDIA brings agents to life with DGX Spark and Reachy Mini Published January 5, 2026 Update on GitHub Upvote 58 +52 Jeff Boudier jeffboudier Follow Nader Khalil nader-at-nvidia Follow nvidia Alec Fong alecfong Follow nvidia Ingredients Giving agentic powers to Reachy Building the agent Step 0: Set up and get access to models and services Step 1: Build a chat interface Step 2: Add NeMo Agent Toolkit’s built-in ReAct agent for tool calling Step 3: Add a router to direct queries to different models Step 4: Add a Pipecat bot for real-time voice + vision Step 5: Hook everything up to Reachy (hardware or simulation) Run the full system Try these example prompts Where to go next Today at CES 2026, NVIDIA unveiled a world of new open models to enable the future of agents, online and in the real world. From the recently released NVIDIA Nemotron reasoning LLMs to the new NVIDIA Isaac GR00T N1.6 open reasoning VLA and NVIDIA Cosmos world foundation models , all the building...</description><pubDate>Mon, 05 Jan 2026 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/nvidia-reachy-mini</guid></item></channel></rss>