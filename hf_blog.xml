<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>CodeAgents + Structure: A¬†Better Way to Execute Actions</title><link>https://huggingface.co/blog/structured-codeagent</link><description>Back to Articles CodeAgents + Structure: A Better Way to Execute Actions Published May 28, 2024 Update on GitHub Upvote 28 +22 akseljoonas Aksel Joonas Reedi m-ric Aymeric Roucher ü§î The Evolution of Agent Actions ‚û°Ô∏è Adding Structured outputs to Code Agent üß™ Benchmark Results üí° Why Structure (Generally) Helps The Parsing Problem is Real The Structure Tax üöÄ When to Use Structured CodeAgents How to use with smolagents: Implementation Tips The Bigger Picture - What's Next? Today we're sharing research that bridges two powerful paradigms in AI agent design: the expressiveness of code-based actions and the reliability of structured generation. Our findings show that forcing CodeAgents to generate both thoughts and code in a structured JSON format can significantly outperform traditional approaches across multiple benchmarks. Figure 1: Accuracy comparison of three approaches: Structured CodeAgent (blue), CodeAgent (orange), and ToolCallingAgent (gray) on SmolBench (GAIA, MATH, SimpleQA,...</description><pubDate>Tue, 28 May 2024 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/structured-codeagent</guid></item><item><title>üêØ Liger GRPO meets TRL</title><link>https://huggingface.co/blog/liger-grpo</link><description>Back to Articles üêØ Liger GRPO meets TRL Published May 25, 2025 Update on GitHub Upvote 26 +20 shisahni Shivam Sahni LinkedIn kashif Kashif Rasul smohammadi Salman Mohammadi axolotl-ai-co ShirinYamani Shirin Yamani m0m0chen Yanning Chen guest liberty4321 Liberty guest Motivation How Liger Kernel slashes memory for GRPO Plug-and-Play integration with TRL Benchmarks Scaling further with FSDP and PEFT Scaling even further with vLLM Conclusion TL; DR Liger supercharges TRL ‚Äôs Group Relative Policy Optimization GRPO Trainer by slashing memory usage by 40% with zero drop in model quality. We also added support for FSDP and PEFT , making it easier than ever to scale GRPO across multiple GPUs. Motivation Fine-tuning language models using reinforcement learning (RL) is a crucial step in a model's training lifecycle for steering models towards desirable behaviours which are more complex than can be achieved through typical supervised fine-tuning. RL has traditionally been applied to optimize...</description><pubDate>Sun, 25 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/liger-grpo</guid></item><item><title>Dell Enterprise Hub is all you need to build AI on premises</title><link>https://huggingface.co/blog/dell-ai-applications</link><description>Back to Articles Dell Enterprise Hub is all you need to build AI on premises Published May 23, 2025 Update on GitHub Upvote 16 +10 jeffboudier Jeff Boudier andrewrreed Andrew Reed pagezyhf Simon Pagezy alvarobartt Alvaro Bartolome beurkinger Thibault Goehringer florentgbelidji Florent Gbelidji ark393 Arjuna balaatdell Balachandran Rajendran DellTechnologies Models Ready for Action Introducing AI Applications Powered by NVIDIA, AMD and Intel On-Device Models for Dell AI PC Now with CLI and Python SDK Wrapping up This week at Dell Tech World, we announced the new version of Dell Enterprise Hub , with a complete suite of models and applications to easily build AI running on premises with Dell AI servers and AI PCs. Models Ready for Action If you go to the Dell Enterprise Hub today, you can find some of the most popular models, like Meta Llama 4 Maverick , DeepSeek R1 or Google Gemma 3 , available for deployment and training in a few clicks. But what you get is much more than a model,...</description><pubDate>Fri, 23 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/dell-ai-applications</guid></item><item><title>Tiny Agents in Python: a MCP-powered agent in ~70 lines of code</title><link>https://huggingface.co/blog/python-tiny-agents</link><description>Back to Articles Tiny Agents in Python: an MCP-powered agent in ~70 lines of code Published May 23, 2025 Update on GitHub Upvote 96 +90 celinah C√©lina Hanouti julien-c Julien Chaumond Wauplin Lucain Pouget evalstate shaun smith guest How to Run the Demo Agent Configuration LLMs Can Use Tools Building our Python MCP Client Using the Tools: Streaming and Processing 1. Prepare tools and calling the LLM 2. Executing tools Our Tiny Python Agent: It's (Almost) Just a Loop! 1. Initializing the Agent 2. The agent‚Äôs core: the Loop Next Steps Inspired by Tiny Agents in JS , we ported the idea to Python üêç and extended the huggingface_hub client SDK to act as a MCP Client so it can pull tools from MCP servers and pass them to the LLM during inference. MCP ( Model Context Protocol ) is an open protocol that standardizes how Large Language Models (LLMs) interact with external tools and APIs. Essentially, it removed the need to write custom integrations for each tool, making it simpler to plug new...</description><pubDate>Fri, 23 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/python-tiny-agents</guid></item><item><title>Exploring Quantization Backends in Diffusers</title><link>https://huggingface.co/blog/diffusers-quantization</link><description>Back to Articles Exploring Quantization Backends in Diffusers Published May 21, 2025 Update on GitHub Upvote 25 +19 derekl35 Derek Liu marcsun13 Marc Sun sayakpaul Sayak Paul Spot The Quantized Model Quantization Backends in Diffusers bitsandbytes (BnB) torchao Quanto GGUF FP8 Layerwise Casting ( enable_layerwise_casting ) Combining with More Memory Optimizations and torch.compile Ready to use quantized checkpoints Conclusion Large diffusion models like Flux (a flow-based text-to-image generation model) can create stunning images, but their size can be a hurdle, demanding significant memory and compute resources. Quantization offers a powerful solution, shrinking these models to make them more accessible without drastically compromising performance. But the big question always is: can you actually tell the difference in the final image? Before we dive into the technical details of how various quantization backends in Hugging Face Diffusers work, why not test your own perception?...</description><pubDate>Wed, 21 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/diffusers-quantization</guid></item><item><title>nanoVLM: The simplest repository to train your VLM in pure PyTorch</title><link>https://huggingface.co/blog/nanovlm</link><description>Back to Articles nanoVLM: The simplest repository to train your VLM in pure PyTorch Published May 21, 2025 Update on GitHub Upvote 115 +109 ariG23498 Aritra Roy Gosthipaty lusxvr Luis Wiedmann andito Andres Marafioti sergiopaniego Sergio Paniego merve Merve Noyan pcuenq Pedro Cuenca reach-vb Vaibhav Srivastav Table of contents: TL;DR What is a Vision Language Model? Working with the repository Architecture Train your own VLM Run inference on a pre-trained model Conclusion References nanoVLM is the simplest way to get started with training your very own Vision Language Model (VLM) using pure PyTorch. It is lightweight toolkit which allows you to launch a VLM training on a free tier colab notebook . We were inspired by Andrej Karpathy ‚Äôs nanoGPT , and provide a similar project for the vision domain. At its heart, nanoVLM is a toolkit that helps you build and train a model that can understand both images and text, and then generate text based on that. The beauty of nanoVLM lies in its...</description><pubDate>Wed, 21 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/nanovlm</guid></item><item><title>Microsoft and Hugging Face expand collaboration</title><link>https://huggingface.co/blog/azure-ai-foundry</link><description>Back to Articles Microsoft and Hugging Face expand collaboration to make open models easy to use on Azure Published May 19, 2025 Update on GitHub Upvote 20 +14 jeffboudier Jeff Boudier pagezyhf Simon Pagezy alvarobartt Alvaro Bartolome It‚Äôs time to build - an expanded collaboration How to use Hugging Face in Azure AI Foundry More Hugging Face to come in Azure AI Foundry Today at the Microsoft Build conference, Satya Nadella announced an expanded collaboration with Hugging Face, to make its wide diversity of open models easy to deploy on Azure secure infrastructure. If you head over to Azure AI Foundry today, you will find a vastly expanded collection of 10,000+ Hugging Face models you can deploy in a couple clicks to power AI applications working with text, audio and images. And we‚Äôre just getting started! It‚Äôs time to build - an expanded collaboration 2 years ago, Microsoft and Hugging Face started a collaboration to make open models more easily accessible on Azure - back then the...</description><pubDate>Mon, 19 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/azure-ai-foundry</guid></item><item><title>The Transformers Library: standardizing model definitions</title><link>https://huggingface.co/blog/transformers-model-definition</link><description>Back to Articles The Transformers Library: standardizing model definitions Published May 15, 2025 Update on GitHub Upvote 104 +98 lysandre Lysandre ArthurZ Arthur Zucker pcuenq Pedro Cuenca julien-c Julien Chaumond A model-definition library Striving for even simpler model contributions How does this affect you? What this means for you, as a model user What this means for you, as a model creator TLDR: Going forward, we're aiming for Transformers to be the pivot across frameworks: if a model architecture is supported by transformers, you can expect it to be supported in the rest of the ecosystem. Transformers was created in 2019, shortly following the release of the BERT Transformer model. Since then, we've continuously aimed to add state-of-the-art architectures, initially focused on NLP, then growing to Audio and computer vision. Today, transformers is the default library for LLMs and VLMs in the Python ecosystem. Transformers now supports 300+ model architectures, with an average...</description><pubDate>Thu, 15 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/transformers-model-definition</guid></item><item><title>Improving Hugging Face Model Access for Kaggle Users</title><link>https://huggingface.co/blog/kaggle-integration</link><description>Back to Articles Improving Hugging Face Model Access for Kaggle Users Published May 14, 2025 Update on GitHub Upvote 25 +19 roseberryv Vincent Roseberry kaggle megrisdal Meg Risdal kaggle julien-c Julien Chaumond pcuenq Pedro Cuenca reach-vb Vaibhav Srivastav How to get started How does this work with private and consent-gated Hugging Face models? What‚Äôs next Kaggle and Hugging Face users are part of one AI community. That‚Äôs why we‚Äôre excited to announce our plans to bring our platforms and communities closer to better serve AI developers everywhere. Beginning today, Kaggle is launching an integration that enhances visibility and discoverability for Hugging Face models directly on Kaggle. How to get started You can navigate from Hugging Face models to Kaggle and vice versa. Start by visiting a Hugging Face model page like Qwen/Qwen3-1.7B . To use it in a Kaggle Notebook, you can click on ‚ÄúUse this model‚Äù and select ‚ÄúKaggle‚Äù to open up a Kaggle notebook with a pre-populated code...</description><pubDate>Wed, 14 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/kaggle-integration</guid></item><item><title>Blazingly fast whisper transcriptions with Inference Endpoints</title><link>https://huggingface.co/blog/fast-whisper-endpoints</link><description>Back to Articles Blazingly fast whisper transcriptions with Inference Endpoints Published May 13, 2025 Update on GitHub Upvote 65 +59 mfuntowicz Morgan Funtowicz freddyaboulton Freddy Boulton Steveeeeeeen Steven Zheng reach-vb Vaibhav Srivastav erikkaum Erik Kaunism√§ki michellehbn Michelle Habonneau Inference Stack Benchmarks How to deploy Inference FastRTC Demo Today we are happy to introduce a new blazing fast OpenAI Whisper deployment option on Inference Endpoints . It provides up to 8x performance improvements compared to the previous version, and makes everyone one click away from deploying dedicated, powerful transcription models in a cost-effective way, leveraging the amazing work done by the AI community. Through this release, we would like to make Inference Endpoints more community-centric and allow anyone to come and contribute to create incredible inference deployments on the Hugging Face Platform. Along with the community, we would like to propose optimized deployments...</description><pubDate>Tue, 13 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/fast-whisper-endpoints</guid></item><item><title>Vision Language Models (Better, Faster, Stronger)</title><link>https://huggingface.co/blog/vlms-2025</link><description>Back to Articles Vision Language Models (Better, Faster, Stronger) Published May 12, 2025 Update on GitHub Upvote 391 +385 merve Merve Noyan sergiopaniego Sergio Paniego ariG23498 Aritra Roy Gosthipaty pcuenq Pedro Cuenca andito Andres Marafioti Motivation Table of Contents New model trends Any-to-any models Reasoning Models Smol yet Capable Models Mixture-of-Experts as Decoders Vision-Language-Action Models Specialized Capabilities Object Detection, Segmentation, Counting with Vision Language Models Multimodal Safety Models Multimodal RAG: retrievers, rerankers Multimodal Agents Video Language Models New Alignment Techniques for Vision Language Models New benchmarks MMT-Bench MMMU-Pro Useful Resources Motivation Vision Language Models (VLMs) are the talk of the town. In a previous blog post (from April 2024 ), we talked a lot about VLMs. A major chunk was about LLaVA , the first successful and easily reproducible open-source vision language model, along with tips on how to...</description><pubDate>Mon, 12 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/vlms-2025</guid></item><item><title>LeRobot Community Datasets: The ‚ÄúImageNet‚Äù of Robotics ‚Äî When and How?</title><link>https://huggingface.co/blog/lerobot-datasets</link><description>Back to Articles LeRobot Community Datasets: The ‚ÄúImageNet‚Äù of Robotics ‚Äî When and How? Published May 11, 2025 Update on GitHub Upvote 49 +43 danaaubakirova Dana Aubakirova Beegbrain Alexandre Chapin guest mshukor Mustafa Shukor m1b Marina guest villekuosmanen Ville Kuosmanen guest cadene Remi Cadene pcuenq Pedro Cuenca Introduction From Models to Data: Shifting the Perspective Why does Robotics lack its ImageNet Moment? Building a LeRobot Community Scaling Responsibly Better data = Better models Challenges with Current Community Datasets 1. Incomplete or Inconsistent Task Annotations 2. Feature Mapping Inconsistencies 3. Low-Quality or Incomplete Episodes 4. Inconsistent Action/State Dimensions What Makes a Good Dataset? Image Quality Metadata &amp; Recording Protocol Feature Naming Conventions Task Annotation How Can You Help? üß≠ TL;DR ‚Äî Why This Blogpost? In this post, we: Recognize the growing impact of community-contributed LeRobot datasets Highlight the current challenges in...</description><pubDate>Sun, 11 May 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/lerobot-datasets</guid></item><item><title>How to Build an MCP Server with Gradio</title><link>https://huggingface.co/blog/gradio-mcp</link><description>Back to Articles How to Build an MCP Server in 5 Lines of Python Published April 30, 2025 Update on GitHub Upvote 128 +122 abidlabs Abubakar Abid ysharma yuvraj sharma Why Build an MCP Server? Example: Counting Letters in a Word Key features of the Gradio &lt;&gt; MCP Integration Further Reading Gradio is a Python library used by more than 1 million developers each month to build interfaces for machine learning models. Beyond just creating UIs, Gradio also exposes API capabilities and ‚Äî now! ‚Äî Gradio apps can be launched Model Context Protocol (MCP) servers for LLMs. This means that your Gradio app, whether it's an image generator or a tax calculator or something else entirely, can be called as a tool by an LLM. This guide will show you how to use Gradio to build an MCP server in just a few lines of Python. Prerequisites If not already installed, please install Gradio with the MCP extra: pip install "gradio[mcp]" This will install the necessary dependencies, including the mcp package....</description><pubDate>Wed, 30 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gradio-mcp</guid></item><item><title>Welcoming Llama Guard 4 on Hugging Face Hub</title><link>https://huggingface.co/blog/llama-guard-4</link><description>Back to Articles Welcoming Llama Guard 4 on Hugging Face Hub Published April 29, 2025 Update on GitHub Upvote 37 +31 merve Merve Noyan ariG23498 Aritra Roy Gosthipaty sergiopaniego Sergio Paniego pcuenq Pedro Cuenca Table-of-Contents What is Llama Guard 4? Model Details Llama Guard 4 Llama Prompt Guard 2 Getting Started using ü§ó transformers Llama Prompt Guard 2 Useful Resources TL;DR: Today, Meta releases Llama Guard 4, a 12B dense (not a MoE!) multimodal safety model, and two new Llama Prompt Guard 2 models. This release comes with multiple open model checkpoints, along with an interactive notebook for you to get started easily ü§ó. Model checkpoints can be found in Llama 4 Collection . Table-of-Contents What is Llama Guard 4? Model Details Llama Guard 4 Llama Prompt Guard 2 Getting Started using ü§ótransformers Llama Guard 4 Llama Prompt Guard 2 Useful Resources What is Llama Guard 4? Vision and large language models deployed to production can be exploited to generate unsafe output...</description><pubDate>Tue, 29 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/llama-guard-4</guid></item><item><title>The 4 Things Qwen-3's Chat Template Teaches Us</title><link>https://huggingface.co/blog/qwen-3-chat-template-deep-dive</link><description>Back to Articles The 4 Things Qwen-3‚Äôs Chat Template Teaches Us Published April 30, 2025 Update on GitHub Upvote 47 +41 cfahlgren1 Caleb Fahlgren What is a Chat Template? 1. Reasoning doesn't have to be forced 2. Context Management Should be Dynamic Example 3. Tool Arguments Need Better Serialization 4. There's No Need for a Default System Prompt Conclusion What a boring Jinja snippet tells us about the new Qwen-3 model. The new Qwen-3 model by Qwen ships with a much more sophisticated chat template than its predecessors Qwen-2.5 and QwQ. By taking a look at the differences in the Jinja template, we can find interesting insights into the new model. Chat Templates Qwen-3 Chat Template Qwen-2.5 Chat Template Qwen-QwQ Chat Template What is a Chat Template? A chat template defines how conversations between users and models are structured and formatted. The template acts as a translator, converting a human-readable conversation: [ { role : "user" , content : "Hi there!" }, { role :...</description><pubDate>Wed, 30 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/qwen-3-chat-template-deep-dive</guid></item></channel></rss>