<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Tricks from OpenAI gpt-oss YOU ü´µ can use with transformers</title><link>https://huggingface.co/blog/faster-transformers</link><description>Back to Articles Tricks from OpenAI gpt-oss YOU ü´µ can use with transformers Published September 11, 2025 Update on GitHub Upvote 96 +90 Aritra Roy Gosthipaty ariG23498 Follow Sergio Paniego sergiopaniego Follow Vaibhav Srivastav reach-vb Follow Pedro Cuenca pcuenq Follow Arthur Zucker ArthurZ Follow Nathan Habib SaylorTwift Follow Cyril Vallez cyrilvallez Follow Zero-build Kernels, downloadable from the Hub Custom Kernels for GPT-OSS Flash Attention 3 MXFP4 Quantization What is MXFP4 MXFP4 in transformers Requirements and fallbacks Kernels for MXFP4 Tensor Parallelism What this enables in transformers When to reach for TP Expert Parallelism Dynamic Sliding Window Layer &amp; Cache How to use it Continuous Batching &amp; Paged Attention Load larger models faster Conclusion Read More OpenAI recently released their GPT-OSS series of models . The models feature some novel techniques like MXFP4 quantization, efficient kernels, a brand new chat format, and more. To enable the release of gpt-oss...</description><pubDate>Thu, 11 Sep 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/faster-transformers</guid></item><item><title>Jupyter Agents: training LLMs to reason with notebooks</title><link>https://huggingface.co/blog/jupyter-agent-2</link><description>Back to Articles Jupyter Agents: training LLMs to reason with notebooks Published September 10, 2025 Update on GitHub Upvote 18 +12 Baptiste Colle baptistecolle Follow Hanna Yukhymenko hannayukhymenko Follow Leandro von Werra lvwerra Follow üèÅ Primer: the DABStep Benchmark üéØ First Baseline üîß Primer on Scaffolding üèÉ‚Äç‚ôÇÔ∏è Training Pipeline ‚öôÔ∏è Dataset Pipeline 1. Large-scale deduplication 2. Downloading linked datasets 3. Edu scoring 4. Filtering irrelevant notebooks 5. QA generation 6. Trace generation 7. Final curation üèÉ‚Äç‚ôÇÔ∏è Training Pipeline üìä Results Try Jupyter Agent Yourself üîÆ Next Steps The past year has been all about giving LLMs more tools and autonomy to solve more complex and open ended tasks. The goal of the Jupyter Agent is to give the model the ultimate tool: code execution. A natural way to display multi-step code execution together with reasoning is within a Jupyter Notebook, which consists of code and markdown cells. So we built Jupyter Agent to act as an agent that can...</description><pubDate>Wed, 10 Sep 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/jupyter-agent-2</guid></item><item><title>mmBERT: ModernBERT goes Multilingual</title><link>https://huggingface.co/blog/mmbert</link><description>Back to Articles mmBERT: ModernBERT goes Multilingual Published September 9, 2025 Update on GitHub Upvote 77 +71 Marc Marone mmarone Follow jhu-clsp Orion Weller orionweller Follow jhu-clsp William Fleshman will-fleshman Follow guest Eugene Yang eugene-yang Follow jhu-clsp Dawn Lawrie dlawrie Follow jhu-clsp Ben Van Durme vandurme Follow jhu-clsp TL;DR Training Data Training Recipe and Novel Components Architecture Three-Phase Training Approach Novel Training Techniques Results Natural Language Understanding (NLU) Retrieval Performance Learning Languages in the Decay Phase Efficiency Improvements Usage Examples Fine-tuning Examples Encoders Model Family and Links TL;DR This blog post introduces mmBERT , a state-of-the-art massively multilingual encoder model trained on 3T+ tokens of text in over 1800 languages. It shows significant performance and speed improvements over previous multilingual models, being the first to improve upon XLM-R, while also developing new strategies for...</description><pubDate>Tue, 09 Sep 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/mmbert</guid></item><item><title>Welcome EmbeddingGemma, Google's new efficient embedding model</title><link>https://huggingface.co/blog/embeddinggemma</link><description>Back to Articles Welcome EmbeddingGemma, Google's new efficient embedding model Published September 4, 2025 Update on GitHub Upvote 201 +195 Tom Aarsen tomaarsen Follow Joshua Xenova Follow Alvaro Bartolome alvarobartt Follow Aritra Roy Gosthipaty ariG23498 Follow Pedro Cuenca pcuenq Follow Sergio Paniego sergiopaniego Follow TL;DR Table of Contents Introduction Architecture Evaluation Demo Usage Sentence Transformers LangChain LlamaIndex Haystack txtai Transformers.js Text Embeddings Inference ONNX Runtime Finetuning Full Finetuning Script Training Finetuned Evaluation Further Reading TL;DR Today, Google releases EmbeddingGemma , a state-of-the-art multilingual embedding model perfect for on-device use cases. Designed for speed and efficiency, the model features a compact size of 308M parameters and a 2K context window , unlocking new possibilities for mobile RAG pipelines, agents, and more. EmbeddingGemma is trained to support over 100 languages and is the highest-ranking text-...</description><pubDate>Thu, 04 Sep 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/embeddinggemma</guid></item><item><title>Make your ZeroGPU Spaces go brrr with PyTorch ahead-of-time compilation</title><link>https://huggingface.co/blog/zerogpu-aoti</link><description>Back to Articles Make your ZeroGPU Spaces go brrr with ahead-of-time compilation Published September 2, 2025 Update on GitHub Upvote 52 +46 Charles Bensimon cbensimon Follow Sayak Paul sayakpaul Follow Linoy Tsaban linoyts Follow Apolin√°rio from multimodal AI art multimodalart Follow Table of Contents What is ZeroGPU PyTorch compilation Ahead-of-time compilation on ZeroGPU 1. Getting example inputs 2. Exporting the model 3. Compiling the exported model 4. Using the compiled model in the pipeline 5. Wrapping it all together Gotchas Quantization Dynamic shapes Multi-compile / shared weights FlashAttention-3 Regional compilation Use a compiled graph from the Hub AoT compiled ZeroGPU Spaces demos Speedup comparison Featured AoTI Spaces Regional compilation Conclusion Resources ZeroGPU lets anyone spin up powerful Nvidia H200 hardware in Hugging Face Spaces without keeping a GPU locked for idle traffic. It‚Äôs efficient, flexible, and ideal for demos but it doesn‚Äôt always make full use of...</description><pubDate>Tue, 02 Sep 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/zerogpu-aoti</guid></item><item><title>Generate Images with Claude and Hugging Face</title><link>https://huggingface.co/blog/claude-and-mcp</link><description>Back to Articles Generate Images with Claude and Hugging Face Published August 19, 2025 Update on GitHub Upvote 33 +27 shaun smith evalstate Follow Introduction Natural Images with Flux.1 Krea Dev Qwen Image Conclusion TL;DR: It's easier than ever to generate detailed pictures with state-of-the-art AI models by connecting Claude to Hugging Face Spaces. This article describes how and why, and introduces recently launched models which excel at producing natural images or images that include text. Introduction Recent advances in image generation models have improved their ability to produce realistic outputs and incorporate high quality text. It's easier than ever to use these models by connecting them directly to Claude. The advantages of generating pictures this way are: The AI can assist in building detailed prompts that may improve the quality of generated images. The AI can "see" the generated images, then help iterate on designs and techniques to get perfect results. You can...</description><pubDate>Tue, 19 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/claude-and-mcp</guid></item><item><title>From Zero to GPU: A Guide to Building and Scaling Production-Ready CUDA Kernels</title><link>https://huggingface.co/blog/kernel-builder</link><description>Back to Articles From Zero to GPU: A Guide to Building and Scaling Production-Ready CUDA Kernels Published August 18, 2025 Update on GitHub Upvote 64 +58 David Holtz drbh Follow Dani√´l de Kok danieldk Follow What You‚Äôll Learn Part 1: Anatomy of a Modern CUDA Kernel Step 1: Project Structure Step 2: The build.toml Manifest Step 3: The flake.nix Reproducibility File Step 4: Writing the CUDA Kernel Step 5: Registering a Native PyTorch Operator Step 6: Building the Kernel Step 7: Sharing with the World Step 8: Loading and Testing Your Custom Op Part 2: From One Kernel to Many: Solving Production Challenges Kernel Versions Pre-downloading Locked Kernels Creating Legacy Python Wheels Custom CUDA kernels give your models a serious performance edge, but building them for the real world can feel daunting. How do you move beyond a simple GPU function to create a robust, scalable system without getting bogged down by endless build times and dependency nightmares? We created the kernel-builder...</description><pubDate>Mon, 18 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/kernel-builder</guid></item><item><title>MCP for Research: How to Connect AI to Research Tools</title><link>https://huggingface.co/blog/mcp-for-research</link><description>Back to Articles MCP for Research: How to Connect AI to Research Tools Published August 18, 2025 Update on GitHub Upvote 49 +43 Dylan Ebert dylanebert Follow Research Discovery: Three Layers of Abstraction 1. Manual Research 2. Scripted Tools 3. MCP Integration Setup and Usage Quick Setup Learn More Academic research involves frequent research discovery : finding papers, code, related models and datasets. This typically means switching between platforms like arXiv , GitHub , and Hugging Face , manually piecing together connections. The Model Context Protocol (MCP) is a standard that allows agentic models to communicate with external tools and data sources. For research discovery, this means AI can use research tools through natural language requests, automating platform switching and cross-referencing. Research Discovery: Three Layers of Abstraction Much like software development, research discovery can be framed in terms of layers of abstraction. 1. Manual Research At the lowest...</description><pubDate>Mon, 18 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/mcp-for-research</guid></item><item><title>TextQuests: How Good are LLMs at Text-Based Video Games?</title><link>https://huggingface.co/blog/textquests</link><description>Back to Articles TextQuests: How Good are LLMs at Text-Based Video Games? Published August 12, 2025 Update on GitHub Upvote 33 +27 Long Phan justinphan3110 Follow cais Cl√©mentine Fourrier clefourrier Follow TextQuests Evaluations Discussion Citations The rapid advancement of Large Language Models (LLMs) has enabled remarkable progress on established academic and industrial benchmarks. Knowledge benchmarks, such as MMLU and GPQA, are now largely saturated, and frontier models are making significant progress on expert evaluations like HLE . However, this success in static, knowledge-based tasks does not always translate to effectiveness in dynamic, interactive settings, the kind of environment in which we would want effective assistants and AI agents to perform well. Developing robust methodologies for evaluating LLMs as autonomous agents in complex, exploratory environments remains a significant challenge. Two core avenues exist to evaluate autonomous agents: either use real-world...</description><pubDate>Tue, 12 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/textquests</guid></item><item><title>Introducing AI Sheets: a tool to work with datasets using open AI models!</title><link>https://huggingface.co/blog/aisheets</link><description>Back to Articles Introducing AI Sheets: a tool to work with datasets using open AI models! Published August 8, 2025 Update on GitHub Upvote 93 +87 Daniel Vila dvilasuero Follow Ame Vi Ameeeee Follow Francisco Aranda frascuchon Follow Dami√°n Pumar damianpumar Follow Leandro von Werra lvwerra Follow Thomas Wolf thomwolf Follow Useful links What is AI Sheets What can I use it for How to use it Getting started Working with your dataset Refining and expanding the dataset Exporting your final dataset to the Hub Running data generation scripts using HF Jobs Examples Vibe testing and comparing models Add categories to a Hub dataset Evaluate models with LLMs-as-Judge Next steps üß≠TL;DR Hugging Face AI Sheets is a new, open-source tool for building, enriching, and transforming datasets using AI models with no code . The tool can be deployed locally or on the Hub. It lets you use thousands of open models from the Hugging Face Hub via Inference Providers or local models, including gpt-oss from...</description><pubDate>Fri, 08 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/aisheets</guid></item><item><title>Accelerate ND-Parallel: A Guide to Efficient Multi-GPU Training</title><link>https://huggingface.co/blog/accelerate-nd-parallel</link><description>Back to Articles Accelerate ND-Parallel: A guide to Efficient Multi-GPU Training Published August 8, 2025 Update on GitHub Upvote 64 +58 Salman Mohammadi smohammadi Follow axolotl-ai-co Matej Sirovatka siro1 Follow wing lian winglian Follow axolotl-ai-co Marc Sun marcsun13 Follow Dan Saunders djsaunde Follow axolotl-ai-co Contents Data Parallelism Fully Sharded Data Parallelism Tensor Parallelism Context Parallelism ND Parallelisms Hybrid Sharded Data Parallelism Fully Sharded Data Parallelism + Tensor Parallelism Fully Sharded Data Parallelism + Context Parallelism Hybrid Sharded Data Parallelism + Tensor Parallelism Usage notes Training large models across multiple GPUs can be challenging due to the complexities of different parallelism strategies. In Accelerate, together with Axolotl , we have integrated a quick and easy way to use any combination of parallelism strategies in your training script! Here is how to add it to your training script: from transformers import...</description><pubDate>Fri, 08 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/accelerate-nd-parallel</guid></item><item><title>üáµüá≠ FilBench - Can LLMs Understand and Generate Filipino?</title><link>https://huggingface.co/blog/filbench</link><description>Back to Articles üáµüá≠ FilBench - Can LLMs Understand and Generate Filipino? Published August 12, 2025 Update on GitHub Upvote 15 +9 Lj V. Miranda ljvmiranda921 Follow UD-Filipino Elyanah Aco acocodes Follow UD-Filipino Conner Manuel connermanuel Follow UD-Filipino Jan Christian Blaise Cruz jcblaise Follow UD-Filipino Jan Christian Blaise Cruz jcblaise Follow SEACrowd Joseph Imperial josephimperial Follow SEACrowd Daniel van Strien davanstrien Follow Nathan Habib SaylorTwift Follow Cl√©mentine Fourrier clefourrier Follow FilBench What did we learn from FilBench? Finding #1: Although region-specific LLMs still lag behind GPT-4, collecting data to train these models is still a promising direction Finding #2: Filipino translation is still a difficult task for LLMs Finding #3: Open LLMs Remain a Cost-Effective Choice for Filipino Language Tasks Does your LLM work on Philippine Languages? Try it on FilBench! Acknowledgements Citation As large language models (LLMs) become increasingly...</description><pubDate>Tue, 12 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/filbench</guid></item><item><title>Vision Language Model Alignment in TRL ‚ö°Ô∏è</title><link>https://huggingface.co/blog/trl-vlm-alignment</link><description>Back to Articles Vision Language Model Alignment in TRL ‚ö°Ô∏è Published August 7, 2025 Update on GitHub Upvote 81 +75 Sergio Paniego sergiopaniego Follow merve merve Follow Quentin Gallou√©dec qgallouedec Follow Kashif Rasul kashif Follow Aritra Roy Gosthipaty ariG23498 Follow Introduction Table of Contents Alignment for Vision Language Models Mixed Preference Optimization (MPO) Multimodal Group Relative Policy Optimization (GRPO) Group Sequence Policy Optimization (GSPO) Comparison Native Supervised Fine-tuning Support vLLM Integration in TRL Useful Resources Introduction Vision Language Models (VLMs) are getting stronger, but aligning them to human preferences still matters. In TRL, we already showed how to post-train VLMs with Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) . This time, we‚Äôre going further. tl;dr We have added two new multimodal alignment methods to TRL: Group Relative Policy Optimization (GRPO) , its variant Group Sequence Policy Optimization...</description><pubDate>Thu, 07 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/trl-vlm-alignment</guid></item><item><title>Welcome GPT OSS, the new open-source model family from OpenAI!</title><link>https://huggingface.co/blog/welcome-openai-gpt-oss</link><description>Back to Articles Welcome GPT OSS, the new open-source model family from OpenAI! Published August 5, 2025 Update on GitHub Upvote 492 +486 Vaibhav Srivastav reach-vb Follow Pedro Cuenca pcuenq Follow Lewis Tunstall lewtun Follow Clem ü§ó clem Follow Matthew Carrigan Rocketknight1 Follow Cl√©mentine Fourrier clefourrier Follow C√©lina Hanouti celinah Follow Lucain Pouget Wauplin Follow Marc Sun marcsun13 Follow Simon Pagezy pagezyhf Follow √Åkos Hadnagy ahadnagy Follow Joao Gante joaogante Follow Contents Overview of Capabilities and Architecture API access through Inference Providers Local Inference Using Transformers Llama.cpp vLLM transformers serve Fine-Tuning Deploy on Hugging Face Partners Azure Dell Evaluating the Model Chats and Chat Templates System and Developer Messages Tool Use With transformers Acknowledgements GPT OSS is a hugely anticipated open-weights release by OpenAI, designed for powerful reasoning, agentic tasks, and versatile developer use cases. It comprises two...</description><pubDate>Tue, 05 Aug 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/welcome-openai-gpt-oss</guid></item><item><title>Build an AI Shopping Assistant with Gradio MCP Servers</title><link>https://huggingface.co/blog/gradio-vton-mcp</link><description>Back to Articles Implementing MCP Servers in Python: An AI Shopping Assistant with Gradio Published July 31, 2025 Update on GitHub Upvote 57 +51 Freddy Boulton freddyaboulton Follow The Goal: Your Personal AI Stylist Building the Gradio MCP Server Configuring VS Code Putting It All Together Conclusion Python Developers, want to give your LLM superpowers? Gradio is the fastest way to do it! With Gradio's Model Context Protocol (MCP) integration, your LLM can plug directly into the thousands of AI models and Spaces hosted on the Hugging Face Hub . By pairing the general reasoning capabilities of LLMs with the specialized abilities of models found on Hugging Face, your LLM can go beyond simply answering text questions to actually solving problems in your daily life. For Python developers, Gradio makes implementing powerful MCP servers a breeze, offering features like: Automatic conversion of python functions into LLM tools: Each API endpoint in your Gradio app is automatically...</description><pubDate>Thu, 31 Jul 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gradio-vton-mcp</guid></item></channel></rss>