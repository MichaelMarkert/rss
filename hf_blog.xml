<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Blog</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Blog.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>How to Build an MCP Server with Gradio</title><link>https://huggingface.co/blog/gradio-mcp</link><description>Back to Articles How to Build an MCP Server in 5 Lines of Python Published April 30, 2025 Update on GitHub Upvote 67 +61 abidlabs Abubakar Abid ysharma yuvraj sharma Why Build an MCP Server? Example: Counting Letters in a Word Key features of the Gradio &lt;&gt; MCP Integration Further Reading Gradio is a Python library used by more than 1 million developers each month to build interfaces for machine learning models. Beyond just creating UIs, Gradio also exposes API capabilities and â€” now! â€” Gradio apps can be launched Model Context Protocol (MCP) servers for LLMs. This means that your Gradio app, whether it's an image generator or a tax calculator or something else entirely, can be called as a tool by an LLM. This guide will show you how to use Gradio to build an MCP server in just a few lines of Python. Prerequisites If not already installed, please install Gradio with the MCP extra: pip install "gradio[mcp]" This will install the necessary dependencies, including the mcp package....</description><pubDate>Wed, 30 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gradio-mcp</guid></item><item><title>Welcoming Llama Guard 4 on Hugging Face Hub</title><link>https://huggingface.co/blog/llama-guard-4</link><description>Back to Articles Welcoming Llama Guard 4 on Hugging Face Hub Published April 29, 2025 Update on GitHub Upvote 26 +20 merve Merve Noyan ariG23498 Aritra Roy Gosthipaty sergiopaniego Sergio Paniego pcuenq Pedro Cuenca Table-of-Contents What is Llama Guard 4? Model Details Llama Guard 4 Llama Prompt Guard 2 Getting Started using ðŸ¤— transformers Llama Prompt Guard 2 Useful Resources TL;DR: Today, Meta releases Llama Guard 4, a 12B dense (not a MoE!) multimodal safety model, and two new Llama Prompt Guard 2 models. This release comes with multiple open model checkpoints, along with an interactive notebook for you to get started easily ðŸ¤—. Model checkpoints can be found in Llama 4 Collection . Table-of-Contents What is Llama Guard 4? Model Details Llama Guard 4 Llama Prompt Guard 2 Getting Started using ðŸ¤—transformers Llama Guard 4 Llama Prompt Guard 2 Useful Resources What is Llama Guard 4? Vision and large language models deployed to production can be exploited to generate unsafe output...</description><pubDate>Tue, 29 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/llama-guard-4</guid></item><item><title>The 4 Things Qwen-3's Chat Template Teaches Us</title><link>https://huggingface.co/blog/qwen-3-chat-template-deep-dive</link><description>Back to Articles The 4 Things Qwen-3â€™s Chat Template Teaches Us Published April 30, 2025 Update on GitHub Upvote 18 +12 cfahlgren1 Caleb Fahlgren What is a Chat Template? 1. Reasoning doesn't have to be forced 2. Context Management Should be Dynamic Example 3. Tool Arguments Need Better Serialization 4. There's No Need for a Default System Prompt Conclusion What a boring Jinja snippet tells us about the new Qwen-3 model. The new Qwen-3 model by Qwen ships with a much more sophisticated chat template than it's predecessors Qwen-2.5 and QwQ. By taking a look at the differences in the Jinja template, we can find interesting insights into the new model. Chat Templates Qwen-3 Chat Template Qwen-2.5 Chat Template Qwen-QwQ Chat Template What is a Chat Template? A chat template defines how conversations between users and models are structured and formatted. The template acts as a translator, converting a human-readable conversation: [ { role : "user" , content : "Hi there!" }, { role :...</description><pubDate>Wed, 30 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/qwen-3-chat-template-deep-dive</guid></item><item><title>Tiny Agents: a MCP-powered agent in 50 lines of code</title><link>https://huggingface.co/blog/tiny-agents</link><description>Back to Articles Tiny Agents: an MCP-powered agent in 50 lines of code Published April 25, 2025 Update on GitHub Upvote 207 +201 julien-c Julien Chaumond How to run the complete demo Default model and provider Where does the code live The foundation for this: tool calling native support in LLMs. Implementing an MCP client on top of InferenceClient How to use the tools Our 50-lines-of-code Agent ðŸ¤¯ The complete while loop Next steps Over the past few weeks, I've been diving into MCP ( Model Context Protocol ) to understand what the hype around it was all about. My TL;DR is that it's fairly simple, but still quite powerful: MCP is a standard API to expose sets of Tools that can be hooked to LLMs. It is fairly simple to extend an Inference Client â€“ at HF, we have two official client SDKs: @huggingface/inference in JS, and huggingface_hub in Python â€“ to also act as a MCP client and hook the available tools from MCP servers into the LLM inference. But while doing that, came my second...</description><pubDate>Fri, 25 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/tiny-agents</guid></item><item><title>Introducing AutoRound: Intelâ€™s Advanced Quantization for LLMs and VLMs</title><link>https://huggingface.co/blog/autoround</link><description>Back to Articles What is AutoRound? Published April 29, 2025 Update on GitHub Upvote 15 +9 wenhuach wenhua cheng Intel Haihao Haihao Shen Intel weiweiz1 weiweiz1 Intel n1ck-guo Heng Guo Intel isaacmac Huang, Tai Intel kding1 Ke Ding Intel IlyasMoutawwakil Ilyas Moutawwakil marcsun13 Marc Sun medmekk Mohamed Mekkouri Superior Accuracy at Low Bit Widths 2. Broad Compatibility Models Devices Quantization Configurations Export Formats 3. Flexible/Efficient Quantization Installation Quantization and Serialization Command Line Usage AutoRound API Usage Inference CPU/Intel GPU/CUDA Convert GPTQ/AWQ to AutoRound As large language models (LLMs) and vision-language models (VLMs) continue to grow in size and complexity, deploying them efficiently becomes increasingly challenging. Quantization offers a solution by reducing model size and inference latency. Intel's AutoRound emerges as a cutting-edge quantization tool that balances accuracy, efficiency, and compatibility. AutoRound is a weight-...</description><pubDate>Tue, 29 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/autoround</guid></item><item><title>17 Reasons Why Gradio Isn't Just Another UI Library</title><link>https://huggingface.co/blog/why-gradio-stands-out</link><description>Back to Articles 17 Reasons Why Gradio Isn't Just Another UI Library Published April 16, 2025 Update on GitHub Upvote 28 +22 ysharma yuvraj sharma abidlabs Abubakar Abid Introduction 1. Universal API Access 2. Interactive API Recorder for Development 3. Fast ML Apps with Server-Side Rendering 4. Automatic Queue Management for ML Tasks 5. High-Performance Streaming for Real-Time ML Outputs 6. Integrated Multi-Page Application Support 7. New Client-Side Function Execution With Groovy 8. A Comprehensive Theming System and Modern UI Components 9. Gradio's Dynamic Interfaces 10. Visual Interface Development with Gradio Sketch 11. Progressive Web App (PWA) Support 12. In-Browser Execution with Gradio Lite 13. Accelerated Development with AI-Assisted Tooling 14. Hassle-Free App Sharing 15. Enterprise-Grade Security and Production Readiness 16. Enhanced Dataframe Component 17. Deep Links for Sharing App States Conclusion Introduction "Oh, Gradio? That's a Python library for building UIs,...</description><pubDate>Wed, 16 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/why-gradio-stands-out</guid></item><item><title>Cohere on Hugging Face Inference Providers ðŸ”¥</title><link>https://huggingface.co/blog/inference-providers-cohere</link><description>Back to Articles Cohere on Hugging Face Inference Providers ðŸ”¥ Published April 16, 2025 Update on GitHub Upvote 124 +118 reach-vb Vaibhav Srivastav burtenshaw ben burtenshaw merve Merve Noyan celinah CÃ©lina Hanouti alexrs Alejandro Rodriguez CohereLabs julien-c Julien Chaumond sbrandeis Simon Brandeis Cohere Models CohereLabs/c4ai-command-a-03-2025 ðŸ”— CohereLabs/aya-expanse-32b ðŸ”— CohereLabs/c4ai-command-r7b-12-2024 ðŸ”— CohereLabs/aya-vision-32b ðŸ”— How it works In the website UI From the client SDKs From OpenAI client Tool Use with Cohere Models Billing We're thrilled to share that Cohere is now a supported Inference Provider on HF Hub! This also marks the first model creator to share and serve their models directly on the Hub. Cohere is committed to building and serving models purpose-built for enterprise use-cases. Their comprehensive suite of secure AI solutions, from cutting-edge Generative AI to powerful Embeddings and Ranking models, are designed to tackle real-world business...</description><pubDate>Wed, 16 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/inference-providers-cohere</guid></item><item><title>Introducing HELMET</title><link>https://huggingface.co/blog/helmet</link><description>Back to Articles Introducing HELMET : Holistically Evaluating Long-context Language Models Published April 16, 2025 Update on GitHub Upvote 23 +17 hyen Howard Yen guest gaotianyu1350 Tianyu Gao guest houminmin Minmin Hou Intel kding1 Ke Ding Intel danf Daniel Fleischer Intel moshew Moshe Wasserblat Intel cdq10131 Danqi Chen guest Evaluating long-context language models is challenging but important Existing evaluations overly rely on synthetic tasks Crafting diverse, controllable, and reliable evaluation for LCLMs Key improvements over existing benchmarks LCLMs still have a long way to go on real-world tasks Diverse evaluation is needed for assessing long-context abilities Models degrade with increasing lengths and task complexity Using HELMET for future developments How to run HELMET Faster development Quick comparison with existing models Looking ahead Acknowledgements Citation Contact: hyen@cs.princeton.edu Paper: https://arxiv.org/abs/2410.02694 Website: https://princeton-...</description><pubDate>Wed, 16 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/helmet</guid></item><item><title>Hugging Face to sell open-source robots thanks to Pollen Robotics acquisition ðŸ¤–</title><link>https://huggingface.co/blog/hugging-face-pollen-robotics-acquisition</link><description>Back to Articles Hugging Face to sell open-source robots thanks to Pollen Robotics acquisition ðŸ¤– Published April 14, 2025 Update on GitHub Upvote 42 +36 thomwolf Thomas Wolf clem Clem ðŸ¤— matthieu-lapeyre Matthieu Lapeyre pollen-robotics Hugging Faceâ€™s Robotics Venture Timeline About Hugging Face About Pollen Robotics About Reachy 2 Simon Alibert and RÃ©mi CadÃ¨ne from the LeRobot team with Reachy 1 â€” Photo: LÃ©a Crespi Since Hugging Face started the LeRobot library in 2024, led by ex-Tesla lead Remi Cadene, the Hugging Face Hub has quickly become the most widely used hub and software platform for open robotics with models, datasets, spaces and libraries. Today, weâ€™re excited to take it a step further by welcoming Pollen Robotics to Hugging Face, a team that's spent the last 9 years building open-source robots and hardware. We believe robotics could be the next frontier unlocked by AI â€” and it should be open, affordable, and private. Our vision: a future where everyone in the community,...</description><pubDate>Mon, 14 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/hugging-face-pollen-robotics-acquisition</guid></item><item><title>4M Models Scanned: Protect AI + Hugging Face 6 Months In</title><link>https://huggingface.co/blog/pai-6-month</link><description>Back to Articles 4M Models Scanned: Protect AI + Hugging Face 6 Months In Published April 14, 2025 Update on GitHub Upvote 27 +21 sean-pai Sean Morgan protectai Maintaining a Zero Trust Approach to Model Security Evolving Guardianâ€™s Model Vulnerability Detection Capabilities Common attack themes Delivering Comprehensive Threat Detection for Hugging Face Users It Only Gets Better from Here Hugging Face and Protect AI partnered in October 2024 to enhance machine learning (ML) model security through Guardianâ€™s scanning technology for the community of developers who explore and use models from the Hugging Face Hub. The partnership has been a natural fit from the startâ€”Hugging Face is on a mission to democratize the use of open source AI, with a commitment to safety and security; and Protect AI is building the guardrails to make open source models safe for all. 4 new threat detection modules launched Since October, Protect AI has significantly expanded Guardian's detection capabilities,...</description><pubDate>Mon, 14 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/pai-6-month</guid></item><item><title>Hugging Face and Cloudflare Partner to Make Real-Time Speech and Video Seamless with FastRTC</title><link>https://huggingface.co/blog/fastrtc-cloudflare</link><description>Back to Articles Hugging Face and Cloudflare Partner to Make Real-Time Speech and Video Seamless with FastRTC Published April 9, 2025 Update on GitHub Upvote 23 +17 freddyaboulton Freddy Boulton Meeting a Gap in the Toolbox of AI Developers Free Access with Your Hugging Face Account Why This Matters for AI Developers Getting Started What's Next? We're excited to announce a new partnership between Cloudflare and Hugging Face that gives FastRTC developers instant access to enterprise-grade WebRTC infrastructure with a Hugging Face token. As a preview of what you can build with FastRTC and Cloudflare, check out this voice chat app built with Meta's new Llama 4 model! Meeting a Gap in the Toolbox of AI Developers As conversational AI becomes a core interface for tools, products, and services, real-time communication infrastructure is increasingly essential to support natural, multimodal interactions. Hugging Face built FastRTC to let AI developers build low-latency AI-powered audio and...</description><pubDate>Wed, 09 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/fastrtc-cloudflare</guid></item><item><title>Arabic Leaderboards: Introducing Arabic Instruction Following, Updating AraGen, and More</title><link>https://huggingface.co/blog/leaderboard-3c3h-aragen-ifeval</link><description>Back to Articles Arabic Leaderboards: Introducing Arabic Instruction Following, Updating AraGen, and More Published April 8, 2025 Update on GitHub Upvote 16 +10 alielfilali01 Ali El Filali inceptionai SarahAlBarri Sarah AlBarri inceptionai Arwa88 Abouelseoud inceptionai samta-kamboj samta kamboj inceptionai neha1710 Neha Sengupta inceptionai preslavnakov Preslav Nakov MBZUAI Arabic-Leaderboards Space Latest Updates in AraGen Leaderboard AraGen-03-25 Release Dynamic Evaluation and Ranking Analysis Instruction Following Leaderboard What is Instruction Following as a Benchmark? Dataset: Arabic IFEval Evaluation Methodology &amp; Metrics Results &amp; Analysis Upcoming Work At Inception, we have been working to enhance AI model evaluations within the Arabic language context. Previously, we introduced AraGen , one of the first generative Arabic leaderboards, serving as a benchmark for evaluating Arabic LLMs on generative tasks. As part of our ongoing efforts, we are excited to share the...</description><pubDate>Tue, 08 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/leaderboard-3c3h-aragen-ifeval</guid></item><item><title>Welcome Llama 4 Maverick &amp; Scout on Hugging Face!</title><link>https://huggingface.co/blog/llama4-release</link><description>Back to Articles Welcome Llama 4 Maverick &amp; Scout on Hugging Face Published April 5, 2025 Update on GitHub Upvote 142 +136 burtenshaw ben burtenshaw reach-vb Vaibhav Srivastav pcuenq Pedro Cuenca clem Clem ðŸ¤— rajatarya Rajat Arya xet-team jsulz Jared Sulzdorf xet-team lysandre Lysandre What is Llama 4? Features and Integrations on Hugging Face Context Length and Architecture Choices How to Use with Transformers Evaluation Scores Pre-trained models Instruction tuned models Acknowledgments References We are incredibly excited to welcome the next generation of large language models from Meta to the Hugging Face Hub: Llama 4 Maverick (~400B) and Llama 4 Scout (~109B)! ðŸ¤— Both are Mixture of Experts (MoE) models with 17B active parameters. Released today, these powerful, natively multimodal models represent a significant leap forward. We've worked closely with Meta to ensure seamless integration into the Hugging Face ecosystem, including both transformers and TGI from day one. This is just...</description><pubDate>Sat, 05 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/llama4-release</guid></item><item><title>Journey to 1 Million Gradio Users!</title><link>https://huggingface.co/blog/gradio-1m</link><description>Back to Articles Journey to 1 Million Gradio Users! Published April 4, 2025 Update on GitHub Upvote 25 +19 abidlabs Abubakar Abid 5 years ago, we launched Gradio as a simple Python library to let researchers at Stanford easily demo computer vision models with a web interface. Today, Gradio is used by &gt;1 million developers each month to build and share AI web apps. This includes some of the most popular open-source projects of all time, like Automatic1111 , Oobaboogaâ€™s Text Generation WebUI , Dall-E Mini , and LLaMA-Factory . How did we get here? How did Gradio keep growing in the very crowded field of open-source Python libraries? I get this question a lot from folks who are building their own open-source libraries. This post distills some of the lessons that I have learned over the past few years: Invest in good primitives, not high-level abstractions Embed virality directly into your library Focus on a (growing) niche Your only roadmap should be rapid iteration Maximize ways users...</description><pubDate>Fri, 04 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/gradio-1m</guid></item><item><title>The NLP Course is becoming the LLM Course!</title><link>https://huggingface.co/blog/llm-course</link><description>Back to Articles The NLP Course is becoming the LLM Course! Published April 3, 2025 Update on GitHub Upvote 87 +81 burtenshaw ben burtenshaw reach-vb Vaibhav Srivastav lewtun Lewis Tunstall fdaudens Florent Daudens pcuenq Pedro Cuenca tomaarsen Tom Aarsen coyotte508 Eliott Coyac mishig Mishig Davaadorj sergiopaniego Sergio Paniego julien-c Julien Chaumond Whatâ€™s going to happen to the NLP course material? Will there be new chapters? Will there be interactive exercises and live sessions? Whatâ€™s next? Education has always been at the heart of Hugging Faceâ€™s mission to democratize AI and weâ€™re doubling down on that by giving hf.co/learn a big upgrade! Our NLP course has been a go-to resource for the open-source AI community for the past 3 years, and itâ€™s now time for a refresh. Weâ€™re updating and expanding it to keep up with all the exciting stuff happening in AI (which is not easy when there are breakthroughs every week!) We felt the excitement during the experimental smol-course and...</description><pubDate>Thu, 03 Apr 2025 00:00:00 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/blog/llm-course</guid></item></channel></rss>