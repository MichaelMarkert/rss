<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Papers</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Papers.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in
  Large Language Models</title><link>https://huggingface.co/papers/2505.24864</link><description>Prolonged reinforcement learning training (ProRL) uncovers novel reasoning strategies in language models, outperforming base models and suggesting meaningful expansion of reasoning capabilities.  					AI-generated summary 				 Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B</description><pubDate>Mon, 02 Jun 2025 04:48:21 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24864</guid></item><item><title>AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time</title><link>https://huggingface.co/papers/2505.24863</link><description>AlphaOne dynamically modulates reasoning in large models by introducing $\alpha$ moment and Bernoulli process for slow thinking, improving efficiency and capability across diverse domains.  					AI-generated summary 				 This paper presents AlphaOne (alpha1), a universal framework for modulating reasoning progress in large reasoning models (LRMs) at test time. alpha1 first introduces alpha moment, which represents the scaled thinking phase with a universal parameter alpha. Within this scaled pre-alpha moment phase, it dynamically schedules slow thinking transitions by modeling the insertion of reasoning transition tokens as a Bernoulli stochastic process. After the alpha moment, alpha1 deterministically terminates slow thinking with the end-of-thinking token, thereby fostering fast reasoning and efficient answer generation. This approach unifies and generalizes existing monotonic scaling methods by enabling flexible and dense slow-to-fast reasoning modulation. Extensive empirical studies on various challenging benchmarks across mathematical, coding, and scientific domains demonstrate alpha1's superior reasoning capability and efficiency. Project page: https://alphaone-project.github.io/</description><pubDate>Mon, 02 Jun 2025 02:25:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24863</guid></item><item><title>Time Blindness: Why Video-Language Models Can't See What Humans Can?</title><link>https://huggingface.co/papers/2505.24867</link><description>SpookyBench is a benchmark for temporal pattern recognition in videos that highlights the limitations of vision-language models in processing noise-like frames without spatial information.  					AI-generated summary 				 Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce SpookyBench, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, text, and patterns in these sequences with over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance gap highlights a critical limitation: an over-reliance on frame-level spatial features and an inability to extract meaning from temporal cues. Furthermore, when trained in data sets with low spatial signal-to-noise ratios (SNR), temporal understanding of models degrades more rapidly than human perception, especially in tasks requiring fine-grained temporal reasoning. Overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing. Our systematic analysis shows that this issue persists across model scales and architectures. We release SpookyBench to catalyze research in temporal pattern recognition and bridge the gap between human and machine video understanding. Dataset and code has been made available on our project website: https://timeblindness.github.io/.</description><pubDate>Mon, 02 Jun 2025 06:01:40 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24867</guid></item><item><title>Don't Look Only Once: Towards Multimodal Interactive Reasoning with
  Selective Visual Revisitation</title><link>https://huggingface.co/papers/2505.18842</link><description>v1 enhances Multimodal Large Language Models by enabling selective and dynamic visual region retrieval during inference, improving performance on multimodal reasoning tasks.  					AI-generated summary 				 We present v1, a lightweight extension to Multimodal Large Language Models (MLLMs) that enables selective visual revisitation during inference. While current MLLMs typically consume visual input only once and reason purely over internal memory, v1 introduces a simple point-and-copy mechanism that allows the model to dynamically retrieve relevant image regions throughout the reasoning process. This mechanism augments existing architectures with minimal modifications, enabling contextual access to visual tokens based on the model's evolving hypotheses. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments on three multimodal mathematical reasoning benchmarks -- MathVista, MathVision, and MathVerse -- demonstrate that v1 consistently improves performance over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. Our results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. Code, models, and data will be released to support future research.</description><pubDate>Mon, 02 Jun 2025 04:28:04 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.18842</guid></item><item><title>Large Language Models for Data Synthesis</title><link>https://huggingface.co/papers/2505.14752</link><description>LLMSynthor enhances LLMs for efficient and statistically accurate data synthesis through distributional feedback and proposal sampling.  					AI-generated summary 				 Generating synthetic data that faithfully captures the statistical structure of real-world distributions is a fundamental challenge in data modeling. Classical approaches often depend on strong parametric assumptions or manual structural design and struggle in high-dimensional or heterogeneous domains. Recent progress in Large Language Models (LLMs) reveals their potential as flexible, high-dimensional priors over real-world distributions. However, when applied to data synthesis, standard LLM-based sampling is inefficient, constrained by fixed context limits, and fails to ensure statistical alignment. Given this, we introduce LLMSynthor, a general framework for data synthesis that transforms LLMs into structure-aware simulators guided by distributional feedback. LLMSynthor treats the LLM as a nonparametric copula simulator for modeling high-order dependencies and introduces LLM Proposal Sampling to generate grounded proposal distributions that improve sampling efficiency without requiring rejection. By minimizing discrepancies in the summary statistics space, the iterative synthesis loop aligns real and synthetic data while gradually uncovering and refining the latent generative structure. We evaluate LLMSynthor in both controlled and real-world settings using heterogeneous datasets in privacy-sensitive domains (e.g., e-commerce, population, and mobility) that encompass both structured and unstructured formats. The synthetic data produced by LLMSynthor shows high statistical fidelity, practical utility, and cross-data adaptability, positioning it as a valuable tool across economics, social science, urban studies, and beyond.</description><pubDate>Mon, 02 Jun 2025 03:40:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.14752</guid></item><item><title>HardTests: Synthesizing High-Quality Test Cases for LLM Coding</title><link>https://huggingface.co/papers/2505.24098</link><description>HARDTESTGEN creates a large, high-quality competitive programming dataset to enhance the precision and recall of verifiers in evaluating LLM-generated code.  					AI-generated summary 				 Verifiers play a crucial role in large language model (LLM) reasoning, needed by post-training techniques such as reinforcement learning. However, reliable verifiers are hard to get for difficult coding problems, because a well-disguised wrong solution may only be detected by carefully human-written edge cases that are difficult to synthesize. To address this issue, we propose HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this pipeline, we curate a comprehensive competitive programming dataset HARDTESTS with 47k problems and synthetic high-quality tests. Compared with existing tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points higher and recall that is 17.5 percentage points higher when evaluating LLM-generated code. For harder problems, the improvement in precision can be as large as 40 points. HARDTESTS also proves to be more effective for model training, measured by downstream code generation performance. We will open-source our dataset and synthesis pipeline at https://leililab.github.io/HardTests/.</description><pubDate>Mon, 02 Jun 2025 04:50:27 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24098</guid></item><item><title>ViStoryBench: Comprehensive Benchmark Suite for Story Visualization</title><link>https://huggingface.co/papers/2505.24862</link><description>ViStoryBench is a comprehensive evaluation benchmark for story visualization frameworks, featuring diverse datasets and metrics to assess model performance across narrative and visual dimensions.  					AI-generated summary 				 Story visualization, which aims to generate a sequence of visually coherent images aligning with a given narrative and reference images, has seen significant progress with recent advancements in generative models. To further enhance the performance of story visualization frameworks in real-world scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We collect a diverse dataset encompassing various story types and artistic styles, ensuring models are evaluated across multiple dimensions such as different plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D renderings). ViStoryBench is carefully curated to balance narrative structures and visual elements, featuring stories with single and multiple protagonists to test models' ability to maintain character consistency. Additionally, it includes complex plots and intricate world-building to challenge models in generating accurate visuals. To ensure comprehensive comparisons, our benchmark incorporates a wide range of evaluation metrics assessing critical aspects. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, fostering targeted improvements.</description><pubDate>Mon, 02 Jun 2025 07:39:52 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24862</guid></item><item><title>Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and
  Benchmarking Multimodal LLM Agents</title><link>https://huggingface.co/papers/2505.24878</link><description>Open CaptchaWorld benchmark evaluates MLLM-powered agents on diverse CAPTCHA puzzles, revealing significant performance gaps compared to humans.  					AI-generated summary 				 CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.</description><pubDate>Mon, 02 Jun 2025 03:10:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24878</guid></item><item><title>CoDA: Coordinated Diffusion Noise Optimization for Whole-Body
  Manipulation of Articulated Objects</title><link>https://huggingface.co/papers/2505.21437</link><description>A coordinated diffusion noise optimization framework improves whole-body manipulation of articulated objects by leveraging specialized diffusion models for body and hand motions and a unified basis point set representation for precise hand-object interaction.  					AI-generated summary 				 Synthesizing whole-body manipulation of articulated objects, including body motion, hand motion, and object motion, is a critical yet challenging task with broad applications in virtual humans and robotics. The core challenges are twofold. First, achieving realistic whole-body motion requires tight coordination between the hands and the rest of the body, as their movements are interdependent during manipulation. Second, articulated object manipulation typically involves high degrees of freedom and demands higher precision, often requiring the fingers to be placed at specific regions to actuate movable parts. To address these challenges, we propose a novel coordinated diffusion noise optimization framework. Specifically, we perform noise-space optimization over three specialized diffusion models for the body, left hand, and right hand, each trained on its own motion dataset to improve generalization. Coordination naturally emerges through gradient flow along the human kinematic chain, allowing the global body posture to adapt in response to hand motion objectives with high fidelity. To further enhance precision in hand-object interaction, we adopt a unified representation based on basis point sets (BPS), where end-effector positions are encoded as distances to the same BPS used for object geometry. This unified representation captures fine-grained spatial relationships between the hand and articulated object parts, and the resulting trajectories serve as targets to guide the optimization of diffusion noise, producing highly accurate interaction motion. We conduct extensive experiments demonstrating that our method outperforms existing approaches in motion quality and physical plausibility, and enables various capabilities such as object pose control, simultaneous walking and manipulation, and whole-body generation from hand-only data.</description><pubDate>Mon, 02 Jun 2025 10:27:36 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.21437</guid></item><item><title>CLaSp: In-Context Layer Skip for Self-Speculative Decoding</title><link>https://huggingface.co/papers/2505.24196</link><description>CLaSp, an in-context layer-skipping strategy for self-speculative decoding, accelerates Large Language Model decoding without additional modules or training, achieving a 1.3x to 1.7x speedup on LLaMA3 models.  					AI-generated summary 				 Speculative decoding (SD) is a promising method for accelerating the decoding process of Large Language Models (LLMs). The efficiency of SD primarily hinges on the consistency between the draft model and the verify model. However, existing drafting approaches typically require additional modules to be trained, which can be challenging to implement and ensure compatibility across various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping strategy for self-speculative decoding. Unlike prior methods, CLaSp does not require additional drafting modules or extra training. Instead, it employs a plug-and-play mechanism by skipping intermediate layers of the verify model to construct a compressed draft model. Specifically, we develop a dynamic programming algorithm that optimizes the layer-skipping process by leveraging the complete hidden states from the last verification stage as an objective. This enables CLaSp to dynamically adjust its layer-skipping strategy after each verification stage, without relying on pre-optimized sets of skipped layers. Experimental results across diverse downstream tasks demonstrate that CLaSp achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the original distribution of the generated text.</description><pubDate>Mon, 02 Jun 2025 04:53:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24196</guid></item><item><title>Vision Language Models are Biased</title><link>https://huggingface.co/papers/2505.23941</link><description>Vision language models exhibit strong biases in counting and identification tasks, demonstrating a failure mode that persist even with additional instructions or context.  					AI-generated summary 				 Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., "Adidas") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io.</description><pubDate>Mon, 02 Jun 2025 04:58:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.23941</guid></item><item><title>UniGeo: Taming Video Diffusion for Unified Consistent Geometry
  Estimation</title><link>https://huggingface.co/papers/2505.24521</link><description>Video generation models leveraging diffusion priors achieve superior global geometric attribute estimation and reconstructions, benefiting from inter-frame consistency and joint training on shared attributes.  					AI-generated summary 				 Recently, methods leveraging diffusion model priors to assist monocular geometric estimation (e.g., depth and normal) have gained significant attention due to their strong generalization ability. However, most existing works focus on estimating geometric properties within the camera coordinate system of individual video frames, neglecting the inherent ability of diffusion models to determine inter-frame correspondence. In this work, we demonstrate that, through appropriate design and fine-tuning, the intrinsic consistency of video generation models can be effectively harnessed for consistent geometric estimation. Specifically, we 1) select geometric attributes in the global coordinate system that share the same correspondence with video frames as the prediction targets, 2) introduce a novel and efficient conditioning method by reusing positional encodings, and 3) enhance performance through joint training on multiple geometric attributes that share the same correspondence. Our results achieve superior performance in predicting global geometric attributes in videos and can be directly applied to reconstruction tasks. Even when trained solely on static video data, our approach exhibits the potential to generalize to dynamic video scenes.</description><pubDate>Mon, 02 Jun 2025 02:55:45 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24521</guid></item><item><title>MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs</title><link>https://huggingface.co/papers/2505.24858</link><description>A study reveals that Large Language Models (LLMs) struggle with expressing uncertainty accurately and introduces MetaFaith, a prompt-based method that enhances their calibration significantly.  					AI-generated summary 				 A critical component in the trustworthiness of LLMs is reliable uncertainty communication, yet LLMs often use assertive language when conveying false claims, leading to over-reliance and eroded trust. We present the first systematic study of faithful confidence calibration of LLMs, benchmarking models' ability to use linguistic expressions of uncertainty that faithfully reflect their intrinsic uncertainty, across a comprehensive array of models, datasets, and prompting strategies. Our results demonstrate that LLMs largely fail at this task, and that existing interventions are insufficient: standard prompt approaches provide only marginal gains, and existing, factuality-based calibration techniques can even harm faithful calibration. To address this critical gap, we introduce MetaFaith, a novel prompt-based calibration approach inspired by human metacognition. We show that MetaFaith robustly improves faithful calibration across diverse models and task domains, enabling up to 61% improvement in faithfulness and achieving an 83% win rate over original generations as judged by humans.</description><pubDate>Mon, 02 Jun 2025 04:43:37 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24858</guid></item><item><title>EasyText: Controllable Diffusion Transformer for Multilingual Text
  Rendering</title><link>https://huggingface.co/papers/2505.24417</link><description>The paper presents EasyText, a multilingual text rendering framework using DiT that enhances rendering precision and visual quality with large datasets.  					AI-generated summary 				 Generating accurate multilingual text with diffusion models has long been desired but remains challenging. Recent methods have made progress in rendering text in a single language, but rendering arbitrary languages is still an unexplored area. This paper introduces EasyText, a text rendering framework based on DiT (Diffusion Transformer), which connects denoising latents with multilingual character tokens encoded as character tokens. We propose character positioning encoding and position encoding interpolation techniques to achieve controllable and precise text rendering. Additionally, we construct a large-scale synthetic text image dataset with 1 million multilingual image-text annotations as well as a high-quality dataset of 20K annotated images, which are used for pretraining and fine-tuning respectively. Extensive experiments and evaluations demonstrate the effectiveness and advancement of our approach in multilingual text rendering, visual quality, and layout-aware text integration.</description><pubDate>Mon, 02 Jun 2025 02:25:24 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24417</guid></item><item><title>Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual
  Large Language Models</title><link>https://huggingface.co/papers/2505.20873</link><description>The Fork-Merge Decoding strategy improves balanced multimodal understanding in audio-visual large language models by separating and then combining modality-specific reasoning.  					AI-generated summary 				 The goal of this work is to enhance balanced multimodal understanding in audio-visual large language models (AV-LLMs) by addressing modality bias without requiring additional training. In current AV-LLMs, audio and video features are typically processed jointly in the decoder. While this strategy facilitates unified multimodal understanding, it may introduce modality bias, where the model tends to over-rely on one modality due to imbalanced training signals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet effective inference-time strategy that requires no additional training or architectural modifications. FMD first performs modality-specific reasoning by processing audio-only and video-only inputs through the early decoder layers (a fork phase), and then merges the resulting hidden states for joint reasoning in the remaining layers (a merge phase). This approach promotes balanced modality contributions and leverages complementary information across modalities. We evaluate our method on two representative AV-LLMs, VideoLLaMA2 and video-SALMONN, using three benchmark datasets. Experimental results demonstrate consistent performance improvements on tasks focused on audio, video, and combined audio-visual reasoning, demonstrating the effectiveness of inference-time interventions for robust multimodal understanding.</description><pubDate>Mon, 02 Jun 2025 08:03:06 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.20873</guid></item><item><title>Harnessing Negative Signals: Reinforcement Distillation from Teacher
  Data for LLM Reasoning</title><link>https://huggingface.co/papers/2505.24850</link><description>Reinforcement Distillation (REDI) leverages both positive and negative traces to enhance large language model reasoning performance offline, outperforming traditional methods and achieving state-of-the-art results with limited open data.  					AI-generated summary 				 Recent advances in model distillation demonstrate that data from advanced reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer complex reasoning abilities to smaller, efficient student models. However, standard practices employ rejection sampling, discarding incorrect reasoning examples -- valuable, yet often underutilized data. This paper addresses the critical question: How can both positive and negative distilled reasoning traces be effectively leveraged to maximize LLM reasoning performance in an offline setting? To this end, We propose Reinforcement Distillation (REDI), a two-stage framework. Stage 1 learns from positive traces via Supervised Fine-Tuning (SFT). Stage 2 further refines the model using both positive and negative traces through our proposed REDI objective. This novel objective is a simple, reference-free loss function that outperforms established methods like DPO and SimPO in this distillation context. Our empirical evaluations demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT combined with DPO/SimPO on mathematical reasoning tasks. Notably, the Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1). Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a model post-trained on 800k proprietary data) across various mathematical reasoning benchmarks, establishing a new state-of-the-art for 1.5B models post-trained offline with openly available data.</description><pubDate>Mon, 02 Jun 2025 03:37:38 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24850</guid></item><item><title>Large Language Models are Locally Linear Mappings</title><link>https://huggingface.co/papers/2505.24293</link><description>LLMs can be approximated as linear systems for inference, offering insights into their internal representations and semantic structures without altering the models or their predictions.  					AI-generated summary 				 We demonstrate that the inference operations of several open-weight large language models (LLMs) can be mapped to an exactly equivalent linear system for an input sequence without modifying the model weights or altering output predictions. Extending techniques from image diffusion models that exhibit local or piecewise linearity, we strategically alter the gradient computation with respect to a given input sequence for a next-token prediction such that the Jacobian of the model nearly exactly reproduces the forward prediction with a linear system. We demonstrate this approach across models (Llama 3, Gemma 3, Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show through the singular value decomposition of the detached Jacobian that these LLMs operate in extremely low-dimensional subspaces where many of the largest singular vectors decode to concepts related to the most-likely output token. This approach also allows us to examine the operation of each successive layer (and its attention and MLP components) as nearly-exact linear systems and observe the emergence of semantic concepts. Despite their expressive power and global nonlinearity, modern LLMs can be interpreted through nearly-exact locally linear decompositions that provide insights into their internal representations and reveal interpretable semantic structures in the next-token prediction process.</description><pubDate>Mon, 02 Jun 2025 02:32:58 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24293</guid></item><item><title>Role-Playing Evaluation for Large Language Models</title><link>https://huggingface.co/papers/2505.13157</link><description>A benchmark called Role-Playing Eval assesses Large Language Models in role-playing across emotional understanding, decision-making, moral alignment, and in-character consistency.  					AI-generated summary 				 Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval</description><pubDate>Mon, 02 Jun 2025 08:43:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.13157</guid></item><item><title>Harnessing Large Language Models for Scientific Novelty Detection</title><link>https://huggingface.co/papers/2505.24615</link><description>A method utilizing large language models to detect scientific novelty by distilling idea-level knowledge and constructing specialized datasets in marketing and NLP domains.  					AI-generated summary 				 In an era of exponential scientific growth, identifying novel research ideas is crucial and challenging in academia. Despite potential, the lack of an appropriate benchmark dataset hinders the research of novelty detection. More importantly, simply adopting existing NLP technologies, e.g., retrieving and then cross-checking, is not a one-size-fits-all solution due to the gap between textual similarity and idea conception. In this paper, we propose to harness large language models (LLMs) for scientific novelty detection (ND), associated with two new datasets in marketing and NLP domains. To construct the considerate datasets for ND, we propose to extract closure sets of papers based on their relationship, and then summarize their main ideas based on LLMs. To capture idea conception, we propose to train a lightweight retriever by distilling the idea-level knowledge from LLMs to align ideas with similar conception, enabling efficient and accurate idea retrieval for LLM novelty detection. Experiments show our method consistently outperforms others on the proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.</description><pubDate>Mon, 02 Jun 2025 06:21:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24615</guid></item><item><title>un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via
  Inverting unCLIP</title><link>https://huggingface.co/papers/2505.24517</link><description>A generative model framework, unCLIP, is inverted to improve CLIP's ability to capture detailed visual information while maintaining text alignment.  					AI-generated summary 				 Contrastive Language-Image Pre-training (CLIP) has become a foundation model and has been applied to various vision and multimodal tasks. However, recent works indicate that CLIP falls short in distinguishing detailed differences in images and shows suboptimal performance on dense-prediction and vision-centric multimodal tasks. Therefore, this work focuses on improving existing CLIP models, aiming to capture as many visual details in images as possible. We find that a specific type of generative models, unCLIP, provides a suitable framework for achieving our goal. Specifically, unCLIP trains an image generator conditioned on the CLIP image embedding. In other words, it inverts the CLIP image encoder. Compared to discriminative models like CLIP, generative models are better at capturing image details because they are trained to learn the data distribution of images. Additionally, the conditional input space of unCLIP aligns with CLIP's original image-text embedding space. Therefore, we propose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this way, the improved image encoder can gain unCLIP's visual detail capturing ability while preserving its alignment with the original text encoder simultaneously. We evaluate our improved CLIP across various tasks to which CLIP has been applied, including the challenging MMVP-VLM benchmark, the dense-prediction open-vocabulary segmentation task, and multimodal large language model tasks. Experiments show that un^2CLIP significantly improves the original CLIP and previous CLIP improvement methods. Code and models will be available at https://github.com/LiYinqi/un2CLIP.</description><pubDate>Mon, 02 Jun 2025 06:25:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24517</guid></item><item><title>Point-MoE: Towards Cross-Domain Generalization in 3D Semantic
  Segmentation via Mixture-of-Experts</title><link>https://huggingface.co/papers/2505.23926</link><description>Point-MoE, a Mixture-of-Experts architecture, enables large-scale, cross-domain generalization in 3D perception by automatically specializing experts without domain labels.  					AI-generated summary 				 While scaling laws have transformed natural language processing and computer vision, 3D point cloud understanding has yet to reach that stage. This can be attributed to both the comparatively smaller scale of 3D datasets, as well as the disparate sources of the data itself. Point clouds are captured by diverse sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor, outdoor), each introducing unique scanning patterns, sampling densities, and semantic biases. Such domain heterogeneity poses a major barrier towards training unified models at scale, especially under the realistic constraint that domain labels are typically inaccessible at inference time. In this work, we propose Point-MoE, a Mixture-of-Experts architecture designed to enable large-scale, cross-domain generalization in 3D perception. We show that standard point cloud backbones degrade significantly in performance when trained on mixed-domain data, whereas Point-MoE with a simple top-k routing strategy can automatically specialize experts, even without access to domain labels. Our experiments demonstrate that Point-MoE not only outperforms strong multi-domain baselines but also generalizes better to unseen domains. This work highlights a scalable path forward for 3D understanding: letting the model discover structure in diverse 3D data, rather than imposing it via manual curation or domain supervision.</description><pubDate>Mon, 02 Jun 2025 05:19:08 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.23926</guid></item><item><title>EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,
  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge</title><link>https://huggingface.co/papers/2505.23009</link><description>A comprehensive TTS benchmark, EmergentTTS-Eval, automates test-case generation and evaluation using LLMs and LALM to assess nuanced and semantically complex text in speech outputs.  					AI-generated summary 				 Text-to-Speech (TTS) benchmarks often fail to capture how well models handle nuanced and semantically complex text. Building on EmergentTTS, we introduce EmergentTTS-Eval, a comprehensive benchmark covering six challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation (e.g. URLs, formulas), and questions. Crucially, our framework automates both test-case generation and evaluation, making the benchmark easily extensible. Starting from a small set of human-written seed prompts, we iteratively extend them using LLMs to target specific structural, phonetic and prosodic challenges, resulting in 1,645 diverse test cases. Moreover, we employ a model-as-a-judge approach, using a Large Audio Language Model (LALM) to assess the speech across multiple dimensions such as expressed emotion, prosodic, intonational, and pronunciation accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems, such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval, demonstrating its ability to reveal fine-grained performance differences. Results show that the model-as-a-judge approach offers robust TTS assessment and a high correlation with human preferences. We open source the evaluation https://github.com/boson-ai/EmergentTTS-Eval-public{code} and the https://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.</description><pubDate>Mon, 02 Jun 2025 02:54:22 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.23009</guid></item><item><title>DexUMI: Using Human Hand as the Universal Manipulation Interface for
  Dexterous Manipulation</title><link>https://huggingface.co/papers/2505.21864</link><description>DexUMI framework utilizes a wearable hand exoskeleton and high-fidelity robot hand inpainting to transfer dexterous manipulation skills from human hands to robot hands, achieving high task success rates.  					AI-generated summary 				 We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI includes hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap using a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. The software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86%.</description><pubDate>Mon, 02 Jun 2025 07:51:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.21864</guid></item><item><title>More Thinking, Less Seeing? Assessing Amplified Hallucination in
  Multimodal Reasoning Models</title><link>https://huggingface.co/papers/2505.21523</link><description>A new metric and benchmark are introduced to evaluate multimodal large language models' ability to maintain visual grounding while performing extended reasoning, revealing that larger models and specific training data types improve this balance.  					AI-generated summary 				 Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.</description><pubDate>Mon, 02 Jun 2025 11:40:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.21523</guid></item><item><title>ReasonGen-R1: CoT for Autoregressive Image generation models through SFT
  and RL</title><link>https://huggingface.co/papers/2505.24875</link><description>A two-stage framework ReasonGen-R1 integrates chain-of-thought reasoning and reinforcement learning to enhance image generation by imbuing models with text-based thinking skills and refining outputs through reward optimization.  					AI-generated summary 				 Although chain-of-thought reasoning and reinforcement learning (RL) have driven breakthroughs in NLP, their integration into generative vision models remains underexplored. We introduce ReasonGen-R1, a two-stage framework that first imbues an autoregressive image generator with explicit text-based "thinking" skills via supervised fine-tuning on a newly generated reasoning dataset of written rationales, and then refines its outputs using Group Relative Policy Optimization. To enable the model to reason through text before generating images, We automatically generate and release a corpus of model crafted rationales paired with visual prompts, enabling controlled planning of object layouts, styles, and scene compositions. Our GRPO algorithm uses reward signals from a pretrained vision language model to assess overall visual quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong baselines and prior state-of-the-art models. More: aka.ms/reasongen.</description><pubDate>Mon, 02 Jun 2025 15:57:16 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24875</guid></item><item><title>Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code
  Workflows</title><link>https://huggingface.co/papers/2505.24189</link><description>Fine-tuning Small Language Models yields higher quality for structured output tasks compared to prompting Large Language Models, despite reduced token costs.  					AI-generated summary 				 Large Language Models (LLMs) such as GPT-4o can handle a wide range of complex tasks with the right prompt. As per token costs are reduced, the advantages of fine-tuning Small Language Models (SLMs) for real-world applications -- faster inference, lower costs -- may no longer be clear. In this work, we present evidence that, for domain-specific tasks that require structured outputs, SLMs still have a quality advantage. We compare fine-tuning an SLM against prompting LLMs on the task of generating low-code workflows in JSON form. We observe that while a good prompt can yield reasonable results, fine-tuning improves quality by 10% on average. We also perform systematic error analysis to reveal model limitations.</description><pubDate>Mon, 02 Jun 2025 13:15:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24189</guid></item><item><title>Enabling Flexible Multi-LLM Integration for Scalable Knowledge
  Aggregation</title><link>https://huggingface.co/papers/2505.23844</link><description>A framework for adaptive selection and dynamic weighted fusion of knowledge from multiple LLMs reduces interference and improves scalability in knowledge aggregation.  					AI-generated summary 				 Large language models (LLMs) have shown remarkable promise but remain challenging to continually improve through traditional finetuning, particularly when integrating capabilities from other specialized LLMs. Popular methods like ensemble and weight merging require substantial memory and struggle to adapt to changing data environments. Recent efforts have transferred knowledge from multiple LLMs into a single target model; however, they suffer from interference and degraded performance among tasks, largely due to limited flexibility in candidate selection and training pipelines. To address these issues, we propose a framework that adaptively selects and aggregates knowledge from diverse LLMs to build a single, stronger model, avoiding the high memory overhead of ensemble and inflexible weight merging. Specifically, we design an adaptive selection network that identifies the most relevant source LLMs based on their scores, thereby reducing knowledge interference. We further propose a dynamic weighted fusion strategy that accounts for the inherent strengths of candidate LLMs, along with a feedback-driven loss function that prevents the selector from converging on a single subset of sources. Experimental results demonstrate that our method can enable a more stable and scalable knowledge aggregation process while reducing knowledge interference by up to 50% compared to existing approaches. Code is avaliable at https://github.com/ZLKong/LLM_Integration</description><pubDate>Mon, 02 Jun 2025 02:22:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.23844</guid></item><item><title>Grammars of Formal Uncertainty: When to Trust LLMs in Automated
  Reasoning Tasks</title><link>https://huggingface.co/papers/2505.20047</link><description>This research explores uncertainty quantification in large language models for generating formal specifications, introducing a PCFG framework to improve error detection and selective verification.  					AI-generated summary 				 Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC&gt;0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.</description><pubDate>Mon, 02 Jun 2025 12:33:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.20047</guid></item><item><title>GATE: General Arabic Text Embedding for Enhanced Semantic Textual
  Similarity with Matryoshka Representation Learning and Hybrid Loss Training</title><link>https://huggingface.co/papers/2505.24581</link><description>GATE models using Matryoshka Representation Learning and a hybrid loss approach achieve state-of-the-art performance on Arabic Semantic Textual Similarity benchmarks.  					AI-generated summary 				 Semantic textual similarity (STS) is a critical task in natural language processing (NLP), enabling applications in retrieval, clustering, and understanding semantic relationships between texts. However, research in this area for the Arabic language remains limited due to the lack of high-quality datasets and pre-trained models. This scarcity of resources has restricted the accurate evaluation and advance of semantic similarity in Arabic text. This paper introduces General Arabic Text Embedding (GATE) models that achieve state-of-the-art performance on the Semantic Textual Similarity task within the MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for Natural Language Inference, which are essential for enhancing model performance in tasks that demand fine-grained semantic understanding. GATE outperforms larger models, including OpenAI, with a 20-25% performance improvement on STS benchmarks, effectively capturing the unique semantic nuances of Arabic.</description><pubDate>Mon, 02 Jun 2025 10:25:53 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24581</guid></item><item><title>LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements
  Generation</title><link>https://huggingface.co/papers/2505.23832</link><description></description><pubDate>Mon, 02 Jun 2025 17:21:14 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.23832</guid></item><item><title>Context is Gold to find the Gold Passage: Evaluating and Training
  Contextual Document Embeddings</title><link>https://huggingface.co/papers/2505.24782</link><description>A context-aware benchmark and contrastive training method improve document retrieval quality by leveraging full-document context and maintaining computational efficiency.  					AI-generated summary 				 A limitation of modern document retrieval embedding methods is that they typically encode passages (chunks) from the same documents independently, often overlooking crucial contextual information from the rest of the document that could greatly improve individual chunk representations.   In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a benchmark designed to evaluate retrieval models on their ability to leverage document-wide context. Our results show that state-of-the-art embedding models struggle in retrieval scenarios where context is required. To address this limitation, we propose InSeNT (In-sequence Negative Training), a novel contrastive post-training approach which combined with late chunking pooling enhances contextual representation learning while preserving computational efficiency. Our method significantly improves retrieval quality on ConTEB without sacrificing base model performance. We further find chunks embedded with our method are more robust to suboptimal chunking strategies and larger retrieval corpus sizes. We open-source all artifacts at https://github.com/illuin-tech/contextual-embeddings.</description><pubDate>Mon, 02 Jun 2025 15:06:39 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24782</guid></item><item><title>The State of Multilingual LLM Safety Research: From Measuring the
  Language Gap to Mitigating It</title><link>https://huggingface.co/papers/2505.24119</link><description>The analysis reveals a significant language gap in LLM safety research, focusing mainly on English, and suggests recommendations for multilingual safety evaluation and crosslingual generalization to enhance global AI safety.  					AI-generated summary 				 This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publications from 2020--2024 across major NLP conferences and workshops at *ACL, we identify a significant and growing language gap in LLM safety research, with even high-resource non-English languages receiving minimal attention. We further observe that non-English languages are rarely studied as a standalone language and that English safety research exhibits poor language documentation practice. To motivate future research into multilingual safety, we make several recommendations based on our survey, and we then pose three concrete future directions on safety evaluation, training data generation, and crosslingual safety generalization. Based on our survey and proposed directions, the field can develop more robust, inclusive AI safety practices for diverse global populations.</description><pubDate>Mon, 02 Jun 2025 14:11:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.24119</guid></item><item><title>ChARM: Character-based Act-adaptive Reward Modeling for Advanced
  Role-Playing Language Agents</title><link>https://huggingface.co/papers/2505.23923</link><description>ChARM, a character-focused adaptive reward model, improves preference learning for role-playing language agents by using an act-adaptive margin and self-evolution with unlabeled data, achieving superior results on dedicated benchmarks.  					AI-generated summary 				 Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, a Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) a self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at https://github.com/calubkk/ChARM.</description><pubDate>Mon, 02 Jun 2025 16:34:55 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.23923</guid></item><item><title>Evaluating and Steering Modality Preferences in Multimodal Large
  Language Model</title><link>https://huggingface.co/papers/2505.20977</link><description>MLLMs exhibit modality bias in multimodal processing, which can be controlled using a representation engineering method to improve tasks like hallucination mitigation and multimodal machine translation.  					AI-generated summary 				 Multimodal large language models (MLLMs) have achieved remarkable performance on complex tasks with multimodal context. However, it is still understudied whether they exhibit modality preference when processing multimodal contexts. To study this question, we first build a MC\textsuperscript{2} benchmark under controlled evidence conflict scenarios to systematically evaluate modality preference, which is the tendency to favor one modality over another when making decisions based on multimodal conflicting evidence. Our extensive evaluation reveals that all 18 tested MLLMs generally demonstrate clear modality bias, and modality preference can be influenced by external interventions. An in-depth analysis reveals that the preference direction can be captured within the latent representations of MLLMs. Built on this, we propose a probing and steering method based on representation engineering to explicitly control modality preference without additional fine-tuning or carefully crafted prompts. Our method effectively amplifies modality preference toward a desired direction and applies to downstream tasks such as hallucination mitigation and multimodal machine translation, yielding promising improvements.</description><pubDate>Mon, 02 Jun 2025 13:37:10 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.20977</guid></item><item><title>Revisiting Bi-Linear State Transitions in Recurrent Neural Networks</title><link>https://huggingface.co/papers/2505.21749</link><description>Bilinear operations in recurrent neural networks are shown to be a natural bias for state tracking tasks, forming a hierarchical structure where linear recurrent networks are the simplest form.  					AI-generated summary 				 The role of hidden units in recurrent neural networks is typically seen as modeling memory, with research focusing on enhancing information retention through gating mechanisms. A less explored perspective views hidden units as active participants in the computation performed by the network, rather than passive memory stores. In this work, we revisit bi-linear operations, which involve multiplicative interactions between hidden units and input embeddings. We demonstrate theoretically and empirically that they constitute a natural inductive bias for representing the evolution of hidden states in state tracking tasks. These are the simplest type of task that require hidden units to actively contribute to the behavior of the network. We also show that bi-linear state updates form a natural hierarchy corresponding to state tracking tasks of increasing complexity, with popular linear recurrent networks such as Mamba residing at the lowest-complexity center of that hierarchy.</description><pubDate>Mon, 02 Jun 2025 14:52:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2505.21749</guid></item></channel></rss>