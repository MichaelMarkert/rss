<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Hugging Face Papers</title><link>https://huggingface.co/</link><description>This is a website scraping RSS feed for the HuggingFace Papers.</description><generator>rfeed v1.1.1</generator><docs>https://github.com/svpino/rfeed/blob/master/README.md</docs><item><title>Agentic Reasoning for Large Language Models</title><link>https://huggingface.co/papers/2601.12538</link><description>Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.  					AI-generated summary 				 Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.</description><pubDate>Thu, 22 Jan 2026 02:57:25 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.12538</guid></item><item><title>Rethinking Video Generation Model for the Embodied World</title><link>https://huggingface.co/papers/2601.15282</link><description>A comprehensive robotics benchmark evaluates video generation models across multiple task domains and embodiments, revealing deficiencies in physical realism and introducing a large-scale dataset to address training data shortages.  					AI-generated summary 				 Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.</description><pubDate>Thu, 22 Jan 2026 02:34:18 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.15282</guid></item><item><title>Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance</title><link>https://huggingface.co/papers/2601.14171</link><description>RebuttalAgent is a multi-agent framework that reframes rebuttal generation as an evidence-centric planning task, improving coverage, faithfulness, and strategic coherence in academic peer review.  					AI-generated summary 				 Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.</description><pubDate>Thu, 22 Jan 2026 03:41:23 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.14171</guid></item><item><title>MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents</title><link>https://huggingface.co/papers/2601.12346</link><description>MMDeepResearch-Bench evaluates multimodal research agents on report generation with visual evidence, revealing trade-offs between prose quality, citation accuracy, and visual grounding.  					AI-generated summary 				 Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.</description><pubDate>Thu, 22 Jan 2026 04:49:13 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.12346</guid></item><item><title>FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments</title><link>https://huggingface.co/papers/2601.07853</link><description>FinVault presents the first execution-grounded security benchmark for financial agents, revealing significant vulnerabilities in current defense mechanisms when applied to real-world financial workflows.  					AI-generated summary 				 Financial agents powered by large language models (LLMs) are increasingly deployed for investment analysis, risk assessment, and automated decision-making, where their abilities to plan, invoke tools, and manipulate mutable state introduce new security risks in high-stakes and highly regulated financial environments. However, existing safety evaluations largely focus on language-model-level content compliance or abstract agent settings, failing to capture execution-grounded risks arising from real operational workflows and state-changing actions. To bridge this gap, we propose FinVault, the first execution-grounded security benchmark for financial agents, comprising 31 regulatory case-driven sandbox scenarios with state-writable databases and explicit compliance constraints, together with 107 real-world vulnerabilities and 963 test cases that systematically cover prompt injection, jailbreaking, financially adapted attacks, as well as benign inputs for false-positive evaluation. Experimental results reveal that existing defense mechanisms remain ineffective in realistic financial agent settings, with average attack success rates (ASR) still reaching up to 50.0\% on state-of-the-art models and remaining non-negligible even for the most robust systems (ASR 6.7\%), highlighting the limited transferability of current safety designs and the need for stronger financial-specific defenses. Our code can be found at https://github.com/aifinlab/FinVault.</description><pubDate>Thu, 22 Jan 2026 04:36:01 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.07853</guid></item><item><title>Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning</title><link>https://huggingface.co/papers/2601.14750</link><description></description><pubDate>Thu, 22 Jan 2026 05:33:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.14750</guid></item><item><title>RoboBrain 2.5: Depth in Sight, Time in Mind</title><link>https://huggingface.co/papers/2601.14352</link><description></description><pubDate>Thu, 22 Jan 2026 05:33:19 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.14352</guid></item><item><title>Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models</title><link>https://huggingface.co/papers/2601.14152</link><description>Research reveals that causal attention in language models creates information bottlenecks when question-answer options follow context, leading to performance drops of over 14 percentage points compared to reversed prompt ordering.  					AI-generated summary 				 Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.</description><pubDate>Thu, 22 Jan 2026 03:00:31 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.14152</guid></item><item><title>AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization</title><link>https://huggingface.co/papers/2601.13918</link><description>AgentEHR presents a benchmark for autonomous EHR navigation requiring complex decision-making, while RetroSum framework improves performance through retrospective summarization and evolving experience strategies.  					AI-generated summary 				 Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.</description><pubDate>Thu, 22 Jan 2026 05:15:57 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.13918</guid></item><item><title>The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems</title><link>https://huggingface.co/papers/2601.15059</link><description>Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.   We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.   We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.   We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.   We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.</description><pubDate>Thu, 22 Jan 2026 04:18:54 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.15059</guid></item><item><title>XR: Cross-Modal Agents for Composed Image Retrieval</title><link>https://huggingface.co/papers/2601.14245</link><description>A multi-agent framework for compositional image retrieval that uses specialized agents for generation, filtering, and verification to improve semantic and visual query matching.  					AI-generated summary 				 Retrieval is being redefined by agentic AI, demanding multimodal reasoning beyond conventional similarity-based paradigms. Composed Image Retrieval (CIR) exemplifies this shift as each query combines a reference image with textual modifications, requiring compositional understanding across modalities. While embedding-based CIR methods have achieved progress, they remain narrow in perspective, capturing limited cross-modal cues and lacking semantic reasoning. To address these limitations, we introduce XR, a training-free multi-agent framework that reframes retrieval as a progressively coordinated reasoning process. It orchestrates three specialized types of agents: imagination agents synthesize target representations through cross-modal generation, similarity agents perform coarse filtering via hybrid matching, and question agents verify factual consistency through targeted reasoning for fine filtering. Through progressive multi-agent coordination, XR iteratively refines retrieval to meet both semantic and visual query constraints, achieving up to a 38% gain over strong training-free and training-based baselines on FashionIQ, CIRR, and CIRCO, while ablations show each agent is essential. Code is available: https://01yzzyu.github.io/xr.github.io/.</description><pubDate>Thu, 22 Jan 2026 03:35:28 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.14245</guid></item><item><title>Facilitating Proactive and Reactive Guidance for Decision Making on the Web: A Design Probe with WebSeek</title><link>https://huggingface.co/papers/2601.15100</link><description></description><pubDate>Thu, 22 Jan 2026 05:33:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.15100</guid></item><item><title>FARE: Fast-Slow Agentic Robotic Exploration</title><link>https://huggingface.co/papers/2601.14681</link><description></description><pubDate>Thu, 22 Jan 2026 05:33:20 GMT</pubDate><guid isPermaLink="true">https://huggingface.co/papers/2601.14681</guid></item></channel></rss>