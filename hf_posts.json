{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/mitkox/598805408500117", "image": "", "title": "We\u2019ve reached a point where on device AI coding that is free, offline, and capable isn\u2019t just a theoretical possibility; it\u2019s sitting on my lap, barely warming my thighs.", "content_text": "We\u2019ve reached a point where on device AI coding that is free, offline, and capable isn\u2019t just a theoretical possibility; it\u2019s sitting on my lap, barely warming my thighs. My local MacBook Air setup includes a Qwen3 Coder Flash with a 1M context, Cline in a VSCode IDE. No internet, no cloud, no ID verification- this is the forbidden tech. Current stats: All agentic tools work great local, sandboxed, and MCP OK model output precision 17 tokens/sec. Not great, not terrible 65K tokens context, the model can do 1M, but let\u2019s be real, my MacBook Air would probably achieve fusion before hitting that smoothly Standard backend and cache off for the test All inference and function calling happen locally, offline, untethered. The cloud didn\u2019t even get a memo. See translation", "url": "https://huggingface.co/posts/mitkox/598805408500117", "date_published": "2025-08-03T13:35:33.002011"}, {"id": "https://huggingface.co/posts/YerbaPage/437092763804097", "image": "", "title": "Latest work on SWE-Bench \ud83d\udc1b", "content_text": "Latest work on SWE-Bench \ud83d\udc1b Our two new papers from the SJTU & Huawei: Powered by DeepSeek-V3, we've achieved a new SOTA on the SWE-Bench benchmark! We introduce two innovative approaches: \u2694\ufe0f SWE-Debate: AI agents compete and \"debate\" to generate the best code fix. \ud83e\udde0 SWE-Exp: An AI agent learns from past repair \"experience\" to solve new issues more efficiently. \ud83d\udc47 Explore the future of software development: SWE-Debate \ud83d\udcc4 Paper: https://arxiv.org/abs/2507.23348 \ud83d\udcbb Code: https://github.com/YerbaPage/SWE-Debate SWE-Exp \ud83d\udcc4 Paper: https://arxiv.org/abs/2507.23361 \ud83d\udcbb Code: https://github.com/YerbaPage/SWE-Exp See translation", "url": "https://huggingface.co/posts/YerbaPage/437092763804097", "date_published": "2025-08-03T13:35:33.002381"}, {"id": "https://huggingface.co/posts/Abhaykoul/625756342268823", "image": "", "title": "\ud83d\ude80 Dhanishtha-2.0-preview-0825 Is Here", "content_text": "\ud83d\ude80 Dhanishtha-2.0-preview-0825 Is Here The Intermediate Thinking Model just leveled up again. With sharper reasoning, better tool use, and expanded capabilities, Dhanishtha-2.0-preview-0825 is now live and ready to impress. \ud83e\udde0 What Makes Dhanishtha Special? Unlike typical CoT models that only thinks one time, Dhanishtha thinks iteratively: > Think \u2192 Answer \u2192 Rethink \u2192 Improve \u2192 Rethink again if needed. \ud83d\udd17 Try it now: HelpingAI/Dhanishtha-2.0-preview-0825 \ud83d\udd1e Dhanishtha NSFW Preview For those exploring more expressive and immersive roleplay scenarios, we\u2019re also releasing: HelpingAI/Dhanishtha-nsfw A specialized version tuned for adult-themed interactions and character-driven roleplay. \ud83d\udd17 Explore it here: HelpingAI/Dhanishtha-nsfw \ud83d\udcac You can also try all of these live at chat.helpingai.co See translation", "url": "https://huggingface.co/posts/Abhaykoul/625756342268823", "date_published": "2025-08-03T13:35:33.002770"}, {"id": "https://huggingface.co/posts/chintankp/680096865746882", "image": "", "title": "We\u2019re excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100.", "content_text": "We\u2019re excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100. Super v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. Besides leading accuracy, it also delivers high throughput. Key features: - Leading accuracy on multi-step reasoning, math, coding, and function-calling - Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples - Fully transparent training data on HF (Nemotron-Post-Training-Dataset-v1) Try Super v1.5 on build.nvidia.com or download from Hugging Face See translation", "url": "https://huggingface.co/posts/chintankp/680096865746882", "date_published": "2025-08-03T13:35:33.003139"}, {"id": "https://huggingface.co/posts/prithivMLmods/210600524016945", "image": "", "title": "Introducing Camel-Doc-OCR-080125(v2), a document content-structure retrieval VLM designed for content extraction and summarization. This is the second model in the Camel Doc OCR VLM series, following Camel-Doc-OCR-062825(v1). The new version fixes formal table reconstruction issues in both en and zh language, achieving optimal performance for long-context inferences.\ud83e\udd17\ud83d\udc2a", "content_text": "Introducing Camel-Doc-OCR-080125(v2), a document content-structure retrieval VLM designed for content extraction and summarization. This is the second model in the Camel Doc OCR VLM series, following Camel-Doc-OCR-062825(v1). The new version fixes formal table reconstruction issues in both en and zh language, achieving optimal performance for long-context inferences.\ud83e\udd17\ud83d\udc2a \u2937 Camel-Doc-OCR(v2) : prithivMLmods/Camel-Doc-OCR-080125 \u2937 Camel-Doc-OCR(v1) : prithivMLmods/Camel-Doc-OCR-062825 \u2937 Demo : prithivMLmods/core-OCR Multimodal Model Collections and Spaces: \u279d Camel-Doc-OCR : prithivMLmods/camel-doc-ocr-080125-688c0c61c5dba648756f31f8 \u279d Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a \u279d Multimodal Spaces : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 \u279d Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 . . . To know more about it, visit the model card of the respective model. !! See...", "url": "https://huggingface.co/posts/prithivMLmods/210600524016945", "date_published": "2025-08-03T13:35:33.003570"}, {"id": "https://huggingface.co/posts/merve/770620321222949", "image": "", "title": "Cohere just dropped", "content_text": "Cohere just dropped CohereLabs/command-a-vision-07-2025 , a 112B (dense!) vision LM > based on SigLIP2 & Command-A > built for enterprise use cases \ud83d\udd25 > use with Inference Providers or transformers \ud83e\udd17 read their blog https://huggingface.co/blog/CohereLabs/introducing-command-a-vision-07-2025 See translation", "url": "https://huggingface.co/posts/merve/770620321222949", "date_published": "2025-08-03T13:35:33.003823"}, {"id": "https://huggingface.co/posts/AdinaY/564352975503737", "image": "", "title": "Qwen team did it again!!", "content_text": "Qwen team did it again!! They just released Qwen3-Coder-30B-A3B-Instruct on the hub\ud83d\udd25 Qwen/Qwen3-Coder-30B-A3B-Instruct \u2728 Apache 2.0 \u272830B total / 3.3B active (128 experts, 8 top-k) \u2728 Native 256K context, extendable to 1M via Yarn \u2728 Built for Agentic Coding See translation", "url": "https://huggingface.co/posts/AdinaY/564352975503737", "date_published": "2025-08-03T13:35:33.004066"}, {"id": "https://huggingface.co/posts/Parveshiiii/913365249348370", "image": "", "title": "\ud83d\ude80 Launch Alert: Dev-Stack-Agents", "content_text": "\ud83d\ude80 Launch Alert: Dev-Stack-Agents Meet your 50-agent senior AI team \u2014 principal-level experts in engineering, AI, DevOps, security, product, and more \u2014 all bundled into one modular repo. + Code. Optimize. Scale. Secure. - Full-stack execution, Claude-powered. No human bottlenecks. \ud83d\udd27 Built for Claude Code Seamlessly plug into Claude\u2019s dev environment: * \ud83e\udde0 Each .md file = a fully defined expert persona * \u2699\ufe0f Claude indexes them as agents with roles, skills & strategy * \ud83e\udd16 You chat \u2192 Claude auto-routes to the right agent(s) * \u270d\ufe0f Want precision? Just call @agent-name directly * \ud83d\udc65 Complex task? Mention multiple agents for team execution Examples: \"@security-auditor please review auth flow for risks\" \"@cloud-architect + @devops-troubleshooter \u2192 design a resilient multi-region setup\" \"@ai-engineer + @legal-advisor \u2192 build a privacy-safe RAG pipeline\" \ud83d\udd17 https://github.com/Parveshiiii/Dev-Stack-Agents MIT License | Claude-Ready | PRs Welcome See translation", "url": "https://huggingface.co/posts/Parveshiiii/913365249348370", "date_published": "2025-08-03T13:35:33.004504"}, {"id": "https://huggingface.co/posts/prithivMLmods/923940739727688", "image": "", "title": "Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, we\u2019ve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources.", "content_text": "Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, we\u2019ve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources. \u2728 Multimodal-VLM-Thinking : prithivMLmods/Multimodal-VLM-Thinking \u2728 Multimodal-VLM-OCR : prithivMLmods/Multimodal-VLM-OCR \u2726 Models used in these spaces: \u2728 Lumian-VLR-7B-Thinking : prithivMLmods/Lumian-VLR-7B-Thinking \u2728 Enesidaon-VLR-7B-no-Thinking : prithivMLmods/Enesidaon-VLR-7B-no-Thinking \u2728 GLM-4.1V-9B-Thinking : zai-org/GLM-4.1V-9B-Thinking \u2728 DREX-062225-exp : prithivMLmods/DREX-062225-exp & more ... \u2726 Multimodal Model Collections and Spaces: \u2728 Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a \u2728 Multimodal Spaces : prithivMLmods/multimodal-...", "url": "https://huggingface.co/posts/prithivMLmods/923940739727688", "date_published": "2025-08-03T13:35:33.004962"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/425722660691765", "image": "", "title": "Wan 2.2 & FLUX Krea Full Tutorial - Automated Install - Ready Perfect Presets - SwarmUI with ComfyUI - Install Wan 2.2 and FLUX Krea with literally 1-click and use our pre-made most amazing quality presets :", "content_text": "Wan 2.2 & FLUX Krea Full Tutorial - Automated Install - Ready Perfect Presets - SwarmUI with ComfyUI - Install Wan 2.2 and FLUX Krea with literally 1-click and use our pre-made most amazing quality presets : https://youtu.be/8MvvuX4YPeo https://youtu.be/8MvvuX4YPeo Video Chapters 0:00 Introduction: The Ultimate Wan 2.2 Tutorial with Optimized Presets 1:03 Free Prompt Generation Tool & Introducing the New FLUX Krea Dev Model 2:01 How SwarmUI & ComfyUI Enable Video Generation on Low-End Hardware 2:46 Quick Start Guide: Downloading the Latest SwarmUI & ComfyUI Installers 3:10 Step-by-Step: How to Update or Perform a Fresh Installation of ComfyUI 3:51 Step-by-Step: How to Update or Perform a Fresh Installation of SwarmUI 4:18 Essential Setup: Configuring the SwarmUI Backend for ComfyUI 4:53 One-Click Setup: Downloading All Required Wan 2.2 Models Automatically 5:46 Importing the Ultimate SwarmUI Presets Pack for Best Results 6:22 Wan 2.2 Image-to-Video Generation: A Complete Step-by-...", "url": "https://huggingface.co/posts/MonsterMMORPG/425722660691765", "date_published": "2025-08-03T13:35:33.005483"}]}