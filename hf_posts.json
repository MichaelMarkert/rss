{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/mike-ravkine/324105560308241", "image": "", "title": "Let's talk about one of the hidden gems in the ReasonScape evaluation results, lucky #13:", "content_text": "Let's talk about one of the hidden gems in the ReasonScape evaluation results, lucky #13: aquif-ai/aquif-3.5-8B-Think Built on top of the solid Qwen3-8B foundation, aquif-3.5-8B-Think successfully preserves the high performance of the original model while consuming 30-50% less reasoning tokens. The most notable regression vs the base model here is in arithmetic - if your workload is math heavy this model demonstrates an unfortunate collapse with performance under growing complexity. The interesting combination of awesome overall performance on SVG simple shapes identification coupled with a total inability to recognize more complex shapes like 'House' or 'Arrow' is a behavior directly inherited from the base model (but with a ~20% improvement in token utilization). If you like your reasoning models token-efficient, Aquif-3.5-8B-Think is well worth a spin. Higher resolution, more detailed, interactive plots are available at the m12X explorer: https://reasonscape.com/m12x/explorer/...", "url": "https://huggingface.co/posts/mike-ravkine/324105560308241", "date_published": "2025-10-16T05:22:05.168308"}, {"id": "https://huggingface.co/posts/kanaria007/662291776092926", "image": "", "title": "\u2705 New Article: *Humor as Structured Protocol*", "content_text": "\u2705 New Article: *Humor as Structured Protocol* Title: \ud83c\udfad Humor as Structured Protocol: Joke-Protocols as Emotion Regulation and AGI Design Resource \ud83d\udd17 https://huggingface.co/blog/kanaria007/humor-as-structured-protocol --- Summary: Humor isn\u2019t a distraction \u2014 it\u2019s a *protocol*. By framing paradox as *benign*, humor vents overload, resets attention, and enables safe re-entry to difficult topics. In Structured Intelligence terms, jokes are *bounded anomalies* that discharge tension without breaking identity or trust. > Laughter is relief. > *Humor is the design that makes relief safe.* --- Why It Matters: \u2022 Turns \u201ccomic timing\u201d into *recoverable state transitions* (no denial, no collapse) \u2022 Gives teams and products a *de-escalation primitive* that preserves dignity \u2022 Informs *AI/UX safety*: sandboxed incongruity, ethical gates, clear exit paths --- What\u2019s Inside: \u2022 The Humor Protocol: trigger \u2192 incongruity \u2192 benign boundary \u2192 release \u2192 re-entry \u2022 Patterns: irony, self-deprecation,...", "url": "https://huggingface.co/posts/kanaria007/662291776092926", "date_published": "2025-10-16T05:22:05.168865"}, {"id": "https://huggingface.co/posts/ronantakizawa/301388923540512", "image": "", "title": "Released an AWQ quantized version of BosonAI\u2019s Higgs-Llama-3-70B model! \ud83c\udf89", "content_text": "Released an AWQ quantized version of BosonAI\u2019s Higgs-Llama-3-70B model! \ud83c\udf89 The Higgs-Llama-3-70B is an LLM specialized in role-playing, useful for game characters. Using an NVIDIA B200 GPU, I was able to compress the huge 140GB model into 37GB while keeping minimal perplexity \ud83d\udc4d ronantakizawa/higgs-llama-3-70b-awq See translation", "url": "https://huggingface.co/posts/ronantakizawa/301388923540512", "date_published": "2025-10-16T05:22:05.169130"}, {"id": "https://huggingface.co/posts/s3nh/172255383269757", "image": "", "title": "Just tried to create an educational assistant for younger people who can struggle with visualsation of 'what is this sorcery all about'.", "content_text": "Just tried to create an educational assistant for younger people who can struggle with visualsation of 'what is this sorcery all about'. Its first step of my spare time projects, sft on Qwen3-8B, EduHelper is a child-friendly tutoring assistant fine-tuned from the Qwen3-8B base model using parameter-efficient fine-tuning (PEFT) with LoRA on the ajibawa-2023/Education-Young-Children dataset. s3nh/EduHelp-8B Glad to share my work, have a wonderful day! See translation", "url": "https://huggingface.co/posts/s3nh/172255383269757", "date_published": "2025-10-16T05:22:05.169409"}, {"id": "https://huggingface.co/posts/andywu-kby/521155221047550", "image": "", "title": "\ud83d\udce2 Product Update: SalesPilot 1.2 Released!", "content_text": "\ud83d\udce2 Product Update: SalesPilot 1.2 Released! \ud83d\udd27 What\u2019s New: - Sales Forecasting, Sales Analysis using Excel - No technical skills required - Dashboard and Delete Functionality - Chatbot Application https://miragic.ai/products/sales-pilot Looking forward to your feedback! See translation", "url": "https://huggingface.co/posts/andywu-kby/521155221047550", "date_published": "2025-10-16T05:22:05.169653"}, {"id": "https://huggingface.co/posts/Nymbo/670905099951674", "image": "", "title": "I've made some improvements to my custom Deep_Research tool in the", "content_text": "I've made some improvements to my custom Deep_Research tool in the Nymbo/Tools MCP server. I've added a second LLM process and it still takes less than 1 minute to complete! The original version of my Deep_Research tool would basically dump up to 50 fetched webpages onto the Researcher model ( Qwen3-235B ), with only a little bit of context shown from each page. # New \"Filterer\" Process The new process includes another LLM call before the researcher process. The Filterer (also Qwen3-235B ) gets the query summary and the original 50 pages with low context, and decides which pages are most relevant to the research topic. The Filterer then outputs the URLs to the relevant pages, which are then re-fetched (with more context) and sent to the Researcher. # Researcher Context The Researcher now gets only the relevant webpages, then begins writing the report. When testing with 50 initial results, the researcher would often end up with 10-20 results of relevant context. It still takes less...", "url": "https://huggingface.co/posts/Nymbo/670905099951674", "date_published": "2025-10-16T05:22:05.170063"}, {"id": "https://huggingface.co/posts/piercus/778833977889788", "image": "", "title": "We have trained a LBM-Eraser with RORD-Dataset in the open \ud83d\udd25", "content_text": "We have trained a LBM-Eraser with RORD-Dataset in the open \ud83d\udd25 \ud83d\ude80 1-step only inference, no distillation \ud83e\udeb6 Light backbone :SD1.5 \ud83e\udde0 Light training : converge in 6k steps Now let's improve this, especially the inpainting capabilities. Stay tuned for more :-) LBM paper : LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) Our LBM fork : https://github.com/finegrain-ai/LBM See translation", "url": "https://huggingface.co/posts/piercus/778833977889788", "date_published": "2025-10-16T05:22:05.170354"}, {"id": "https://huggingface.co/posts/prithivMLmods/280533880488225", "image": "", "title": "Introducing Image-Guard-2.0, an experimental, lightweight vision-language encoder model with a size of 0.1B (<100M parameters), trained on SigLIP2 (siglip2-base-patch16-224). Designed for multi-label image classification tasks, this model functions as an image safety system, serving as an image guard or moderator across a wide range of categories, from anime to realistic imagery.", "content_text": "Introducing Image-Guard-2.0, an experimental, lightweight vision-language encoder model with a size of 0.1B (<100M parameters), trained on SigLIP2 (siglip2-base-patch16-224). Designed for multi-label image classification tasks, this model functions as an image safety system, serving as an image guard or moderator across a wide range of categories, from anime to realistic imagery. \u26a1blog-article: https://huggingface.co/blog/prithivMLmods/image-guard-models It also performs strict moderation and filtering of artificially synthesized content, demonstrating strong detection and handling of explicit images. Image-Guard-2.0 delivers robust performance in streamlined scenarios, ensuring reliable and effective classification across diverse visual inputs. See translation", "url": "https://huggingface.co/posts/prithivMLmods/280533880488225", "date_published": "2025-10-16T05:22:05.170719"}, {"id": "https://huggingface.co/posts/mrmanna/327383930726574", "image": "", "title": "Stop Designing Workflows, Design Capabilities, and Let Models Plan at Runtime", "content_text": "Stop Designing Workflows, Design Capabilities, and Let Models Plan at Runtime > Start Governing Capabilities in Enterprise Agent Development with Agentic Contract Model (ACM) https://cloudoffice.io/stop-designing-workflows-design-capabilities-and-let-models-plan-at-runtime-f49265496196 See translation", "url": "https://huggingface.co/posts/mrmanna/327383930726574", "date_published": "2025-10-16T05:22:05.170937"}, {"id": "https://huggingface.co/posts/jlopez-dl/435039183324892", "image": "", "title": "Just posted", "content_text": "Just posted https://huggingface.co/blog/jlopez-dl/hybrid-attention-game-changer , for those interested in Hybrid Attention See translation", "url": "https://huggingface.co/posts/jlopez-dl/435039183324892", "date_published": "2025-10-16T05:22:05.171124"}]}