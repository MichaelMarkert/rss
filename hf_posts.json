{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Ujjwal-Tyagi/869541950904709", "image": "", "title": "So, Koreans are also doing great progress behind Chinese,", "content_text": "So, Koreans are also doing great progress behind Chinese, Their two open source ai models that are actually good in coding. upstage/Solar-Open-100B skt/A.X-K1 See translation", "url": "https://huggingface.co/posts/Ujjwal-Tyagi/869541950904709", "date_published": "2026-01-21T17:57:00.655065"}, {"id": "https://huggingface.co/posts/marksverdhei/460500590246249", "image": "", "title": "Inspired by the heroes of day zero quants (", "content_text": "Inspired by the heroes of day zero quants ( @ TheBloke @ danielhanchen @ shimmyshimmer @ bartowski ), I decided to join the race by releasing the first FP8 quant of glm-4.7-flash! Not as easy as i expected, but I'm happy i was still able to have it working within a few hours after the original model was released! Interested in feedback if anyone wants to try it out! marksverdhei/GLM-4.7-Flash-FP8 Note: If my PR to vLLM isn't merged yet you might have to use my fork. Cheers! \ud83e\udd17 See translation", "url": "https://huggingface.co/posts/marksverdhei/460500590246249", "date_published": "2026-01-21T17:57:00.655449"}, {"id": "https://huggingface.co/posts/danielhanchen/143027024579647", "image": "", "title": "Run GLM-4.7-Flash locally on your device with 24GB RAM!\ud83d\udd25", "content_text": "Run GLM-4.7-Flash locally on your device with 24GB RAM!\ud83d\udd25 It's the best performing 30B model on SWE-Bench and GPQA. With 200K context, it excels at coding, agents, chat & reasoning. GGUF: unsloth/GLM-4.7-Flash-GGUF Guide: https://unsloth.ai/docs/models/glm-4.7-flash See translation", "url": "https://huggingface.co/posts/danielhanchen/143027024579647", "date_published": "2026-01-21T17:57:00.655735"}, {"id": "https://huggingface.co/posts/AdinaY/893023705771972", "image": "", "title": "Z.ai just released a powerful lightweight option of GLM 4.7", "content_text": "Z.ai just released a powerful lightweight option of GLM 4.7 \u2728 30B total/3B active - MoE zai-org/GLM-4.7-Flash See translation", "url": "https://huggingface.co/posts/AdinaY/893023705771972", "date_published": "2026-01-21T17:57:00.655963"}, {"id": "https://huggingface.co/posts/projectlosangeles/732365874551092", "image": "", "title": "Check out Orpheus Karaoke! Turn any MIDI into a unique Karaoke MIDI!", "content_text": "Check out Orpheus Karaoke! Turn any MIDI into a unique Karaoke MIDI! projectlosangeles/Orpheus-Karaoke See translation", "url": "https://huggingface.co/posts/projectlosangeles/732365874551092", "date_published": "2026-01-21T17:57:00.656170"}, {"id": "https://huggingface.co/posts/paulpham157/197160445199465", "image": "", "title": "Two things to know right before starting:", "content_text": "Two things to know right before starting: - Learn Git. Git is a great versioning tool, even when working alone. It's also essential when working in a team. Don't make excuses that you only do DL and can't do software development. \ud83d\ude1e Don't create files like: main_backup_1.py main_backup_2.py main_backup_3.py anymore... (It sounds ridiculous, but I've actually seen some people do that... weren't students.) - Try to keep everything stable. Imagine encountering errors during a demo. Keep the code clean so it runs smoothly and is maintainable. Always minimize DevOps steps to ensure quick reboot (this can be covered by some platforms; thanks to Hugging Face for making it easy and providing a basic infrastructure that most people can access almost for free). \ud83e\udd24 See translation", "url": "https://huggingface.co/posts/paulpham157/197160445199465", "date_published": "2026-01-21T17:57:00.656571"}, {"id": "https://huggingface.co/posts/efecelik/213200184330880", "image": "", "title": "Interesting paper: PhysRVG", "content_text": "Interesting paper: PhysRVG The core idea: instead of treating physics as a soft condition the model can work around during optimization, enforce it strictly via reinforcement learning. The paper focuses on rigid body dynamics - collisions, pendulums, free fall, rolling. PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models (2601.11087) See translation", "url": "https://huggingface.co/posts/efecelik/213200184330880", "date_published": "2026-01-21T17:57:00.656842"}, {"id": "https://huggingface.co/posts/nyuuzyou/331224318760046", "image": "", "title": "\ud83c\udfdb\ufe0f Google Code Archive Dataset -", "content_text": "\ud83c\udfdb\ufe0f Google Code Archive Dataset - nyuuzyou/google-code-archive Expanding beyond the modern code series, this release presents a massive historical snapshot from the Google Code Archive. This dataset captures the open-source landscape from 2006 to 2016, offering a unique time capsule of software development patterns during the era before GitHub's dominance. Key Stats: - 65,825,565 files from 488,618 repositories - 47 GB compressed Parquet storage - 454 programming languages (Heavily featuring Java, PHP, and C++) - Extensive quality filtering (excluding vendor code and build artifacts) - Rich historical metadata: original repo names, file paths, and era-specific licenses This is one of those releases that I'm most interested in getting feedback on. Would you like to see more old code datasets? See translation", "url": "https://huggingface.co/posts/nyuuzyou/331224318760046", "date_published": "2026-01-21T17:57:00.657253"}, {"id": "https://huggingface.co/posts/unmodeled-tyler/422829843470899", "image": "", "title": "NEW MODEL:", "content_text": "NEW MODEL: vanta-research/mox-small-1 Mox-Small-1 has landed on the Hub! Finetuned from the fantastic Olmo3.1 32B architecture by AllenAI, Mox-Small-1 was trained using the same datasets and methodology as Mox-Tiny-1, making this model our second addition to the Mox-1 family of models. Mox-1 is designed to prioritize clarity, honesty, and genuine utility over blind agreement. These models are perfect for when you want to be challenged in a constructive, helpful way. By utilizing Olmo3.1 32B's architecture, Mox-Small-1 brings greater conversational depth and reasoning quality to the Mox-1 model family. Check it out! See translation", "url": "https://huggingface.co/posts/unmodeled-tyler/422829843470899", "date_published": "2026-01-21T17:57:00.657557"}, {"id": "https://huggingface.co/posts/lorraine2/285073238542136", "image": "", "title": "\ud83d\udcfd\ufe0f New NVIDIA paper: Motion Attribution for Video Generation \ud83d\udcfd\ufe0f", "content_text": "\ud83d\udcfd\ufe0f New NVIDIA paper: Motion Attribution for Video Generation \ud83d\udcfd\ufe0f We propose MOTIVE, a method for taking query video clips and identifying which training data will improve or degrade performance after finetuning, enabling sophisticated data curation and beyond! \ud83d\udd0e Project Page: https://research.nvidia.com/labs/sil/projects/MOTIVE/ \ud83d\udcd6 Full Paper: https://arxiv.org/abs/2601.08828 Check out more work from the NVIDIA Spatial Intelligence Lab here: https://research.nvidia.com/labs/sil/ This project was led by the great work of Xindi(Cindy) Wu, along with Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taix\u00e9, Olga Russakovsky, and Sanja Fidler. See translation", "url": "https://huggingface.co/posts/lorraine2/285073238542136", "date_published": "2026-01-21T17:57:00.657906"}]}