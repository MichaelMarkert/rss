{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Tonic/414083244384754", "image": "", "title": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f Normalize adding compute & runtime traces to your model cards", "content_text": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f Normalize adding compute & runtime traces to your model cards See translation", "url": "https://huggingface.co/posts/Tonic/414083244384754", "date_published": "2025-07-14T13:39:17.157253"}, {"id": "https://huggingface.co/posts/AdinaY/423045666935241", "image": "", "title": "Kimi-K2 is now available on the hub\ud83d\udd25\ud83d\ude80", "content_text": "Kimi-K2 is now available on the hub\ud83d\udd25\ud83d\ude80 This is a trillion-parameter MoE model focused on long context, code, reasoning, and agentic behavior. moonshotai/kimi-k2-6871243b990f2af5ba60617d \u2728 Base & Instruct \u2728 1T total / 32B active - Modified MIT License \u2728 128K context length \u2728 Muon optimizer for stable trillion-scale training See translation", "url": "https://huggingface.co/posts/AdinaY/423045666935241", "date_published": "2025-07-14T13:39:17.157536"}, {"id": "https://huggingface.co/posts/hesamation/850768471232119", "image": "", "title": "in case you didn\u2019t know, Claude now has a developer training course with certificates,", "content_text": "in case you didn\u2019t know, Claude now has a developer training course with certificates, this is better than anything you can find on Coursera. covers Claude Code, MCP and its advanced topics and even more: https://www.anthropic.com/learn/build-with-claude See translation", "url": "https://huggingface.co/posts/hesamation/850768471232119", "date_published": "2025-07-14T13:39:17.157836"}, {"id": "https://huggingface.co/posts/Kseniase/384482543815919", "image": "", "title": "13 New types of LoRA", "content_text": "13 New types of LoRA LoRA (Low-Rank Adaptation) is a popular lightweight method for fine-tuning AI models. It doesn't update the full model, it adds small trainable components, low-rank matrices, while keeping the original weights frozen. Only these adapters are trained. Recently, many interesting new LoRA variations came out, so it\u2019s a great time to take a look at these 13 clever approaches: 1. T-LoRA \u2192 T-LoRA: Single Image Diffusion Model Customization Without Overfitting (2507.05964) A timestep-dependent LoRA method for adapting diffusion models with a single image. It dynamically adjusts updates and uses orthogonal initialization to reduce overlap, achieving better fidelity\u2013alignment balance than standard LoRA 2. SingLoRA \u2192 SingLoRA: Low Rank Adaptation Using a Single Matrix (2507.05566) Simplifies LoRA by using only one small matrix instead of usual two, and multiplying it by its own transpose (like A \u00d7 A\u1d40). It uses half the parameters of LoRA and avoids scale mismatch between...", "url": "https://huggingface.co/posts/Kseniase/384482543815919", "date_published": "2025-07-14T13:39:17.158522"}, {"id": "https://huggingface.co/posts/DawnC/760622875415705", "image": "", "title": "\ud83c\udfaf Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science!", "content_text": "\ud83c\udfaf Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science! This isn't just another computer vision project. VisionScout represents a fundamental shift from simple object detection to genuine scene understanding, where four specialized AI models work together to interpret what's actually happening in an image. \ud83c\udfd7\ufe0f Part 1: Architecture Foundation How careful system design transforms independent models into collaborative intelligence through proper layering and coordination strategies. \u2699\ufe0f Part 2: Deep Technical Implementation The five core algorithms powering the system: dynamic weight adjustment, attention mechanisms, statistical methods, lighting analysis, and CLIP's zero-shot learning. \ud83c\udf0d Part 3: Real-World Validation Concrete case studies from indoor spaces to cultural landmarks, demonstrating how integrated systems deliver insights no single model could achieve. What makes this valuable:...", "url": "https://huggingface.co/posts/DawnC/760622875415705", "date_published": "2025-07-14T13:39:17.159065"}, {"id": "https://huggingface.co/posts/prithivMLmods/700925755780035", "image": "", "title": "Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. \ud83e\udd17\ud83d\udcc4", "content_text": "Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. \ud83e\udd17\ud83d\udcc4 Spaces & Models : > Doc-VLMs-OCR : prithivMLmods/Doc-VLMs-OCR > core-OCR : prithivMLmods/core-OCR > Megalodon-OCR (3B) : prithivMLmods/Megalodon-OCR-Sync-0713 > Perseus-Doc-vl (7B): prithivMLmods/Perseus-Doc-vl-0712 Datasets Caption Mix : > Corvus-OCR-Caption-Mix : prithivMLmods/Corvus-OCR-Caption-Mix > Corvus-OCR-Caption-Mini-Mix : prithivMLmods/Corvus-OCR-Caption-Mini-Mix Collections : > Corvus OCR Caption Mix: prithivMLmods/corvus-ocr-caption-mix-687349bfaceffbd10976f0cc > Captioning / OCR / DocTable : prithivMLmods/captioning-ocr-doctable-687382e1da822008bb5c06f2 GitHub : > OCR-ReportLab :...", "url": "https://huggingface.co/posts/prithivMLmods/700925755780035", "date_published": "2025-07-14T13:39:17.159604"}, {"id": "https://huggingface.co/posts/hba123/150628355666585", "image": "", "title": "In our latest paper, Bourbaki (7b), we show how one can achieve state-of-the-art 7B theorem provers on PutnamBench by applying MCTS to what we call self-generated and goal-conditioned MDPs. I started a series of Blogs on this!", "content_text": "In our latest paper, Bourbaki (7b), we show how one can achieve state-of-the-art 7B theorem provers on PutnamBench by applying MCTS to what we call self-generated and goal-conditioned MDPs. I started a series of Blogs on this! Why a series of Blogs \ud83d\ude1d? I want to try to make everyone understand what Bourbaki (7b) is and what it does. I don't want to just give you a ChatGPT summary with some result hype. I think there are many things to improve, and I am hoping with more exposure to this, beyond experiments and codes, some people would be interested and help us improve it! In this first blog, we will be talking basics: 1) MCTS and why it should be applied to LLMs so that the whole world is not just fine-tuning a 100000000000000000000000 b model on 10 data points (not that i have not done it before \ud83e\udd2a\ud83e\udd2a), 2) the basics of MDPs, and 3) the Vanilla MCTS algorithm. Check it out: https://huggingface.co/blog/hba123/bourbaki7b If you find it useful, consider upvoting and sharing this post and...", "url": "https://huggingface.co/posts/hba123/150628355666585", "date_published": "2025-07-14T13:39:17.160035"}, {"id": "https://huggingface.co/posts/3LC/827651059369427", "image": "", "title": "\ud83d\ude80 Announcing the Synthetic-to-Real Multi-Class Object Detection Challenge!", "content_text": "\ud83d\ude80 Announcing the Synthetic-to-Real Multi-Class Object Detection Challenge! We\u2019re excited to announce the launch of the Synthetic-to-Real Multi-Class Object Detection Challenge\u2014now live on Kaggle! This exciting competition is brought to you by 3LC in partnership with Duality AI, creators of the powerful FalconCloud tool for generating targeted synthetic data. Together, we're offering a unique opportunity to push the boundaries of object detection through high-fidelity, simulation-to-real workflows. \ud83e\uddea What Makes This Challenge Special? \ud83d\udcbb Create customized training data with Duality\u2019s cloud-based scenario \ud83e\udde0 Analyze data weaknesses and take precise, data-driven actions using 3LC's robust tooling \u2699\ufe0f Optimize data for peak model training \ud83c\udfc6 Why Join? \u2022 Win cash prizes, certificates, and global recognition \u2022 Gain exposure to real-world simulation workflows used in top AI companies \u2022 Collaborate and compete with leading minds in computer vision, ML, and AI Whether you're a student,...", "url": "https://huggingface.co/posts/3LC/827651059369427", "date_published": "2025-07-14T13:39:17.160522"}, {"id": "https://huggingface.co/posts/kanaria007/210554569109150", "image": "", "title": "\u2705 New Article on Hugging Face: Teaching AI to Think Like a System \u2014 Not a Toolkit", "content_text": "\u2705 New Article on Hugging Face: Teaching AI to Think Like a System \u2014 Not a Toolkit Title: \ud83c\udfd7\ufe0f Understanding Structured Cognitive Architecture: A Unified Framework for AI Reasoning Systems \ud83d\udd17 Read it here: https://huggingface.co/blog/kanaria007/understanding-structured-cognitive-architecture Summary: After exploring how AI can select reasoning modes or learn from failure, this new article zooms out: *How do all these capabilities form a single mind, not just a menu of functions?* The **Structured Cognitive Architecture** defines a unified framework where protocols interact coherently \u2014 forming a self-organizing, reflective, and ethically grounded reasoning system. This architecture enables agents to: \u2022 Integrate memory, ethics, reasoning, and identity across layers \u2022 Select and execute reasoning jumps with traceable structure \u2022 Coordinate failure recovery and adaptive learning \u2022 Maintain cross-session identity and self-editing capability It\u2019s not modular stacking. It\u2019s **structured...", "url": "https://huggingface.co/posts/kanaria007/210554569109150", "date_published": "2025-07-14T13:39:17.161096"}, {"id": "https://huggingface.co/posts/Quazim0t0/506664241355208", "image": "", "title": "I loved the idea of the Boxing by", "content_text": "I loved the idea of the Boxing by sergiopaniego/vlm_object_understanding And webml-community/fastvlm-webgpu So I tried to combine the two idea, unfortunately I can\u2019t seem to get it consistent and I only worked on the File Upload side. You may have to change the prompt a bit to suite the video you upload but it seems to semi work. If anyone knows a better way to fix this, I really wanted to use this for a project but I can\u2019t seem to figure it out. Quazim0t0/FastVLMBoxes I used videos from here and uploaded them to try it out. https://pixabay.com/videos/search/branch+birds/ See translation", "url": "https://huggingface.co/posts/Quazim0t0/506664241355208", "date_published": "2025-07-14T13:39:17.161430"}]}