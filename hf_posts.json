{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/giadap/999915316832908", "image": "", "title": "One of the hardest challenges in AI safety is finding the right balance: how do we protect people from harm without undermining their agency? This tension is especially visible in conversational systems, where safeguards can sometimes feel more paternalistic than supportive.", "content_text": "One of the hardest challenges in AI safety is finding the right balance: how do we protect people from harm without undermining their agency? This tension is especially visible in conversational systems, where safeguards can sometimes feel more paternalistic than supportive. In my latest piece for Hugging Face, I argue that open source and community-driven approaches offer a promising (though not exclusive) way forward. \u2728 Transparency can make safety mechanisms into learning opportunities. \u2728 Collaboration with diverse communities makes safeguards more relevant across contexts. \u2728 Iteration in the open lets protections evolve rather than freeze into rigid, one-size-fits-all rules. Of course, this isn\u2019t a silver bullet. Top-down safety measures will still be necessary in some cases. But if we only rely on corporate control, we risk building systems that are safe at the expense of trust and autonomy. Read the blog post here: https://huggingface.co/blog/giadap/preserving-agency See...", "url": "https://huggingface.co/posts/giadap/999915316832908", "date_published": "2025-10-02T17:17:47.535258"}, {"id": "https://huggingface.co/posts/AdinaY/926192442043020", "image": "", "title": "GLM-4.6 is here\ud83d\ude80", "content_text": "GLM-4.6 is here\ud83d\ude80 zai-org/GLM-4.6 \u2728 200K context window \u2728 Superior coding & polished UI generation \u2728 Stronger reasoning & tool use \u2728 More capable agents & agent frameworks See translation", "url": "https://huggingface.co/posts/AdinaY/926192442043020", "date_published": "2025-10-02T17:17:47.535500"}, {"id": "https://huggingface.co/posts/Parveshiiii/228189451590505", "image": "", "title": "\ud83d\ude80 Big news from XenArcAI!", "content_text": "\ud83d\ude80 Big news from XenArcAI! We\u2019ve just released our new dataset: **Bhagwat\u2011Gita\u2011Infinity** \ud83c\udf38\ud83d\udcd6 \u2728 What\u2019s inside: - Verse\u2011aligned Sanskrit, Hindi, and English - Clean, structured, and ready for ML/AI projects - Perfect for research, education, and open\u2011source exploration \ud83d\udd17 Hugging Face: XenArcAI/Bhagwat-Gita-Infinity Let\u2019s bring timeless wisdom into modern AI together \ud83d\ude4c See translation", "url": "https://huggingface.co/posts/Parveshiiii/228189451590505", "date_published": "2025-10-02T17:17:47.535752"}, {"id": "https://huggingface.co/posts/andywu-kby/790599686035068", "image": "", "title": "Hello everyone,", "content_text": "Hello everyone, I hope you\u2019re doing well. We\u2019re currently developing a chatbot that can analyze and forecast sales directly from Excel files. Do you think this would be useful? Miragic-AI/Miragic-Sales-Pilot Please share your feedback by \ud83d\udc4d or \ud83d\udc4e this post. Best regards, See translation", "url": "https://huggingface.co/posts/andywu-kby/790599686035068", "date_published": "2025-10-02T17:17:47.535972"}, {"id": "https://huggingface.co/posts/YerbaPage/558970453952386", "image": "", "title": "How to compress long code context? \ud83d\udcda", "content_text": "How to compress long code context? \ud83d\udcda Check out our LongCodeZip! Paper just got accepted to ASE 2025. \ud83d\udd25 Code: https://github.com/YerbaPage/LongCodeZip Paper: LongCodeZip: Compress Long Context for Code Language Models (2510.00446) See translation", "url": "https://huggingface.co/posts/YerbaPage/558970453952386", "date_published": "2025-10-02T17:17:47.536176"}, {"id": "https://huggingface.co/posts/Sri-Vigneshwar-DJ/891594547382960", "image": "", "title": "\ud83d\ude80 Qwen3-Omni for Marketing: A Game-Changer", "content_text": "\ud83d\ude80 Qwen3-Omni for Marketing: A Game-Changer Just wanted to share something exciting I've been exploring\u2014Qwen3-Omni and how it's transforming marketing workflows. What makes it special? At Hawky.ai we are started experimenting with Qwen3 recently for Analysis and Optimization. Unlike traditional tools that look at text, images, or audio separately, Qwen3-Omni analyzes everything together. It handles 119 languages, processes 40-minute audio sequences, and understands both images and videos\u2014all at once. The cool part? It's 2-3x faster than similar models thanks to its MoE architecture. Real applications I'm seeing: Ad Analysis: It scores video ads by combining visual elements, audio tone, and text\u2014giving 25% better CTR predictions than single-mode tools. Campaign Localization: Drop in one ad, get 10 localized versions with native voiceovers in under a minute. Perfect for testing across markets. Market Research: Feed it competitor content, podcasts, or UGC videos. It extracts actionable...", "url": "https://huggingface.co/posts/Sri-Vigneshwar-DJ/891594547382960", "date_published": "2025-10-02T17:17:47.536642"}, {"id": "https://huggingface.co/posts/hba123/508032894003486", "image": "", "title": "Hey, amazing, awesome people of the beautiful internet \ud83d\ude0d\ud83e\udd70", "content_text": "Hey, amazing, awesome people of the beautiful internet \ud83d\ude0d\ud83e\udd70 Distillation has been (from my point of view) a main driving factor for the success of hashtag#LLMs - like distilling the knowledge of an amazing big model (say hashtag#DeepSeekv3, or hashtag#GeminiAI) into yours. Probably, you have done it with minimising a KL divergence, and it somehow worked. Well, not that well, right? 1\ufe0f\u20e3 Your model tends to memorise! 2\ufe0f\u20e3 Your model might get the right answer, but its reasoning might be flawed. To fix those problems, we rethink distillation and process a new approach! A method that is based on constrained RL that comes with nice theoretical guarantees and excellent performance! Check it out: Rethinking Large Language Model Distillation: A Constrained Markov Decision Process Perspective (2509.22921) Let us do distillation right! Please upvote if you find it useful! See translation", "url": "https://huggingface.co/posts/hba123/508032894003486", "date_published": "2025-10-02T17:17:47.537049"}, {"id": "https://huggingface.co/posts/SelmaNajih001/721687692996128", "image": "", "title": "Introducing", "content_text": "Introducing SelmaNajih001/StockPredictionExplanation , built with GRPO and RAG: -GRPO trains the model to predict and explain stock direction. -RAG grounds explanations in historical financial news and central bank speeches. Together, they create a system that forecasts stock movements and shows the reasoning behind them. Full article: Explainable Financial Predictions \u2014 https://huggingface.co/blog/SelmaNajih001/explainable-financial-predictions Try it here: StockPredictionExplanation Space \u2014 SelmaNajih001/StockPredictionExplanation See translation", "url": "https://huggingface.co/posts/SelmaNajih001/721687692996128", "date_published": "2025-10-02T17:17:47.537318"}, {"id": "https://huggingface.co/posts/yeonseok-zeticai/773315710052361", "image": "", "title": "\u26a1 ColBERT-ko-v1.0 Complete On-device Study: SOTA Korean Retrieval on Mobile", "content_text": "\u26a1 ColBERT-ko-v1.0 Complete On-device Study: SOTA Korean Retrieval on Mobile Major Release: Comprehensive mobile deployment study of yoonjong's ColBERT-ko-v1.0 with detailed performance benchmarks across 50+ mobile devices! \ud83c\udfaf Model Overview: Architecture: Korean-optimized ColBERT (late interaction) Parameters: 0.1B (compact and efficient) Specialty: Korean document retrieval and semantic search Performance: 1.0 recall@10, 0.966 nDCG@10 on AutoRAGRetrieval Benchmark: Outperforms Jina-ColBERT-v2 on Korean MTEB tasks \ud83d\udcca Mobile Performance Results: Latency Metrics: NPU (Best): 3.17ms average inference GPU: 11.67ms average CPU: 21.36ms average NPU Advantage: 18.46x speedup over CPU Memory Efficiency: Model Size: 567.89 MB (production optimized) Runtime Memory: 170.87 MB peak consumption Load Range: 4-614 MB across device categories FP32 Memory: 642.65 MB (optional high precision) Accuracy Preservation: - FP16 Precision: 53.51 dB maintained - Quantized Mode: 32.77 dB (available for memory...", "url": "https://huggingface.co/posts/yeonseok-zeticai/773315710052361", "date_published": "2025-10-02T17:17:47.537876"}, {"id": "https://huggingface.co/posts/Znilsson/836359160713115", "image": "", "title": "Hi there, I'm an amateur AI enthusiast. I've managed to gather a fairly large dataset of .pdfs of varying quality. I have about 10gb total. I want to use it to fine tune a model and/or a RAG system. I've been having a terrible time trying to get the data prepared so I can train with it. I've tried google collab, I've tried Kaggle and paid for GPUs on both, but I'm struggling. This is because I'm not a data scientist and have zero background in programming, etc. I'm just using AI to teach me along the way.  I'm willing to pay some money for GPU power, but because I fail so frequently I fear I'm just burning through it. I don't have enough horsepower on my laptop - I've tried to train on it, but to no avail. I've had minor successes, but have not been able to complete the dataset with anything comprehensible. Any and all suggestions are welcome. I need some serious help. Thanks!!", "content_text": "Hi there, I'm an amateur AI enthusiast. I've managed to gather a fairly large dataset of .pdfs of varying quality. I have about 10gb total. I want to use it to fine tune a model and/or a RAG system. I've been having a terrible time trying to get the data prepared so I can train with it. I've tried google collab, I've tried Kaggle and paid for GPUs on both, but I'm struggling. This is because I'm not a data scientist and have zero background in programming, etc. I'm just using AI to teach me along the way. I'm willing to pay some money for GPU power, but because I fail so frequently I fear I'm just burning through it. I don't have enough horsepower on my laptop - I've tried to train on it, but to no avail. I've had minor successes, but have not been able to complete the dataset with anything comprehensible. Any and all suggestions are welcome. I need some serious help. Thanks!! See translation", "url": "https://huggingface.co/posts/Znilsson/836359160713115", "date_published": "2025-10-02T17:17:47.538181"}]}