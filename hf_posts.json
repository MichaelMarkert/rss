{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/burtenshaw/742649076372470", "image": "", "title": "I made a real time voice agent with FastRTC, smolagents, and hugging face inference providers. Check it out in this space:", "content_text": "I made a real time voice agent with FastRTC, smolagents, and hugging face inference providers. Check it out in this space: \ud83d\udd17 burtenshaw/coworking_agent See translation", "url": "https://huggingface.co/posts/burtenshaw/742649076372470", "date_published": "2025-02-28T17:17:03.163660"}, {"id": "https://huggingface.co/posts/jasoncorkill/366338815946143", "image": "", "title": "Has OpenGVLab Lumina Outperformed OpenAI\u2019s Model?", "content_text": "Has OpenGVLab Lumina Outperformed OpenAI\u2019s Model? We\u2019ve just released the results from a large-scale human evaluation (400k annotations) of OpenGVLab\u2019s newest text-to-image model, Lumina. Surprisingly, Lumina outperforms OpenAI\u2019s DALL-E 3 in terms of alignment, although it ranks #6 in our overall human preference benchmark. To support further development in text-to-image models, we\u2019re making our entire human-annotated dataset publicly available. If you\u2019re working on model improvements and need high-quality data, feel free to explore. We welcome your feedback and look forward to any insights you might share! Rapidata/OpenGVLab_Lumina_t2i_human_preference See translation", "url": "https://huggingface.co/posts/jasoncorkill/366338815946143", "date_published": "2025-02-28T17:17:03.164058"}, {"id": "https://huggingface.co/posts/singhsidhukuldeep/419917039256941", "image": "", "title": "O1 Embedder: Transforming Retrieval Models with Reasoning Capabilities", "content_text": "O1 Embedder: Transforming Retrieval Models with Reasoning Capabilities Researchers from University of Science and Technology of China and Beijing Academy of Artificial Intelligence have developed a novel retrieval model that mimics the slow-thinking capabilities of reasoning-focused LLMs like OpenAI's O1 and DeepSeek's R1. Unlike traditional embedding models that directly match queries with documents, O1 Embedder first generates thoughtful reflections about the query before performing retrieval. This two-step process significantly improves performance on complex retrieval tasks, especially those requiring intensive reasoning or zero-shot generalization to new domains. The technical implementation is fascinating: - The model integrates two essential functions: Thinking and Embedding - It uses an \"Exploration-Refinement\" data synthesis workflow where initial thoughts are generated by an LLM and refined by a retrieval committee - A multi-task training method fine-tunes a pre-trained...", "url": "https://huggingface.co/posts/singhsidhukuldeep/419917039256941", "date_published": "2025-02-28T17:17:03.164529"}, {"id": "https://huggingface.co/posts/burtenshaw/352638065928004", "image": "", "title": "Now the Hugging Face agent course is getting real! With frameworks like smolagents, LlamaIndex, and LangChain.", "content_text": "Now the Hugging Face agent course is getting real! With frameworks like smolagents, LlamaIndex, and LangChain. \ud83d\udd17 Follow the org for updates https://huggingface.co/agents-course This week we are releasing the first framework unit in the course and it\u2019s on smolagents. This is what the unit covers: - why should you use smolagents vs another library? - how to build agents that use code - build multiagents systems - use vision language models for browser use The team has been working flat out on this for a few weeks. Led by @ sergiopaniego and supported by smolagents author @ m-ric . See translation", "url": "https://huggingface.co/posts/burtenshaw/352638065928004", "date_published": "2025-02-28T17:17:03.164907"}, {"id": "https://huggingface.co/posts/ginipick/923354082387927", "image": "", "title": "\ud83d\ude80 Introducing MOUSE: Space Research Thinking on HuggingFace Spaces", "content_text": "\ud83d\ude80 Introducing MOUSE: Space Research Thinking on HuggingFace Spaces \ud83d\ude80 How to Get Started ginipick/spaces-research-think Welcome to **MOUSE: Space Research Thinking** \u2013 an innovative HuggingFace Spaces project designed to transform how you analyze and interact with Python code. Whether you're a developer, researcher, or simply passionate about coding, this tool provides state-of-the-art analysis, summarization, and usage guidance, all powered by advanced AI. --- ## \ud83c\udf1f Key Features - **Real-Time Code Analysis** Instantly dissect your Python code to reveal its structure, functionality, and potential applications. Our tool delivers: - **Background & Necessity**: Understand the context behind the code. - **Functional Utility & Value**: Highlight core functionalities and benefits. - **Distinctive Features**: Discover what sets the project apart. - **Target Audience & Applications**: Identify who can benefit and how. - **Expected Impact**: Envision the improvements and innovations the code...", "url": "https://huggingface.co/posts/ginipick/923354082387927", "date_published": "2025-02-28T17:17:03.165525"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/331073083077260", "image": "", "title": "\u2728\ud83c\udf89Duality.ai just released a multiclass object detection dataset for YOLOv8, as well as a tutorial on how to create your own multiclass dataset!", "content_text": "\u2728\ud83c\udf89Duality.ai just released a multiclass object detection dataset for YOLOv8, as well as a tutorial on how to create your own multiclass dataset! Carefully crafted (not GenAI created) synthetic data that ACTUALLY trains a model that works in the physical world. Create a free FalconEDU account, and download the 1000 image and annotation dataset - https://falcon.duality.ai/secure/documentation/ex3-dataset?sidebarMode=learn -or- Follow along with Exercise 3: Multiclass Object Detection to start creating - https://falcon.duality.ai/secure/documentation/ex3-objdetection-multiclass -or- Download this Colab notebook to see the data work, no hardware required - https://falcon.duality.ai/secure/documentation/ex3-dataset?sidebarMode=learn See translation", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/331073083077260", "date_published": "2025-02-28T17:17:03.165913"}, {"id": "https://huggingface.co/posts/fdaudens/636972739498133", "image": "", "title": "Is this the best tool to extract clean info from PDFs, handwriting and complex documents yet?", "content_text": "Is this the best tool to extract clean info from PDFs, handwriting and complex documents yet? Open source olmOCR just dropped and the results are impressive. Tested the free demo with various documents, including a handwritten Claes Oldenburg letter. The speed is impressive: 3000 tokens/second on your own GPU - that's 1/32 the cost of GPT-4o ($190/million pages). Game-changer for content extraction and digital archives. To achieve this, Ai2 trained a 7B vision language model on 260K pages from 100K PDFs using \"document anchoring\" - combining PDF metadata with page images. Best part: it actually understands document structure (columns, tables, equations) instead of just jumbling everything together like most OCR tools. Their human eval results back this up. \ud83d\udc49 Try the demo: https://olmocr.allenai.org Going right into the AI toolkit: JournalistsonHF/ai-toolkit See translation", "url": "https://huggingface.co/posts/fdaudens/636972739498133", "date_published": "2025-02-28T17:17:03.166324"}, {"id": "https://huggingface.co/posts/prithivMLmods/305640045790864", "image": "", "title": "Dropping some of the custom fine-tunes based on SigLIP2,", "content_text": "Dropping some of the custom fine-tunes based on SigLIP2, with a single-label classification problem type! \ud83c\udf00\ud83e\udde4 - AI vs Deepfake vs Real : prithivMLmods/AI-vs-Deepfake-vs-Real-Siglip2 - Deepfake Detect : prithivMLmods/Deepfake-Detect-Siglip2 - Fire Detection : prithivMLmods/Fire-Detection-Siglip2 - Deepfake Quality Assess : prithivMLmods/Deepfake-Quality-Assess-Siglip2 - Guard Against Unsafe Content : prithivMLmods/Guard-Against-Unsafe-Content-Siglip2 \ud83c\udf20Collection : prithivMLmods/siglip2-custom-67bcdb2de8fe96b99fb4e19e See translation", "url": "https://huggingface.co/posts/prithivMLmods/305640045790864", "date_published": "2025-02-28T17:17:03.166652"}, {"id": "https://huggingface.co/posts/davanstrien/855251141208457", "image": "", "title": "Quick POC: Turn a Hugging Face dataset card into a short podcast introducing the dataset using all open models.", "content_text": "Quick POC: Turn a Hugging Face dataset card into a short podcast introducing the dataset using all open models. I think I'm the only weirdo who would enjoy listening to something like this though \ud83d\ude05 Here is an example for eth-nlped/stepverify See translation", "url": "https://huggingface.co/posts/davanstrien/855251141208457", "date_published": "2025-02-28T17:17:03.166926"}, {"id": "https://huggingface.co/posts/ngxson/723185341195650", "image": "", "title": "A comprehensive matrix for which format should you use.", "content_text": "A comprehensive matrix for which format should you use. Read more on my blog post: https://huggingface.co/blog/ngxson/common-ai-model-formats | Hardware | GGUF | PyTorch | Safetensors | ONNX | | ----------------- | ----------- | ------------------------ | -------------------------- | ------- | | CPU | \u2705 (best) | \ud83d\udfe1 | \ud83d\udfe1 | \u2705 | | GPU | \u2705 | \u2705 | \u2705 | \u2705 | | Mobile | \u2705 | \ud83d\udfe1 (via executorch) | \u274c | \u2705 | | Apple silicon | \u2705 | \ud83d\udfe1 | \u2705 (via MLX framework) | \u2705 | See translation", "url": "https://huggingface.co/posts/ngxson/723185341195650", "date_published": "2025-02-28T17:17:03.167277"}]}