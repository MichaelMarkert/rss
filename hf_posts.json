{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/mrfakename/579691668854907", "image": "", "title": "Trained a model for emotion-controllable TTS based on MiMo audio on LAION's dataset.", "content_text": "Trained a model for emotion-controllable TTS based on MiMo audio on LAION's dataset. Still very early and does have an issue with hallucinating but results seem pretty good so far, given that it is very early into the training run. Will probably kick off a new run later with some settings tweaked. Put up a demo here: mrfakename/EmoAct-MiMo (Turn \ud83d\udd0a on to hear audio samples) See translation", "url": "https://huggingface.co/posts/mrfakename/579691668854907", "date_published": "2025-10-29T13:36:20.144200"}, {"id": "https://huggingface.co/posts/onekq/651277400400501", "image": "", "title": "Context rot is such a catchy phrase, but the problem has been identified 2+ years ago, called attention decay.", "content_text": "Context rot is such a catchy phrase, but the problem has been identified 2+ years ago, called attention decay. Lost in the Middle: How Language Models Use Long Contexts (2307.03172) I spotted the same problem in coding tasks, and documented in my book ( https://www.amazon.com/dp/9999331130 ). Why did this problem become hot again? This is because many of us thought the problem has been solved by long context models, which is not true. Here we were misled by benchmarks. Most long-context benchmarks build around the QA scenario, i.e. \"finding needle in haystack\". But in agentic scenarios, the model needs to find EVERYTHING in the haystack, and just can't afford enough attention for this challenge. See translation", "url": "https://huggingface.co/posts/onekq/651277400400501", "date_published": "2025-10-29T13:36:20.144617"}, {"id": "https://huggingface.co/posts/DmitryRyumin/256377930602220", "image": "", "title": "\ud83d\ude80\ud83e\udd16\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\udd16\ud83d\ude80", "content_text": "\ud83d\ude80\ud83e\udd16\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\udd16\ud83d\ude80 \ud83d\udcc4 Title: Variance-based Pruning for Accelerating and Compressing Trained Networks \ud83d\udd1d \ud83d\udcdd Description: The one-shot pruning method efficiently compresses networks, reducing computation and memory usage while retaining almost full performance and requiring minimal fine-tuning. \ud83d\udc65 Authors: Uranik Berisha, Jens Mehnert, and Alexandru Paul Condurache \ud83d\udcc5 Conference: ICCV, 19 \u2013 23 Oct, 2025 | Honolulu, Hawai'i, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: Variance-Based Pruning for Accelerating and Compressing Trained Networks (2507.12988) \ud83d\ude80 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers \ud83d\ude80 Added to the Efficient Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/efficient-learning.md \ud83d\udcda More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers curated by @ DmitryRyumin \ud83d\udd0d Keywords: #VarianceBasedPruning #NetworkCompression #ModelAcceleration...", "url": "https://huggingface.co/posts/DmitryRyumin/256377930602220", "date_published": "2025-10-29T13:36:20.145074"}, {"id": "https://huggingface.co/posts/DmitryRyumin/213442382070723", "image": "", "title": "\ud83d\ude80\ud83d\udca1\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\ude84\ud83d\ude80", "content_text": "\ud83d\ude80\ud83d\udca1\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\ude84\ud83d\ude80 \ud83d\udcc4 Title: LoftUp: Learning a Coordinate-based Feature Upsampler for Vision Foundation Models \ud83d\udd1d \ud83d\udcdd Description: LoftUp is a coordinate-based transformer that upscales the low-resolution features of VFMs (e.g. DINOv2 and CLIP) using cross-attention and self-distilled pseudo-ground truth (pseudo-GT) from SAM. \ud83d\udc65 Authors: Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang \ud83d\udcc5 Conference: ICCV, 19 \u2013 23 Oct, 2025 | Honolulu, Hawai'i, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models (2504.14032) \ud83c\udf10 Github Page: https://andrehuang.github.io/loftup-site \ud83d\udcc1 Repository: https://github.com/andrehuang/loftup \ud83d\ude80 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers \ud83d\ude80 Added to the Foundation Models and Representation Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/foundation-models-and-representation-learning.md \ud83d\udcda...", "url": "https://huggingface.co/posts/DmitryRyumin/213442382070723", "date_published": "2025-10-29T13:36:20.145573"}, {"id": "https://huggingface.co/posts/sourceoftruthdata/665062314942834", "image": "", "title": "What a fantastic community!", "content_text": "What a fantastic community! See translation", "url": "https://huggingface.co/posts/sourceoftruthdata/665062314942834", "date_published": "2025-10-29T13:36:20.145797"}, {"id": "https://huggingface.co/posts/jbilcke-hf/233313729271846", "image": "", "title": "I made a code sniping agent to detect when new AI papers with code (and weights) are released, and then automatically create a Gradio demo on Hugging Face \ud83e\uddd9", "content_text": "I made a code sniping agent to detect when new AI papers with code (and weights) are released, and then automatically create a Gradio demo on Hugging Face \ud83e\uddd9 Here are some examples generated 100% automatically: https://huggingface.co/collections/jbilcke-hf/sniped I call this agent CheatCode ( https://github.com/jbilcke-hf/CheatCode ) because it skips so many steps that it kinda feels like breaking the rules of the AI tech release game \ud83d\ude05 As with any experimental technology, there is still room for improvement \ud83d\udc69\ud83c\udffb\u200d\ud83d\udd2c: - Currently the demos are all generated in one go and not built or tested by the agent itself. A more robust version should loop over the deployed app to fix build/runtime issues. - There is still a bit of human curation done to avoid making demos for things that can\u2019t really be demonstrated on ZeroGPU (eg. tasks taking several minutes) - Some papers can actually be showcased in a variety of ways, which isn\u2019t really supported (see Demo 2) See translation", "url": "https://huggingface.co/posts/jbilcke-hf/233313729271846", "date_published": "2025-10-29T13:36:20.146231"}, {"id": "https://huggingface.co/posts/gokaygokay/758462412009896", "image": "", "title": "FlashPack: Lightning-Fast Model Loading for PyTorch", "content_text": "FlashPack: Lightning-Fast Model Loading for PyTorch https://github.com/fal-ai/flashpack FlashPack \u2014 a new, high-throughput file format and loading mechanism for PyTorch that makes model checkpoint I/O blazingly fast, even on systems without access to GPU Direct Storage (GDS). With FlashPack, loading any model can be 3\u20136\u00d7 faster than with the current state-of-the-art methods like accelerate or the standard load_state_dict() and to() flow \u2014 all wrapped in a lightweight, pure-Python package that works anywhere. See translation", "url": "https://huggingface.co/posts/gokaygokay/758462412009896", "date_published": "2025-10-29T13:36:20.146559"}, {"id": "https://huggingface.co/posts/DmitryRyumin/687304943131343", "image": "", "title": "\ud83d\ude80\ud83c\udff7\ufe0f\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\udde9\ud83d\ude80", "content_text": "\ud83d\ude80\ud83c\udff7\ufe0f\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\udde9\ud83d\ude80 \ud83d\udcc4 Title: Heavy Labels Out! Dataset Distillation with Label Space Lightening \ud83d\udd1d \ud83d\udcdd Description: The HeLlO framework is a new corpus distillation method that removes the need for large soft labels. It uses a lightweight, online image-to-label projector based on CLIP. This projector has been adapted using LoRA-style, parameter-efficient tuning. It has also been initialized with text embeddings. \ud83d\udc65 Authors: @ roseannelexie , @ Huage001 , Zigeng Chen, Jingwen Ye, and Xinchao Wang \ud83d\udcc5 Conference: ICCV, 19 \u2013 23 Oct, 2025 | Honolulu, Hawai'i, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: Heavy Labels Out! Dataset Distillation with Label Space Lightening (2408.08201) \ud83d\udcfa Video: https://www.youtube.com/watch?v=kAyK_3wskgA \ud83d\ude80 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers \ud83d\ude80 Added to the Efficient Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/efficient-learning.md \ud83d\udcda More Papers: more cutting-edge research...", "url": "https://huggingface.co/posts/DmitryRyumin/687304943131343", "date_published": "2025-10-29T13:36:20.147064"}, {"id": "https://huggingface.co/posts/prithivMLmods/131495845206291", "image": "", "title": "Here is the official Florence-2 Transformers-converted demo for the following vision models:", "content_text": "Here is the official Florence-2 Transformers-converted demo for the following vision models: florence-community/Florence-2-large , florence-community/Florence-2-large-ft , florence-community/Florence-2-base , and florence-community/Florence-2-base-ft . These models support tasks such as object detection, captioning, detailed captioning, more detailed captioning, dense region captioning, region proposal, OCR, and OCR with region. Try the official demo at the link below: > Space: prithivMLmods/florence2-vision-models > Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 > To know more about it, visit the app page or the respective model page!! See translation", "url": "https://huggingface.co/posts/prithivMLmods/131495845206291", "date_published": "2025-10-29T13:36:20.147385"}, {"id": "https://huggingface.co/posts/AdinaY/516473804279541", "image": "", "title": "Glyph \ud83d\udd25 a framework that scales context length by compressing text into images and processing them with vision\u2013language models, released by Z.ai.", "content_text": "Glyph \ud83d\udd25 a framework that scales context length by compressing text into images and processing them with vision\u2013language models, released by Z.ai. Paper:https://huggingface.co/papers/2510.17800 Model:https://huggingface.co/zai-org/Glyph \u2728 Compresses long sequences visually to bypass token limits \u2728 Reduces computational and memory costs \u2728 Preserves meaning through multimodal encoding \u2728 Built on GLM-4.1V-9B-Base See translation", "url": "https://huggingface.co/posts/AdinaY/516473804279541", "date_published": "2025-10-29T13:36:20.147663"}]}