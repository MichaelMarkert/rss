{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/wolfram/819510719695955", "image": "", "title": "Finally finished my extensive **Qwen 3 evaluations** across a range of formats and quantisations, focusing on **MMLU-Pro** (Computer Science).", "content_text": "Finally finished my extensive **Qwen 3 evaluations** across a range of formats and quantisations, focusing on **MMLU-Pro** (Computer Science). A few take-aways stood out - especially for those interested in local deployment and performance trade-offs: 1\ufe0f\u20e3 **Qwen3-235B-A22B** (via Fireworks API) tops the table at **83.66%** with ~55 tok/s. 2\ufe0f\u20e3 But the **30B-A3B Unsloth** quant delivered **82.20%** while running locally at ~45 tok/s and with zero API spend. 3\ufe0f\u20e3 The same Unsloth build is ~5x faster than Qwen's **Qwen3-32B**, which scores **82.20%** as well yet crawls at <10 tok/s. 4\ufe0f\u20e3 On Apple silicon, the **30B MLX** port hits **79.51%** while sustaining ~64 tok/s - arguably today's best speed/quality trade-off for Mac setups. 5\ufe0f\u20e3 The **0.6B** micro-model races above 180 tok/s but tops out at **37.56%** - that's why it's not even on the graph (50 % performance cut-off). All local runs were done with LM Studio on an M4 MacBook Pro, using Qwen's official recommended settings....", "url": "https://huggingface.co/posts/wolfram/819510719695955", "date_published": "2025-05-09T13:31:51.788192"}, {"id": "https://huggingface.co/posts/DawnC/287598166225995", "image": "", "title": "VisionScout \u2014 Now with Video Analysis! \ud83d\ude80", "content_text": "VisionScout \u2014 Now with Video Analysis! \ud83d\ude80 I\u2019m excited to announce a major update to VisionScout, my interactive vision tool that now supports VIDEO PROCESSING, in addition to powerful object detection and scene understanding! \u2b50\ufe0f NEW: Video Analysis Is Here! \ud83c\udfac Upload any video file to detect and track objects using YOLOv8. \u23f1\ufe0f Customize processing intervals to balance speed and thoroughness. \ud83d\udcca Get comprehensive statistics and summaries showing object appearances across the entire video. What else can VisionScout do? \ud83d\uddbc\ufe0f Analyze any image and detect 80 object types with YOLOv8. \ud83d\udd04 Switch between Nano, Medium, and XLarge models for speed or accuracy. \ud83c\udfaf Filter by object classes (people, vehicles, animals, etc.) to focus on what matters. \ud83d\udcca View detailed stats on detections, confidence levels, and distributions. \ud83e\udde0 Understand scenes \u2014 interpreting environments and potential activities. \u26a0\ufe0f Automatically identify possible safety concerns based on detected objects. What\u2019s coming next? \ud83d\udd0e Expanding...", "url": "https://huggingface.co/posts/DawnC/287598166225995", "date_published": "2025-05-09T13:31:51.788817"}, {"id": "https://huggingface.co/posts/giadap/315154856088110", "image": "", "title": "Ever notice how some AI assistants feel like tools while others feel like companions? Turns out, it's not always about fancy tech upgrades, because sometimes it's just clever design.", "content_text": "Ever notice how some AI assistants feel like tools while others feel like companions? Turns out, it's not always about fancy tech upgrades, because sometimes it's just clever design. Our latest blog post at Hugging Face dives into how minimal design choices can completely transform how users experience AI. We've seen our community turn the same base models into everything from swimming coaches to interview prep specialists with surprisingly small tweaks. The most fascinating part? When we tested identical models with different \"personalities\" in our Inference Playground, the results were mind-blowing. Want to experiment yourself? Our Inference Playground lets anyone (yes, even non-coders!) test these differences in real-time. You can: - Compare multiple models side-by-side - Customize system prompts - Adjust parameters like temperature - Test multi-turn conversations It's fascinating how a few lines of instruction text can transform the same AI from strictly professional to...", "url": "https://huggingface.co/posts/giadap/315154856088110", "date_published": "2025-05-09T13:31:51.789191"}, {"id": "https://huggingface.co/posts/clem/655781573301725", "image": "", "title": "nvidia", "content_text": "nvidia dominating the top trending open datasets these days! http://hf.co/datasets See translation", "url": "https://huggingface.co/posts/clem/655781573301725", "date_published": "2025-05-09T13:31:51.789386"}, {"id": "https://huggingface.co/posts/RiverZ/221754259422855", "image": "", "title": "\ud83d\udd25 We're thrilled to share some exciting news about ICEdit! Currently, ICEdit app (", "content_text": "\ud83d\udd25 We're thrilled to share some exciting news about ICEdit! Currently, ICEdit app ( RiverZ/ICEdit ) has soared to the second place on the weekly trend list of Hugging Face Space, just trailing behind Qwen3. What's more, it also holds the second position on the overall space trend list. This achievement wouldn't have been possible without your incredible support and love. A huge thank you to each and every one of you\u2764! \ud83c\udf89 The ICEdit community has been incredibly active, and we've seen a plethora of amazing ComfyUI workflows being shared. For instance, with the help of ComfyUI - nunchaku, you can run ICEdit locally with just 4GB of VRAM. This makes it much more accessible for those with limited hardware resources. \ud83c\udf87 If you're interested in the detailed information, please head over to our repository. We highly encourage you to give these workflows a try and explore the creative possibilities that ICEdit offers. Github Repo: https://github.com/River-Zhang/ICEdit Hugging Face Space:...", "url": "https://huggingface.co/posts/RiverZ/221754259422855", "date_published": "2025-05-09T13:31:51.789839"}, {"id": "https://huggingface.co/posts/VirtualOasis/885212606719735", "image": "", "title": "Agents vs. Workflows", "content_text": "Agents vs. Workflows Agents are systems where LLMs dynamically direct their processes and tool usage, maintaining control over how they accomplish tasks. Workflows are through predefined code paths, ensuring that each step is executed in a deterministic manner. Agents are like smart assistants that can think on their own. They understand situations, make decisions, and act, whatever the task is new or unpredictable. Think of the Agent as a chef who can make a meal based on what they have. Workflows are like a recipe with fixed steps. They\u2019re a series of tasks done in order, like following a checklist for approving a loan. They\u2019re great for tasks that don\u2019t change much. See translation", "url": "https://huggingface.co/posts/VirtualOasis/885212606719735", "date_published": "2025-05-09T13:31:51.790187"}, {"id": "https://huggingface.co/posts/nomadicsynth/333343195611996", "image": "", "title": "I Did a Thing!", "content_text": "I Did a Thing! I made an embedding model to find answers in research papers. It goes deeper than plain \"semantic search\" by identifying deeply reasoned connections and interdisciplinary insights that might have been overlooked. The goal is to find the solutions that might have been missed and to uncover answers that are already out there. I\u2019ve set up a demo Space - nomadicsynth/inkling . It\u2019s early days, and I\u2019d love some feedback on the model\u2019s results. Try it out and let me know what you think! Oh, and if it finds your Nobel-winning answer, I want a cut! \ud83d\ude09 See translation", "url": "https://huggingface.co/posts/nomadicsynth/333343195611996", "date_published": "2025-05-09T13:31:51.790523"}, {"id": "https://huggingface.co/posts/sequelbox/254248082050134", "image": "", "title": "NEW RELEASE: Esper 3 for Qwen 3!", "content_text": "NEW RELEASE: Esper 3 for Qwen 3! - A full-stack software assistant: a reasoning finetune focused on coding, architecture, and DevOps using the Titanium and Tachibana datasets! - Improved general and creative reasoning skills, powered by the Raiden dataset. 4B model: ValiantLabs/Qwen3-4B-Esper3 8B model: ValiantLabs/Qwen3-8B-Esper3 We'll also be bringing Esper 3 to larger Qwen 3 models as soon as we can - if you want these, consider helping us out: sequelbox/SupportOpenSource More models and datasets to come soon! with my love and enthusiasm, allegra See translation", "url": "https://huggingface.co/posts/sequelbox/254248082050134", "date_published": "2025-05-09T13:31:51.790829"}, {"id": "https://huggingface.co/posts/merve/742287367367358", "image": "", "title": "A ton of impactful models and datasets in open AI past week, let's summarize the best \ud83e\udd29", "content_text": "A ton of impactful models and datasets in open AI past week, let's summarize the best \ud83e\udd29 merve/releases-apr-21-and-may-2-6819dcc84da4190620f448a3 \ud83d\udcac Qwen made it rain! They released Qwen3: new dense and MoE models ranging from 0.6B to 235B \ud83e\udd2f as well as Qwen2.5-Omni, any-to-any model in 3B and 7B! > Microsoft AI released Phi4 reasoning models (that also come in mini and plus sizes) > NVIDIA released new CoT reasoning datasets \ud83d\uddbc\ufe0f > ByteDance released UI-TARS-1.5, native multimodal UI parsing agentic model > Meta released EdgeTAM, an on-device object tracking model (SAM2 variant) \ud83d\udde3\ufe0f NVIDIA released parakeet-tdt-0.6b-v2, a smol 600M automatic speech recognition model > Nari released Dia, a 1.6B text-to-speech model > Moonshot AI released Kimi Audio, a new audio understanding, generation, conversation model \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb JetBrains released Melium models in base and SFT for coding > Tesslate released UIGEN-T2-7B, a new text-to-frontend-code model \ud83e\udd29 See translation", "url": "https://huggingface.co/posts/merve/742287367367358", "date_published": "2025-05-09T13:31:51.791266"}, {"id": "https://huggingface.co/posts/juhoinkinen/699503520254777", "image": "", "title": "We (", "content_text": "We ( @ osma , @ MonaLehtinen & me, i.e. the Annif team at the National Library of Finland) recently took part in the LLMs4Subjects challenge at the SemEval-2025 workshop. The task was to use large language models (LLMs) to generate good quality subject indexing for bibliographic records, i.e. titles and abstracts. We are glad to report that our system performed well; it was ranked \ud83e\udd47 1st in the category where the full vocabulary was used \ud83e\udd48 2nd in the smaller vocabulary category \ud83c\udfc5 4th in the qualitative evaluations. 14 participating teams developed their own solutions for generating subject headings and the output of each system was assessed using both quantitative and qualitative evaluations. Research papers about most of the systems are going to be published around the time of the workshop in late July, and many pre-prints are already available. We applied Annif together with several LLMs that we used to preprocess the data sets: translated the GND vocabulary terms to English,...", "url": "https://huggingface.co/posts/juhoinkinen/699503520254777", "date_published": "2025-05-09T13:31:51.791902"}]}