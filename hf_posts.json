{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/mlabonne/443122762320210", "image": "", "title": "\u2702\ufe0f Gemma 3 Abliterated", "content_text": "\u2702\ufe0f Gemma 3 Abliterated I noticed that Gemma 3 was much more resilient to refusal removal than other models like Qwen 2.5. I experimented with different recipes and improved the abliteration technique I wrote about last year. It's still experimental but the refusal rate is super low in my tests. Enjoy! mlabonne/gemma-3-4b-it-abliterated mlabonne/gemma-3-12b-it-abliterated mlabonne/gemma-3-27b-it-abliterated See translation", "url": "https://huggingface.co/posts/mlabonne/443122762320210", "date_published": "2025-03-19T09:24:54.865940"}, {"id": "https://huggingface.co/posts/mlabonne/714992455492422", "image": "", "title": "\u2702\ufe0f AutoAbliteration", "content_text": "\u2702\ufe0f AutoAbliteration I made a Colab notebook to automatically abliterate models. It's quite general, so you can do interesting stuff like blocking a given language in the model outputs. \ud83d\udcbb Colab: https://colab.research.google.com/drive/1RmLv-pCMBBsQGXQIM8yF-OdCNyoylUR1?usp=sharing See translation", "url": "https://huggingface.co/posts/mlabonne/714992455492422", "date_published": "2025-03-19T09:24:54.866255"}, {"id": "https://huggingface.co/posts/Kseniase/624548696865407", "image": "", "title": "15 types of attention mechanisms", "content_text": "15 types of attention mechanisms Attention mechanisms allow models to dynamically focus on specific parts of their input when performing tasks. In our recent article, we discussed Multi-Head Latent Attention (MLA) in detail and now it's time to summarize other existing types of attention. Here is a list of 15 types of attention mechanisms used in AI models: 1. Soft attention (Deterministic attention) -> Neural Machine Translation by Jointly Learning to Align and Translate (1409.0473) Assigns a continuous weight distribution over all parts of the input. It produces a weighted sum of the input using attention weights that sum to 1. 2. Hard attention (Stochastic attention) -> Effective Approaches to Attention-based Neural Machine Translation (1508.04025) Makes a discrete selection of some part of the input to focus on at each step, rather than attending to everything. 3. Self-attention -> Attention Is All You Need (1706.03762) Each element in the sequence \"looks\" at other elements and...", "url": "https://huggingface.co/posts/Kseniase/624548696865407", "date_published": "2025-03-19T09:24:54.866936"}, {"id": "https://huggingface.co/posts/jasoncorkill/521063845119914", "image": "", "title": "At Rapidata, we compared DeepL with LLMs like DeepSeek-R1, Llama, and Mixtral for translation quality using feedback from over 51,000 native speakers. Despite the costs, the performance makes it a valuable investment, especially in critical applications where translation quality is paramount. Now we can say that Europe is more than imposing regulations.", "content_text": "At Rapidata, we compared DeepL with LLMs like DeepSeek-R1, Llama, and Mixtral for translation quality using feedback from over 51,000 native speakers. Despite the costs, the performance makes it a valuable investment, especially in critical applications where translation quality is paramount. Now we can say that Europe is more than imposing regulations. Our dataset, based on these comparisons, is now available on Hugging Face. This might be useful for anyone working on AI translation or language model evaluation. Rapidata/Translation-deepseek-llama-mixtral-v-deepl See translation", "url": "https://huggingface.co/posts/jasoncorkill/521063845119914", "date_published": "2025-03-19T09:24:54.867261"}, {"id": "https://huggingface.co/posts/aifeifei798/845431141575759", "image": "", "title": "\ud83d\ude0a This program is designed to remove emojis from a given text. It uses a regular expression (regex) pattern to match and replace emojis with an empty string, effectively removing them from the text. The pattern includes a range of Unicode characters that correspond to various types of emojis, such as emoticons, symbols, and flags. By using this program, you can clean up text data by removing any emojis that may be present, which can be useful for text processing, analysis, or other applications where emojis are not desired. \ud83d\udcbb", "content_text": "\ud83d\ude0a This program is designed to remove emojis from a given text. It uses a regular expression (regex) pattern to match and replace emojis with an empty string, effectively removing them from the text. The pattern includes a range of Unicode characters that correspond to various types of emojis, such as emoticons, symbols, and flags. By using this program, you can clean up text data by removing any emojis that may be present, which can be useful for text processing, analysis, or other applications where emojis are not desired. \ud83d\udcbb import re def remove_emojis ( text ): # Define a broader emoji pattern emoji_pattern = re. compile ( \"[\" u\"\\U0001F600-\\U0001F64F\" # emoticons u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS) u\"\\U00002702-\\U000027B0\" u\"\\U000024C2-\\U0001F251\" u\"\\U0001F900-\\U0001F9FF\" # supplemental symbols and pictographs u\"\\U0001FA00-\\U0001FA6F\" # chess symbols and more emojis...", "url": "https://huggingface.co/posts/aifeifei798/845431141575759", "date_published": "2025-03-19T09:24:54.867779"}, {"id": "https://huggingface.co/posts/AdinaY/835178247040445", "image": "", "title": "New 3D models from Tencent Hunyuan  are now available on the hub \ud83d\udd25", "content_text": "New 3D models from Tencent Hunyuan are now available on the hub \ud83d\udd25 \u2728 Hunyuan3D-2mv: multiview shape model for high quality generation \u2728 Hunyuan3D-2mini: 0.6B lightweight model for efficient workflows Model: tencent/Hunyuan3D-2mv tencent/Hunyuan3D-2mini Demo: tencent/Hunyuan3D-2mv See translation", "url": "https://huggingface.co/posts/AdinaY/835178247040445", "date_published": "2025-03-19T09:24:54.868056"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/887864437059007", "image": "", "title": "\ud83e\udd29 Ready to start creating synthetic data that bridges the Sim2Real gap and trains robust AI models?", "content_text": "\ud83e\udd29 Ready to start creating synthetic data that bridges the Sim2Real gap and trains robust AI models? \ud83c\udf1f Join Duality AI's free \u201cIntro to Falcon\u201d class this Thursday, March 20th. Dive into the Digital Twin Simulation workflows\u2014from setting up the simulation software, FalconEditor, to generating custom data. Get hands-on guidance, ask questions, and receive direct support to build a solid foundation for your simulations! Sign up here: https://forms.gle/xViMMSMsA5GP468J6 In the class, you will: \u2714\ufe0fExplore FalconCloud features like the digital twin library and cloud simulations \u2714\ufe0fSet up and navigate FalconEditor as a first-time user \u2714\ufe0fConfigure FalconEditor for scenario and data creation \u2714\ufe0fAssemble a sim-ready scenario using Falcon library twins \u2714\ufe0fGenerate multiple data types with Falcon\u2019s virtual sensors New: This class also covers the latest Falcon 5.1 features! Interested in checking out FalconEditor? Start your FREE EDU tier account to access online resources and lessons designed to...", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/887864437059007", "date_published": "2025-03-19T09:24:54.868589"}, {"id": "https://huggingface.co/posts/abhishek/496057967241133", "image": "", "title": "\ud83d\ude80 I'm thrilled to announce the launch of Arcee Conductor, a game-changing platform that's about to revolutionize the way you interact with AI models! \ud83e\udd16 As the pioneers of small language models (SLMs), we've been working tirelessly to bring you the most exciting innovation in the AI space.", "content_text": "\ud83d\ude80 I'm thrilled to announce the launch of Arcee Conductor, a game-changing platform that's about to revolutionize the way you interact with AI models! \ud83e\udd16 As the pioneers of small language models (SLMs), we've been working tirelessly to bring you the most exciting innovation in the AI space. Here's a quick TL;DR of what Arcee Conductor is all about: \ud83c\udf1f Choice and flexibility: Get access to multiple models, including our powerful SLMs and third-party LLMs, to choose the best one for your specific use case \ud83e\udd16 Intelligent routing: Our platform evaluates which model is best-suited for each of your queries, ensuring you get the most accurate results \ud83d\udcc8 Cost savings: Reduce your AI costs with our affordable SLMs, while still having access to leading LLMs when needed \ud83d\ude80 Easy to get started: Sign up now and try Arcee Conductor today, with 400 million tokens (a $200 value) on us! \ud83c\udf81 \ud83d\udcca Proven track record: Our SLMs have already racked up 222K+ downloads on Hugging Face, with customers seeing...", "url": "https://huggingface.co/posts/abhishek/496057967241133", "date_published": "2025-03-19T09:24:54.869075"}, {"id": "https://huggingface.co/posts/giux78/524924454012414", "image": "", "title": "@ mii-llm with", "content_text": "@ mii-llm with @ efederici @ mferraretto @ FinancialSupport and @ DeepMount00 we just released #Propaganda a framework designed to evaluate and train LLMs on political opinions and bias. We aim to analyze both open-source and closed-source LLMs to understand the political positions and biases expressed in their outputs. Moreover we provide a set of recipes to enforce political positions into the models by creating ad hoc curated datasets and by applying fine tuning techniques. By releasing our work in the open, we hope to foster contributions: https://github.com/mii-llm/propaganda This framework offers opportunities for expansion in various directions and could become the standard reference for evaluating LLMs on political topics, particularly those that influence public opinion. See translation", "url": "https://huggingface.co/posts/giux78/524924454012414", "date_published": "2025-03-19T09:24:54.869454"}, {"id": "https://huggingface.co/posts/etemiz/135866882319158", "image": "", "title": "My 1 year of work summarized.", "content_text": "My 1 year of work summarized. TLDR: by carefully curating datasets we can fix misinformation in AI. Then we can use that to measure misinformation in other AI. https://huggingface.co/blog/etemiz/building-a-beneficial-ai See translation", "url": "https://huggingface.co/posts/etemiz/135866882319158", "date_published": "2025-03-19T09:24:54.869690"}]}