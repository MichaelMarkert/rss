{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/mallocode200/873699618032778", "image": "", "title": "Hello everyone, happy to share with you my experimentation of a Deep Research Assistant, using 7 agents and a quality assurance pipeline :", "content_text": "Hello everyone, happy to share with you my experimentation of a Deep Research Assistant, using 7 agents and a quality assurance pipeline : \ud83e\udd16 What makes this special: \u2705 Agent-Based Architecture - 7 specialised AI agents working together: - Planner Agent - Strategic search planning - Search Agent - Multi-source web research - Writer Agent - Comprehensive report generation - Evaluator Agent - Automatic quality assessment - Optimiser Agent - Iterative improvement when needed - Email Agent - Professional report delivery - Clarifier Agent - Interactive query refinement \u2705 Quality Assurance Pipeline - Every report is scored (1-10) and automatically improved if it scores below 7/10 \u2705 Multiple Research Modes - From quick queries to deep, clarification-driven analysis \u2705 Production-Ready - Deployed on Hugging Face Spaces with comprehensive documentation \ud83d\udd27 Technical Stack: - Frontend: Gradio with theme-adaptive UI - Backend: OpenAI Agents framework - Integration: SendGrid for email delivery -...", "url": "https://huggingface.co/posts/mallocode200/873699618032778", "date_published": "2025-06-23T17:22:20.238067"}, {"id": "https://huggingface.co/posts/merve/432717492221522", "image": "", "title": "fav open-source multimodal reasoning model just got an update \ud83d\udd25", "content_text": "fav open-source multimodal reasoning model just got an update \ud83d\udd25 moonshotai/Kimi-VL-A3B-Thinking-2506 has > smarter with less tokens, small size (only 3B active params!!!) > better accuracy > video reasoning > higher resolution \ud83e\udd2f Read their blog https://huggingface.co/blog/moonshotai/kimi-vl-a3b-thinking-2506 See translation", "url": "https://huggingface.co/posts/merve/432717492221522", "date_published": "2025-06-23T17:22:20.238375"}, {"id": "https://huggingface.co/posts/Kseniase/930317125764918", "image": "", "title": "10 Techniques for Boosting LLM Reasoning in 2025", "content_text": "10 Techniques for Boosting LLM Reasoning in 2025 Everyone\u2019s chasing top reasoning, but sometimes it's still the bottleneck for many real-world tasks. This week, let's spotlight some powerful techniques that have shown promise in helping LLMs achieve more consistent logic, planning, and depth: 1. Retrieval-Augmented CoT Chaining (RAG+CoT) -> CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models (2504.13534) Combines Chain-of-Thought prompting with retrieval augmentation at intermediate steps. Relevant documents are fetched after each reasoning subgoal, updating context dynamically. Great for open-domain QA, math, logic and multi-hop fact-checking 2. Tool-use by example injection -> Self-Training Large Language Models for Tool-Use Without Demonstrations (2502.05867) Injects few-shot tool interaction examples during training to implicitly teach calling patterns. Helps in plug-and-play tool use without training new...", "url": "https://huggingface.co/posts/Kseniase/930317125764918", "date_published": "2025-06-23T17:22:20.239078"}, {"id": "https://huggingface.co/posts/yeonseok-zeticai/882369268235109", "image": "", "title": "\ud83d\udcab Next-Level On-Device AI Showdown", "content_text": "\ud83d\udcab Next-Level On-Device AI Showdown \ud83e\udebd See It to Believe It, How QWEN4b works at On-device environment without expensive GPU Cloud server? We\u2019ve crafted a side-by-side demo video showcasing both Jan-Nano and QWEN 4B in action\u2014no more wondering which model reigns supreme. Click play, compare their speeds, accuracy, and memory footprints, and decide which one fits your needs best! \ud83d\udc4b Why You Can\u2019t Miss This We are actively creating runnable sLLM environments for On-device AI. You can just build On-device AI apps within few hours. Including Jan-Nano, QWEN4b, there are several sLLM models ready to be used on your AI application!. \ud83e\udd11 Please feel free to use, because it is free to use!. Ready to Compare? Watch now, draw your own conclusions, and let us know which model you\u2019d deploy in your next edge-AI project! \ud83c\udf0d\ud83d\udca1 #OnDeviceAI #EdgeAI #AIShowdown #MLOptimization #DemoVideo #AIComparison See translation", "url": "https://huggingface.co/posts/yeonseok-zeticai/882369268235109", "date_published": "2025-06-23T17:22:20.239518"}, {"id": "https://huggingface.co/posts/merve/300457326273979", "image": "", "title": "y'all have been asking my opinion on how OCR models compare to each other \ud83d\udc40", "content_text": "y'all have been asking my opinion on how OCR models compare to each other \ud83d\udc40 I will leave three apps to compare newest models by @ prithivMLmods instead \u2935\ufe0f > compare Nanonets-OCR-s, Qwen2-VL-OCR-2B-Instruct, RolmOCR, Aya-Vision prithivMLmods/Multimodal-OCR > SmolDocling, Nanonets-OCR-s, MonkeyOCR, Typhoon-OCR-7B prithivMLmods/Multimodal-OCR2 > docscopeOCR, MonkeyOCR, coreOCR prithivMLmods/core-OCR See translation", "url": "https://huggingface.co/posts/merve/300457326273979", "date_published": "2025-06-23T17:22:20.239809"}, {"id": "https://huggingface.co/posts/codelion/825512802076392", "image": "", "title": "Adaptive Classifier: Dynamic Text Classification with Strategic Learning", "content_text": "Adaptive Classifier: Dynamic Text Classification with Strategic Learning New text classification system that learns continuously without catastrophic forgetting. Achieved 22.2% robustness improvement on adversarial datasets while maintaining clean data performance. \ud83c\udfaf THE PROBLEM Traditional classifiers require complete retraining when adding new classes. Expensive and time-consuming, especially with adversarial users trying to game the system. \ud83d\ude80 KEY INNOVATIONS \u2022 Hybrid memory-neural architecture (prototype-based + neural adaptation) \u2022 Strategic classification using game theory to predict and defend against manipulation \u2022 Elastic Weight Consolidation prevents catastrophic forgetting \ud83d\udcca RESULTS Tested on AI-Secure/adv_glue dataset: \u2022 Clean data: 80.0% \u2192 82.2% (+2.2%) \u2022 Manipulated data: 60.0% \u2192 82.2% (+22.2%) \u2022 Zero performance drop under adversarial attacks \ud83d\udd2c APPLICATIONS \u2022 Hallucination detection: 80.7% recall for RAG safety \u2022 LLM routing: 26.6% cost optimization improvement \u2022...", "url": "https://huggingface.co/posts/codelion/825512802076392", "date_published": "2025-06-23T17:22:20.240372"}, {"id": "https://huggingface.co/posts/brainhome/128948549150065", "image": "", "title": "Trinity-Synthesis: A Multi-Agent Architecture for AI Agents That Think Before They Speak", "content_text": "Trinity-Synthesis: A Multi-Agent Architecture for AI Agents That Think Before They Speak Ever felt your AI agent is \"shooting from the hip\"? It latches onto a single line of thought and fails to produce a robust, well-rounded plan. This is a common struggle I've called the \"AI Reasoning Paradox.\" To tackle this, I developed Trinity-Synthesis, a multi-agent architecture designed to force reflection and synthesis before delivering a final answer. The philosophy is simple: constructive conflict between different perspectives leads to better solutions. Here\u2019s the core idea: Instead of one agent, it uses four agents running on the same base model but with different \"personalities\" defined by their system prompts and temperature settings: \ud83e\udde0 The Visionary: Thinks outside the box (high temp: 1.0). \ud83d\udcca The Analyst: Focuses on logic, data, and structure (low temp: 0.3). \ud83d\udee0\ufe0f The Pragmatist: Evaluates feasibility, costs, and risks (mid temp: 0.5). These three \"thinkers\" work in parallel on the...", "url": "https://huggingface.co/posts/brainhome/128948549150065", "date_published": "2025-06-23T17:22:20.240982"}, {"id": "https://huggingface.co/posts/merve/128480916969769", "image": "", "title": "Release picks of the past week is here!  Find more models, datasets, Spaces here", "content_text": "Release picks of the past week is here! Find more models, datasets, Spaces here merve/june-20-releases-68594824d1f4dfa61aee3433 \ud83d\uddbc\ufe0f VLMs/OCR > moonshotai/Kimi-VL-A3B-Thinking-2506 is a powerful reasoning vision LM, 3B active params, smarter with less tokens, supports long documents, videos \ud83d\udc4f (OS) > nanonets/Nanonets-OCR-s is 3.75B params OCR model based on Qwen2.5VL-3B-Instruct (OS) \ud83d\udcac LLMs > moonshotai/Kimi-Dev-72B is a strong coding model based on Qwen2.5-72B (OS) > Mistral released mistralai/Mistral-Small-3.2-24B-Instruct-2506 , an update to their former model with better function calling & instruction following (OS) \ud83d\udde3\ufe0f Audio > Google released google/magenta-realtime , real time music generation & audio synthesis (cc-by-4) > kyutai released new speech-to-text models that come in 1B & 2B ( kyutai/stt-1b-en_fr , stt-2b-en_fr) with 0.5s and 2.5s delay 3D > Tencent released tencent/Hunyuan3D-2.1 an image-to-3D model (see below) See translation", "url": "https://huggingface.co/posts/merve/128480916969769", "date_published": "2025-06-23T17:22:20.241442"}, {"id": "https://huggingface.co/posts/mallocode200/392195427390890", "image": "", "title": "Hi everyone, I have updated the space from its original version.", "content_text": "Hi everyone, I have updated the space from its original version. mallocode200/Deep_Research_Assistant The main changes are: - The possibility for users to insert their own OpenAI API Key - The possibility for users to select the model they want from OpenAI - An improved UI to see the process going on when the research is running - A better Gradio interface for both light and dark themes Why use only OpenAI models? For this research assistant, I've chosen to focus exclusively on OpenAI models for several key reasons: - Reliability & Consistency: OpenAI's models provide consistent, high-quality responses that are crucial for research tasks requiring accuracy and depth. - Advanced Reasoning: Models like GPT-4o and o1-preview excel at complex analytical thinking, making them ideal for comprehensive research synthesis and evaluation. - Proven Performance: The research pipeline has been extensively tested and optimised specifically for OpenAI's model behaviours and capabilities. - API...", "url": "https://huggingface.co/posts/mallocode200/392195427390890", "date_published": "2025-06-23T17:22:20.241884"}, {"id": "https://huggingface.co/posts/seawolf2357/835684207699630", "image": "", "title": "\ud83d\ude80 VEO3 Real-Time: Real-time AI Video Generation with Self-Forcing", "content_text": "\ud83d\ude80 VEO3 Real-Time: Real-time AI Video Generation with Self-Forcing \ud83c\udfaf Core Innovation: Self-Forcing Technology VEO3 Real-Time, an open-source project challenging Google's VEO3, achieves real-time video generation through revolutionary Self-Forcing technology. Heartsync/VEO3-RealTime \u26a1 What is Self-Forcing? While traditional methods require 50-100 steps, Self-Forcing achieves the same quality in just 1-2 steps. Through self-correction and rapid convergence, this Distribution Matching Distillation (DMD) technique maintains quality while delivering 50x speed improvement. \ud83d\udca1 Technical Advantages of Self-Forcing 1. Extreme Speed Generates 4-second videos in under 30 seconds, with first frame streaming in just 3 seconds. This represents 50x faster performance than traditional diffusion methods. 2. Consistent Quality Maintains cinematic quality despite fewer steps, ensures temporal consistency, and minimizes artifacts. 3. Efficient Resource Usage Reduces GPU memory usage by 70% and heat...", "url": "https://huggingface.co/posts/seawolf2357/835684207699630", "date_published": "2025-06-23T17:22:20.242491"}]}