{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/prithivMLmods/844227545389355", "image": "", "title": "The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board \u2014 including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. \ud83e\udd17\ud83d\udd25", "content_text": "The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board \u2014 including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. \ud83e\udd17\ud83d\udd25 \u26a1 Space / App: prithivMLmods/Qwen3-VL-HF-Demo \u26a1 Github: https://github.com/prithivsakthiur/qwen3-vl-hf-demo The model\u2019s demo supports a wide range of tasks, including; Image Inference, Video Inference, PDF Inference, Image Captioning (VLA), GIF Inference. \u26a1 Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Thanks for granting the blazing-fast Zero GPU access, @ merve \ud83d\ude4f \u26a1 Other Pages > Multimodal VLMs July'25 : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 > VL caption \u2014 < Sep 15 \u201925 : prithivMLmods/vl-caption-sep-15-25-68c7f6d737985c63c13e2391 > Multimodal...", "url": "https://huggingface.co/posts/prithivMLmods/844227545389355", "date_published": "2025-10-13T05:23:55.311672"}, {"id": "https://huggingface.co/posts/sondhiArm/422203128655094", "image": "", "title": "Arm will be @ PyTorch Conference, Join Us!", "content_text": "Arm will be @ PyTorch Conference, Join Us! Join us on site October 22-23 to see how Arm empowers developers to build and deploy AI applications with ease using PyTorch and ExecuTorch. Learn about the latest AI technologies from Arm and our ecosystem while expanding your professional network alongside like-minded AI engineers. Learn more here: https://huggingface.co/blog/Arm/arm-at-pytorch-conference See translation", "url": "https://huggingface.co/posts/sondhiArm/422203128655094", "date_published": "2025-10-13T05:23:55.311977"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/350754844731753", "image": "", "title": "Ovi is Local Version of VEO 3 & SORA 2 - The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer on Windows even with a 6GB GPUs - Full Tutorial for Windows, RunPod and Massed Compute - Gradio App >", "content_text": "Ovi is Local Version of VEO 3 & SORA 2 - The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer on Windows even with a 6GB GPUs - Full Tutorial for Windows, RunPod and Massed Compute - Gradio App > https://youtu.be/T00VmkMQRPQ Tutorial : https://youtu.be/T00VmkMQRPQ Forget waiting lists and expensive APIs. The era of closed-off, corporate-controlled AI video generation is soon over. This is Ovi : The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer\u2014even with a 6GB GPU! This isn't just a demo; it's a full, step-by-step revolution. Tutorial Info In this ultimate A-Z guide, I'll show you EVERYTHING you need to know to install and master this Sora 2 and VEO3 like AI. We'll go from zero to generating incredible talking videos from text or a single image. \ud83d\udd25 In This Tutorial, You Will Learn To: \ud83c\udf93 Master the Ultimate SORA 2 and VEO 3...", "url": "https://huggingface.co/posts/MonsterMMORPG/350754844731753", "date_published": "2025-10-13T05:23:55.312567"}, {"id": "https://huggingface.co/posts/Ethank01/902315574816069", "image": "", "title": "\ud83d\ude80 No invite. No watermark. Just pure AI magic.", "content_text": "\ud83d\ude80 No invite. No watermark. Just pure AI magic. Experience Sora 2 on iMini \u2014 free for members \ud83d\udc49 https://imini.com/ See translation", "url": "https://huggingface.co/posts/Ethank01/902315574816069", "date_published": "2025-10-13T05:23:55.312793"}, {"id": "https://huggingface.co/posts/vincentg64/231735241059410", "image": "", "title": "Benchmarking xLLM and Specialized Language Models: New Approach & Results", "content_text": "Benchmarking xLLM and Specialized Language Models: New Approach & Results https://mltblog.com/4nzaKUb Standard benchmarking techniques using LLM as a judge have strong limitations. First it creates a circular loop and reflects the flaws present in the AI judges. Then, the perceived quality depends on the end user: an enterprise LLM appeals to professionals and business people, while a generic one appeals to laymen. The two have almost opposite criteria to assess the value. Finally, benchmarking metrics currently in use fail to capture many of the unique features of specialized LLMs, such as exhaustivity, or the quality of the relevancy and trustworthiness scores attached to each element in the response. In fact, besides xLLM, very few if any LLMs display such scores to the user. I now discuss these points, as well as the choice of test prompts, and preliminary results about xLLM, compared to others. -- Structured output vs standard response -- A peculiarity of xLLM is that if offers...", "url": "https://huggingface.co/posts/vincentg64/231735241059410", "date_published": "2025-10-13T05:23:55.313417"}, {"id": "https://huggingface.co/posts/Ethank01/441451500060564", "image": "", "title": "No invitation code needed \u2014 create AI videos with one click!", "content_text": "No invitation code needed \u2014 create AI videos with one click! Experience Sora 2, Veo 3, and Wan 2.2 all in one place on iMini. \ud83d\udc49 Try it here: https://imini.com/ See translation", "url": "https://huggingface.co/posts/Ethank01/441451500060564", "date_published": "2025-10-13T05:23:55.313643"}, {"id": "https://huggingface.co/posts/sergiopaniego/617301570898525", "image": "", "title": "Super nice intro to fine-tuning with TRL, just dropped by", "content_text": "Super nice intro to fine-tuning with TRL, just dropped by @ google (runs free on Colab)! They use SFT + QLoRA to fine-tune the tiny Gemma 3 270M model for emoji generation Here\u2019s what the fine-tuned model generates for the prompt: \u201cI'm learning to tweet\u201d \u2192 \ud83d\udc26\ud83d\udde3\ud83d\udcbb Colab: https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Demos/Emoji-Gemma-on-Web/resources/Fine_tune_Gemma_3_270M_for_emoji_generation.ipynb Try it out: google/emoji-gemma Learn more: https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/ See translation", "url": "https://huggingface.co/posts/sergiopaniego/617301570898525", "date_published": "2025-10-13T05:23:55.313973"}, {"id": "https://huggingface.co/posts/kanaria007/689097395708339", "image": "", "title": "\u2705 New Article: *Envy \u2014 The Structural Law of Relative Evaluation*", "content_text": "\u2705 New Article: *Envy \u2014 The Structural Law of Relative Evaluation* Title: \ud83e\udded Envy, Comparison, and the Structural Law of Relative Evaluation: Why \u201cThey Got More\u201d Hurts \u2014 and How Protocols Contain It \ud83d\udd17 https://huggingface.co/blog/kanaria007/envy-structural-law-of-relative-evaluation --- Summary: Envy isn\u2019t a moral glitch \u2014 it\u2019s a *relative-evaluation loop*. When identity meets comparison, the mind computes *delta-to-others* (status, attention, resources). If that delta breaches a threshold without a repair path, the loop escalates into resentment. Structured Intelligence makes this computable \u2014 and containable. > Envy compares levels. > *Integrity compares trajectories.* --- Why It Matters: \u2022 Turns envy from shame into an *auditable signal* (where, when, and why comparison spikes) \u2022 Provides *de-escalation protocols* for individuals, teams, and platforms \u2022 Guides product/organization design to reduce perceived unfairness and burnout --- What\u2019s Inside: \u2022 The Envy Loop: *trigger \u2192...", "url": "https://huggingface.co/posts/kanaria007/689097395708339", "date_published": "2025-10-13T05:23:55.314538"}, {"id": "https://huggingface.co/posts/Monica997/949496784214909", "image": "", "title": "\ud83c\udf0a Create Your Own Underwater World with iMini AI \u2013 No Invite, No Watermark!", "content_text": "\ud83c\udf0a Create Your Own Underwater World with iMini AI \u2013 No Invite, No Watermark! Just tried something incredible on iMini AI \u2014 I used the nano banana model to generate a stunning underwater world image \ud83d\udc20, then turned it into a dynamic video using the latest Sora 2 model. The result? It looks so real, like I actually filmed it while scuba diving! \ud83e\udd3f\u2728 The water ripples, the light rays, even the fish movement \u2014 all generated by AI in seconds. The best part? \u2705 No invite code required \u2705 No watermark \u2705 Members can generate without using credits If you\u2019re into AI visuals or creative video experiments, this one\u2019s a must-try. \ud83d\udc49 Experience it now at https://imini.com/ See translation", "url": "https://huggingface.co/posts/Monica997/949496784214909", "date_published": "2025-10-13T05:23:55.314925"}, {"id": "https://huggingface.co/posts/etemiz/180950130032944", "image": "", "title": "Another abliteration by huihui which had big positive impact!", "content_text": "Another abliteration by huihui which had big positive impact! huihui-ai/Huihui-GLM-4.5-Air-abliterated-GGUF @ huihui-ai See translation", "url": "https://huggingface.co/posts/etemiz/180950130032944", "date_published": "2025-10-13T05:23:55.315134"}]}