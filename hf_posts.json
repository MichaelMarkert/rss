{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/openfree/729932908902684", "image": "", "title": "Huggingface Space Leaderboard \ud83d\ude80", "content_text": "Huggingface Space Leaderboard \ud83d\ude80 Hello Huggingface Community! VIDraft/Space-Leaderboard We are excited to introduce the Huggingface Space Leaderboard, a service that lets you view the latest trending Spaces on the Huggingface platform at a glance. This service helps you quickly explore a wide range of creative projects and will spark new inspiration for your own ideas. \ud83c\udf89 Detailed Feature Overview 1. Real-time Trend Reflection Automated Aggregation: Analyzes and ranks over 500 popular Spaces on Huggingface in real time. Accurate Ranking: Combines various metrics such as likes, engagement, and creation time to accurately reflect the latest trends. Instant Updates: Data is continuously updated, so you always see the most current popular Spaces. 2. Intuitive Preview 70% Scaled Preview: Each Space is displayed at 70% scale, providing a neat and clear preview at a glance. Easy Visual Comparison: View multiple Spaces side by side to easily compare their designs and functionalities. Error...", "url": "https://huggingface.co/posts/openfree/729932908902684", "date_published": "2025-03-13T05:20:23.728636"}, {"id": "https://huggingface.co/posts/BrigitteTousi/467635548040677", "image": "", "title": "LeRobot goes to driving school! \ud83d\ude97\ud83d\ude97\ud83d\ude97", "content_text": "LeRobot goes to driving school! \ud83d\ude97\ud83d\ude97\ud83d\ude97 Hugging Face just announced a new collab with Yaak to bring the largest open-source self-driving dataset to LeRobot! Major kudos to HF's @ cadene , as well as @ sandhawalia , @ Shnissen and the Yaak team! Check out the blog post here: https://huggingface.co/blog/lerobot-goes-to-driving-school See translation", "url": "https://huggingface.co/posts/BrigitteTousi/467635548040677", "date_published": "2025-03-13T05:20:23.728968"}, {"id": "https://huggingface.co/posts/jasoncorkill/287919306604058", "image": "", "title": "Benchmarking Google's Veo2: How Does It Compare?", "content_text": "Benchmarking Google's Veo2: How Does It Compare? The results did not meet expectations. Veo2 struggled with style consistency and temporal coherence, falling behind competitors like Runway, Pika, Tencent, and even Alibaba. While the model shows promise, its alignment and quality are not yet there. Google recently launched Veo2, its latest text-to-video model, through select partners like fal.ai. As part of our ongoing evaluation of state-of-the-art generative video models, we rigorously benchmarked Veo2 against industry leaders. We generated a large set of Veo2 videos spending hundreds of dollars in the process and systematically evaluated them using our Python-based API for human and automated labeling. Check out the ranking here: https://www.rapidata.ai/leaderboard/video-models Rapidata/text-2-video-human-preferences-veo2 See translation", "url": "https://huggingface.co/posts/jasoncorkill/287919306604058", "date_published": "2025-03-13T05:20:23.729319"}, {"id": "https://huggingface.co/posts/prithivMLmods/635315600525918", "image": "", "title": "Gemma-3-4B : Image and Video Inference \ud83d\uddbc\ufe0f\ud83c\udfa5", "content_text": "Gemma-3-4B : Image and Video Inference \ud83d\uddbc\ufe0f\ud83c\udfa5 \ud83e\udde4Space: prithivMLmods/Imagineo-Chat @ gemma3-4b : {Tag + Space_+ 'prompt'} @ gemma3-4b-video : {Tag + Space_+ 'prompt'} By default, it runs: prithivMLmods/Qwen2-VL-OCR-2B-Instruct Additionally, I have also tested Aya-Vision 8B vs Custom Qwen2-VL-OCR for OCR with test case samples on messy handwriting for experimental purposes to optimize edge device VLMs for Optical Character Recognition. \ud83d\udcdcRead the blog here: https://huggingface.co/blog/prithivMLmods/aya-vision-vs-qwen2vl-ocr-2b See translation", "url": "https://huggingface.co/posts/prithivMLmods/635315600525918", "date_published": "2025-03-13T05:20:23.729663"}, {"id": "https://huggingface.co/posts/prithivMLmods/506493789164421", "image": "", "title": "Variable Demo for Two Image-to-Text-to-Text Multimodals \ud83c\udf20", "content_text": "Variable Demo for Two Image-to-Text-to-Text Multimodals \ud83c\udf20 \ud83d\udcdcSpace: prithivMLmods/Multimodal-OCR By default, it will use: prithivMLmods/Qwen2-VL-OCR-2B-Instruct or prithivMLmods/Qwen2-VL-OCR2-2B-Instruct To trigger Aya-Vision's 8B by @ aya-vision , use the prompt: CohereForAI/aya-vision-8b See translation", "url": "https://huggingface.co/posts/prithivMLmods/506493789164421", "date_published": "2025-03-13T05:20:23.729952"}, {"id": "https://huggingface.co/posts/BrigitteTousi/858963061028741", "image": "", "title": "Regardless of X being down or not, so glad I can rely on HF Posts for AI news \u2764\ufe0f\ud83e\udd17", "content_text": "Regardless of X being down or not, so glad I can rely on HF Posts for AI news \u2764\ufe0f\ud83e\udd17 See translation", "url": "https://huggingface.co/posts/BrigitteTousi/858963061028741", "date_published": "2025-03-13T05:20:23.730175"}, {"id": "https://huggingface.co/posts/julien-c/158943939527784", "image": "", "title": "Important notice \ud83d\udea8", "content_text": "Important notice \ud83d\udea8 For Inference Providers who have built support for our Billing API (currently: Fal, Novita, HF-Inference \u2013 with more coming soon), we've started enabling Pay as you go (=PAYG) What this means is that you can use those Inference Providers beyond the free included credits, and they're charged to your HF account. You can see it on this view: any provider that does not have a \"Billing disabled\" badge, is PAYG-compatible. See translation", "url": "https://huggingface.co/posts/julien-c/158943939527784", "date_published": "2025-03-13T05:20:23.730483"}, {"id": "https://huggingface.co/posts/thomwolf/597591144299421", "image": "", "title": "We've kept pushing our Open-R1 project, an open initiative to replicate and extend the techniques behind DeepSeek-R1.", "content_text": "We've kept pushing our Open-R1 project, an open initiative to replicate and extend the techniques behind DeepSeek-R1. And even we were mind-blown by the results we got with this latest model we're releasing: \u26a1\ufe0fOlympicCoder ( open-r1/OlympicCoder-7B and open-r1/OlympicCoder-32B ) It's beating Claude 3.7 on (competitive) programming \u2013a domain Anthropic has been historically really strong at\u2013 and it's getting close to o1-mini/R1 on olympiad level coding with just 7B parameters! And the best part is that we're open-sourcing all about its training dataset, the new IOI benchmark, and more in our Open-R1 progress report #3: https://huggingface.co/blog/open-r1/update-3 Datasets are are releasing: - open-r1/codeforces - open-r1/codeforces-cots - open-r1/ioi - open-r1/ioi-test-cases - open-r1/ioi-sample-solutions - open-r1/ioi-cots - open-r1/ioi-2024-model-solutions See translation", "url": "https://huggingface.co/posts/thomwolf/597591144299421", "date_published": "2025-03-13T05:20:23.730905"}, {"id": "https://huggingface.co/posts/fdaudens/732602870953765", "image": "", "title": "\ud83d\udd25The Open R1 team just dropped OlympicCoder and it's wild:", "content_text": "\ud83d\udd25The Open R1 team just dropped OlympicCoder and it's wild: - 7B model outperforms Claude 3.7 Sonnet on IOI benchmark (yes, 7B!!) - 32B crushes all open-weight models tested, even those 100x larger \ud83e\udd2f Open-sourcing the future of code reasoning! \ud83d\ude80 Check it out https://huggingface.co/blog/open-r1/update-3 See translation", "url": "https://huggingface.co/posts/fdaudens/732602870953765", "date_published": "2025-03-13T05:20:23.731198"}, {"id": "https://huggingface.co/posts/tomaarsen/782540332014764", "image": "", "title": "An assembly of 18 European companies, labs, and universities have banded together to launch \ud83c\uddea\ud83c\uddfa EuroBERT! It's a state-of-the-art multilingual encoder for 15 European languages, designed to be finetuned for retrieval, classification, etc.", "content_text": "An assembly of 18 European companies, labs, and universities have banded together to launch \ud83c\uddea\ud83c\uddfa EuroBERT! It's a state-of-the-art multilingual encoder for 15 European languages, designed to be finetuned for retrieval, classification, etc. \ud83c\uddea\ud83c\uddfa 15 Languages: English, French, German, Spanish, Chinese, Italian, Russian, Polish, Portuguese, Japanese, Vietnamese, Dutch, Arabic, Turkish, Hindi 3\ufe0f\u20e3 3 model sizes: 210M, 610M, and 2.1B parameters - very very useful sizes in my opinion \u27a1\ufe0f Sequence length of 8192 tokens! Nice to see these higher sequence lengths for encoders becoming more common. \u2699\ufe0f Architecture based on Llama, but with bi-directional (non-causal) attention to turn it into an encoder. Flash Attention 2 is supported. \ud83d\udd25 A new Pareto frontier (stronger *and* smaller) for multilingual encoder models \ud83d\udcca Evaluated against mDeBERTa, mGTE, XLM-RoBERTa for Retrieval, Classification, and Regression (after finetuning for each task separately): EuroBERT punches way above its weight. \ud83d\udcdd...", "url": "https://huggingface.co/posts/tomaarsen/782540332014764", "date_published": "2025-03-13T05:20:23.731730"}]}