{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Jaward/982484477481896", "image": "", "title": "made a few improvements on custom grpo trainer:", "content_text": "made a few improvements on custom grpo trainer: - added sequence similarity reward (seems to work) - improved vllm support (5x inference speed) - adjusted reward scores (this helped with format/accuracy) - can now push to hf hub (already pushed mine lol: Jaward/smollm2_360m_grpo_gsm8k_reasoner ) Code: https://github.com/Jaykef/ai-algorithms/blob/main/smollm2_360M_135M_grpo_gsm8k.ipynb See translation", "url": "https://huggingface.co/posts/Jaward/982484477481896", "date_published": "2025-03-03T09:24:55.521034"}, {"id": "https://huggingface.co/posts/Kseniase/433849056207490", "image": "", "title": "9 types of \"Chain-of-...\" approaches:", "content_text": "9 types of \"Chain-of-...\" approaches: Chain-of-Thought (CoT) prompting enhances reasoning in AI models by breaking down complex problems into step-by-step logical sequences. It continues proving its effectiveness, especially in top-performing reasoning models. However, there are other similar methods, that expand CoT and can be used for different purposes. Here are 9 of them: 1. Chain-of-Action-Thought (COAT) -> Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search (2502.02508) Helps model decide when to keep thinking, double-check their work, or try a different approach, using special guiding tokens. 2. Chain of Draft (CoD) -> Chain of Draft: Thinking Faster by Writing Less (2502.18600) It helps model generate short but meaningful reasoning steps, cutting costs and making processing faster 3. Chain-of-Agents -> Chain of Agents: Large Language Models Collaborating on Long-Context Tasks (2406.02818) Uses multi-agent...", "url": "https://huggingface.co/posts/Kseniase/433849056207490", "date_published": "2025-03-03T09:24:55.521634"}, {"id": "https://huggingface.co/posts/mkurman/255930071052172", "image": "", "title": "Introducing a new architecture, MedIT One \u2013 a single-token transformer with LSTM-like recurrence.", "content_text": "Introducing a new architecture, MedIT One \u2013 a single-token transformer with LSTM-like recurrence. It is extremely fast in training and inference, but we lack funding for large-scale training. Enjoy \ud83c\udf53 https://github.com/MedITSolutionsKurman/medit-one See translation", "url": "https://huggingface.co/posts/mkurman/255930071052172", "date_published": "2025-03-03T09:24:55.522003"}, {"id": "https://huggingface.co/posts/prithivMLmods/305640045790864", "image": "", "title": "Dropping some of the custom fine-tunes based on SigLIP2,", "content_text": "Dropping some of the custom fine-tunes based on SigLIP2, with a single-label classification problem type! \ud83c\udf00\ud83e\udde4 - AI vs Deepfake vs Real : prithivMLmods/AI-vs-Deepfake-vs-Real-Siglip2 - Deepfake Detect : prithivMLmods/Deepfake-Detect-Siglip2 - Fire Detection : prithivMLmods/Fire-Detection-Siglip2 - Deepfake Quality Assess : prithivMLmods/Deepfake-Quality-Assess-Siglip2 - Guard Against Unsafe Content : prithivMLmods/Guard-Against-Unsafe-Content-Siglip2 \ud83c\udf20Collection : prithivMLmods/siglip2-custom-67bcdb2de8fe96b99fb4e19e See translation", "url": "https://huggingface.co/posts/prithivMLmods/305640045790864", "date_published": "2025-03-03T09:24:55.522349"}, {"id": "https://huggingface.co/posts/singhsidhukuldeep/815565847250252", "image": "", "title": "Exciting New Tool for Knowledge Graph Extraction from Plain Text!", "content_text": "Exciting New Tool for Knowledge Graph Extraction from Plain Text! I just came across a groundbreaking new tool called KGGen that's solving a major challenge in the AI world - the scarcity of high-quality knowledge graph data. KGGen is an open-source Python package that leverages language models to extract knowledge graphs (KGs) from plain text. What makes it special is its innovative approach to clustering related entities, which significantly reduces sparsity in the extracted KGs. The technical approach is fascinating: 1. KGGen uses a multi-stage process involving an LLM (GPT-4o in their implementation) to extract entities and relations from source text 2. It aggregates graphs across sources to reduce redundancy 3. Most importantly, it applies iterative LM-based clustering to refine the raw graph The clustering stage is particularly innovative - it identifies which nodes and edges refer to the same underlying entities or concepts. This normalizes variations in tense, plurality,...", "url": "https://huggingface.co/posts/singhsidhukuldeep/815565847250252", "date_published": "2025-03-03T09:24:55.522820"}, {"id": "https://huggingface.co/posts/eaddario/832567461491467", "image": "", "title": "Squeezing out tensor bits?", "content_text": "Squeezing out tensor bits? I have been tinkering with quantization and pruning to reduce model sizes. So far, I've had modest success in producing, on average, 8% smaller versions with negligible loss of quality, and I think further reductions in the 10-15% range are realistic, but I've come across a behaviour I wasn't expecting! Part of the process I'm following consists of quantizing the embedding and output layers aggressively. Since the embedding layer is more about lookup than complex computation, the vectors representing the relative distances between embeddings are usually preserved well enough making this layer fairly robust to quantization. So far, so good. The output layer, on the other hand, maps the final hidden state to the vocabulary logits and therefore, small changes in these logits could lead to a different probability distribution over the vocabulary, resulting in incorrect word predictions, or so I thought. Surprisingly, I'm finding that even at Q2_K the loss of...", "url": "https://huggingface.co/posts/eaddario/832567461491467", "date_published": "2025-03-03T09:24:55.523256"}, {"id": "https://huggingface.co/posts/rizavelioglu/517500066145145", "image": "", "title": "Comparing reconstruction quality of various VAEs with an interactive demo", "content_text": "Comparing reconstruction quality of various VAEs with an interactive demo rizavelioglu/vae-comparison See translation", "url": "https://huggingface.co/posts/rizavelioglu/517500066145145", "date_published": "2025-03-03T09:24:55.523473"}, {"id": "https://huggingface.co/posts/lingvanex-mt/205421793754165", "image": "", "title": "Dear HF Community!", "content_text": "Dear HF Community! Our company open-sourced machine translation models for 12 rare languages under MIT license. You can use them freely with OpenNMT translation framework. Each model is about 110 mb and has an excellent performance, ( about 40000 characters / s on Nvidia RTX 3090 ) Download models there https://huggingface.co/lingvanex You can test translation quality there: https://lingvanex.com/translate/ See translation", "url": "https://huggingface.co/posts/lingvanex-mt/205421793754165", "date_published": "2025-03-03T09:24:55.523745"}, {"id": "https://huggingface.co/posts/Quazim0t0/128052320188055", "image": "", "title": "Debugging Tags:", "content_text": "Debugging Tags: Imagine, Associated Thoughts, Dialectical Analysis, Backwards Induction, Metacognition, and Normal Thought Processes such as <think> or <begin_of_thought> Edit: Uploaded new images w/ a Open WebUI function to organize the tags. Open WebUI Function: https://openwebui.com/f/quaz93/imagine_phi This Phi-4 model is part of a test project that I called Micro-Dose. My goal was to use a small dataset to activate reasoning and other cognitive processes without relying on a large dataset. I found that this was possible with a tiny dataset of just 90 rows, specifically designed as math problems. In the initial iterations, the dataset only activated reasoning when a math-related question was asked. I then made a few changes to the dataset\u2019s structure, including the order of information and the naming of tags. You can see the sample results in the pictures. Not really anything special, just thought I'd share. Tweaked the dataset a bit: Quazim0t0/Imagine-Phi-v0.2-GGUF...", "url": "https://huggingface.co/posts/Quazim0t0/128052320188055", "date_published": "2025-03-03T09:24:55.524220"}, {"id": "https://huggingface.co/posts/Bils/313728910739163", "image": "", "title": "create amazing audio ads in just a few steps", "content_text": "create amazing audio ads in just a few steps Bils/AIPromoStudio See translation", "url": "https://huggingface.co/posts/Bils/313728910739163", "date_published": "2025-03-03T09:24:55.524426"}]}