{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/seawolf2357/796388354612946", "image": "", "title": "\ud83d\udd25 AgenticAI: The Ultimate Multimodal AI with 16 MBTI Girlfriend Personas! \ud83d\udd25", "content_text": "\ud83d\udd25 AgenticAI: The Ultimate Multimodal AI with 16 MBTI Girlfriend Personas! \ud83d\udd25 Hello AI community! Today, our team is thrilled to introduce AgenticAI, an innovative open-source AI assistant that combines deep technical capabilities with uniquely personalized interaction. \ud83d\udc98 \ud83d\udee0\ufe0f MBTI 16 Types SPACES Collections link seawolf2357/heartsync-mbti-67f793d752ef1fa542e16560 \u2728 16 MBTI Girlfriend Personas Complete MBTI Implementation: All 16 MBTI female personas modeled after iconic characters (Dana Scully, Lara Croft, etc.) Persona Depth: Customize age groups and thinking patterns for hyper-personalized AI interactions Personality Consistency: Each MBTI type demonstrates consistent problem-solving approaches, conversation patterns, and emotional expressions \ud83d\ude80 Cutting-Edge Multimodal Capabilities Integrated File Analysis: Deep analysis and cross-referencing of images, videos, CSV, PDF, and TXT files Advanced Image Understanding: Interprets complex diagrams, mathematical equations, charts, and...", "url": "https://huggingface.co/posts/seawolf2357/796388354612946", "date_published": "2025-04-11T05:23:14.863324"}, {"id": "https://huggingface.co/posts/hesamation/789492772324435", "image": "", "title": "Google published a 69-page whitepaper on Prompt Engineering and its best practices, a must-read if you are using LLMs in production:", "content_text": "Google published a 69-page whitepaper on Prompt Engineering and its best practices, a must-read if you are using LLMs in production: > zero-shot, one-shot, few-shot > system prompting > chain-of-thought (CoT) > ReAct LINK: https://www.kaggle.com/whitepaper-prompt-engineering > code prompting > best practices See translation", "url": "https://huggingface.co/posts/hesamation/789492772324435", "date_published": "2025-04-11T05:23:14.863607"}, {"id": "https://huggingface.co/posts/merterbak/235850739835485", "image": "", "title": "Qwen 3 can launch very soon. \ud83d\udc40", "content_text": "Qwen 3 can launch very soon. \ud83d\udc40 https://github.com/ggml-org/llama.cpp/pull/12828 See translation", "url": "https://huggingface.co/posts/merterbak/235850739835485", "date_published": "2025-04-11T05:23:14.863828"}, {"id": "https://huggingface.co/posts/fdaudens/513864434208106", "image": "", "title": "\ud83c\udfa8 Designers, meet OmniSVG! This new model helps you create professional vector graphics from text/images, generate editable SVGs from icons to detailed characters, convert rasters to vectors, maintain style consistency with references, and integrate into your workflow.", "content_text": "\ud83c\udfa8 Designers, meet OmniSVG! This new model helps you create professional vector graphics from text/images, generate editable SVGs from icons to detailed characters, convert rasters to vectors, maintain style consistency with references, and integrate into your workflow. @ OmniSVG See translation", "url": "https://huggingface.co/posts/fdaudens/513864434208106", "date_published": "2025-04-11T05:23:14.864150"}, {"id": "https://huggingface.co/posts/ajibawa-2023/282296415348325", "image": "", "title": "Hi All,  I recently released two Audio datasets  which are generated using my earlier released dataset:", "content_text": "Hi All, I recently released two Audio datasets which are generated using my earlier released dataset: ajibawa-2023/Children-Stories-Collection First Audio Dataset:https://huggingface.co/datasets/ajibawa-2023/Audio-Children-Stories-Collection-Large has 5600++ stories in .mp3 format. Second Audio Dataset:https://huggingface.co/datasets/ajibawa-2023/Audio-Children-Stories-Collection has 600 stories in .mp3 format. See translation", "url": "https://huggingface.co/posts/ajibawa-2023/282296415348325", "date_published": "2025-04-11T05:23:14.864429"}, {"id": "https://huggingface.co/posts/Steven10429/887336506731659", "image": "", "title": "I got rejected from llama4.", "content_text": "I got rejected from llama4. So that means I can use quantinized model without following their TOS. Interesting. See translation", "url": "https://huggingface.co/posts/Steven10429/887336506731659", "date_published": "2025-04-11T05:23:14.864648"}, {"id": "https://huggingface.co/posts/jasoncorkill/726469711226418", "image": "", "title": "\ud83d\udd25 Yesterday was a fire day!", "content_text": "\ud83d\udd25 Yesterday was a fire day! We dropped two brand-new datasets capturing Human Preferences for text-to-video and text-to-image generations powered by our own crowdsourcing tool! Whether you're working on model evaluation, alignment, or fine-tuning, this is for you. 1. Text-to-Video Dataset (Pika 2.2 model): Rapidata/text-2-video-human-preferences-pika2.2 2. Text-to-Image Dataset (Reve-AI Halfmoon): Rapidata/Reve-AI-Halfmoon_t2i_human_preference Let\u2019s train AI on AI-generated content with humans in the loop. Let\u2019s make generative models that actually get us. See translation", "url": "https://huggingface.co/posts/jasoncorkill/726469711226418", "date_published": "2025-04-11T05:23:14.864981"}, {"id": "https://huggingface.co/posts/danielhanchen/859959880164586", "image": "", "title": "You can now run Llama 4 on your own local device! \ud83e\udd99", "content_text": "You can now run Llama 4 on your own local device! \ud83e\udd99 Run our Dynamic 1.78-bit and 2.71-bit Llama 4 GGUFs: unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF You can run them on llama.cpp and other inference engines. See our guide here: https://docs.unsloth.ai/basics/tutorial-how-to-run-and-fine-tune-llama-4 See translation", "url": "https://huggingface.co/posts/danielhanchen/859959880164586", "date_published": "2025-04-11T05:23:14.865269"}, {"id": "https://huggingface.co/posts/onekq/215048320445842", "image": "", "title": "We desperately need GPU for model inference. CPU can't replace GPU.", "content_text": "We desperately need GPU for model inference. CPU can't replace GPU. I will start with the basics. GPU is designed to serve predictable workloads with many parallel units (pixels, tensors, tokens). So a GPU allocates as much transistor budget as possible to build thousands of compute units (Cuda cores in NVidia or execution units in Apple Silicon), each capable of running a thread. But CPU is designed to handle all kinds of workloads. CPU cores are much larger (hence a lot fewer) with branch prediction and other complex things. In addition, more and more transistors are allocated to build larger cache (~50% now) to house the unpredictable, devouring the compute budget. Generalists can't beat specialists. See translation", "url": "https://huggingface.co/posts/onekq/215048320445842", "date_published": "2025-04-11T05:23:14.865596"}, {"id": "https://huggingface.co/posts/AdinaY/423063846745216", "image": "", "title": "Moonshot AI \u6708\u4e4b\u6697\u9762 \ud83c\udf1b @Kimi_Moonshotis just dropped an MoE VLM  and an MoE Reasoning VLM on the hub!!", "content_text": "Moonshot AI \u6708\u4e4b\u6697\u9762 \ud83c\udf1b @Kimi_Moonshotis just dropped an MoE VLM and an MoE Reasoning VLM on the hub!! Model:https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85 \u27283B with MIT license \u2728Long context windows up to 128K \u2728Strong multimodal reasoning (36.8% on MathVision, on par with 10x larger models) and agent skills (34.5% on ScreenSpot-Pro) See translation", "url": "https://huggingface.co/posts/AdinaY/423063846745216", "date_published": "2025-04-11T05:23:14.865878"}]}