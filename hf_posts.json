{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Monica997/874620000877286", "image": "", "title": "AI Just Made My Cat the King of Emojis \ud83d\udc51\ud83d\udc31\ud83d\ude02", "content_text": "AI Just Made My Cat the King of Emojis \ud83d\udc51\ud83d\udc31\ud83d\ude02 Never thought I\u2019d see this \u2014 but with iMini\u2019s nano banana model, my cat is now a full emoji + sticker pack \ud83c\udfa8\u2728 Used the 9-grid meme template + cartoon sticker generator, and in just ONE click \ud83d\udc49 my ordinary cat photo turned into a hilarious, cute, and super shareable set of stickers \ud83d\udcac\ud83d\udd25 No need to master complicated nano banana prompts \u2014 iMini handles everything. Perfect for chats, socials, or just showing off your pet\u2019s new \u201cdigital identity.\u201d \ud83d\udc49 Try it here: https://imini.com/nano-banana Who else wants their pet to be the next emoji star? \ud83c\udf1f See translation", "url": "https://huggingface.co/posts/Monica997/874620000877286", "date_published": "2025-09-25T09:24:50.051054"}, {"id": "https://huggingface.co/posts/prithivMLmods/355225487543965", "image": "", "title": "Photo-Mate-i2i \u2013 a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages!", "content_text": "Photo-Mate-i2i \u2013 a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages! \u26a1Demo: prithivMLmods/Photo-Mate-i2i \u2699\ufe0fHow to Use: prithivMLmods/Photo-Mate-i2i#2 \ud83d\udc68\u200d\ud83d\udd27i2i-Kontext(Experimental LoRAs): prithivMLmods/i2i-kontext-exp-68ce573b5c0623476b636ec7 See translation", "url": "https://huggingface.co/posts/prithivMLmods/355225487543965", "date_published": "2025-09-25T09:24:50.051368"}, {"id": "https://huggingface.co/posts/nroggendorff/916862110503909", "image": "", "title": "I'm sorry, what?", "content_text": "I'm sorry, what?", "url": "https://huggingface.co/posts/nroggendorff/916862110503909", "date_published": "2025-09-25T09:24:50.051546"}, {"id": "https://huggingface.co/posts/yeonseok-zeticai/506441566129403", "image": "", "title": "YOLOv11 Complete On-device Study", "content_text": "YOLOv11 Complete On-device Study - {NPU vs GPU vs CPU} Across All Model Variants We've just completed comprehensive benchmarking of the entire YOLOv11 family on ZETIC.MLange. Here's what every ML engineer needs to know. \ud83d\udcca Key Findings Across 5 Model Variants (XL to Nano): 1. NPU Dominance in Efficiency: - YOLOv11n: 1.72ms on NPU vs 53.60ms on CPU (31x faster) - Memory footprint: 0-65MB across all variants - Consistent sub-10ms inference even on XL models 2. The Sweet Spot - YOLOv11s: - NPU: 3.23ms @ 95.57% mAP - Perfect balance: 36MB model, production-ready speed - 10x faster than GPU, 30x faster than CPU 3. Surprising Discovery: Medium models (YOLOv11m) show unusual GPU performance patterns - NPU outperforms GPU by 4x (9.55ms vs 35.82ms), suggesting current GPU kernels aren't optimized for mid-size architectures. 4. Production Insights: - XL/Large: GPU still competitive for batch processing - Small/Nano: NPU absolutely crushes everything else - Memory scaling: Linear from 10MB...", "url": "https://huggingface.co/posts/yeonseok-zeticai/506441566129403", "date_published": "2025-09-25T09:24:50.052103"}, {"id": "https://huggingface.co/posts/yeonseok-zeticai/752870941871415", "image": "", "title": "\ud83c\udfaf RetinaFace On-Device Deployment Study: NPU Acceleration Breakthrough!", "content_text": "\ud83c\udfaf RetinaFace On-Device Deployment Study: NPU Acceleration Breakthrough! (Check details at :https://mlange.zetic.ai/p/Steve/RetinaFace) TL;DR: Successfully deployed RetinaFace with ZETIC.MLange achieving 1.43ms inference on mobile NPU! \ud83d\udd0d Complete Performance Analysis: Latency Comparison: - NPU: 1.43ms (Winner! \ud83c\udfc6) - GPU: 3.75ms - CPU: 21.42ms Accuracy Metrics - SNR: - FP16: 56.98 dB - Integer Quantized: 48.03 dB (Precision-Performance: Excellent trade-off maintained) Memory Footprint: - Model Size: 2.00 MB (highly compressed) - Runtime Memory: 14.58 MB peak - Deployment Ready: \u2705 Production optimized \ud83d\udee0 Technical Implementation: (Runnable with Copy & Paste at the MLange link!) \ud83d\udcca Device Compatibility Matrix: Tested on 50+ devices including Samsung Galaxy series, Google Pixel lineup, and Xiaomi devices, iPhones and iPads. Consistent sub-5ms performance across the board! \ud83d\ude80 Applications Unlocked: - Real-time AR/VR face tracking - Privacy-preserving edge authentication - Live video...", "url": "https://huggingface.co/posts/yeonseok-zeticai/752870941871415", "date_published": "2025-09-25T09:24:50.052610"}, {"id": "https://huggingface.co/posts/AdinaY/290924120685458", "image": "", "title": "BAAI has released ROME\ud83d\udd25 evaluating 30+ large reasoning models on text & visual reasoning", "content_text": "BAAI has released ROME\ud83d\udd25 evaluating 30+ large reasoning models on text & visual reasoning FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions (2509.17177) \u2728Tests visual reasoning, not just recognition \u2728Covers capability \u00d7 alignment \u00d7 safety \u00d7 efficiency \u2728More transparent & reliable (less data contamination) \u2728Helps make real-world deployment choices See translation", "url": "https://huggingface.co/posts/AdinaY/290924120685458", "date_published": "2025-09-25T09:24:50.052908"}, {"id": "https://huggingface.co/posts/prithivMLmods/322831563234696", "image": "", "title": "Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.\ud83e\udd17\u2728", "content_text": "Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.\ud83e\udd17\u2728 prithivMLmods/PhotoCleanser-i2i : Remove objects while preserving the rest of the image. prithivMLmods/Photo-Restore-i2i : Restore old photos into moderately colorized, detailed images. prithivMLmods/Polaroid-Warm-i2i : Seamless vintage Polaroid-style images with warm, faded tones. prithivMLmods/Yarn-Photo-i2i : Convert images into yarn-stitched artwork while retaining key details. prithivMLmods/Monochrome-Pencil : Turn images into monochrome pencil sketches while keeping original features. \u2728Note: All the above models share the same auto-labeling multimodal VLM captioning model, prithivMLmods/DeepCaption-VLA-7B , which is used...", "url": "https://huggingface.co/posts/prithivMLmods/322831563234696", "date_published": "2025-09-25T09:24:50.053376"}, {"id": "https://huggingface.co/posts/salma-remyx/828425996651513", "image": "", "title": "We're joining the", "content_text": "We're joining the @ ag2 team in discord to present a deep-dive into how we've used the framework to build GitRank in their Community Talks The GitRank pipeline is used to: \ud83d\udcf0 power personalized paper recommendations \ud83d\udc33 build environments as Docker Images \ud83c\udfaf implement core-methods as PRs for your target repo Don't miss it! Tomorrow, Sept 25 at 9:00 am PST: https://calendar.app.google/3soCpuHupRr96UaF8 See translation", "url": "https://huggingface.co/posts/salma-remyx/828425996651513", "date_published": "2025-09-25T09:24:50.053660"}, {"id": "https://huggingface.co/posts/sergiopaniego/566597314485869", "image": "", "title": "\ud83d\udca5 Tons of new material just landed in the smol-course! \ud83e\uddd1\u200d\ud83d\udcbb", "content_text": "\ud83d\udca5 Tons of new material just landed in the smol-course! \ud83e\uddd1\u200d\ud83d\udcbb > evaluation > alignment > VLMs > quizzes > assignments! > certificates!\ud83d\udc69\u200d\ud83c\udf93 go learn! \ud83d\udc49 https://huggingface.co/learn/smol-course/unit0/1 See translation", "url": "https://huggingface.co/posts/sergiopaniego/566597314485869", "date_published": "2025-09-25T09:24:50.053907"}, {"id": "https://huggingface.co/posts/AdinaY/244715855875832", "image": "", "title": "Qwen3Guard \ud83d\udee1\ufe0f a series of safety moderation models built upon Qwen3", "content_text": "Qwen3Guard \ud83d\udee1\ufe0f a series of safety moderation models built upon Qwen3 Qwen/qwen3guard-68d2729abbfae4716f3343a1 \u2728 0.6B/4B/8B - Apache2.0 \u2728 Two variants: Gen & Steam \u2728 Trained on a dataset of 1.19 million prompts \u2728 Classifies content into Safe / Unsafe / Controversial \u2728 Supports 119 languages & dialects See translation", "url": "https://huggingface.co/posts/AdinaY/244715855875832", "date_published": "2025-09-25T09:24:50.054161"}]}