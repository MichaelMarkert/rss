{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/mlabonne/713929804806596", "image": "", "title": "New family of 1B models just dropped!", "content_text": "New family of 1B models just dropped! > LiquidAI/LFM2.5-1.2B-Base : 10T \u2192 28T tokens > LiquidAI/LFM2.5-1.2B-Instruct : new large-scale multi-stage RL > LiquidAI/LFM2.5-1.2B-JP : our most polite model > LiquidAI/LFM2.5-VL-1.6B : multi-image multilingual > LiquidAI/LFM2.5-Audio-1.5B : 8x times faster, no quality loss Super proud of this release \ud83e\udd17 See translation", "url": "https://huggingface.co/posts/mlabonne/713929804806596", "date_published": "2026-01-08T09:33:39.651506"}, {"id": "https://huggingface.co/posts/MaziyarPanahi/255552518498714", "image": "", "title": "\ud83c\udf89 OpenMed 2025 Year in Review: 6 Months of Open Medical AI", "content_text": "\ud83c\udf89 OpenMed 2025 Year in Review: 6 Months of Open Medical AI I'm thrilled to share what the OpenMed community has accomplished since our July 2025 launch! \ud83d\udcca The Numbers 29,700,000 downloads Thank you! \ud83d\ude4f - 481 total models (475 medical NER models + 6 fine-tuned LLMs) - 475 medical NER models in [OpenMed]( OpenMed ) organization - 6 fine-tuned LLMs in [openmed-community]( openmed-community ) - 551,800 PyPI downloads of the [openmed package]( https://pypi.org/project/openmed/ ) - 707 followers on HuggingFace (you!) - 97 GitHub stars on the [toolkit repo]( https://github.com/maziyarpanahi/openmed ) \ud83c\udfc6 Top Models by Downloads 1. [OpenMed-NER-PharmaDetect-SuperClinical-434M]( OpenMed/OpenMed-NER-PharmaDetect-SuperClinical-434M ) \u2014 147,305 downloads 2. [OpenMed-NER-ChemicalDetect-ElectraMed-33M]( OpenMed/OpenMed-NER-ChemicalDetect-ElectraMed-33M ) \u2014 126,785 downloads 3. [OpenMed-NER-BloodCancerDetect-TinyMed-65M]( OpenMed/OpenMed-NER-BloodCancerDetect-TinyMed-65M ) \u2014 126,465 downloads \ud83d\udd2c Model...", "url": "https://huggingface.co/posts/MaziyarPanahi/255552518498714", "date_published": "2026-01-08T09:33:39.652121"}, {"id": "https://huggingface.co/posts/vincentg64/731522720644742", "image": "", "title": "New Book: No-Blackbox, Secure, Efficient AI and LLM Solutions", "content_text": "New Book: No-Blackbox, Secure, Efficient AI and LLM Solutions https://mltblog.com/4aRwvM5 Large language models and modern AI is often presented as technology that needs deep neural networks (DNNs) with billions of Blackbox parameters, expensive and time consuming training, along with GPU farms, yet prone to hallucinations. This book presents alternatives that rely on explainable AI, featuring new algorithms based on radically different technology with trustworthy, auditable, fast, accurate, secure, replicable Enterprise AI. Most of the material is proprietary and made from scratch, showcasing the culmination of decades of research away from standard models to establish a new framework in machine learning and AI technology. I discuss an efficient DNN architecture based on a new type of universal functions in chapter 4, with DNN distillation and protection via watermarking in chapter 5. Then, in chapter 6, I discuss non-DNN alternatives that yield exact interpolation on the training...", "url": "https://huggingface.co/posts/vincentg64/731522720644742", "date_published": "2026-01-08T09:33:39.652587"}, {"id": "https://huggingface.co/posts/mindchain/258347414806989", "image": "", "title": "Skill Reflect: A Concept for Automated AI Skill Mastery", "content_text": "Skill Reflect: A Concept for Automated AI Skill Mastery Let\u2019s be real for a second: most of us are using AI all wrong. We send a prompt, get a \"meh\" answer, and then spend twenty minutes fixing it ourselves. That\u2019s not a workflow; that\u2019s just a digital chore. I wanted to see if I could push Claude further\u2014to see if I could build a system that actually learns and refines itself. That\u2019s how the Claude-Reflect-System (Skill Reflect) was born. But here\u2019s the thing: this isn\u2019t some polished, final product. It\u2019s a concept. It\u2019s a blueprint. I\u2019ve built the foundation of a recursive reflection loop that forces the AI to step back, look at its work, and act as its own harshest critic. It identifies the \"skill delta\"\u2014the gap between \"okay\" and \"mastery\"\u2014and closes it. This logic isn't just for Claude; you can grab this architecture and drop it right into codex-cli, terminal agents, or whatever stack you're building. I\u2019m a big believer in the law of causality. Action, reaction. Cause and...", "url": "https://huggingface.co/posts/mindchain/258347414806989", "date_published": "2026-01-08T09:33:39.653173"}, {"id": "https://huggingface.co/posts/MikeDoes/265606187790986", "image": "", "title": "We can't build more private AI if we can't measure privacy intelligence.", "content_text": "We can't build more private AI if we can't measure privacy intelligence. That's why we're highlighting the Priv-IQ benchmark, a new, solution-oriented framework for evaluating LLMs on eight key privacy competencies, from visual privacy to knowledge of privacy law. The direct connection to our work is clear: the researchers relied on samples from the Ai4Privacy dataset to build out questions for Privacy Risk Assessment and Multilingual Entity Recognition. This is the power of open-source collaboration. We provide the data building blocks, and researchers construct powerful new evaluation tools on top of them. It's a win-win for the entire ecosystem when we can all benefit from transparent, data-driven benchmarks that help push for better, safer AI. Kudos to Sakib Shahriar and Rozita A. Dara for this important contribution. Read the paper to see the results: https://www.proquest.com/docview/3170854914?pq-origsite=gscholar&fromopenview=true&sourcetype=Scholarly%20Journals #OpenSource...", "url": "https://huggingface.co/posts/MikeDoes/265606187790986", "date_published": "2026-01-08T09:33:39.653523"}, {"id": "https://huggingface.co/posts/tsungyi/839679910683551", "image": "", "title": "Big news from CES \u2014 Cosmos Reason 2 is here \u2014 our most advanced reasoning vision-language model for physical AI, now topping the Physical AI Bench leaderboard\ud83c\udfc6", "content_text": "Big news from CES \u2014 Cosmos Reason 2 is here \u2014 our most advanced reasoning vision-language model for physical AI, now topping the Physical AI Bench leaderboard\ud83c\udfc6 shi-labs/physical-ai-bench-leaderboard What\u2019s new: - Enhanced physical reasoning & spatio-temporal understanding - Flexible deployment with 2B & 8B model sizes - Long-context understanding (up to 256K tokens) - Object detection with 2D/3D point localizations and trajectory data - New Cosmos Cookbook Recipes for faster onboarding Read the full blog \ud83d\udcd6 https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning Download Cosmos Reason 2 \ud83d\udc49 nvidia/Cosmos-Reason2-8B On top of Cosmos Reason 2, we also rolled out other new updates, including: - Cosmos Predict 2.5 \u2013 Unified Text2World/Image2World/Video2World model for higher-quality synthetic video worlds - Cosmos Transfer 2.5-2B \u2013 Lightweight, high-fidelity world-to-world translation with stronger physics alignment - NVIDIA GR00T N1.6 \u2013 Open robot foundation...", "url": "https://huggingface.co/posts/tsungyi/839679910683551", "date_published": "2026-01-08T09:33:39.653966"}, {"id": "https://huggingface.co/posts/dhruv3006/728892692419632", "image": "", "title": "Need a variable that's not fixed but depends on another request's response?", "content_text": "Need a variable that's not fixed but depends on another request's response? Runtime variables let you capture values from one API call and reuse them in subsequent requests. What are Runtime Variables? Runtime variables are dynamic values that get set during request execution. They're perfect for scenarios like: - Capturing an auth token from login and using it in authenticated requests - Storing a user ID from a create-user response - Saving an order ID to use in later order management calls Use Runtime Variables in Voiden : https://voiden.md/ See translation", "url": "https://huggingface.co/posts/dhruv3006/728892692419632", "date_published": "2026-01-08T09:33:39.654245"}, {"id": "https://huggingface.co/posts/AdinaY/239073779898851", "image": "", "title": "Chinese open source AI in December 2025 was about the stack coming together: open, end to end, and ready to ship \ud83d\udd25", "content_text": "Chinese open source AI in December 2025 was about the stack coming together: open, end to end, and ready to ship \ud83d\udd25 https://huggingface.co/collections/zh-ai-community/december-2025-china-open-source-highlights \u2728 Big wave of foundation models: still scaling, but efficiency, reasoning, and deployment now matter more than size - DeepSeek-V3.2 - Z.ai GLM-4.7 - MiniMax-M2.1 - Xiaomi: MiMo-V2-Flash \u2728 Multimodal reasoning is now default - Z.ai GLM-4.6V - Z.ai AutoGLM-Phone 9B - Bytedance: Dolphin-v2 \u2728 Image & video: editable assets and real workflows - Qwen-Image-Layered / Image-2512 - Meituan: LongCat-Image & Image Edit - AIDC: Ovis-Image-7B - Live-Avatar / LongCat-Video-Avatar - HY-WorldPlay / RealVideo \u2728 Audio goes edge ready - GLM-ASR-Nano / Fun-ASR-Nano - GLM-TTS / VoxCPM1.5 - CosyVoice 0.5B \u2728 The quiet backbone: data & infrastructure - Finch (FinWorkBench) - Tencent ARC: TimeLens-100K - BIGAI: TongSIM-Asset - MiniMax: VTP-Base \u2728 Also congrats on Minimax and Z.ai announced their IPOs...", "url": "https://huggingface.co/posts/AdinaY/239073779898851", "date_published": "2026-01-08T09:33:39.654678"}, {"id": "https://huggingface.co/posts/pcuenq/421927498996428", "image": "", "title": "\ud83d\udc49 What happened in AI in 2025? \ud83d\udc48", "content_text": "\ud83d\udc49 What happened in AI in 2025? \ud83d\udc48 We prepared the 2025 version of the HF AI Timeline Grid, highlighting open vs API-based model releases, and allowing you to browse and filter by access, modality, and release type! Play with it here: 2025-ai-timeline/2025-ai-timeline Here's my personal quarterly TL;DR: 1\ufe0f\u20e3 Q1 \u2014 Learning to Reason Deepseek not only releases a top-notch reasoning model, but shows how to train them and compete with closed frontier models. OpenAI debuts Deep Research. Significant milestones: DeepSeek R1 & R1-Zero, Qwen 2.5 VL, OpenAI Deep Research, Gemini 2.5 Pro (experimental) 2\ufe0f\u20e3 Q2 \u2014 Multimodality and Coding More LLMs embrace multimodality by default, and there's a surge in coding agents. Strong vision, audio, and generative models emerge. Significant milestones: Llama 4, Qwen 3, Imagen 4, OpenAI Codex, Google Jules, Claude 4 3\ufe0f\u20e3 Q3 \u2014 \"Gold\" rush, OpenAI opens up, the community goes bananas Flagship models get gold in Math olympiads and hard benchmarks. OpenAI...", "url": "https://huggingface.co/posts/pcuenq/421927498996428", "date_published": "2026-01-08T09:33:39.655260"}, {"id": "https://huggingface.co/posts/Akhil-Theerthala/396893563842558", "image": "", "title": "Is it better to show a model too many images once (Diversity), or extract as much information from a small set of images?", "content_text": "Is it better to show a model too many images once (Diversity), or extract as much information from a small set of images? I have always wanted to do an ablation study on this and recently I got the chance to do exactly that. Why? In applied domains like robotics, manufacturing, or banking, we rarely have the luxury of internet-scale diverse image datasets. We are often \"Data Poor\" in terms of diversity but \"Data Rich\" in depth. The takeaway? Density is efficient for facts but dangerous for reasoning (logical collapse) if you don't have larger scale data. More details: https://huggingface.co/blog/Akhil-Theerthala/diversity-density-for-vision-language-models See translation", "url": "https://huggingface.co/posts/Akhil-Theerthala/396893563842558", "date_published": "2026-01-08T09:33:39.655535"}]}