{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Abhaykoul/997219525730173", "image": "", "title": "Introducing Dhanishtha 2.0: World's first Intermediate Thinking Model", "content_text": "Introducing Dhanishtha 2.0: World's first Intermediate Thinking Model Dhanishtha 2.0 is the world's first LLM designed to think between the responses. Unlike other Reasoning LLMs, which think just once. Dhanishtha can think, rethink, self-evaluate, and refine in between responses using multiple <think> blocks. This technique makes it Hinghlt Token efficient it Uses up to 79% fewer tokens than DeepSeek R1 --- You can try our model from: https://helpingai.co/chat Also, we're gonna Open-Source Dhanistha on July 1st. --- For Devs: \ud83d\udd11 Get your API key at https://helpingai.co/dashboard from HelpingAI import HAI # pip install HelpingAI ==1.1.1 from rich import print hai = HAI( api_key = \"hl-***********************\" ) response = hai.chat.completions.create( model = \"Dhanishtha-2.0-preview\" , messages=[{ \"role\" : \"user\" , \"content\" : \"What is the value of \u222b0\u221e\ud835\udc653/\ud835\udc65\u22121\ud835\udc51\ud835\udc65 ?\" }], stream = True , hide_think = False # Hide or show models thinking ) for chunk in response: print...", "url": "https://huggingface.co/posts/Abhaykoul/997219525730173", "date_published": "2025-06-26T17:21:33.926396"}, {"id": "https://huggingface.co/posts/Jaward/445538723467397", "image": "", "title": "Awesome intro to LLM course \"Language Modeling from Scratch\" by stanford. love the aesthetics behind the lecture notes, notes-in-code genius idea\ud83d\udc4d", "content_text": "Awesome intro to LLM course \"Language Modeling from Scratch\" by stanford. love the aesthetics behind the lecture notes, notes-in-code genius idea\ud83d\udc4d Course site: https://stanford-cs336.github.io/spring2025/ Repo: https://github.com/stanford-cs336/spring2025-lectures Videos: https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_ See translation", "url": "https://huggingface.co/posts/Jaward/445538723467397", "date_published": "2025-06-26T17:21:33.926686"}, {"id": "https://huggingface.co/posts/yeonseok-zeticai/727857049396772", "image": "", "title": "Hi everyone,", "content_text": "Hi everyone, I\u2019ve been running small language models (SLLMs) directly on smartphones \u2014 completely offline, with no cloud backend or server API calls. I wanted to share: 1. \u26a1 Tokens/sec performance across several SLLMs 2. \ud83e\udd16 Observations on hardware utilization (where the workload actually runs) 3. \ud83d\udccf Trade-offs between model size, latency, and feasibility for mobile apps There are reports for below models - QWEN3 0.6B - NVIDIA/Nemotron QWEN 1.5B - SimpleScaling S1 - TinyLlama - Unsloth tuned Llama 3.2 1B - Naver HyperClova 0.5B \ud83d\udcdcComparable Benchmark reports (no cloud, all on-device): I\u2019d really value your thoughts on: - Creative ideas to further optimize inference under these hardware constraints - Other compact LLMs worth testing on-device - Experiences you\u2019ve had trying to deploy LLMs at the edge If there\u2019s interest, I\u2019m happy to share more details on the test setup, hardware specs, or the tooling we used for these comparisons. Thanks for taking a look, and you can build your own...", "url": "https://huggingface.co/posts/yeonseok-zeticai/727857049396772", "date_published": "2025-06-26T17:21:33.927146"}, {"id": "https://huggingface.co/posts/pagezyhf/610118153924016", "image": "", "title": "Hackathons in Paris on July 5th and 6th!", "content_text": "Hackathons in Paris on July 5th and 6th! Hugging Face just wrapped 4 months of deep work with AMD to push kernel-level optimization on their MI300X GPUs. Now, it's time to share everything we learned. Join us in Paris at STATION F for a hands-on weekend of workshops and a hackathon focused on making open-source LLMs faster and more efficient on AMD. Prizes, amazing host speakers, ... if you want more details, navigate to https://lu.ma/fmvdjmur ! See translation", "url": "https://huggingface.co/posts/pagezyhf/610118153924016", "date_published": "2025-06-26T17:21:33.927409"}, {"id": "https://huggingface.co/posts/bartowski/460622149989234", "image": "", "title": "Was going to post this on /r/LocalLLaMa, but apparently it's without moderation at this time :')", "content_text": "Was going to post this on /r/LocalLLaMa, but apparently it's without moderation at this time :') bartowski/mistralai_Mistral-Small-3.2-24B-Instruct-2506-GGUF Was able to use previous mistral chat templates, some hints from Qwen templates, and Claude to piece together a seemingly working chat template, tested it with llama.cpp server and got perfect results, though lmstudio still seems to be struggling for some reason (don't know how to specify a jinja file there) Outlined the details of the script and results in my llama.cpp PR to add the jinja template: https://github.com/ggml-org/llama.cpp/pull/14349 Start server with a command like this: ./llama-server -m /models/mistralai_Mistral-Small -3 .2 -24 B-Instruct -2506 -Q4_K_M.gguf --jinja --chat-template-file /models/Mistral-Small -3 .2 -24 B-Instruct -2506 .jinja and it should be perfect! Hoping it'll work for ALL tools if lmstudio gets an update or something, not just llama.cpp, but very happy to see it works flawlessly in llama.cpp...", "url": "https://huggingface.co/posts/bartowski/460622149989234", "date_published": "2025-06-26T17:21:33.927798"}, {"id": "https://huggingface.co/posts/BFFree/190836494252067", "image": "", "title": "Working on some chess set concepts. I went towards minimal sculpted shapes then returned to some traditionalism.", "content_text": "Working on some chess set concepts. I went towards minimal sculpted shapes then returned to some traditionalism. See translation", "url": "https://huggingface.co/posts/BFFree/190836494252067", "date_published": "2025-06-26T17:21:33.927990"}, {"id": "https://huggingface.co/posts/jsulz/866221600890917", "image": "", "title": "It's been a bit since I took a step back and looked at", "content_text": "It's been a bit since I took a step back and looked at xet-team progress to migrate Hugging Face from Git LFS to Xet, but every time I do it boggles the mind. A month ago there were 5,500 users/orgs on Xet with 150K repos and 4PB. Today? \ud83e\udd17 700,000 users/orgs \ud83d\udcc8 350,000 repos \ud83d\ude80 15PB Meanwhile, our migrations have pushed throughput to numbers that are bonkers. In June, we hit upload speeds of 577Gb/s (crossing 500Gb/s for the first time). These are hard numbers to put into context, but let's try: The latest run of the Common Crawl from commoncrawl was 471 TB. We now have ~32 crawls stored in Xet. At peak upload speed we could move the latest crawl into Xet in about two hours. We're moving to a new phase in the process, so stay tuned. This shift in gears means it's also time to roll up our sleeves and look at all the bytes we have and the value we're adding to the community. I already have some homework from @ RichardErkhov to look at the dedupe across, and I'll be doing the same for...", "url": "https://huggingface.co/posts/jsulz/866221600890917", "date_published": "2025-06-26T17:21:33.928486"}, {"id": "https://huggingface.co/posts/cgeorgiaw/508678634223532", "image": "", "title": "Huge new bio datasets just dropped!!!", "content_text": "Huge new bio datasets just dropped!!! Check out them out @ ginkgo-datapoints Read the blog for more info: https://huggingface.co/blog/cgeorgiaw/gdp See translation", "url": "https://huggingface.co/posts/cgeorgiaw/508678634223532", "date_published": "2025-06-26T17:21:33.928709"}, {"id": "https://huggingface.co/posts/yeonseok-zeticai/894951796717498", "image": "", "title": "\ud83d\ude80 Real-Time On-Device AI Agent with Polaris-4B \u2014 Run It Yourself, No Cloud, No Cost", "content_text": "\ud83d\ude80 Real-Time On-Device AI Agent with Polaris-4B \u2014 Run It Yourself, No Cloud, No Cost We just deployed a real-time on-device AI agent using the Polaris-4B-Preview model \u2014 one of the top-performing <6B open LLMs on Hugging Face. \ud83d\udcf1 What\u2019s remarkable? This model runs entirely on a mobile device, without cloud, and without any manual optimization. It was built using ZETIC.MLange, and the best part? \u27a1\ufe0f It\u2019s totally automated, free to use, and anyone can do it. You don\u2019t need to write deployment code, tweak backends, or touch device-specific SDKs. Just upload your model \u2014 and ZETIC.MLange handles the rest. \ud83e\udde0 About the Model - Model: Polaris-4B-Preview - Size: ~4B parameters - Ranking: Top 3 on Hugging Face LLM Leaderboard (<6B) - Tokenizer: Token-incremental inference supported - Modifications: None \u2014 stock weights, just optimized for mobile \u2699\ufe0f What ZETIC.MLange Does ZETIC.MLange is a fully automated deployment framework for On-Device AI, built for AI engineers who want to focus on models \u2014...", "url": "https://huggingface.co/posts/yeonseok-zeticai/894951796717498", "date_published": "2025-06-26T17:21:33.929379"}, {"id": "https://huggingface.co/posts/freddyaboulton/509614991477853", "image": "", "title": "The new", "content_text": "The new multimodalart/self-forcing model and demo are truly impressive! See translation", "url": "https://huggingface.co/posts/freddyaboulton/509614991477853", "date_published": "2025-06-26T17:21:33.929576"}]}