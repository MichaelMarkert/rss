{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/mitkox/706030667212965", "image": "", "title": "Got to 1199.8 tokens/sec with Devstral Small -2 on my desktop GPU workstation. vLLM nightly.", "content_text": "Got to 1199.8 tokens/sec with Devstral Small -2 on my desktop GPU workstation. vLLM nightly. Works out of the box with Mistral Vibe. Next is time to test the big one. See translation", "url": "https://huggingface.co/posts/mitkox/706030667212965", "date_published": "2025-12-11T17:30:14.928385"}, {"id": "https://huggingface.co/posts/melvindave/358781763788308", "image": "", "title": "Currently having a blast learning the transformers library.", "content_text": "Currently having a blast learning the transformers library. I noticed that model cards usually have Transformers code as usage examples. So I tried to figure out how to load a model just using the transformers library without using ollama, lmstudio, or llamacpp. Learned how to install dependencies required to make it work like pytorch and CUDA. I also used Conda for python environment dependencies. Once I got the model loaded and sample inference working, I made an API to serve it. I know it's very basic stuff for machine learning experts here in HF but I'm completely new to this so I'm happy to get it working! Model used: Qwen/Qwen3-VL-8B-Instruct GPU: NVIDIA GeForce RTX 3090 Here's the result of my experimentation See translation", "url": "https://huggingface.co/posts/melvindave/358781763788308", "date_published": "2025-12-11T17:30:14.928767"}, {"id": "https://huggingface.co/posts/sergiopaniego/384415208092213", "image": "", "title": "ICYMI, you can fine-tune open LLMs using Claude Code", "content_text": "ICYMI, you can fine-tune open LLMs using Claude Code just tell it: \u201cFine-tune Qwen3-0.6B on open-r1/codeforces-cots\u201d and Claude submits a real training job on HF GPUs using TRL. it handles everything: > dataset validation > GPU selection > training + Trackio monitoring > job submission + cost estimation when it\u2019s done, your model is on the Hub, ready to use read more about the process: https://huggingface.co/blog/hf-skills-training See translation", "url": "https://huggingface.co/posts/sergiopaniego/384415208092213", "date_published": "2025-12-11T17:30:14.929109"}, {"id": "https://huggingface.co/posts/sergiopaniego/627155943228357", "image": "", "title": "We just released TRL v0.26.0!", "content_text": "We just released TRL v0.26.0! It comes packed with updates: > Agent training with tools in GRPO > New CISPO & SAPO losses + reasoning rewards > vLLM quantization in colocate mode > Dataset shuffling in SFT > Lots of NEW examples > Tons of fixes and documentation improvements See translation", "url": "https://huggingface.co/posts/sergiopaniego/627155943228357", "date_published": "2025-12-11T17:30:14.929358"}, {"id": "https://huggingface.co/posts/tomaarsen/853653818134091", "image": "", "title": "\ud83d\udc26\u200d\ud83d\udd25 I've just published Sentence Transformers v5.2.0! It introduces multi-processing for CrossEncoder (rerankers), multilingual NanoBEIR evaluators, similarity score outputs in mine_hard_negatives, Transformers v5 support and more. Details:", "content_text": "\ud83d\udc26\u200d\ud83d\udd25 I've just published Sentence Transformers v5.2.0! It introduces multi-processing for CrossEncoder (rerankers), multilingual NanoBEIR evaluators, similarity score outputs in mine_hard_negatives, Transformers v5 support and more. Details: - CrossEncoder multi-processing: Similar to SentenceTransformer and SparseEncoder, you can now use multi-processing with CrossEncoder rerankers. Useful for multi-GPU and CPU settings, and simple to configure: just device=[\"cuda:0\", \"cuda:1\"] or device=[\"cpu\"]*4 on the model.predict or model.rank calls. - Multilingual NanoBEIR Support: You can now use community translations of the tiny NanoBEIR retrieval benchmark instead of only the English one, by passing dataset_id , e.g. dataset_id=\"lightonai/NanoBEIR-de\" for the German benchmark. - Similarity scores in Hard Negatives Mining: When mining for hard negatives to create a strong training dataset, you can now pass output_scores=True to get similarity scores returned. This can be useful for some...", "url": "https://huggingface.co/posts/tomaarsen/853653818134091", "date_published": "2025-12-11T17:30:14.929939"}, {"id": "https://huggingface.co/posts/angt/186034800220690", "image": "", "title": "installama.sh at the TigerBeetle 1000x World Tour !", "content_text": "installama.sh at the TigerBeetle 1000x World Tour ! Last week I had the chance to give a short talk during the TigerBeetle 1000x World Tour (organized by @ jedisct1 \ud83d\udc4f ) a fantastic event celebrating high-performance engineering and the people who love pushing systems to their limits! In the talk, I focused on the CPU and Linux side of things, with a simple goal in mind: making the installation of llama.cpp instant, automatic, and optimal, no matter your OS or hardware setup. For the curious, here are the links worth checking out: Event page: https://tigerbeetle.com/event/1000x GitHub repo: https://github.com/angt/installama.sh Talk: https://youtu.be/pg5NOeJZf0o?si=9Dkcfi2TqjnT_30e More improvements are coming soon. Stay tuned! See translation", "url": "https://huggingface.co/posts/angt/186034800220690", "date_published": "2025-12-11T17:30:14.930340"}, {"id": "https://huggingface.co/posts/StJohnDeakins/147365886941567", "image": "", "title": "Hey all  \ud83d\udc4b", "content_text": "Hey all \ud83d\udc4b A Quick one for any founders building with Small Language Models in mobile apps: We\u2019re opening 10 Innovation Partner spots this month for our Device Native AI (DNA) platform. What you get: - Device Native AI SDK \u2060(AI processes data on-device, not cloud \ud83d\udcf2) - 99% off for 3 months, then 90% off for the rest of the year (no lock-in) - Direct engineering access + feature releases - It's an Innovation community, so at least some participation is required Perfect if you're building consumer apps and want: \u2713 Hyper-personalization without privacy risks \u2713 Zero cloud AI token costs \u2713 Early access to next-gen mobile AI Limited spots, and on a first-come basis, so DM me \"DNA\" for more info and an access code. Cheers Singe \ud83d\udc35 See translation", "url": "https://huggingface.co/posts/StJohnDeakins/147365886941567", "date_published": "2025-12-11T17:30:14.930722"}, {"id": "https://huggingface.co/posts/RakshitAralimatti/606573836468382", "image": "", "title": "I built something crazy you never saw before.", "content_text": "I built something crazy you never saw before. Please check - https://huggingface.co/blog/RakshitAralimatti/streaming-data-rag A real-time Streaming Data to RAG system that listens to live radio, transcribes it on-the-fly, and lets you query across TIME. Not just \"what was discussed\" \u2013 but \"what happened in the last 10 minutes on channel 0?\" or \"at 9 AM, what was the breaking news?\" This is RAG that understands temporal context. See translation", "url": "https://huggingface.co/posts/RakshitAralimatti/606573836468382", "date_published": "2025-12-11T17:30:14.931012"}, {"id": "https://huggingface.co/posts/leonardlin/280604558183575", "image": "", "title": "We just released our latest Shisa V2.1 Japanese multi-lingual models:", "content_text": "We just released our latest Shisa V2.1 Japanese multi-lingual models: https://huggingface.co/collections/shisa-ai/shisa-v21 Besides updates to our 14B, and 70B, we have a new LFM2-based 1.2B, Llama 3.2-based 3B, and Qwen 3-based 8B, all with class-leading Japanese language capabilities. Per usual, lots of details in the Model Cards for those interested. See translation", "url": "https://huggingface.co/posts/leonardlin/280604558183575", "date_published": "2025-12-11T17:30:14.931268"}, {"id": "https://huggingface.co/posts/NicoBBQ1/471268005218525", "image": "", "title": "What do you think of my LLM Chat app so far?", "content_text": "What do you think of my LLM Chat app so far? Here are some of the features already included (and more are coming): - Chat with AI models \u2013 Local inference via Ollama - Reasoning support \u2013 View model thinking process (DeepSeek-R1, Qwen-QwQ, etc.) - Vision models \u2013 Analyze images with llava, bakllava, moondream - Image generation \u2013 Local GGUF models with GPU acceleration (CUDA) - Fullscreen images \u2013 Click generated images to view in fullscreen - Image attachments \u2013 File picker or clipboard paste (Ctrl+V) - DeepSearch \u2013 Web search with tool use - Inference Stats \u2013 Token counts, speed, duration (like Ollama verbose) - Regenerate \u2013 Re-run any AI response - Copy \u2013 One-click copy AI responses See translation", "url": "https://huggingface.co/posts/NicoBBQ1/471268005218525", "date_published": "2025-12-11T17:30:14.931626"}]}