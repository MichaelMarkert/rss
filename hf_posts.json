{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/YerbaPage/727846915147423", "image": "", "title": "Is 100% Pass Rate on HumanEval possible? Yes! \u2705", "content_text": "Is 100% Pass Rate on HumanEval possible? Yes! \u2705 Meet MGDebugger if you are tired of LLMs failing on complex bugs \ud83e\udd14 Our MGDebugger, just hit 100% accuracy on HumanEval using the DeepSeek-R1 model. \ud83d\ude80 \u2728 Demo: learnmlf/MGDebugger \ud83d\udcdd Paper: From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging (2410.01215) \ud83d\udcbb Code: https://github.com/YerbaPage/MGDebugger HumanEval may be retired, we're ready for the next challenge In more complex scenarios! You may also take look at this repo for a collection of awesome repo-level coding tasks! \ud83d\udda5\ufe0f https://github.com/YerbaPage/Awesome-Repo-Level-Code-Generation See translation", "url": "https://huggingface.co/posts/YerbaPage/727846915147423", "date_published": "2025-07-06T13:31:03.113519"}, {"id": "https://huggingface.co/posts/Parveshiiii/373499845630863", "image": "", "title": "\ud83e\udde0 MathX-5M by XenArcAI \u2014 Scalable Math Reasoning for Smarter LLMs", "content_text": "\ud83e\udde0 MathX-5M by XenArcAI \u2014 Scalable Math Reasoning for Smarter LLMs Introducing MathX-5M, a high-quality, instruction-tuned dataset built to supercharge mathematical reasoning in large language models. With 5 million rigorously filtered examples, it spans everything from basic arithmetic to advanced calculus\u2014curated from public sources and enhanced with synthetic data. \ud83d\udd0d Key Highlights: - Step-by-step reasoning with verified answers - Covers algebra, geometry, calculus, logic, and more - RL-validated correctness and multi-stage filtering - Ideal for fine-tuning, benchmarking, and educational AI \ud83d\udcc2 - XenArcAI/MathX-5M See translation", "url": "https://huggingface.co/posts/Parveshiiii/373499845630863", "date_published": "2025-07-06T13:31:03.113895"}, {"id": "https://huggingface.co/posts/zamal/249688504470467", "image": "", "title": "Hey all", "content_text": "Hey all Finally it's happening. DeepGit lite is back now, running on cpu only devices. Just smartly search across Github and spin up conversational agents in the background and have grounded conversation with repositories Try it out now!!!! zamal/DeepGit See translation", "url": "https://huggingface.co/posts/zamal/249688504470467", "date_published": "2025-07-06T13:31:03.114127"}, {"id": "https://huggingface.co/posts/prithivMLmods/606950317422768", "image": "", "title": "Multimodal OCR with ReportLab? On Colab T4? (Nanonets OCR, Monkey OCR, OCRFlux 3B, Typhoo OCR 3B?) .. Yeah, it\u2019s possible. I\u2019ve made a dedicated Colab notebook to experiment with these models (all built on top of Qwen2.5 VL). \ud83e\udd17\ud83d\ude80", "content_text": "Multimodal OCR with ReportLab? On Colab T4? (Nanonets OCR, Monkey OCR, OCRFlux 3B, Typhoo OCR 3B?) .. Yeah, it\u2019s possible. I\u2019ve made a dedicated Colab notebook to experiment with these models (all built on top of Qwen2.5 VL). \ud83e\udd17\ud83d\ude80 Download notebooks here : \u2726\ufe0e NanonetsOCR : https://colab.research.google.com/drive/1VvA-amvSVxGdWgIsh4_by6KWOtEs_Iqp \u2726\ufe0e MonkeyOCR : https://colab.research.google.com/drive/1vPCojbmlXjDFUt06FJ1tjgnj_zWK4mUo \u2726\ufe0e OCRFluxOCR : https://colab.research.google.com/drive/1TDoCXzWdF2hxVLbISqW6DjXAzOyI7pzf \u2726\ufe0e TyphoonOCR : https://colab.research.google.com/drive/1_59zvLNnn1kvbiSFxzA1WiqhpbW8RKbz \ud83d\udf32 Github : https://github.com/PRITHIVSAKTHIUR/OCR-ReportLab What does it do? 1. Performs OCR on the input image 2. Generates a DOCX or PDF file with the input image and the extracted text . . . To know more about it, visit the model card of the respective model. !! See translation", "url": "https://huggingface.co/posts/prithivMLmods/606950317422768", "date_published": "2025-07-06T13:31:03.114511"}, {"id": "https://huggingface.co/posts/stefan-french/381121888136998", "image": "", "title": "\ud83d\ude80 We just released the WASM Agent Blueprint!", "content_text": "\ud83d\ude80 We just released the WASM Agent Blueprint! It shows how to run Python-based AI agents directly in your browser using WebAssembly (WASM) via Pyodide and the OpenAI Agents SDK. There are no installs, it runs straight in your browser. Try it out and explore the code \ud83d\udc49 https://github.com/mozilla-ai/wasm-agents-blueprint See translation", "url": "https://huggingface.co/posts/stefan-french/381121888136998", "date_published": "2025-07-06T13:31:03.114783"}, {"id": "https://huggingface.co/posts/andito/535116877809522", "image": "", "title": "\ud83e\udde0\ud83d\udc41\ufe0f Can AI visualize solutions?", "content_text": "\ud83e\udde0\ud83d\udc41\ufe0f Can AI visualize solutions? Humans often solve visual problems by sketching ideas in our minds. What if Vision-Language Models (VLMs) could do something similar, not by generating full images, but by using internal \u201cmental sketches\u201d? That\u2019s the idea behind Mirage, a new framework that empowers VLMs to reason using latent visual tokens. Instead of just thinking in words, Mirage mixes in abstract visual representations that help the model solve complex tasks. These aren't photorealistic images. They're compact, internal representations optimized purely to support reasoning. \ud83d\udd27 Mirage is trained in two phases: 1) Grounding: It learns to produce latent tokens anchored in real images. 2) Refinement: The model drops the images and learns to generate visual tokens on its own. \ud83d\udcc8 And yes, it works! On challenging benchmarks like Visual Spatial Planning, Jigsaw puzzles, and Spatial Attention Tasks, Mirage clearly outperforms GPT-4o and other strong baselines. Smart sketches > empty words....", "url": "https://huggingface.co/posts/andito/535116877809522", "date_published": "2025-07-06T13:31:03.115273"}, {"id": "https://huggingface.co/posts/AdinaY/711789989544718", "image": "", "title": "The Chinese Open Source Heatmap is live \ud83d\udd25", "content_text": "The Chinese Open Source Heatmap is live \ud83d\udd25 You can now track the companies/ research labs/ communities powering China\u2019s open source AI movement. zh-ai-community/model-release-heatmap-zh Some highlights: \u2728Giant Tech are investing more in open source. -Alibaba: Full stack open ecosystem -Tecent: Hunyuan image/video/3D -Bytedance: Catching up fast in 2025 -Baidu: New player in open LLM \u2728New players emerging post\u2013DeepSeek moment. -Xiaomi -Red Note -Bilibili -MiniMax -Moonshot AI \u2728Startup list is shifting fast! Those who find a direction aligned with their strengths are the ones who endure. -DeepSeek -MiniMax -StepFun -Moonshot AI -Zhipu AI -OpenBMB \u2728Research Lab & Community are making key contributions. -BAAI -Shanghai AI Lab -OpenMOSS -MAP See translation", "url": "https://huggingface.co/posts/AdinaY/711789989544718", "date_published": "2025-07-06T13:31:03.115638"}, {"id": "https://huggingface.co/posts/sergiopaniego/776509639548053", "image": "", "title": "Updated my HF Space for vibe testing smol VLMs on object detection, visual grounding, keypoint detection & counting! \ud83d\udc53", "content_text": "Updated my HF Space for vibe testing smol VLMs on object detection, visual grounding, keypoint detection & counting! \ud83d\udc53 \ud83c\udd95 Compare Qwen2.5 VL 3B vs Moondream 2B side-by-side with annotated images & text outputs. Try examples or test your own images! \ud83c\udfc3 \ud83d\udcf1Space: sergiopaniego/vlm_object_understanding See translation", "url": "https://huggingface.co/posts/sergiopaniego/776509639548053", "date_published": "2025-07-06T13:31:03.115907"}, {"id": "https://huggingface.co/posts/eaddario/837008676792926", "image": "", "title": "Layer-wise and Pruned versions of cognitivecomputations/Dolphin-Mistral-24B-Venice-Edition", "content_text": "Layer-wise and Pruned versions of cognitivecomputations/Dolphin-Mistral-24B-Venice-Edition * Tesor-wise: eaddario/Dolphin-Mistral-24B-Venice-Edition-GGUF * Pruned: eaddario/Dolphin-Mistral-24B-Venice-Edition-pruned-GGUF Summary in the model's card and test results in the ./scores directory. Questions/feedback is always welcomed. See translation", "url": "https://huggingface.co/posts/eaddario/837008676792926", "date_published": "2025-07-06T13:31:03.116138"}, {"id": "https://huggingface.co/posts/aiqtech/840192921912390", "image": "", "title": "\ud83d\udd25 HuggingFace Heatmap Leaderboard", "content_text": "\ud83d\udd25 HuggingFace Heatmap Leaderboard Visualizing AI ecosystem activity at a glance aiqtech/Heatmap-Leaderboard \ud83c\udfaf Introduction A leaderboard that visualizes the vibrant HuggingFace community activity through heatmaps. \u2728 Key Features \ud83d\udcca Real-time Tracking - Model/dataset/app releases from AI labs and developers \ud83c\udfc6 Auto Ranking - Rankings based on activity over the past year \ud83c\udfa8 Responsive UI - Unique colors per organization, mobile optimized \u26a1 Auto Updates - Hourly data refresh for latest information \ud83c\udf0d Major Participants Big Tech: OpenAI, Google, Meta, Microsoft, Apple, NVIDIA AI Startups: Anthropic, Mistral, Stability AI, Cohere, DeepSeek Chinese Companies: Tencent, Baidu, ByteDance, Qwen HuggingFace Official: HuggingFaceH4, HuggingFaceM4, lerobot, etc. Active Developers: prithivMLmods, lllyasviel, multimodalart and many more \ud83d\ude80 Value Trend Analysis \ud83d\udcc8 Real-time open source contribution insights Inspiration \ud83d\udcaa Learn from other developers' activity patterns Ecosystem Growth \ud83c\udf31 Visualize AI...", "url": "https://huggingface.co/posts/aiqtech/840192921912390", "date_published": "2025-07-06T13:31:03.116915"}]}