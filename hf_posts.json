{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Jaward/982484477481896", "image": "", "title": "made a few improvements on custom grpo trainer:", "content_text": "made a few improvements on custom grpo trainer: - added sequence similarity reward (seems to work) - improved vllm support (5x inference speed) - adjusted reward scores (this helped with format/accuracy) - can now push to hf hub (already pushed mine lol: Jaward/smollm2_360m_grpo_gsm8k_reasoner ) Code: https://github.com/Jaykef/ai-algorithms/blob/main/smollm2_360M_135M_grpo_gsm8k.ipynb See translation", "url": "https://huggingface.co/posts/Jaward/982484477481896", "date_published": "2025-03-03T05:21:08.363887"}, {"id": "https://huggingface.co/posts/mkurman/255930071052172", "image": "", "title": "Introducing a new architecture, MedIT One \u2013 a single-token transformer with LSTM-like recurrence.", "content_text": "Introducing a new architecture, MedIT One \u2013 a single-token transformer with LSTM-like recurrence. It is extremely fast in training and inference, but we lack funding for large-scale training. Enjoy \ud83c\udf53 https://github.com/MedITSolutionsKurman/medit-one See translation", "url": "https://huggingface.co/posts/mkurman/255930071052172", "date_published": "2025-03-03T05:21:08.364177"}, {"id": "https://huggingface.co/posts/prithivMLmods/305640045790864", "image": "", "title": "Dropping some of the custom fine-tunes based on SigLIP2,", "content_text": "Dropping some of the custom fine-tunes based on SigLIP2, with a single-label classification problem type! \ud83c\udf00\ud83e\udde4 - AI vs Deepfake vs Real : prithivMLmods/AI-vs-Deepfake-vs-Real-Siglip2 - Deepfake Detect : prithivMLmods/Deepfake-Detect-Siglip2 - Fire Detection : prithivMLmods/Fire-Detection-Siglip2 - Deepfake Quality Assess : prithivMLmods/Deepfake-Quality-Assess-Siglip2 - Guard Against Unsafe Content : prithivMLmods/Guard-Against-Unsafe-Content-Siglip2 \ud83c\udf20Collection : prithivMLmods/siglip2-custom-67bcdb2de8fe96b99fb4e19e See translation", "url": "https://huggingface.co/posts/prithivMLmods/305640045790864", "date_published": "2025-03-03T05:21:08.364523"}, {"id": "https://huggingface.co/posts/singhsidhukuldeep/815565847250252", "image": "", "title": "Exciting New Tool for Knowledge Graph Extraction from Plain Text!", "content_text": "Exciting New Tool for Knowledge Graph Extraction from Plain Text! I just came across a groundbreaking new tool called KGGen that's solving a major challenge in the AI world - the scarcity of high-quality knowledge graph data. KGGen is an open-source Python package that leverages language models to extract knowledge graphs (KGs) from plain text. What makes it special is its innovative approach to clustering related entities, which significantly reduces sparsity in the extracted KGs. The technical approach is fascinating: 1. KGGen uses a multi-stage process involving an LLM (GPT-4o in their implementation) to extract entities and relations from source text 2. It aggregates graphs across sources to reduce redundancy 3. Most importantly, it applies iterative LM-based clustering to refine the raw graph The clustering stage is particularly innovative - it identifies which nodes and edges refer to the same underlying entities or concepts. This normalizes variations in tense, plurality,...", "url": "https://huggingface.co/posts/singhsidhukuldeep/815565847250252", "date_published": "2025-03-03T05:21:08.365024"}, {"id": "https://huggingface.co/posts/Kseniase/433849056207490", "image": "", "title": "9 types of \"Chain-of-...\" approaches:", "content_text": "9 types of \"Chain-of-...\" approaches: Chain-of-Thought (CoT) prompting enhances reasoning in AI models by breaking down complex problems into step-by-step logical sequences. It continues proving its effectiveness, especially in top-performing reasoning models. However, there are other similar methods, that expand CoT and can be used for different purposes. Here are 9 of them: 1. Chain-of-Action-Thought (COAT) -> Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search (2502.02508) Helps model decide when to keep thinking, double-check their work, or try a different approach, using special guiding tokens. 2. Chain of Draft (CoD) -> Chain of Draft: Thinking Faster by Writing Less (2502.18600) It helps model generate short but meaningful reasoning steps, cutting costs and making processing faster 3. Chain-of-Agents -> Chain of Agents: Large Language Models Collaborating on Long-Context Tasks (2406.02818) Uses multi-agent...", "url": "https://huggingface.co/posts/Kseniase/433849056207490", "date_published": "2025-03-03T05:21:08.365579"}, {"id": "https://huggingface.co/posts/eaddario/832567461491467", "image": "", "title": "Squeezing out tensor bits?", "content_text": "Squeezing out tensor bits? I have been tinkering with quantization and pruning to reduce model sizes. So far, I've had modest success in producing, on average, 8% smaller versions with negligible loss of quality, and I think further reductions in the 10-15% range are realistic, but I've come across a behaviour I wasn't expecting! Part of the process I'm following consists of quantizing the embedding and output layers aggressively. Since the embedding layer is more about lookup than complex computation, the vectors representing the relative distances between embeddings are usually preserved well enough making this layer fairly robust to quantization. So far, so good. The output layer, on the other hand, maps the final hidden state to the vocabulary logits and therefore, small changes in these logits could lead to a different probability distribution over the vocabulary, resulting in incorrect word predictions, or so I thought. Surprisingly, I'm finding that even at Q2_K the loss of...", "url": "https://huggingface.co/posts/eaddario/832567461491467", "date_published": "2025-03-03T05:21:08.366014"}, {"id": "https://huggingface.co/posts/rizavelioglu/517500066145145", "image": "", "title": "Comparing reconstruction quality of various VAEs with an interactive demo", "content_text": "Comparing reconstruction quality of various VAEs with an interactive demo rizavelioglu/vae-comparison See translation", "url": "https://huggingface.co/posts/rizavelioglu/517500066145145", "date_published": "2025-03-03T05:21:08.366231"}, {"id": "https://huggingface.co/posts/Bils/313728910739163", "image": "", "title": "create amazing audio ads in just a few steps", "content_text": "create amazing audio ads in just a few steps Bils/AIPromoStudio See translation", "url": "https://huggingface.co/posts/Bils/313728910739163", "date_published": "2025-03-03T05:21:08.366443"}, {"id": "https://huggingface.co/posts/lingvanex-mt/205421793754165", "image": "", "title": "Dear HF Community!", "content_text": "Dear HF Community! Our company open-sourced machine translation models for 12 rare languages under MIT license. You can use them freely with OpenNMT translation framework. Each model is about 110 mb and has an excellent performance, ( about 40000 characters / s on Nvidia RTX 3090 ) Download models there https://huggingface.co/lingvanex You can test translation quality there: https://lingvanex.com/translate/ See translation", "url": "https://huggingface.co/posts/lingvanex-mt/205421793754165", "date_published": "2025-03-03T05:21:08.366716"}, {"id": "https://huggingface.co/posts/nyuuzyou/326966093381385", "image": "", "title": "\ud83d\udcda Historical Russian Technical Journal Images Dataset -", "content_text": "\ud83d\udcda Historical Russian Technical Journal Images Dataset - nyuuzyou/journals \u0421ollection of digitized pages from vintage Russian technical journals featuring: - 7.47k high-quality images - Machine-generated descriptions in Russian - Valuable historical technical content for image-to-text applications Content descriptions are dedicated to the public domain under the CC0 1.0 license, allowing unrestricted use without attribution. See translation", "url": "https://huggingface.co/posts/nyuuzyou/326966093381385", "date_published": "2025-03-03T05:21:08.367024"}]}