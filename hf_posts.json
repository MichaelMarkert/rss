{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/AdinaY/251517572573964", "image": "", "title": "\u2728 DeepSeek V3.1 just dropped on the hub.", "content_text": "\u2728 DeepSeek V3.1 just dropped on the hub. deepseek-ai/DeepSeek-V3.1-Base See translation", "url": "https://huggingface.co/posts/AdinaY/251517572573964", "date_published": "2025-08-22T09:25:14.994165"}, {"id": "https://huggingface.co/posts/wcy1122/435759509322871", "image": "", "title": "\ud83d\ude80 Introducing MGM-Omni, an omni-chatbot capable of processing text, image, video, and speech inputs, and can generate both text and speech responses.", "content_text": "\ud83d\ude80 Introducing MGM-Omni, an omni-chatbot capable of processing text, image, video, and speech inputs, and can generate both text and speech responses. \ud83d\udc42 MGM-Omni support hour-level audio understanding. \ud83d\udde3\ufe0f MGM-Omni support 10-minute speech generation and voice cloning. For more details, please check: \ud83d\udcdd Blog: https://mgm-omni.notion.site/MGM-Omni-An-Open-source-Omni-Chatbot-2395728e0b0180149ac9f24683fc9907 \ud83c\udf1f Code: https://github.com/dvlab-research/MGM-Omni \ud83e\udd16 Model: wcy1122/mgm-omni-6896075e97317a88825032e1 \ud83c\udfae Demo: wcy1122/MGM-Omni See translation", "url": "https://huggingface.co/posts/wcy1122/435759509322871", "date_published": "2025-08-22T09:25:14.994503"}, {"id": "https://huggingface.co/posts/ProCreations/419010322512677", "image": "", "title": "why did 36 people unfollow me \ud83d\ude2d", "content_text": "why did 36 people unfollow me \ud83d\ude2d we are back in the hundreds. if you become my 500th follower and have proof I'll give you 5 dollars worth of openrouter credits as an API key See translation", "url": "https://huggingface.co/posts/ProCreations/419010322512677", "date_published": "2025-08-22T09:25:14.994732"}, {"id": "https://huggingface.co/posts/pagezyhf/949187233847606", "image": "", "title": "We've improved the Deploy button on Hugging Face model pages for Microsoft Azure", "content_text": "We've improved the Deploy button on Hugging Face model pages for Microsoft Azure 1/ no more long waits before seeing model support status 2/ ready-to-use CLI and Python snippets 3/ redirection to Azure AI Foundry rather than Azure ML \u270b if you see any bugs or have feedback, open an issue on our repo: https://github.com/huggingface/Microsoft-Azure See translation", "url": "https://huggingface.co/posts/pagezyhf/949187233847606", "date_published": "2025-08-22T09:25:14.994999"}, {"id": "https://huggingface.co/posts/frimelle/763316628765853", "image": "", "title": "\ud83d\uddfa\ufe0f New blog post \ud83d\uddfa\ufe0f", "content_text": "\ud83d\uddfa\ufe0f New blog post \ud83d\uddfa\ufe0f Old Maps, New Terrain: Updating Labour Taxonomies for the AI Era For decades, we\u2019ve relied on labour taxonomies like O*NET to understand how technology changes work. These taxonomies break down jobs into tasks and skills, but they were built in a world before most work became digital-first, and long before generative AI could create marketing campaigns, voiceovers, or even whole professions in one step. That leaves us with a mismatch: we\u2019re trying to measure the future of work with tools from the past. With @ yjernite we describe why these frameworks are falling increasingly short in the age of generative AI. We argue that instead of discarding taxonomies, we need to adapt them. Imagine taxonomies that: \u2728 Capture new AI-native tasks and hybrid human-AI workflows \u2728 Evolve dynamically as technology shifts \u2728 Give workers a voice in deciding what gets automated and what stays human If we don\u2019t act, we\u2019ll keep measuring the wrong things. If we do, we can design...", "url": "https://huggingface.co/posts/frimelle/763316628765853", "date_published": "2025-08-22T09:25:14.995472"}, {"id": "https://huggingface.co/posts/Jaward/861738398724390", "image": "", "title": "fascinating read!", "content_text": "fascinating read! staying bullish on search with rl might just help us get rid of hallucination entirely. I really like their approach: 1) <think>on prompt/context && what u know </think> 2) self <search>when u don\u2019t know</search> (iteratively) with no external tool 3) <information>cite sources to support claim(s)</information> 4) <answer>final answer</answer> their rl training was done cost efficiently too, see code: https://github.com/TsinghuaC3I/SSRL See translation", "url": "https://huggingface.co/posts/Jaward/861738398724390", "date_published": "2025-08-22T09:25:14.995758"}, {"id": "https://huggingface.co/posts/seawolf2357/250380945868372", "image": "", "title": "\ud83c\udfa8 Open Nano-Banana: Revolution in Ultra-Fast AI Image Editing!", "content_text": "\ud83c\udfa8 Open Nano-Banana: Revolution in Ultra-Fast AI Image Editing! \ud83d\ude80 Introduction **Open Nano-Banana** is an innovative image editing tool based on the Qwen-Image-Edit model. Experience amazing quality image editing in just 8 steps! Heartsync/Nano-Banana \u2728 Core Features \u26a1 Lightning-Fast Editing * **8-Step Generation**: Ultra-fast processing with Qwen-Image-Lightning LoRA * **Real-time Editing**: 10x faster than conventional methods * **GPU Optimization**: Maximized memory efficiency with xformers \ud83e\udd16 AI Prompt Enhancement * **Automatic Prompt Improvement**: Intelligent rewriting with Cerebras' Qwen3-235B model * **Multilingual Support**: Auto-detection for Korean/Chinese/English * **Context Understanding**: Sophisticated command generation aligned with image context \ud83c\udfaf Versatile Editing Functions \u2705 Add/Delete/Replace objects \u2705 Text editing and style transformation \u2705 Person editing (expressions, hairstyles) \u2705 Vintage restoration and style conversion \u2705 Background replacement and enhancement...", "url": "https://huggingface.co/posts/seawolf2357/250380945868372", "date_published": "2025-08-22T09:25:14.996334"}, {"id": "https://huggingface.co/posts/ccocks-deca/499605656909204", "image": "", "title": "12 hours ago:", "content_text": "12 hours ago: Something big* coming * big = biggest in the world Annnnnd... here it is! deca-ai/3-alpha-ultra \u2014the largest AI model in the world by deca-ai , clocking in at a whopping 4.6T parameters. Apologies for the delay, but we\u2019re stoked to finally drop this, even in its alpha stage. Before you dive in, here are a few things to keep in mind: 1. **No commercial use yet**: We're still working on Deca 2.5 (Proprietary), and releasing Deca 3 for commercial use right now would impact that. Once Deca 3.5 hits in early '26, we\u2019ll be opening it up with a more permissive license. 2. **Built on existing models**: Deca 3 isn\u2019t a ground-up creation\u2014it\u2019s a huge step forward, building on what\u2019s already out there. 3. **It\u2019s experimental**: As much as we\u2019re hyped about its scale, it\u2019s still in testing. 4. **DynaMoE architecture**: Run a (very) small part of the model with 64GB of RAM/VRAM (when quantized - quants coming soon), or the whole thing with 1TB. It\u2019s that scalable. 5. **Not widely...", "url": "https://huggingface.co/posts/ccocks-deca/499605656909204", "date_published": "2025-08-22T09:25:14.996850"}, {"id": "https://huggingface.co/posts/AdinaY/149610366794720", "image": "", "title": "Seed-OSS \ud83d\udd25 The latest open LLM from Bytedance Seed team", "content_text": "Seed-OSS \ud83d\udd25 The latest open LLM from Bytedance Seed team ByteDance-Seed/seed-oss-68a609f4201e788db05b5dcd \u2728 36B - Base & Instruct \u2728 Apache 2.0 \u2728 Native 512K long context \u2728 Strong reasoning & agentic intelligence \u2728 2 Base versions: with & without synthetic data See translation", "url": "https://huggingface.co/posts/AdinaY/149610366794720", "date_published": "2025-08-22T09:25:14.997094"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/826512832075444", "image": "", "title": "Wan 2.2, FLUX, FLUX Krea & Qwen Image Just got Upgraded: Ultimate Tutorial for Open Source SOTA Image & Video Gen Models - With easy to use SwarmUI with ComfyUI Backend :", "content_text": "Wan 2.2, FLUX, FLUX Krea & Qwen Image Just got Upgraded: Ultimate Tutorial for Open Source SOTA Image & Video Gen Models - With easy to use SwarmUI with ComfyUI Backend : https://youtu.be/3BFDcO2Ysu4 Tutorial Video : https://youtu.be/3BFDcO2Ysu4 Wan 2.2, Qwen Image, FLUX, FLUX Krea, all these models are the SOTA open-source models and in this master tutorial I will show you how to use these models in the easiest, most performant, and most accurate way. After doing almost one week of research, I have determined the very best presets and prepared this tutorial. With literally one click you will be able to install, download models, set presets, and use these amazing models. Wan 2.2 is currently the king of video generation models and now it is super fast with lightx2v Wan2.2-Lightning LoRAs. Moreover, Qwen Image is now ultra-fast with the recently released 8-step LoRA with almost no quality loss. Furthermore, I have updated FLUX and FLUX Krea presets to improve image generation...", "url": "https://huggingface.co/posts/MonsterMMORPG/826512832075444", "date_published": "2025-08-22T09:25:14.997510"}]}