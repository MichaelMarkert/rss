{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/445000542637232", "image": "", "title": "10 awesome advanced LoRA approaches", "content_text": "10 awesome advanced LoRA approaches Low-Rank Adaptation (LoRA) is the go-to method for efficient model fine-tuning that adds small low-rank matrices instead of retraining full models. The field isn\u2019t standing still \u2013 new LoRA variants push the limits of efficiency, generalization, and personalization. So we\u2019re sharing 10 of the latest LoRA approaches you should know about: 1. Mixture-of-LoRA-experts \u2192 Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection (2509.13878) Adds multiple low-rank adapters (LoRA) into a model\u2019s layers, and a routing mechanism activates the most suitable ones for each input. This lets the model adapt better to new unseen conditions 2. Amortized Bayesian Meta-Learning for LoRA (ABMLL) \u2192 Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models (2508.14285) Balances global and task-specific parameters within a Bayesian framework to improve uncertainty calibration and generalization to new tasks without high...", "url": "https://huggingface.co/posts/Kseniase/445000542637232", "date_published": "2025-09-24T05:21:04.814856"}, {"id": "https://huggingface.co/posts/prithivMLmods/322831563234696", "image": "", "title": "Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.\ud83e\udd17\u2728", "content_text": "Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.\ud83e\udd17\u2728 prithivMLmods/PhotoCleanser-i2i : Remove objects while preserving the rest of the image. prithivMLmods/Photo-Restore-i2i : Restore old photos into moderately colorized, detailed images. prithivMLmods/Polaroid-Warm-i2i : Seamless vintage Polaroid-style images with warm, faded tones. prithivMLmods/Yarn-Photo-i2i : Convert images into yarn-stitched artwork while retaining key details. prithivMLmods/Monochrome-Pencil : Turn images into monochrome pencil sketches while keeping original features. \u2728Note: All the above models share the same auto-labeling multimodal VLM captioning model, prithivMLmods/DeepCaption-VLA-7B , which is used...", "url": "https://huggingface.co/posts/prithivMLmods/322831563234696", "date_published": "2025-09-24T05:21:04.815355"}, {"id": "https://huggingface.co/posts/merve/604366247415617", "image": "", "title": "large AI labs open-sourced a ton of models last week \ud83d\udd25", "content_text": "large AI labs open-sourced a ton of models last week \ud83d\udd25 here's few picks, find even more here merve/sep-16-releases-68d13ea4c547f02f95842f05 \ud83e\udd1d > IBM released a new Docling model with 258M params based on Granite (A2.0) \ud83d\udcdd ibm-granite/granite-docling-258M > Xiaomi released 7B audio LM with base and instruct variants (MIT) XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0 > DecartAI released Lucy Edit, open Nano Banana \ud83c\udf4c (NC) decart-ai/Lucy-Edit-Dev > OpenGVLab released a family of agentic computer use models (3B/7B/32B) with the dataset \ud83d\udcbb OpenGVLab/scalecua-68c912cf56f7ff4c8e034003 > Meituan Longcat released thinking version of LongCat-Flash \ud83d\udcad meituan-longcat/LongCat-Flash-Thinking See translation", "url": "https://huggingface.co/posts/merve/604366247415617", "date_published": "2025-09-24T05:21:04.815714"}, {"id": "https://huggingface.co/posts/prithivMLmods/355225487543965", "image": "", "title": "Photo-Mate-i2i \u2013 a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages!", "content_text": "Photo-Mate-i2i \u2013 a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages! \u26a1Demo: prithivMLmods/Photo-Mate-i2i \u2699\ufe0fHow to Use: prithivMLmods/Photo-Mate-i2i#2 \ud83d\udc68\u200d\ud83d\udd27i2i-Kontext(Experimental LoRAs): prithivMLmods/i2i-kontext-exp-68ce573b5c0623476b636ec7 See translation", "url": "https://huggingface.co/posts/prithivMLmods/355225487543965", "date_published": "2025-09-24T05:21:04.816003"}, {"id": "https://huggingface.co/posts/yeonseok-zeticai/506441566129403", "image": "", "title": "YOLOv11 Complete On-device Study", "content_text": "YOLOv11 Complete On-device Study - {NPU vs GPU vs CPU} Across All Model Variants We've just completed comprehensive benchmarking of the entire YOLOv11 family on ZETIC.MLange. Here's what every ML engineer needs to know. \ud83d\udcca Key Findings Across 5 Model Variants (XL to Nano): 1. NPU Dominance in Efficiency: - YOLOv11n: 1.72ms on NPU vs 53.60ms on CPU (31x faster) - Memory footprint: 0-65MB across all variants - Consistent sub-10ms inference even on XL models 2. The Sweet Spot - YOLOv11s: - NPU: 3.23ms @ 95.57% mAP - Perfect balance: 36MB model, production-ready speed - 10x faster than GPU, 30x faster than CPU 3. Surprising Discovery: Medium models (YOLOv11m) show unusual GPU performance patterns - NPU outperforms GPU by 4x (9.55ms vs 35.82ms), suggesting current GPU kernels aren't optimized for mid-size architectures. 4. Production Insights: - XL/Large: GPU still competitive for batch processing - Small/Nano: NPU absolutely crushes everything else - Memory scaling: Linear from 10MB...", "url": "https://huggingface.co/posts/yeonseok-zeticai/506441566129403", "date_published": "2025-09-24T05:21:04.816551"}, {"id": "https://huggingface.co/posts/AdinaY/290924120685458", "image": "", "title": "BAAI has released ROME\ud83d\udd25 evaluating 30+ large reasoning models on text & visual reasoning", "content_text": "BAAI has released ROME\ud83d\udd25 evaluating 30+ large reasoning models on text & visual reasoning FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions (2509.17177) \u2728Tests visual reasoning, not just recognition \u2728Covers capability \u00d7 alignment \u00d7 safety \u00d7 efficiency \u2728More transparent & reliable (less data contamination) \u2728Helps make real-world deployment choices See translation", "url": "https://huggingface.co/posts/AdinaY/290924120685458", "date_published": "2025-09-24T05:21:04.816831"}, {"id": "https://huggingface.co/posts/Ethank01/541492789102968", "image": "", "title": "Discussing  iMini Nano Banana \u2013 Practical AI Templates for Creative Workflows", "content_text": "Discussing iMini Nano Banana \u2013 Practical AI Templates for Creative Workflows \ud83c\udf99\ufe0fI\u2019ve been experimenting with imini nano banana recently, and the new batch of 100+ AI templates feels like a real shift in how we handle creative workflows. \ud83c\udfa8One example: the photo-to-hand-drawn sketch template. I tested it with a travel photo (a blonde girl in front of the Eiffel Tower) \u2192 the output was a clean, consistent hand-drawn sketch in seconds. Normally, this would take manual editing in Photoshop/Illustrator, but the AI template cut the process down to a single click. \ud83d\udca1From a workflow perspective, this could streamline tasks for: Designers: quick iterations on style variations Marketers: generating campaign visuals at scale Content creators: producing unique artwork without advanced editing skills \ud83d\udd8c\ufe0fIf anyone here has tested imini nano banana in professional projects, I\u2019d like to hear how it fits into your pipeline. Full details: https://imini.com/nano-banana See translation", "url": "https://huggingface.co/posts/Ethank01/541492789102968", "date_published": "2025-09-24T05:21:04.817243"}, {"id": "https://huggingface.co/posts/Monica997/874620000877286", "image": "", "title": "AI Just Made My Cat the King of Emojis \ud83d\udc51\ud83d\udc31\ud83d\ude02", "content_text": "AI Just Made My Cat the King of Emojis \ud83d\udc51\ud83d\udc31\ud83d\ude02 Never thought I\u2019d see this \u2014 but with iMini\u2019s nano banana model, my cat is now a full emoji + sticker pack \ud83c\udfa8\u2728 Used the 9-grid meme template + cartoon sticker generator, and in just ONE click \ud83d\udc49 my ordinary cat photo turned into a hilarious, cute, and super shareable set of stickers \ud83d\udcac\ud83d\udd25 No need to master complicated nano banana prompts \u2014 iMini handles everything. Perfect for chats, socials, or just showing off your pet\u2019s new \u201cdigital identity.\u201d \ud83d\udc49 Try it here: https://imini.com/nano-banana Who else wants their pet to be the next emoji star? \ud83c\udf1f See translation", "url": "https://huggingface.co/posts/Monica997/874620000877286", "date_published": "2025-09-24T05:21:04.817575"}, {"id": "https://huggingface.co/posts/kanaria007/142877905661382", "image": "", "title": "\u2705 New Article: *Epilogue \u2014 The Structured Intelligence Computer*", "content_text": "\u2705 New Article: *Epilogue \u2014 The Structured Intelligence Computer* Title: \ud83d\udda5\ufe0f SIC-Epilogue: The Structured Intelligence Computer \ud83d\udd17 https://huggingface.co/blog/kanaria007/sic-epilogue --- Summary: With *SPU* (processing), *GSPU* (simulation), and *SIM/SIS* (memory & storage), the architecture of the *Structured Intelligence Computer (SIC)* is now complete. This epilogue unifies the pieces into one design, and shows why even peripherals like an *AmuSphere-class interface* become inevitable extensions once SIC exists. > The point is not BIC vs AmuSphere. > *It is structured, safe, explainable immersion under one law of intelligence.* --- Why It Matters: \u2022 Marks the closure of the hardware PoC arc \u2022 Shows the computer as not only faster, but *resilient, ethical, and auditable* \u2022 Positions SIC as the substrate for AGI, education, and exploration --- What\u2019s Inside: \u2022 Integration of SPU, GSPU, SIM/SIS into a single SIC stack \u2022 Peripheral emergence (AmuSphere-class) as natural consequence \u2022...", "url": "https://huggingface.co/posts/kanaria007/142877905661382", "date_published": "2025-09-24T05:21:04.818140"}, {"id": "https://huggingface.co/posts/sergiopaniego/447744302131594", "image": "", "title": "This summer TRL leveled up for multimodal alignment \ud83c\udf1e", "content_text": "This summer TRL leveled up for multimodal alignment \ud83c\udf1e \u2705 New VLM alignment methods (MPO, GRPO, GSPO) \u2705 Extended RLOO & Online DPO for VLMs \u2705 Native SFT support \u2705 Ready-to-use training scripts \ud83d\udd17 https://huggingface.co/blog/trl-vlm-alignment See translation", "url": "https://huggingface.co/posts/sergiopaniego/447744302131594", "date_published": "2025-09-24T05:21:04.818396"}]}