{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/762937246285628", "image": "", "title": "12 Types of JEPA", "content_text": "12 Types of JEPA Since Yann LeCun together with Randall Balestriero released a new paper on JEPA (Joint-Embedding Predictive Architecture), laying out its theory and introducing an efficient practical version called LeJEPA, we figured you might need even more JEPA. Here are 7 recent JEPA variants plus 5 iconic ones: 1. LeJEPA \u2192 LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics (2511.08544) Explains a full theory for JEPAs, defining the \u201cideal\u201d JEPA embedding as an isotropic Gaussian, and proposes the SIGReg objective to push JEPA toward this ideal, resulting in practical LeJEPA 2. JEPA-T \u2192 JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation (2510.00974) A text-to-image model that tokenizes images and captions with a joint predictive Transformer, enhances fusion with cross-attention and text embeddings before training loss, and generates images by iteratively denoising visual tokens conditioned on text 3. Text-JEPA \u2192...", "url": "https://huggingface.co/posts/Kseniase/762937246285628", "date_published": "2025-11-19T09:27:40.676867"}, {"id": "https://huggingface.co/posts/sergiopaniego/332334875092196", "image": "", "title": "Who wants a TRL sticker? \ud83d\ude4b", "content_text": "Who wants a TRL sticker? \ud83d\ude4b https://github.com/huggingface/trl See translation", "url": "https://huggingface.co/posts/sergiopaniego/332334875092196", "date_published": "2025-11-19T09:27:40.677144"}, {"id": "https://huggingface.co/posts/flozi00/635605102777732", "image": "", "title": "Running large language models efficiently is more than just raw GPU power. The latest guide breaks down the essential math to determine if your LLM workload is compute-bound or memory-bound.", "content_text": "Running large language models efficiently is more than just raw GPU power. The latest guide breaks down the essential math to determine if your LLM workload is compute-bound or memory-bound. We apply these principles to a real-world example: Qwen's 32B parameter model on the new NVIDIA RTX PRO 6000 Blackwell Edition. In this guide, you will learn how to: Calculate your GPU's operational intensity (Ops:Byte Ratio) Determine your model's arithmetic intensity Identify whether your workload is memory-bound or compute-bound Read the full guide here: https://flozi.net/en/guides/ai/llm-inference-math See translation", "url": "https://huggingface.co/posts/flozi00/635605102777732", "date_published": "2025-11-19T09:27:40.677437"}, {"id": "https://huggingface.co/posts/cjerzak/918588861809536", "image": "", "title": ">>> We're writing a new book,  <Planetary Causal Inference>, on how to model counterfactuals at planetary scale by combining satellite imagery + other global data with local studies and RCTs. Forthcoming in 2026+.", "content_text": ">>> We're writing a new book, <Planetary Causal Inference>, on how to model counterfactuals at planetary scale by combining satellite imagery + other global data with local studies and RCTs. Forthcoming in 2026+. >>> Book info: https://planetarycausalinference.org/book-launch >>> All datasets used in the book will be openly available on our lab\u2019s Hugging Face hub: theaidevlab See translation", "url": "https://huggingface.co/posts/cjerzak/918588861809536", "date_published": "2025-11-19T09:27:40.677723"}, {"id": "https://huggingface.co/posts/prithivMLmods/902988810263838", "image": "", "title": "Made a demo for multimodal understanding of Qwen3-VL space for tasks including point annotation, detection, captioning, guided text inferences, and more. Find the demo link below. \ud83e\udd17\u2197\ufe0f", "content_text": "Made a demo for multimodal understanding of Qwen3-VL space for tasks including point annotation, detection, captioning, guided text inferences, and more. Find the demo link below. \ud83e\udd17\u2197\ufe0f \u2b9e Space[Demo]: prithivMLmods/Qwen3-VL-HF-Demo \u2b9e Model Used: Qwen/Qwen3-VL-4B-Instruct \u2b9e Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations \u2b9e GitHub: https://github.com/PRITHIVSAKTHIUR/Qwen-3VL-Multimodal-Understanding To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/902988810263838", "date_published": "2025-11-19T09:27:40.678028"}, {"id": "https://huggingface.co/posts/vikhyatk/769322438249892", "image": "", "title": "Announcing RefCOCO-M, a refreshed RefCOCO with pixel-accurate masks and the problematic prompts removed.", "content_text": "Announcing RefCOCO-M, a refreshed RefCOCO with pixel-accurate masks and the problematic prompts removed. moondream/refcoco-m See translation", "url": "https://huggingface.co/posts/vikhyatk/769322438249892", "date_published": "2025-11-19T09:27:40.678224"}, {"id": "https://huggingface.co/posts/ZennyKenny/159598235519685", "image": "", "title": "\ud83c\udf89 Wow. Congratulations", "content_text": "\ud83c\udf89 Wow. Congratulations @ bfirsh and the Replicate team on the CloudFlare acquisition! \u270c\ufe0f You've really built an incredible ecosystem and product offering and should be super proud. See translation", "url": "https://huggingface.co/posts/ZennyKenny/159598235519685", "date_published": "2025-11-19T09:27:40.678444"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/695072229497754", "image": "", "title": "Next level Realism with Qwen Image is now possible after new realism LoRA workflow - Top images are new realism workflow - Bottom ones are older default - Full tutorial published - 4+4 Steps only", "content_text": "Next level Realism with Qwen Image is now possible after new realism LoRA workflow - Top images are new realism workflow - Bottom ones are older default - Full tutorial published - 4+4 Steps only Tutorial of realism : https://youtu.be/XWzZ2wnzNuQ Tutorial of training : https://youtu.be/DPX3eBTuO_Y This is a full comprehensive step-by-step tutorial for how to train Qwen Image models. This tutorial covers how to do LoRA training and full Fine-Tuning / DreamBooth training on Qwen Image models. It covers both the Qwen Image base model and the Qwen Image Edit Plus 2509 model. This tutorial is the product of 21 days of full R&D, costing over $800 in cloud services to find the best configurations for training. Furthermore, we have developed an amazing, ultra-easy-to-use Gradio app to use the legendary Kohya Musubi Tuner trainer with ease. You will be able to train locally on your Windows computer with GPUs with as little as 6 GB of VRAM for both LoRA and Fine-Tuning. See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/695072229497754", "date_published": "2025-11-19T09:27:40.678815"}, {"id": "https://huggingface.co/posts/ronantakizawa/582063212338418", "image": "", "title": "Introducing the Japanese Character Difficulty Dataset: a collection of 3,003 Japanese characters (Kanji) labeled with official educational difficulty grades. It includes elementary (grades 1\u20136), secondary (grade 8), and advanced (grade 9) characters, making it useful for language learning, text difficulty analysis, and educational tool development \ud83c\udf89", "content_text": "Introducing the Japanese Character Difficulty Dataset: a collection of 3,003 Japanese characters (Kanji) labeled with official educational difficulty grades. It includes elementary (grades 1\u20136), secondary (grade 8), and advanced (grade 9) characters, making it useful for language learning, text difficulty analysis, and educational tool development \ud83c\udf89 ronantakizawa/japanese-character-difficulty #japanese #kanji #japanesedataset See translation", "url": "https://huggingface.co/posts/ronantakizawa/582063212338418", "date_published": "2025-11-19T09:27:40.679088"}, {"id": "https://huggingface.co/posts/theainerd/926652286906905", "image": "", "title": "Hindi Speech to Text just crossed 20 million downloads. Grateful for everyone using it.", "content_text": "Hindi Speech to Text just crossed 20 million downloads. Grateful for everyone using it. theainerd/Wav2Vec2-large-xlsr-hindi See translation", "url": "https://huggingface.co/posts/theainerd/926652286906905", "date_published": "2025-11-19T09:27:40.679281"}]}