{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/jasoncorkill/614528339803376", "image": "", "title": "Do you remember", "content_text": "Do you remember https://thispersondoesnotexist.com/ ? It was one of the first cases where the future of generative media really hit us. Humans are incredibly good at recognizing and analyzing faces, so they are a very good litmus test for any generative image model. But none of the current benchmarks measure the ability of models to generate humans independently. So we built our own. We measure the models ability to generate a diverse set of human faces and using over 20'000 human annotations we ranked all of the major models on their ability to generate faces. Find the full ranking here: https://app.rapidata.ai/mri/benchmarks/68af24ae74482280b62f7596 We have release the full underlying data publicly here on huggingface: Rapidata/Face_Generation_Benchmark See translation", "url": "https://huggingface.co/posts/jasoncorkill/614528339803376", "date_published": "2025-11-12T09:26:36.701806"}, {"id": "https://huggingface.co/posts/wang12390/272731401856149", "image": "", "title": "How to Use the AI Image Generator", "content_text": "How to Use the AI Image Generator https://miragic.ai/products/image-generator 1. Describe Your Vision: Enter a text prompt describing what you want to create. Example: \"A futuristic city skyline at sunset with glowing airships.\" 2. Select a Style: Choose an art style\u2014Realistic, Anime, Painterly, Surreal, or Minimalist\u2014to match your idea. 3. Generate and Refine: Click \"Generate Image\" and let the AI do its magic. Want to tweak it? Refine your prompt or try a new style. 4. Download and Share: Save your creation in high resolution or share it directly on social media. See translation", "url": "https://huggingface.co/posts/wang12390/272731401856149", "date_published": "2025-11-12T09:26:36.702156"}, {"id": "https://huggingface.co/posts/Locutusque/780653085928819", "image": "", "title": "\ud83d\ude80 AutoXLA - Accelerating Large Models on TPU", "content_text": "\ud83d\ude80 AutoXLA - Accelerating Large Models on TPU AutoXLA is an experimental library that automates the distribution, optimization, and quantization of large language models for TPUs using PyTorch/XLA. It extends the Hugging Face Transformers interface with TPU-aware features such as automatic sharding, custom attention kernels, and quantization-aware loading, making large-scale deployment and training both simpler and faster. With quantization and Splash Attention kernels, AutoXLA achieves up to 4\u00d7 speedups over standard Flash Attention implementations, significantly improving throughput for both inference and training workloads. Whether you\u2019re experimenting with distributed setups (FSDP, 2D, or 3D sharding) or optimizing memory via LanguageModelQuantizer, AutoXLA is built to make scaling LLMs on TPU seamless. \u26a0\ufe0f Note: This is an experimental repository. Expect rough edges! Please report bugs or unexpected behavior through GitHub issues. \ud83d\udd17 GitHub Repository:...", "url": "https://huggingface.co/posts/Locutusque/780653085928819", "date_published": "2025-11-12T09:26:36.702586"}, {"id": "https://huggingface.co/posts/wang12390/363647142063020", "image": "", "title": "Garment Try-On: Transforming Fashion Design and Shopping with AI", "content_text": "Garment Try-On: Transforming Fashion Design and Shopping with AI Discover how AI-powered Garment Try-On technology transforms fashion design, e-commerce, and sustainability with realistic 3D garment visualization. Miragic-AI/Miragic-Virtual-Try-On See translation", "url": "https://huggingface.co/posts/wang12390/363647142063020", "date_published": "2025-11-12T09:26:36.702822"}, {"id": "https://huggingface.co/posts/codelion/147786491495276", "image": "", "title": "Want to experiment with pre-training dataset mixtures but don't want to process terabytes of data? We've got you covered.", "content_text": "Want to experiment with pre-training dataset mixtures but don't want to process terabytes of data? We've got you covered. We're releasing a collection of several carefully curated 1B token dataset samples specifically designed for rapid prototyping and pretraining experiments: https://huggingface.co/collections/codelion/pre-training-dataset-samples These samples were created using reservoir sampling - an algorithm that guarantees statistically unbiased random samples from massive source datasets. This means results you get at the 1B token scale are representative of how these datasets behave at 100B+ token scales, letting you iterate quickly without the computational overhead. The collection includes: - finePDFs-1B: High-quality textbook-style educational content - DCLM-baseline-1B: Filtered, diverse web content - FineWeb-Edu-1B: Curated educational web resources We used these exact samples to run 50+ systematic experiments on dataset mixing strategies, ultimately discovering that a...", "url": "https://huggingface.co/posts/codelion/147786491495276", "date_published": "2025-11-12T09:26:36.703238"}, {"id": "https://huggingface.co/posts/Kseniase/707388483660581", "image": "", "title": "7+ Main precision formats used in AI:", "content_text": "7+ Main precision formats used in AI: Precision is very important in AI as it shapes how accurate and efficient models are. It controls how finely numbers are represented, approximating real-world values with formats like fixed-point and floating-point. A recent BF16 \u2192 FP16 study renewed attention to precision impact. Here are the main precision types used in AI, from full precision for training to ultra-low precision for inference: 1. FP32 (Float32): Standard full-precision float used in most training: 1 sign bit, 8 exponent bits, 23 mantissa bits. Default for backward-compatible training and baseline numerical stability 2. FP16 (Float16) \u2192 https://arxiv.org/abs/2305.10947v6 Half-precision float. It balances accuracy and efficiency. 1 sign bit, 5 exponent bits, 10 mantissa bits. Common on NVIDIA Tensor Cores and mixed-precision setups. There\u2019s now a new wave of using it in reinforcement learning: https://www.turingpost.com/p/fp16 3. BF16 (BFloat16) \u2192...", "url": "https://huggingface.co/posts/Kseniase/707388483660581", "date_published": "2025-11-12T09:26:36.703856"}, {"id": "https://huggingface.co/posts/onekq/821878778302944", "image": "", "title": "Instead of architectural upgade, each major model drop nowadays perfects a regional innovation. What Kimi brought to spot light this time is quantization aware training (QAT). I wrote an article to explain it and why it matters to reasoning models.", "content_text": "Instead of architectural upgade, each major model drop nowadays perfects a regional innovation. What Kimi brought to spot light this time is quantization aware training (QAT). I wrote an article to explain it and why it matters to reasoning models. https://huggingface.co/blog/onekq/qat-bonsai If you are interested in this kind of posts, I will introduce the Muon optimizers, another technology behind Kimi success. See translation", "url": "https://huggingface.co/posts/onekq/821878778302944", "date_published": "2025-11-12T09:26:36.704113"}, {"id": "https://huggingface.co/posts/mitkox/956503968449347", "image": "", "title": "I just threw Qwen3-0.6B in BF16 into an on device AI drag race on AMD Strix Halo with vLLM:", "content_text": "I just threw Qwen3-0.6B in BF16 into an on device AI drag race on AMD Strix Halo with vLLM: 564 tokens/sec on short 100-token sprints 96 tokens/sec on 8K-token marathons TL;DR You don't just run AI on AMD. You negotiate with it. The hardware absolutely delivers. Spoiler alert; there is exactly ONE configuration where vLLM + ROCm + Triton + PyTorch + Drivers + Ubuntu Kernel to work at the same time. Finding it required the patience of a saint Consumer AMD for AI inference is the ultimate \"budget warrior\" play, insane performance-per-euro, but you need hardcore technical skills that would make a senior sysadmin nod in quiet respect. See translation", "url": "https://huggingface.co/posts/mitkox/956503968449347", "date_published": "2025-11-12T09:26:36.704416"}, {"id": "https://huggingface.co/posts/Parveshiiii/257570757344007", "image": "", "title": "SparkEmbedding - SoTA cross lingual retrieval", "content_text": "SparkEmbedding - SoTA cross lingual retrieval Iam very happy to announce our latest embedding model sparkembedding-300m base on embeddinggemma-300m we fine tuned it on 1m extra examples spanning over 119 languages and result is this model achieves exceptional cross lingual retrieval Model: XenArcAI/SparkEmbedding-300m See translation", "url": "https://huggingface.co/posts/Parveshiiii/257570757344007", "date_published": "2025-11-12T09:26:36.704642"}, {"id": "https://huggingface.co/posts/nouamanetazi/972464132222376", "image": "", "title": "After training \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25\ud835\udc0b\ud835\udc0c\ud835\udfd1 on \ud835\udfd1\ud835\udfd6\ud835\udfd2 \ud835\udc07\ud835\udfcf\ud835\udfce\ud835\udfce\ud835\udc2c for nearly a month, I've come to realize something most people overlook: \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc24\ud835\udc1e-\ud835\udc28\ud835\udc2b-\ud835\udc1b\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc24 \ud835\udc1f\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc22\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20. \ud83d\udd25", "content_text": "After training \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25\ud835\udc0b\ud835\udc0c\ud835\udfd1 on \ud835\udfd1\ud835\udfd6\ud835\udfd2 \ud835\udc07\ud835\udfcf\ud835\udfce\ud835\udfce\ud835\udc2c for nearly a month, I've come to realize something most people overlook: \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc24\ud835\udc1e-\ud835\udc28\ud835\udc2b-\ud835\udc1b\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc24 \ud835\udc1f\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc22\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20. \ud83d\udd25 Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious \ud835\udc0d\ud835\udc02\ud835\udc02\ud835\udc0b \ud835\udc1e\ud835\udc2b\ud835\udc2b\ud835\udc28\ud835\udc2b\ud835\udc2c, or when your expensive GPU cluster is running at \ud835\udfd4\ud835\udfce% \ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc32, the problem isn't your model. It's most probably a \ud835\udc26\ud835\udc22\ud835\udc2c\ud835\udc2e\ud835\udc2c\ud835\udc1e \ud835\udc28\ud835\udc1f \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc21\ud835\udc1a\ud835\udc2b\ud835\udc1d\ud835\udc30\ud835\udc1a\ud835\udc2b\ud835\udc1e. \ud83d\udee0\ufe0f Questions that seemed simple but had no clear answers: Why is \ud835\udc0c\ud835\udc28\ud835\udc04 \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc2c\ud835\udc25\ud835\udc28\ud835\udc30\ud835\udc1e\ud835\udc2b \ud835\udc2d\ud835\udc21\ud835\udc1a\ud835\udc27 \ud835\udc1d\ud835\udc1e\ud835\udc27\ud835\udc2c\ud835\udc1e \ud835\udc26\ud835\udc28\ud835\udc1d\ud835\udc1e\ud835\udc25\ud835\udc2c? Which \ud835\udc0d\ud835\udc02\ud835\udc02\ud835\udc0b \ud835\udc1f\ud835\udc25\ud835\udc1a\ud835\udc20\ud835\udc2c should we actually set? How often should we checkpoint without killing throughput? That's why we built \ud835\udc13\ud835\udc21\ud835\udc1e \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25 \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc0f\ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1b\ud835\udc28\ud835\udc28\ud835\udc24 \ud83d\udcd6: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1e\ud835\udc2b that most teams get wrong. We validated real vs...", "url": "https://huggingface.co/posts/nouamanetazi/972464132222376", "date_published": "2025-11-12T09:26:36.705227"}]}