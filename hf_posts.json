{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/ginipick/692850049335646", "image": "", "title": "\ud83c\udfa8 Renoir Studio: Impressionist Masterpieces Reborn Through AI \u2728", "content_text": "\ud83c\udfa8 Renoir Studio: Impressionist Masterpieces Reborn Through AI \u2728 \ud83c\udf1f Experience Renoir's Magical Brushstrokes with AI! \ud83d\udd17 Try it now: ginigen/flux-lora-renoir \ud83d\udd17 Model page: openfree/pierre-auguste-renoir \ud83d\udd17 Collection: openfree/painting-art-ai-681453484ec15ef5978bbeb1 Hello, AI art enthusiasts! \ud83d\udc96 Today I'm introducing a special model - Pierre-Auguste Renoir Studio. Create your own beautiful artwork in the style of the 19th century French Impressionist master! \ud83d\uddbc\ufe0f \u2728 Why Renoir's Style? Renoir is famous for his luminous colors and soft brushstrokes. His works feature: \ud83c\udf1e Warm sunshine and dancing light \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66 The beauty of everyday life and joyful moments \ud83c\udf38 Vibrant nature and portraits of beautiful women \ud83c\udfad Lively Parisian social gatherings and outdoor scenes \ud83d\udd2c Technical Features This model was developed as a flux-based learning model trained on a curated collection of high-resolution masterpieces from renowned global artists. The LoRA fine-tuning process leveraged exceptional quality open-...", "url": "https://huggingface.co/posts/ginipick/692850049335646", "date_published": "2025-05-02T13:31:09.581998"}, {"id": "https://huggingface.co/posts/abidlabs/810486848644944", "image": "", "title": "HOW TO ADD MCP SUPPORT TO ANY \ud83e\udd17 SPACE", "content_text": "HOW TO ADD MCP SUPPORT TO ANY \ud83e\udd17 SPACE Gradio now supports MCP! If you want to convert an existing Space, like this one hexgrad/Kokoro-TTS , so that you can use it with Claude Desktop / Cursor / Cline / TinyAgents / or any LLM that supports MCP, here's all you need to do: 1. Duplicate the Space (in the Settings Tab) 2. Upgrade the Gradio sdk_version to 5.28 (in the README.md ) 3. Set mcp_server=True in launch() 4. (Optionally) add docstrings to the function so that the LLM knows how to use it, like this: def generate ( text, speed= 1 ): \"\"\" Convert text to speech audio. Parameters: text (str): The input text to be converted to speech. speed (float, optional): Playback speed of the generated speech. That's it! Now your LLM will be able to talk to you \ud83e\udd2f See translation", "url": "https://huggingface.co/posts/abidlabs/810486848644944", "date_published": "2025-05-02T13:31:09.582463"}, {"id": "https://huggingface.co/posts/abidlabs/767083410530735", "image": "", "title": "Hi folks! Excited to share a new feature from the Gradio team along with a tutorial.", "content_text": "Hi folks! Excited to share a new feature from the Gradio team along with a tutorial. If you don't already know, Gradio is an open-source Python library used to build interfaces for machine learning models. Beyond just creating UIs, Gradio also exposes API capabilities and now, Gradio apps can be launched Model Context Protocol (MCP) servers for LLMs. If you already know how to use Gradio, there are only two additional things you need to do: * Add standard docstrings to your function (these will be used to generate the descriptions for your tools for the LLM) * Set mcp_server=True in launch() Here's a complete example (make sure you already have the latest version of Gradio installed): import gradio as gr def letter_counter ( word, letter ): \"\"\"Count the occurrences of a specific letter in a word. Args: word: The word or phrase to analyze letter: The letter to count occurrences of Returns: The number of times the letter appears in the word \"\"\" return...", "url": "https://huggingface.co/posts/abidlabs/767083410530735", "date_published": "2025-05-02T13:31:09.582933"}, {"id": "https://huggingface.co/posts/lukmanaj/906857593942972", "image": "", "title": "I\u2019m excited to share that I\u2019ve completed the Hugging Face Agents Course and earned my certificate.", "content_text": "I\u2019m excited to share that I\u2019ve completed the Hugging Face Agents Course and earned my certificate. Over the past few months, I explored how to build intelligent, autonomous agents using cutting-edge tools like smolagents, LlamaIndex, and LangGraph. The course covered everything from the fundamentals of agents to advanced topics like fine-tuning for function-calling, observability, evaluation, and even agents in games. Some key content included: 1. Introduction to AI Agents 2. Agentic RAG use cases 3. Multi-framework implementation: smolagents, LlamaIndex, and LangGraph 4. Building, testing, and certifying a complete agent project This was a hands-on, practical experience that deepened my understanding of how to design reliable, tool-using LLM agents. Looking forward to leveraging these skills in real-world applications in healthcare, logistics, and beyond. Many thanks to the Hugging Face team for putting this together. Let\u2019s build safe and useful agents! See translation", "url": "https://huggingface.co/posts/lukmanaj/906857593942972", "date_published": "2025-05-02T13:31:09.583357"}, {"id": "https://huggingface.co/posts/AdinaY/438609555040169", "image": "", "title": "DeepSeek, Alibaba, Skywork,  Xiaomi, Bytedance.....", "content_text": "DeepSeek, Alibaba, Skywork, Xiaomi, Bytedance..... And that\u2019s just part of the companies from the Chinese community that released open models in April \ud83e\udd2f zh-ai-community/april-2025-open-releases-from-the-chinese-community-67ea699965f6e4c135cab10f \ud83c\udfac Video > MAGI-1 by SandAI > SkyReels-A2 & SkyReels-V2 by Skywork > Wan2.1-FLF2V by Alibaba-Wan \ud83c\udfa8 Image > HiDream-I1 by Vivago AI > Kimi-VL by Moonshot AI > InstantCharacter by InstantX & Tencent-Hunyuan > Step1X-Edit by StepFun > EasyControl by Shanghai Jiaotong University \ud83e\udde0 Reasoning > MiMo by Xiaomi > Skywork-R1V 2.0 by Skywork > ChatTS by ByteDance > Kimina by Moonshot AI & Numina > GLM-Z1 by Zhipu AI > Skywork OR1 by Skywork > Kimi-VL-Thinking by Moonshot AI \ud83d\udd0a Audio > Kimi-Audio by Moonshot AI > IndexTTS by BiliBili > MegaTTS3 by ByteDance > Dolphin by DataOceanAI \ud83d\udd22 Math > DeepSeek Prover V2 by Deepseek \ud83c\udf0d LLM > Qwen by Alibaba-Qwen > InternVL3 by Shanghai AI lab > Ernie4.5 (demo) by Baidu \ud83d\udcca Dataset > PHYBench by Eureka-Lab >...", "url": "https://huggingface.co/posts/AdinaY/438609555040169", "date_published": "2025-05-02T13:31:09.583835"}, {"id": "https://huggingface.co/posts/jsulz/177281245715492", "image": "", "title": "At", "content_text": "At xet-team we've been hard at work bringing a new generation of storage to the Hugging Face community, and we\u2019ve crossed some major milestones: \ud83d\udc77 Over 2,000 builders and nearing 100 organizations with access to Xet \ud83d\ude80 Over 70,000 model and dataset repositories are Xet-backed \ud83e\udd2f 1.4 petabytes managed by Xet As we move repos from LFS to Xet for everyone we onboard, we\u2019re pushing our content-addressed store (CAS). Check out the chart below \ud83d\udc47 of CAS hitting up to 150 Gb/s throughput this past week. All of this growth is helping us build richer insights. We expanded our repo graph, which maps how Xet-backed repositories on the Hub share bytes with each other. Check out the current network in the image below (nodes are repositories, edges are where repos share bytes) and visit the space to see how different versions of Qwen, Llama, and Phi models are grouped together xet-team/repo-graph Join the waitlist to get access! https://huggingface.co/join/xet See translation", "url": "https://huggingface.co/posts/jsulz/177281245715492", "date_published": "2025-05-02T13:31:09.584288"}, {"id": "https://huggingface.co/posts/sanaka87/703703147958180", "image": "", "title": "\ud83d\ude80 Excited to Share Our Latest Work: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer\uff5e", "content_text": "\ud83d\ude80 Excited to Share Our Latest Work: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer\uff5e \ud83c\udfa8 Daily Paper: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer (2504.20690) \ud83d\udd13 Code is now open source! \ud83d\udd25 Huggingface DEMO: RiverZ/ICEdit \ud83c\udf10 Project Website: https://river-zhang.github.io/ICEdit-gh-pages/ \ud83c\udfe0 GitHub Repository: https://github.com/River-Zhang/ICEdit/blob/main/scripts/gradio_demo.py \ud83e\udd17 Huggingface: sanaka87/ICEdit-MoE-LoRA \ud83d\udcc4 arxiv Paper: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer (2504.20690) \ud83d\udd25 Why it\u2019s cool: - Achieves high-quality, multi-task image editing. - Uses only 1% of the training parameters and 0.1% of the training data compared to existing methods \u2014 extremely efficient - Beats several commercial models on background preservation, ID control, and consistency - Open-...", "url": "https://huggingface.co/posts/sanaka87/703703147958180", "date_published": "2025-05-02T13:31:09.584775"}, {"id": "https://huggingface.co/posts/openfree/886239913784185", "image": "", "title": "\ud83d\ude80 Introducing Phi-4-reasoning-plus: Powerful 14B Reasoning Model by Microsoft!", "content_text": "\ud83d\ude80 Introducing Phi-4-reasoning-plus: Powerful 14B Reasoning Model by Microsoft! VIDraft/phi-4-reasoning-plus \ud83c\udf1f Key Highlights Compact Size (14B parameters): Efficient for use in environments with limited computing resources, yet powerful in performance. Extended Context (32k tokens): Capable of handling lengthy and complex input sequences. Enhanced Reasoning: Excels at multi-step reasoning, particularly in mathematics, science, and coding challenges. Chain-of-Thought Methodology: Provides a detailed reasoning process, followed by concise, accurate summaries. \ud83c\udfc5 Benchmark Achievements Despite its smaller size, Phi-4-reasoning-plus has delivered impressive results, often surpassing significantly larger models: Mathematical Reasoning (AIME 2025): Achieved an accuracy of 78%, significantly outperforming larger models like DeepSeek-R1 Distilled (51.5%) and Claude-3.7 Sonnet (58.7%). Olympiad-level Math (OmniMath): Strong performance with an accuracy of 81.9%, surpassing DeepSeek-R1...", "url": "https://huggingface.co/posts/openfree/886239913784185", "date_published": "2025-05-02T13:31:09.585415"}, {"id": "https://huggingface.co/posts/fdaudens/447589021070314", "image": "", "title": "Want to know which AI models are least likely to hallucinate \u2014 and how to keep yours from spiking hallucinations by 20%?", "content_text": "Want to know which AI models are least likely to hallucinate \u2014 and how to keep yours from spiking hallucinations by 20%? A new benchmark called Phare, by Giskard, tested leading models across multiple languages, revealing three key findings: 1\ufe0f\u20e3 Popular models aren't necessarily factual. Some models ranking highest in user satisfaction benchmarks like LMArena are actually more prone to hallucination. 2\ufe0f\u20e3 The way you ask matters - a lot. When users present claims confidently (\"My teacher said...\"), models are 15% less likely to correct misinformation vs. neutral framing (\"I heard...\"). 3\ufe0f\u20e3 Telling models to \"be concise\" can increase hallucination by up to 20%. What's also cool is that the full dataset is public - use them to test your own models or dive deeper into the results! H/t @ davidberenstein1957 for the link. - Study: https://www.giskard.ai/knowledge/good-answers-are-not-necessarily-factual-answers-an-analysis-of-hallucination-in-leading-llms - Leaderboard:...", "url": "https://huggingface.co/posts/fdaudens/447589021070314", "date_published": "2025-05-02T13:31:09.585842"}, {"id": "https://huggingface.co/posts/danielhanchen/815477177728331", "image": "", "title": "\ud83d\udc9c Qwen3 128K Context Length: We've released Dynamic 2.0 GGUFs + 4-bit safetensors!", "content_text": "\ud83d\udc9c Qwen3 128K Context Length: We've released Dynamic 2.0 GGUFs + 4-bit safetensors! Fixed: Now works on any inference engine and fixed issues with the chat template. Qwen3 GGUFs: 30B-A3B: unsloth/Qwen3-30B-A3B-GGUF 235-A22B: unsloth/Qwen3-235B-A22B-GGUF 32B: unsloth/Qwen3-32B-GGUF Read our guide on running Qwen3 here: https://docs.unsloth.ai/basics/qwen3-how-to-run-and-finetune 128K Context Length: 30B-A3B: unsloth/Qwen3-30B-A3B-128K-GGUF 235-A22B: unsloth/Qwen3-235B-A22B-128K-GGUF 32B: unsloth/Qwen3-32B-128K-GGUF All Qwen3 uploads: unsloth/qwen3-680edabfb790c8c34a242f95 See translation", "url": "https://huggingface.co/posts/danielhanchen/815477177728331", "date_published": "2025-05-02T13:31:09.586233"}]}