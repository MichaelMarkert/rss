{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/andito/535116877809522", "image": "", "title": "\ud83e\udde0\ud83d\udc41\ufe0f Can AI visualize solutions?", "content_text": "\ud83e\udde0\ud83d\udc41\ufe0f Can AI visualize solutions? Humans often solve visual problems by sketching ideas in our minds. What if Vision-Language Models (VLMs) could do something similar, not by generating full images, but by using internal \u201cmental sketches\u201d? That\u2019s the idea behind Mirage, a new framework that empowers VLMs to reason using latent visual tokens. Instead of just thinking in words, Mirage mixes in abstract visual representations that help the model solve complex tasks. These aren't photorealistic images. They're compact, internal representations optimized purely to support reasoning. \ud83d\udd27 Mirage is trained in two phases: 1) Grounding: It learns to produce latent tokens anchored in real images. 2) Refinement: The model drops the images and learns to generate visual tokens on its own. \ud83d\udcc8 And yes, it works! On challenging benchmarks like Visual Spatial Planning, Jigsaw puzzles, and Spatial Attention Tasks, Mirage clearly outperforms GPT-4o and other strong baselines. Smart sketches > empty words....", "url": "https://huggingface.co/posts/andito/535116877809522", "date_published": "2025-07-04T13:34:21.558339"}, {"id": "https://huggingface.co/posts/m-ric/145810386001131", "image": "", "title": "If you're using any HF libraries, you should enable the Hub MCP in your agentic coding tool!", "content_text": "If you're using any HF libraries, you should enable the Hub MCP in your agentic coding tool! The brand new Docs Semantic Search tool is intravenous caffeine supply for Cursor, enables to correct API errors in a few seconds, gj @ mishig \u26a1\ufe0f\u26a1\ufe0f \ud83d\udc49 To enable Hub MCP, head to your account setting, under MCP, and it will give you everything you need! See translation", "url": "https://huggingface.co/posts/m-ric/145810386001131", "date_published": "2025-07-04T13:34:21.558631"}, {"id": "https://huggingface.co/posts/tomaarsen/190568030432786", "image": "", "title": "\u203c\ufe0fSentence Transformers v5.0 is out! The biggest update yet introduces Sparse Embedding models, encode methods improvements, Router module for asymmetric models & much more. Sparse + Dense = \ud83d\udd25 hybrid search performance! Details:", "content_text": "\u203c\ufe0fSentence Transformers v5.0 is out! The biggest update yet introduces Sparse Embedding models, encode methods improvements, Router module for asymmetric models & much more. Sparse + Dense = \ud83d\udd25 hybrid search performance! Details: 1\ufe0f\u20e3 Sparse Encoder Models Brand new support for sparse embedding models that generate high-dimensional embeddings (30,000+ dims) where <1% are non-zero: - Full SPLADE, Inference-free SPLADE, and CSR architecture support - 4 new modules, 12 new losses, 9 new evaluators - Integration with @ elastic-co , @ opensearch-project , @ NAVER LABS Europe, @ qdrant , @ IBM , etc. - Decode interpretable embeddings to understand token importance - Hybrid search integration to get the best of both worlds 2\ufe0f\u20e3 Enhanced Encode Methods & Multi-Processing - Introduce encode_query & encode_document automatically use predefined prompts - No more manual pool management - just pass device list directly to encode() - Much cleaner and easier to use than the old multi-process approach...", "url": "https://huggingface.co/posts/tomaarsen/190568030432786", "date_published": "2025-07-04T13:34:21.559286"}, {"id": "https://huggingface.co/posts/AdinaY/923543336919000", "image": "", "title": "\ud83d\udd25 June highlights from China\u2019s open source ecosystem.", "content_text": "\ud83d\udd25 June highlights from China\u2019s open source ecosystem. zh-ai-community/june-2025-open-works-from-the-chinese-community-683d66c188f782dc5570ba15 \u2728Baidu & MiniMax both launched open foundation models - Baidu: Ernie 4.5 ( from 0.3B -424B ) \ud83e\udd2f - MiniMax: MiniMax -M1 ( Hybrid MoE reasoning model ) \u2728Multimodal AI is moving from fusion to full-stack reasoning: unified Any-to-Any pipelines across text, vision, audio, and 3D - Baidu: ERNIE-4.5-VL-424B - Moonshot AI: Kimi-VL-A3B - Alibaba: Ovis-U1 - BAAI: Video-XL-2/OmniGen2 - AntGroup: Ming-Lite-Omni - Chinese Academy of Science: Stream-Omni - Bytedance: SeedVR2-3B - Tencent: Hunyuan 3D 2.1/ SongGeneration - FishAudio: Openaudio-s1-mini \u2728Domain specific models are rapidly emerging - Alibaba DAMO: Lingshu-7B (medical MLLM) - BAAI: RoboBrain (Robotics) \u2728 So many small models! - OpenBMB: MiciCPM4 ( on device ) - Qwen: Embedding/Reranker (0.6B) - Alibaba: Ovis-U1-3B - Moonshot AI: Kimi-VL-A3B - Bytedance: SeedVR2-3B See translation", "url": "https://huggingface.co/posts/AdinaY/923543336919000", "date_published": "2025-07-04T13:34:21.559721"}, {"id": "https://huggingface.co/posts/Jaward/639375924369190", "image": "", "title": "I played around with the new RXTX paper (XX^T) and was able to train nanogpt with 4x4 RXTX matmuls in both attention layer and optimizer\ud83e\udd15", "content_text": "I played around with the new RXTX paper (XX^T) and was able to train nanogpt with 4x4 RXTX matmuls in both attention layer and optimizer\ud83e\udd15 It just works (well I had to add some guardrails) but still saves 5% of memory usage: The Patch: - Computes attention scores with a 4x4 blockwise RXTX matmuls (no pytorch dot prod) - Handles arbitrary sequence lengths by padding to the nearest multiple of 4. - An RXTX variant of shampoo with params reshaped into 4x4 blocks during each optimizer step. - Uses 5% less ops Code: https://github.com/Jaykef/ai-algorithms/blob/main/nanogpt-rxtx.ipynb Paper: https://arxiv.org/pdf/2505.09814 See translation", "url": "https://huggingface.co/posts/Jaward/639375924369190", "date_published": "2025-07-04T13:34:21.560069"}, {"id": "https://huggingface.co/posts/kanaria007/563668515609459", "image": "", "title": "\u2705 New Article on Hugging Face: Teaching AI to Learn from Its Own Thinking", "content_text": "\u2705 New Article on Hugging Face: Teaching AI to Learn from Its Own Thinking Title: \ud83d\udd01 Understanding the Pattern Learning Bridge: Adaptive Learning from Problem-Solving Experience \ud83d\udd17 Read it here: https://huggingface.co/blog/kanaria007/understanding-the-pattern-learning-bridge Summary: After exploring structural selfhood in the Identity-Construct Protocol, this new piece introduces a next step in cognitive development: learning from one\u2019s own problem-solving patterns. The Pattern Learning Bridge equips AI with the ability to reflect structurally \u2014 not just on results, but on *why* certain reasoning paths succeed or fail. This protocol enables agents to: \u2022 Log reasoning attempts in a structured format \u2022 Analyze success/failure correlations across problem types \u2022 Extract reusable frame-jump patterns with confidence scoring \u2022 Proactively adapt future reasoning choices It\u2019s not just about having memory \u2014 it\u2019s about having *experience*. Key Features: \u2022 Detects recurring reasoning traps \u2022...", "url": "https://huggingface.co/posts/kanaria007/563668515609459", "date_published": "2025-07-04T13:34:21.560677"}, {"id": "https://huggingface.co/posts/aiqtech/840192921912390", "image": "", "title": "\ud83d\udd25 HuggingFace Heatmap Leaderboard", "content_text": "\ud83d\udd25 HuggingFace Heatmap Leaderboard Visualizing AI ecosystem activity at a glance aiqtech/Heatmap-Leaderboard \ud83c\udfaf Introduction A leaderboard that visualizes the vibrant HuggingFace community activity through heatmaps. \u2728 Key Features \ud83d\udcca Real-time Tracking - Model/dataset/app releases from AI labs and developers \ud83c\udfc6 Auto Ranking - Rankings based on activity over the past year \ud83c\udfa8 Responsive UI - Unique colors per organization, mobile optimized \u26a1 Auto Updates - Hourly data refresh for latest information \ud83c\udf0d Major Participants Big Tech: OpenAI, Google, Meta, Microsoft, Apple, NVIDIA AI Startups: Anthropic, Mistral, Stability AI, Cohere, DeepSeek Chinese Companies: Tencent, Baidu, ByteDance, Qwen HuggingFace Official: HuggingFaceH4, HuggingFaceM4, lerobot, etc. Active Developers: prithivMLmods, lllyasviel, multimodalart and many more \ud83d\ude80 Value Trend Analysis \ud83d\udcc8 Real-time open source contribution insights Inspiration \ud83d\udcaa Learn from other developers' activity patterns Ecosystem Growth \ud83c\udf31 Visualize AI...", "url": "https://huggingface.co/posts/aiqtech/840192921912390", "date_published": "2025-07-04T13:34:21.561451"}, {"id": "https://huggingface.co/posts/prithivMLmods/861488499348917", "image": "", "title": "The bunch of comparable demos for Multimodal VLMs (excels in OCR, cinematography understanding, spatial reasoning, etc.) now up on the Hub \ud83e\udd17 \u2014 max recent till Jun'25.", "content_text": "The bunch of comparable demos for Multimodal VLMs (excels in OCR, cinematography understanding, spatial reasoning, etc.) now up on the Hub \ud83e\udd17 \u2014 max recent till Jun'25. \u2726 Demo Spaces \u2014 > [Nanonets-OCR-s, MonkeyOCR, Typhoon-OCR-7B, SmolDocling] : prithivMLmods/Multimodal-OCR2 > [GLM-4.1v, docscopeOCR-7B, MonkeyOCR, coreOCR-7B] : prithivMLmods/core-OCR > [Camel-Doc-OCR, ViLaSR-7B, OCRFlux-3B, ShotVL-7B] : prithivMLmods/Doc-VLMs-v2-Localization > [SkyCaptioner-V1, SpaceThinker-3B, coreOCR-7B, SpaceOm-3B] : prithivMLmods/VisionScope-R2 > [RolmOCR-7B, Qwen2-VL-OCR-2B, Aya-Vision-8B, Nanonets-OCR-s] : prithivMLmods/Multimodal-OCR > [DREX-062225-7B, Typhoon-OCR-3B, olmOCR-7B-0225, VIREX-062225-7B] : prithivMLmods/Doc-VLMs-OCR > [Cosmos-Reason1-7B, docscopeOCR-7B, Captioner-7B, visionOCR-3B] : prithivMLmods/DocScope-R1 \u2726 Space Collection : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See...", "url": "https://huggingface.co/posts/prithivMLmods/861488499348917", "date_published": "2025-07-04T13:34:21.561873"}, {"id": "https://huggingface.co/posts/merve/119393817203316", "image": "", "title": "visual reasoning is now in  transformers \ud83d\udd25", "content_text": "visual reasoning is now in transformers \ud83d\udd25 THUDM/GLM-4.1V-9B-Thinking is just released and merged into transformers, we gave it a vibe test run \ud83e\udd20 it's very good, comes with 64k context length and MIT license \ud83d\ude0d it supports 4k image tokens and any aspect ratio as well! Notebook: http://colab.research.google.com/drive/1atODIiV57hOZLv16Bjzwd6fwx0yoTorj?usp=sharing Demo: THUDM/GLM-4.1V-9B-Thinking-Demo See translation", "url": "https://huggingface.co/posts/merve/119393817203316", "date_published": "2025-07-04T13:34:21.562166"}, {"id": "https://huggingface.co/posts/Abhaykoul/404767027882987", "image": "", "title": "\ud83c\udf89 Dhanishtha 2.0 Preview is Now Open Source!", "content_text": "\ud83c\udf89 Dhanishtha 2.0 Preview is Now Open Source! The world's first Intermediate Thinking Model is now available to everyone! Dhanishtha 2.0 Preview brings revolutionary intermediate thinking capabilities to the open-source community. Unlike traditional reasoning models that think once, Dhanishtha can think, answer, rethink, answer again, and continue rethinking as needed using multiple blocks between responses. \ud83d\ude80 Key Features - Intermediate thinking: Think \u2192 Answer \u2192 Rethink \u2192 Answer \u2192 Rethink if needed... - Token efficient: Uses up to 79% fewer tokens than DeepSeek R1 on similar queries - Transparent thinking: See the model's reasoning process in real-time - Open source: Freely available for research and development HelpingAI/Dhanishtha-2.0-preview https://helpingai.co/chat See translation", "url": "https://huggingface.co/posts/Abhaykoul/404767027882987", "date_published": "2025-07-04T13:34:21.562523"}]}