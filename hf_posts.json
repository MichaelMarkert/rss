{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/daavoo/629869039060445", "image": "", "title": "2025: The Year of Agents.", "content_text": "2025: The Year of Agents. 2026: The Year of Local Agents? Relying on cloud-hosted LLMs is often overkill. While frontier models still lead in complex coding, local models are now more than capable of handling many agentic workflows\u2014with zero latency and total privacy. To help bridge the gap between local inference and usable agents, I\u2019m releasing agent.cpp: https://github.com/mozilla-ai/agent.cpp It provides minimal, high-performance building blocks for agents in C++, built directly around the awesome llama.cpp ecosystem. Stop sending your data to a remote API. Start building and running agents on your own hardware. See translation", "url": "https://huggingface.co/posts/daavoo/629869039060445", "date_published": "2025-12-22T09:33:54.875720"}, {"id": "https://huggingface.co/posts/John1604/712509372180068", "image": "", "title": "\u6211\u5373\u5c06\u8fbe\u5230\u516c\u5171\u5b58\u50a8\u7a7a\u95f4\u4e0a\u9650\u3002\u6211\u53d1\u73b0\u6211\u7684\u4ed3\u5e93 John1604/Kimi-K2-Thinking-q6K-gguf \u6ca1\u6709\u83b7\u5f97\u8db3\u591f\u7684\u4e0b\u8f7d\u91cf\uff0c\u51e0\u4e4e\u5360\u7528\u4e86 1T \u5b58\u50a8\u7a7a\u95f4\u3002\u5c3d\u7ba1\u6211\u559c\u7231 Kimi K2 \u7684\u601d\u8003\u65b9\u5f0f\uff0c\u4f46\u53ef\u80fd\u4e0d\u5f97\u4e0d\u5220\u9664\u8fd9\u4e2a\u6a21\u578b\u3002\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u771f\u6b63\u7684\u5f00\u6e90 1T LLM\uff0c\u4e0e\u4efb\u4f55\u524d\u6cbf\u7684 LLM \u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u5728 AI \u7ade\u4e89\u4e2d\uff0c\u7f8e\u56fd\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1axAI,  OpenAI, \u8c37\u6b4c\u548cAnthropologie\u3002\u4e2d\u56fd\u4e5f\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1a\u963f\u91cc\u5df4\u5df4, Kimi, DeepSeek\u548cGLM\u3002\u76ee\u524d\u53cc\u65b9\u52bf\u5747\u529b\u654c\u3002", "content_text": "\u6211\u5373\u5c06\u8fbe\u5230\u516c\u5171\u5b58\u50a8\u7a7a\u95f4\u4e0a\u9650\u3002\u6211\u53d1\u73b0\u6211\u7684\u4ed3\u5e93 John1604/Kimi-K2-Thinking-q6K-gguf \u6ca1\u6709\u83b7\u5f97\u8db3\u591f\u7684\u4e0b\u8f7d\u91cf\uff0c\u51e0\u4e4e\u5360\u7528\u4e86 1T \u5b58\u50a8\u7a7a\u95f4\u3002\u5c3d\u7ba1\u6211\u559c\u7231 Kimi K2 \u7684\u601d\u8003\u65b9\u5f0f\uff0c\u4f46\u53ef\u80fd\u4e0d\u5f97\u4e0d\u5220\u9664\u8fd9\u4e2a\u6a21\u578b\u3002\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u771f\u6b63\u7684\u5f00\u6e90 1T LLM\uff0c\u4e0e\u4efb\u4f55\u524d\u6cbf\u7684 LLM \u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u5728 AI \u7ade\u4e89\u4e2d\uff0c\u7f8e\u56fd\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1axAI, OpenAI, \u8c37\u6b4c\u548cAnthropologie\u3002\u4e2d\u56fd\u4e5f\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1a\u963f\u91cc\u5df4\u5df4, Kimi, DeepSeek\u548cGLM\u3002\u76ee\u524d\u53cc\u65b9\u52bf\u5747\u529b\u654c\u3002 I'm about to reach my public storage limit. I've discovered that my repository John1604/Kimi-K2-Thinking-q6K-gguf isn't getting enough downloads and is nearly consuming 1TB of storage. While I love Kimi K2's way of thinking, I have to delete this model because it's a true open-source 1TB LLM, comparable to any cutting-edge LLM model. In the AI \u200b\u200brace, four US companies have 1TB+ models: xAI, OpenAI, Google, and Anthropic. China also has four companies with 1TB+ models: Alibaba, Kimi, DeepSeek, and GLM. Currently, the two sides are evenly matched. Only American team and Chinese team have LLM with 1T+ parameters. Let's cheer for them to reach AGI in next 5 to 10 years. Maybe a 64T chinese model will do it -- Human and cat brain...", "url": "https://huggingface.co/posts/John1604/712509372180068", "date_published": "2025-12-22T09:33:54.876181"}, {"id": "https://huggingface.co/posts/DawnC/213826857024975", "image": "", "title": "PawMatchAI \u2014 Smarter, Safer, and More Thoughtful Recommendations \ud83d\udc15\u2728", "content_text": "PawMatchAI \u2014 Smarter, Safer, and More Thoughtful Recommendations \ud83d\udc15\u2728 \ud83d\udc3e Recommendation system update \u2014 deeper reasoning, safer decisions Over the past weeks, user feedback led me to rethink how PawMatchAI handles description-based breed recommendations. Instead of only matching surface-level preferences, the system now implements a multi-dimensional semantic reasoning architecture that emphasizes real-life compatibility and risk awareness. Key technical improvements: - SBERT-powered semantic understanding with dynamic weight allocation across six constraint dimensions (space, activity, noise, grooming, experience, family) - Hierarchical constraint management distinguishing critical safety constraints from flexible preferences, with progressive relaxation when needed -Multi-head scoring system combining semantic matching (15%), lifestyle compatibility (70%), constraint adherence (10%), and confidence calibration (5%) -Intelligent risk filtering that applies graduated penalties (-10% to...", "url": "https://huggingface.co/posts/DawnC/213826857024975", "date_published": "2025-12-22T09:33:54.876781"}, {"id": "https://huggingface.co/posts/prithivMLmods/761836377624422", "image": "", "title": "Introducing demos for new SOTA models from AI2: SAGE-MM (Smart Any-Horizon Agents for Long-Video Reasoning) and Molmo-2, an open vision-language model that supports multi-image (QA and pointing) and video (QA, pointing, and tracking). The respective demo-related collections are listed below. \ud83c\udf83\ud83d\udd25", "content_text": "Introducing demos for new SOTA models from AI2: SAGE-MM (Smart Any-Horizon Agents for Long-Video Reasoning) and Molmo-2, an open vision-language model that supports multi-image (QA and pointing) and video (QA, pointing, and tracking). The respective demo-related collections are listed below. \ud83c\udf83\ud83d\udd25 \u2728 SAGE-MM [Video-Reasoning]: prithivMLmods/SAGE-MM-Video-Reasoning \u2728 Molmo2 [Demo]: prithivMLmods/Molmo2-HF-Demo \ud83c\udf83 GitHub[SAGE-MM]: https://github.com/PRITHIVSAKTHIUR/SAGE-MM-Video-Reasoning \ud83c\udf83 GitHub[Molmo2]: https://github.com/PRITHIVSAKTHIUR/Molmo2-HF-Demo \ud83c\udf83 Multimodal Implementations: https://huggingface.co/collections/prithivMLmods/multimodal-implementations To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/761836377624422", "date_published": "2025-12-22T09:33:54.877147"}, {"id": "https://huggingface.co/posts/prithivMLmods/787095126804028", "image": "", "title": "Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... \ud83e\udd17\ud83d\udd25", "content_text": "Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... \ud83e\udd17\ud83d\udd25 \u2728 TRELLIS.2-Text-to-3D [Demo]: prithivMLmods/TRELLIS.2-Text-to-3D \u2728 Multimodal Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations \u2728 Github: https://github.com/PRITHIVSAKTHIUR/TRELLIS.2-Text-to-3D To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/787095126804028", "date_published": "2025-12-22T09:33:54.877494"}, {"id": "https://huggingface.co/posts/Kseniase/746586334382009", "image": "", "title": "From Prompt Engineering to Context Engineering: Main Design Patterns", "content_text": "From Prompt Engineering to Context Engineering: Main Design Patterns Earlier on, we relied on clever prompt wording, but now structured, complete context matters more than just magic phrasing. The next year is going to be a year of context engineering which expands beyond prompt engineering. The two complement each other: prompt engineering shapes how we ask, while context engineering shapes what the model knows, sees, and can do. To keep things clear, here are the main techniques and design patterns in both areas, with some useful resources for further exploration: \u25aa\ufe0f 9 Prompt Engineering Techniques (configuring input text) 1. Zero-shot prompting \u2013 giving a single instruction without examples. Relies entirely on pretrained knowledge. 2. Few-shot prompting \u2013 adding input\u2013output examples to encourage model to show the desired behavior. \u27f6 https://arxiv.org/abs/2005.14165 3. Role prompting \u2013 assigning a persona or role (e.g. \"You are a senior researcher,\" \"Say it as a specialist in...", "url": "https://huggingface.co/posts/Kseniase/746586334382009", "date_published": "2025-12-22T09:33:54.878118"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/210519935872315", "image": "", "title": "Wan 2.2 Complete Training Tutorial - Text to Image, Text to Video, Image to Video, Windows & Cloud :", "content_text": "Wan 2.2 Complete Training Tutorial - Text to Image, Text to Video, Image to Video, Windows & Cloud : https://youtu.be/ocEkhAsPOs4 Wan 2.2 training is now so easy. I have done over 64 different unique Wan 2.2 trainings to prepare the very best working training configurations for you. The configurations are fully working locally with as low as 6 GB GPUs. So you will be able to train your awesome Wan 2.2 image or video generation LoRAs on your Windows computer with easiness. Moreover, I have shown how to train on cloud platforms RunPod and Massed Compute so even if you have no GPU or you want faster training, you can train on cloud for very cheap prices fully privately. Full step by step tutorial : https://youtu.be/ocEkhAsPOs4 \u23f1\ufe0f Video Chapters: 0:00 Introduction to Wan 2.2 Training & Capabilities 0:56 Installing & Updating Musubi Tuner Locally 2:20 Explanation of Optimized Presets & Research Logic 4:00 Differences Between T2I, T2V, and I2V Configs 5:36 Extracting Files & Running...", "url": "https://huggingface.co/posts/MonsterMMORPG/210519935872315", "date_published": "2025-12-22T09:33:54.878743"}, {"id": "https://huggingface.co/posts/projectlosangeles/440962027896970", "image": "", "title": "\ud83d\udd25Check out Project Los Angeles new SOTA searchable MIDI dataset! \ud83d\udd25", "content_text": "\ud83d\udd25Check out Project Los Angeles new SOTA searchable MIDI dataset! \ud83d\udd25 projectlosangeles/Discover-MIDI-Dataset The dataset features over 6.74M+ unique searchable MIDIs and is tailored for MIDI music discovery and symbolic music AI! If you like the dataset, please\u2764\ufe0f Sincerely, Alex Project Los Angeles Tegridy Code 2025 See translation", "url": "https://huggingface.co/posts/projectlosangeles/440962027896970", "date_published": "2025-12-22T09:33:54.879008"}, {"id": "https://huggingface.co/posts/danielhanchen/264398594064230", "image": "", "title": "Google releases FunctionGemma, a new 270M parameter model that runs on just 0.5 GB RAM.\u2728", "content_text": "Google releases FunctionGemma, a new 270M parameter model that runs on just 0.5 GB RAM.\u2728 Built for tool-calling, run locally on your phone at 50+ tokens/s, or fine-tune with Unsloth & deploy to your phone. GGUF: unsloth/functiongemma-270m-it-GGUF Docs + Notebook: https://docs.unsloth.ai/models/functiongemma See translation", "url": "https://huggingface.co/posts/danielhanchen/264398594064230", "date_published": "2025-12-22T09:33:54.879344"}, {"id": "https://huggingface.co/posts/YatharthS/190514854652270", "image": "", "title": "\ud83e\udd2f \ud83e\udd2f Released a high quality finetuned LLM based TTS model that can generate realistic and clear 48khz audio at over 100x realtime speed! \ud83e\udd2f \ud83e\udd2f", "content_text": "\ud83e\udd2f \ud83e\udd2f Released a high quality finetuned LLM based TTS model that can generate realistic and clear 48khz audio at over 100x realtime speed! \ud83e\udd2f \ud83e\udd2f Github link: https://github.com/ysharma3501/MiraTTS Model link: https://github.com/ysharma3501/MiraTTS Blog explaining llm tts models: https://huggingface.co/blog/YatharthS/llm-tts-models See translation", "url": "https://huggingface.co/posts/YatharthS/190514854652270", "date_published": "2025-12-22T09:33:54.879599"}]}