{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/ProCreations/321100188234240", "image": "", "title": "Eyyyy 50 followers \ud83e\udd2f", "content_text": "Eyyyy 50 followers \ud83e\udd2f See translation", "url": "https://huggingface.co/posts/ProCreations/321100188234240", "date_published": "2025-05-29T13:33:51.317286"}, {"id": "https://huggingface.co/posts/fdaudens/719212082746895", "image": "", "title": "Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity!", "content_text": "Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity! The final project uses the GAIA dataset - your agent has to solve tasks like analyzing Excel files, processing audio recordings, answering questions about YouTube videos, and diving into research papers. This isn't toy examples, it's the messy, multimodal stuff agents need to handle in practice. Whether you\u2019re just getting started with agents or want to go deeper with tools like LangChain, LlamaIndex, and SmolAgents, this course has tons of useful stuff. A few key insights: - Code agents are incredibly versatile once you get the architecture right - The sweet spot is finding the right balance of guidance vs autonomy for each use case - Once the logic clicks, the possibilities really are endless - it's like letting LLMs break free from the chatbox The course is free and the certification deadline is July 1st, 2025. The Hugging Face...", "url": "https://huggingface.co/posts/fdaudens/719212082746895", "date_published": "2025-05-29T13:33:51.317790"}, {"id": "https://huggingface.co/posts/DawnC/538322807718464", "image": "", "title": "VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration", "content_text": "VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration I'm excited to share significant improvements to VisionScout that substantially enhance accuracy and analytical capabilities. \u2b50\ufe0f Key Enhancements - CLIP Zero-Shot Landmark Detection: The system now identifies famous landmarks and architectural features without requiring specific training data, expanding scene understanding beyond generic object detection. - Places365 Environmental Classification: Integration of MIT's Places365 model provides robust scene baseline classification across 365 categories, significantly improving lighting analysis accuracy and overall scene identification precision. - Enhanced Multi-Modal Fusion: Advanced algorithms now dynamically combine insights from YOLOv8, CLIP, and Places365 to optimize accuracy across diverse scenarios. - Refined LLM Narratives: Llama 3.2 integration continues to transform analytical data into fluent, contextually rich descriptions while maintaining...", "url": "https://huggingface.co/posts/DawnC/538322807718464", "date_published": "2025-05-29T13:33:51.318312"}, {"id": "https://huggingface.co/posts/dhruv3006/675063918098240", "image": "", "title": "Cua : Docker for computer-use agents", "content_text": "Cua : Docker for computer-use agents Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers. Github : https://github.com/trycua See translation", "url": "https://huggingface.co/posts/dhruv3006/675063918098240", "date_published": "2025-05-29T13:33:51.318545"}, {"id": "https://huggingface.co/posts/BestWishYsh/693532821570217", "image": "", "title": "Introducing our new work: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\u200b\u200b \ud83d\ude80", "content_text": "Introducing our new work: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\u200b\u200b \ud83d\ude80 We tackle the core challenges of \u200b\u200bSubject-to-Video Generation (S2V)\u200b\u200b by systematically building the first complete infrastructure\u2014featuring an evaluation benchmark and a million-scale dataset! \u2728 \ud83e\udde0 Introducing \u200b\u200bOpenS2V-Eval\u200b\u200b\u2014the first \u200b\u200bfine-grained S2V benchmark\u200b\u200b, with \u200b\u200b180 multi-domain prompts + real/synthetic test pairs\u200b\u200b. We propose \u200b\u200bNexusScore\u200b\u200b, \u200b\u200bNaturalScore\u200b\u200b, and \u200b\u200bGmeScore\u200b\u200b to precisely quantify model performance across \u200b\u200bsubject consistency, naturalness, and text alignment\u200b\u200b \u2714 \ud83d\udcca Using this framework, we conduct a \u200b\u200bcomprehensive evaluation of 16 leading S2V models\u200b\u200b, revealing their strengths/weaknesses in complex scenarios! \ud83d\udd25 \u200b\u200bOpenS2V-5M dataset\u200b\u200b now available! A \u200b\u200b5.4M 720P HD\u200b\u200b collection of \u200b\u200bsubject-text-video triplets\u200b\u200b, enabled by \u200b\u200bcross-video association segmentation + multi-view synthesis\u200b\u200b for \u200b\u200bdiverse subjects & high-quality...", "url": "https://huggingface.co/posts/BestWishYsh/693532821570217", "date_published": "2025-05-29T13:33:51.319060"}, {"id": "https://huggingface.co/posts/lukmanaj/495766537273785", "image": "", "title": "I am so happy  to share to all that I\u2019ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I\u2019m doing my best to keep up. Excited for what\u2019s ahead!", "content_text": "I am so happy to share to all that I\u2019ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I\u2019m doing my best to keep up. Excited for what\u2019s ahead! See translation", "url": "https://huggingface.co/posts/lukmanaj/495766537273785", "date_published": "2025-05-29T13:33:51.319313"}, {"id": "https://huggingface.co/posts/hesamation/260011784391977", "image": "", "title": "I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book.", "content_text": "I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book. It gives an overview, then goes into detail for each stage, even providing best practices. It\u2019s 115 pages on arxiv, definitely worth a read. Check it out: https://arxiv.org/abs/2408.13296 See translation", "url": "https://huggingface.co/posts/hesamation/260011784391977", "date_published": "2025-05-29T13:33:51.319561"}, {"id": "https://huggingface.co/posts/AdinaY/668826641388828", "image": "", "title": "HunyuanPortrait \ud83d\udd25 video model by Tencent Hunyuan team.", "content_text": "HunyuanPortrait \ud83d\udd25 video model by Tencent Hunyuan team. HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation (2503.18860) tencent/HunyuanPortrait \u2728Portrait animation from just one image + a video prompt \u2728Diffusion-based, implicit motion control \u2728Superior temporal consistency & detail See translation", "url": "https://huggingface.co/posts/AdinaY/668826641388828", "date_published": "2025-05-29T13:33:51.319815"}, {"id": "https://huggingface.co/posts/clem/541152505631903", "image": "", "title": "It's just become easier to share your apps on the biggest AI app store (aka HF spaces) for unlimited storage, more visibility and community interactions.", "content_text": "It's just become easier to share your apps on the biggest AI app store (aka HF spaces) for unlimited storage, more visibility and community interactions. Just pick a React, Svelte, or Vue template when you create your space or add app_build_command: npm run build in your README's YAML and app_file: build/index.html in your README's YAML block. Or follow this link: https://huggingface.co/new-space?sdk=static Let's build! See translation", "url": "https://huggingface.co/posts/clem/541152505631903", "date_published": "2025-05-29T13:33:51.320063"}, {"id": "https://huggingface.co/posts/fdaudens/323840314242853", "image": "", "title": "\ud83c\udfb5 Dream come true for content creators! TIGER AI can extract voice, effects & music from ANY audio file \ud83e\udd2f", "content_text": "\ud83c\udfb5 Dream come true for content creators! TIGER AI can extract voice, effects & music from ANY audio file \ud83e\udd2f This lightweight model uses frequency band-split technology to separate speech like magic. Kudos to @ fffiloni for the amazing demo! fffiloni/TIGER-audio-extraction See translation", "url": "https://huggingface.co/posts/fdaudens/323840314242853", "date_published": "2025-05-29T13:33:51.320316"}]}