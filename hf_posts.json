{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Jofthomas/993866418471203", "image": "", "title": "The new Mistral 3 models are here !", "content_text": "The new Mistral 3 models are here ! Today, we announce Mistral 3, the next generation of Mistral models. Mistral 3 includes three state-of-the-art small, dense models (14B, 8B, and 3B) and Mistral Large 3 \u2013 our most capable model to date \u2013 a sparse mixture-of-experts trained with 41B active and 675B total parameters. All models are released under the Apache 2.0 license. Ministrals : https://huggingface.co/collections/mistralai/ministral-3 Mistral Large 3: https://huggingface.co/collections/mistralai/mistral-large-3 See translation", "url": "https://huggingface.co/posts/Jofthomas/993866418471203", "date_published": "2025-12-04T17:26:42.670816"}, {"id": "https://huggingface.co/posts/hesamation/869653062191419", "image": "", "title": "this is big... 50 AI researchers from Bytedance, Alibaba, Tencent, and other labs/universities just published a 300-page paper with surprising lessons about coding models and agents (data, pre and post-training, etc).", "content_text": "this is big... 50 AI researchers from Bytedance, Alibaba, Tencent, and other labs/universities just published a 300-page paper with surprising lessons about coding models and agents (data, pre and post-training, etc). key highlights: > small LLMs can beat proprietary giants RL (RLVR specifically) gives small open-source models an edge over big models in reasoning. a 14B model trained with RLVR on high-quality verified problems can match the performance of OpenAI's o3. > models have a hard time learning Python. mixing language models during pre-training is good, but Python behaves different from statically typed languages. languages with similar syntax (Java and C#, or JavaScript and TypeScript) creates high positive synergy. mixing Python heavily into the training of statically typed languages can actually hurt because of Python's dynamic typing. > not all languages are equal (coding scaling laws) the amount of data required to specialize a model on a language drastically depends on...", "url": "https://huggingface.co/posts/hesamation/869653062191419", "date_published": "2025-12-04T17:26:42.671413"}, {"id": "https://huggingface.co/posts/danielhanchen/849127033892624", "image": "", "title": "Mistral's new Ministral 3 models can now be Run & Fine-tuned locally! (16GB RAM)", "content_text": "Mistral's new Ministral 3 models can now be Run & Fine-tuned locally! (16GB RAM) Ministral 3 have vision support and the best-in-class performance for their sizes. 14B Instruct GGUF: unsloth/Ministral-3-14B-Instruct-2512-GGUF 14B Reasoning GGUF: unsloth/Ministral-3-14B-Reasoning-2512-GGUF \ud83d\udc31 Step-by-step Guide: https://docs.unsloth.ai/new/ministral-3 All GGUFs, BnB, FP8 etc. variants uploads: https://huggingface.co/collections/unsloth/ministral-3 See translation", "url": "https://huggingface.co/posts/danielhanchen/849127033892624", "date_published": "2025-12-04T17:26:42.671741"}, {"id": "https://huggingface.co/posts/sergiopaniego/920736659206393", "image": "", "title": "ICYMI, transformers v5 is out!", "content_text": "ICYMI, transformers v5 is out! Grab a coffee \u2615 and go read the announcement blog https://huggingface.co/blog/transformers-v5 See translation", "url": "https://huggingface.co/posts/sergiopaniego/920736659206393", "date_published": "2025-12-04T17:26:42.671963"}, {"id": "https://huggingface.co/posts/Juanxi/977874890039450", "image": "", "title": "ScalingOpt | Welcome to join and co-build the Optimization Community!", "content_text": "ScalingOpt | Welcome to join and co-build the Optimization Community! ScalingOpt is a professional platform focusing on optimization for large-scale deep learning, aiming to advocate for \"Optimization at Scale,\" which means verifiable and scalable optimization algorithms. This community platform is dedicated to gathering, discovering, comparing, and contributing various cutting-edge optimizers and optimization algorithms. It's not just a simple Awesome List, it also includes: Visualizations: Covers visualization scripts for the Rosenbrock Function and the Rastrigin Function for users to freely explore. Benchmark: We recommend Algoperf as the primary source, along with other verifiable benchmarks and analysis articles, for users to reference the best optimizer. Papers & Blogs Recommendation: The platform summarizes high-quality papers and blogs from recent years, and continuously adds the latest papers based on daily arXiv updates, currently totaling nearly a hundred articles....", "url": "https://huggingface.co/posts/Juanxi/977874890039450", "date_published": "2025-12-04T17:26:42.672558"}, {"id": "https://huggingface.co/posts/melvindave/497232003536172", "image": "", "title": "Deployed my first Space!", "content_text": "Deployed my first Space! Moved my PDF to Images Converter app from streamlit cloud to Spaces Upload a PDF and get a zip file of pages as PNGs or JPEGs, perfect for posts or decks Hope it's useful! melvindave/pdf-to-images See translation", "url": "https://huggingface.co/posts/melvindave/497232003536172", "date_published": "2025-12-04T17:26:42.672805"}, {"id": "https://huggingface.co/posts/branikita/887548171409943", "image": "", "title": "We've published a comprehensive evaluation of the Feetech STS3250 servo actuator.", "content_text": "We've published a comprehensive evaluation of the Feetech STS3250 servo actuator. Key Findings: - Speed: 77.6 RPM (exceeds spec by 3.2%) - Backlash: 0.43\u00b0 (within 0.5\u00b0 limit) - Repeatability: \u00b10.02mm at 95mm radius - Peak torque: 48 kg\u00b7cm - Sustained torque: ~25 kg\u00b7cm after thermal protection Full review: https://robonine.com/feetech-sts3250-smart-actuator-evaluation-of-accuracy-torque-and-backlash/ See translation", "url": "https://huggingface.co/posts/branikita/887548171409943", "date_published": "2025-12-04T17:26:42.673102"}, {"id": "https://huggingface.co/posts/sergiopaniego/946135410159058", "image": "", "title": "NEW:", "content_text": "NEW: @ mistralai released a fantastic family of multimodal models, Ministral 3. You can fine-tune them for free on Colab using TRL \u26a1\ufe0f, supporting both SFT and GRPO Link to the notebooks: - SFT: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_ministral3_vl.ipynb - GRPO: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_ministral3_vl.ipynb - TRL and more examples: https://huggingface.co/docs/trl/index See translation", "url": "https://huggingface.co/posts/sergiopaniego/946135410159058", "date_published": "2025-12-04T17:26:42.673419"}, {"id": "https://huggingface.co/posts/sequelbox/532546402634546", "image": "", "title": "NEW RELEASE: Esper 3.1 for Ministral 3 14b, 8b, and 3b!", "content_text": "NEW RELEASE: Esper 3.1 for Ministral 3 14b, 8b, and 3b! - Esper is our full-stack, full-cycle coding, DevOps, and architecture specialist! - Our newest, best DeepSeek technical datasets emphasize more challenging queries and tough real-world coding tasks across a variety of programming languages and development paradigms: - Titanium 3 for coding and reasoning in DevOps and architecture: sequelbox/Titanium3-DeepSeek-V3.1-Terminus - Tachibana 3 for high-difficulty code production in a variety of topics and programming languages: - sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus - sequelbox/Tachibana3-Part2-DeepSeek-V3.2 - Mitakihara for MLOps, AI building, use, expertise, and research: sequelbox/Mitakihara-DeepSeek-R1-0528 Get Esper 3.1 now in all 3 Ministral 3 sizes! (We recommend 14b for general use.) 14b: ValiantLabs/Ministral-3-14B-Reasoning-2512-Esper3.1 8b: ValiantLabs/Ministral-3-8B-Reasoning-2512-Esper3.1 3b: ValiantLabs/Ministral-3-3B-Reasoning-2512-Esper3.1 We'll be...", "url": "https://huggingface.co/posts/sequelbox/532546402634546", "date_published": "2025-12-04T17:26:42.673853"}, {"id": "https://huggingface.co/posts/codelion/532332047008536", "image": "", "title": "I just published Ellora - 6 production-ready LoRA recipes for enhancing LLMs with specific capabilities. Each recipe costs under $100 to run and includes complete training code, data generation, and evaluation.", "content_text": "I just published Ellora - 6 production-ready LoRA recipes for enhancing LLMs with specific capabilities. Each recipe costs under $100 to run and includes complete training code, data generation, and evaluation. The 6 Recipes: Recipe 1: Accuracy Recovery - Recover 75% of quantization losses with self-distillation Recipe 2: Reasoning LoRA - Add structured thinking with GRPO (0% to 60% adoption, 75% quality boost) Recipe 3: Tool Calling - Real execution on actual codebases Recipe 4: Context Extension - Scale from 32K to 2M tokens (61x increase) Recipe 5: Secure Code Generation - 97% vulnerability reduction using automated Semgrep analysis Recipe 6: Execution-Aware World Models - Teaching models runtime behavior Why Recipes? Ellora provides methodologies, not frameworks. Use them with your existing tools (PEFT, LoRAX, vLLM, Unsloth, HuggingFace). Each recipe uses self-supervised data generation (Magpie approach) - no expensive human labeling required. All recipes include Jupyter...", "url": "https://huggingface.co/posts/codelion/532332047008536", "date_published": "2025-12-04T17:26:42.674255"}]}