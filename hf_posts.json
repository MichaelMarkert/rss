{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Wauplin/921235032674409", "image": "", "title": "Say hello to", "content_text": "Say hello to hf : a faster, friendlier Hugging Face CLI \u2728 We are glad to announce a long-awaited quality-of-life improvement: the Hugging Face CLI has been officially renamed from huggingface-cli to hf! So... why this change? Typing huggingface-cli constantly gets old fast. More importantly, the CLI\u2019s command structure became messy as new features were added over time (upload, download, cache management, repo management, etc.). Renaming the CLI is a chance to reorganize commands into a clearer, more consistent format. We decided not to reinvent the wheel and instead follow a well-known CLI pattern: hf <resource> <action>. Isn't hf auth login easier to type and remember? The full rationale, implementation details, and migration notes are in the blog post: https://huggingface.co/blog/hf-cli See translation", "url": "https://huggingface.co/posts/Wauplin/921235032674409", "date_published": "2025-07-27T17:22:37.490941"}, {"id": "https://huggingface.co/posts/nroggendorff/692994583869726", "image": "", "title": "Is it possible to apply for a resources grant for a whole organization, or do you need to apply for each repo individually? I think it'd be pretty cool to have something like the discord-community org for None-yet in terms of resource allocation (multiple spaces running on", "content_text": "Is it possible to apply for a resources grant for a whole organization, or do you need to apply for each repo individually? I think it'd be pretty cool to have something like the discord-community org for None-yet in terms of resource allocation (multiple spaces running on cpu upgrade . I realize the scale of the community is just a tiny bit different, and that having this for a public org (one where anyone can join) isn't super fiscally responsible, but we'll be good. I promise we will! Right, guys? See translation", "url": "https://huggingface.co/posts/nroggendorff/692994583869726", "date_published": "2025-07-27T17:22:37.491251"}, {"id": "https://huggingface.co/posts/prithivMLmods/432897219160306", "image": "", "title": "Excited to introduce the new experimental model \"Qwen2.5-VL-7B-Abliterated-Caption-it\", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.\ud83e\uddea\ud83e\udd17", "content_text": "Excited to introduce the new experimental model \"Qwen2.5-VL-7B-Abliterated-Caption-it\", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.\ud83e\uddea\ud83e\udd17 \u2728 Try the demo here : prithivMLmods/Qwen2.5-VL \u2728 Qwen2.5-VL-7B-Abliterated-Caption-it : prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it \u2728 Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 \u2728 Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See translation", "url": "https://huggingface.co/posts/prithivMLmods/432897219160306", "date_published": "2025-07-27T17:22:37.491646"}, {"id": "https://huggingface.co/posts/Blazgo/275361717193984", "image": "", "title": "\ud83d\ude80 Deca 3 Ultra Alpha is coming in the next 72 hours! \ud83d\ude80", "content_text": "\ud83d\ude80 Deca 3 Ultra Alpha is coming in the next 72 hours! \ud83d\ude80 We're on the verge of something monumental. Right now, we're in the final stages of testing, and we're about to drop a game-changing milestone in the open-source AI community. \ud83c\udf89 In just two weeks, we've managed to almost 4x the size of the largest open-source LLM at that time (and we are still 2.6x bigger than the largest LLM). This is unprecedented and a testament to the power of collaboration, innovation, and the relentless pursuit of pushing AI to its limits. The future of open-source AI is now. Stay tuned for the release \u2013 we\u2019re just getting started. - Model testing finishes: 24hrs from now - Model gets uploaded: 30hrs from now - Related code/inference stack gets published: 70-90hrs from now See translation", "url": "https://huggingface.co/posts/Blazgo/275361717193984", "date_published": "2025-07-27T17:22:37.492044"}, {"id": "https://huggingface.co/posts/FlameF0X/804120995868939", "image": "", "title": "The development of SnowflakeCore-G1-7B-MoE. I can't say when it would be publish yet because it's big and it requires a lot of computational power.", "content_text": "The development of SnowflakeCore-G1-7B-MoE. I can't say when it would be publish yet because it's big and it requires a lot of computational power. See translation", "url": "https://huggingface.co/posts/FlameF0X/804120995868939", "date_published": "2025-07-27T17:22:37.492258"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/822376760397275", "image": "", "title": "NEW ARTICLE: \"Detecting Beyond Sight: Building AI-Enabled SAR Intelligence with Synthetic Data\"", "content_text": "NEW ARTICLE: \"Detecting Beyond Sight: Building AI-Enabled SAR Intelligence with Synthetic Data\" Synthetic Aperture Radar (SAR) reveals what optical sensors can\u2019t. AI can turn that information into actionable intelligence\u2014but only with the right training data. In our latest blog, we explore how Falcon\u2019s new virtual SAR sensor solves the SAR data bottleneck for AI development. As the newest addition to Falcon\u2019s sensor library, it models radar returns with precision\u2014including azimuth, range resolution, signal intensity, and noise. This Falcon-specific, GPU-accelerated raytraced SAR model is exposed via Falcon\u2019s Python API, giving teams precise, control over radar wave propagation and enabling physically grounded, highly customizable, and user-friendly SAR simulation. The result? High-fidelity, automatically labeled synthetic SAR imagery from any scenario\u2014on demand. No custom setup. No external workflows. Just mission-ready data for building AI models across defense, disaster response,...", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/822376760397275", "date_published": "2025-07-27T17:22:37.492701"}, {"id": "https://huggingface.co/posts/AdinaY/243707766122533", "image": "", "title": "Big respect to the Qwen team! They just dropped another model\ud83d\udd25", "content_text": "Big respect to the Qwen team! They just dropped another model\ud83d\udd25 Qwen3-235B-A22B-Thinking-2507 \ud83e\udde0 new reasoning model by Qwen Qwen/Qwen3-235B-A22B-Thinking-2507 \u2728 235B total / 22B active (8 experts) \u2728 256K context window \u2728 Agent-ready with tool use & <think> reasoning mode Hope the team gets some well-deserved rest this weekend after all the massive releases \ud83d\ude4c See translation", "url": "https://huggingface.co/posts/AdinaY/243707766122533", "date_published": "2025-07-27T17:22:37.492987"}, {"id": "https://huggingface.co/posts/Xenova/793837995432659", "image": "", "title": "Introducing Voxtral WebGPU: State-of-the-art audio transcription directly in your browser! \ud83e\udd2f", "content_text": "Introducing Voxtral WebGPU: State-of-the-art audio transcription directly in your browser! \ud83e\udd2f \ud83d\udde3\ufe0f Transcribe videos, meeting notes, songs and more \ud83d\udd10 Runs on-device, meaning no data is sent to a server \ud83c\udf0e Multilingual (8 languages) \ud83e\udd17 Completely free (forever) & open source That's right, we're running Mistral's new Voxtral-Mini-3B model 100% locally in-browser on WebGPU, powered by Transformers.js and ONNX Runtime Web! \ud83d\udd25 Try it out yourself! \ud83d\udc47 webml-community/Voxtral-WebGPU See translation", "url": "https://huggingface.co/posts/Xenova/793837995432659", "date_published": "2025-07-27T17:22:37.493298"}, {"id": "https://huggingface.co/posts/nicolay-r/579222924328271", "image": "", "title": "\ud83d\udce2 For those who planning to start a PhD or research in the UK \ud83c\uddec\ud83c\udde7 (including AI field in particular) but facing ATAS (Academic Technology Approval Scheme) issues.", "content_text": "\ud83d\udce2 For those who planning to start a PhD or research in the UK \ud83c\uddec\ud83c\udde7 (including AI field in particular) but facing ATAS (Academic Technology Approval Scheme) issues. Excited to share the ultimate guide for dealing with ATAS refusals and how to write effective rebuttal letters. \ud83c\udfac https://youtu.be/bfknM3n-SHs \ud83d\udd0d From the video you will find: 1. Why appealing an ATAS decision matters even if your visa is approved 2. Which docments to use in understanding the principles behind sponsorship decisions 3. Key tips for proper rebuttal letter structuring See translation", "url": "https://huggingface.co/posts/nicolay-r/579222924328271", "date_published": "2025-07-27T17:22:37.493609"}, {"id": "https://huggingface.co/posts/codelion/260744865592486", "image": "", "title": "Implemented Test-Time Diffusion Deep Researcher (TTD-DR) in OptiLLM! \ud83d\ude80", "content_text": "Implemented Test-Time Diffusion Deep Researcher (TTD-DR) in OptiLLM! \ud83d\ude80 Just shipped a game-changing feature that turns any LLM into a powerful research agent. TTD-DR applies diffusion-inspired techniques to iteratively refine research reports while grounding them in real web sources. How it works: \u2022 Generates initial draft \u2022 Identifies knowledge gaps \u2022 Searches web for missing info \u2022 Iteratively refines through \"denoising\" steps \u2022 Produces comprehensive reports with 15-30+ sources The magic? It works with ANY model so you can choose your favorite open-source models on HF! Key results: - 47 complex research queries tested - Every report backed by real web sources - Quality rivals human research analysts - No more hallucinations on current events! Try it: pip install optillm Then use \"deep_research-your-model-name\" as the model identifier - Implementation: https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research - Paper: https://arxiv.org/abs/2507.16075v1 - Sample...", "url": "https://huggingface.co/posts/codelion/260744865592486", "date_published": "2025-07-27T17:22:37.494087"}]}