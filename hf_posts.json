{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/openfree/538970335354687", "image": "", "title": "\u2728 DreamO Video: From Customized Images to Videos \u2728", "content_text": "\u2728 DreamO Video: From Customized Images to Videos \u2728 Hello, AI creators! Today I'm introducing a truly special project. DreamO Video is an integrated framework that generates customized images based on reference images and transforms them into videos with natural movement. \ud83c\udfac\u2728 openfree/DreamO-video \ud83d\udd0d Key Features Image Reference (IP): Maintain object appearance while applying to new backgrounds and situations ID Preservation: Retain facial features across various environments Style Transfer: Apply unique styles from reference images to other content \ud83c\udf9e\ufe0f Video Generation: Create natural 2-second videos from generated images \ud83d\udca1 How to Use Upload Reference Images: One or two images (people, objects, landscapes, etc.) Select Task Type: Choose between IP (Image Preservation), ID (Face Feature Retention), or Style Enter Prompt: Describe your desired result (e.g., \"a woman playing guitar on a cloud\") Click Generate Image: \u2728 Create customized AI images! Generate Video: Click the \ud83c\udfac button on the...", "url": "https://huggingface.co/posts/openfree/538970335354687", "date_published": "2025-05-14T09:25:42.126606"}, {"id": "https://huggingface.co/posts/merve/544378273517703", "image": "", "title": "VLMS 2025 UPDATE \ud83d\udd25", "content_text": "VLMS 2025 UPDATE \ud83d\udd25 We just shipped a blog on everything latest on vision language models, including \ud83e\udd16 GUI agents, agentic VLMs, omni models \ud83d\udcd1 multimodal RAG \u23ef\ufe0f video LMs \ud83e\udd0f\ud83c\udffb smol models ..and more! https://huggingface.co/blog/vlms-2025 See translation", "url": "https://huggingface.co/posts/merve/544378273517703", "date_published": "2025-05-14T09:25:42.126882"}, {"id": "https://huggingface.co/posts/ArturoNereu/644085701737970", "image": "", "title": "I\u2019ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things.", "content_text": "I\u2019ve been learning AI for several years (coming from the games industry), and along the way, I curated a list of the tools, courses, books, papers, and models that actually helped me understand things. I turned this into a GitHub repo: https://github.com/ArturoNereu/AI-Study-Group If you\u2019re just getting started, I recommend: \ud83d\udcd8 Deep Learning \u2013 A Visual Approach: https://www.glassner.com/portfolio/deep-learning-a-visual-approach \ud83c\udfa5 Dive into LLMs with Andrej Karpathy: https://youtu.be/7xTGNNLPyMI?si=aUTq_qUzyUx36BsT \ud83e\udde0 The \ud83e\udd17 Agents course]( https://huggingface.co/learn/agents-course/ The repo has grown with help from the community (Reddit, Discord, etc.) and I\u2019ll keep updating it. If you have any favorite resources, I\u2019d love to include them. See translation", "url": "https://huggingface.co/posts/ArturoNereu/644085701737970", "date_published": "2025-05-14T09:25:42.127257"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/869555651580897", "image": "", "title": "Transfer Any Clothing Into A New Person & Turn Any Person Into A 3D Figure - ComfyUI Tutorial", "content_text": "Transfer Any Clothing Into A New Person & Turn Any Person Into A 3D Figure - ComfyUI Tutorial ComfyUI is super hard to use but I have literally prepared 1-click way to install and use 2 amazing workflows. First workflow is generating a person wearing any clothing. The second workflow is turning any person image into a 3D toy like figure image. Tutorial Link : https://youtu.be/ZzYnhKeaJBs Video Chapters 0:00:00 Intro: Two One-Click ComfyUI Workflows (Clothing Gen & 3D Figure) 0:00:34 Effort & Goal: Easy Installation & Use of Complex Workflows 0:00:49 Setup Part 1: ComfyUI Prerequisite & Downloading Project Zip File 0:01:06 Setup Part 2: Extracting Zip into ComfyUI Folder (WinRAR 'Extract Here' Tip) 0:01:18 Setup Part 3: Running update_comfyui.bat for Latest ComfyUI Version 0:01:37 Setup Part 4: Running install_clothing_and_3D.bat (Installs Nodes & Requirements) 0:02:03 Model Downloads: Intro to Swarm UI Auto-Installer & Automatic Updater 0:02:28 Using Swarm UI: Launching...", "url": "https://huggingface.co/posts/MonsterMMORPG/869555651580897", "date_published": "2025-05-14T09:25:42.127737"}, {"id": "https://huggingface.co/posts/ginipick/766230066345476", "image": "", "title": "# \ud83c\udf1f 3D Model to Video: Easy GLB Conversion Tool \ud83c\udf1f", "content_text": "# \ud83c\udf1f 3D Model to Video: Easy GLB Conversion Tool \ud83c\udf1f demo link: ginigen/3D-VIDEO Hello there! Would you like to transform your 3D models into stunning animations? This space can help you! \u2728 ## \ud83d\udd0d What Can It Do? This tool converts your uploaded GLB model into: 1. \ud83c\udfae A transformed GLB file 2. \ud83c\udfac An animated GIF preview 3. \ud83d\udccb A metadata JSON file ## \u2705 Key Features * \ud83d\udda5\ufe0f Works in headless server environments (EGL + pyglet-headless \u2192 pyrender fallback) * \ud83d\udd0d Objects in GIFs appear 3x larger (global scale \u00d73) * \ud83c\udfa8 Clean interface with pastel background ## \ud83c\udfae Animation Types * \ud83d\udd04 Rotate - Object rotates around the Y-axis * \u2b06\ufe0f Float - Object moves smoothly up and down * \ud83d\udca5 Explode - Object moves sideways * \ud83e\udde9 Assemble - Object returns to its original position * \ud83d\udc93 Pulse - Object changes in size * \ud83d\udd04 Swing - Object swings around the Z-axis ## \ud83d\udee0\ufe0f How to Use 1. Upload your GLB model \ud83d\udce4 2. Select your desired animation type \ud83c\udfac 3. Adjust the duration and FPS \u23f1\ufe0f 4. Click the \"Generate Animation\" button \u25b6\ufe0f 5....", "url": "https://huggingface.co/posts/ginipick/766230066345476", "date_published": "2025-05-14T09:25:42.128299"}, {"id": "https://huggingface.co/posts/AdinaY/746841015963520", "image": "", "title": "Matrix Game \ud83c\udfae an interactive foundation model for controllable game world generation, released by Skywork AI.", "content_text": "Matrix Game \ud83c\udfae an interactive foundation model for controllable game world generation, released by Skywork AI. Skywork/Matrix-Game \u2728 17B with MIT licensed \u2728 Diffusion-based image-to-world video generation via keyboard & mouse input \u2728 GameWorld Score benchmark for Minecraft world models \u2728 Massive Matrix Game Dataset with fine-grained action labels See translation", "url": "https://huggingface.co/posts/AdinaY/746841015963520", "date_published": "2025-05-14T09:25:42.128576"}, {"id": "https://huggingface.co/posts/dhruv3006/692657412350660", "image": "", "title": "The era of local Computer Use AI Agents is here.", "content_text": "The era of local Computer Use AI Agents is here. Meet UI-TARS-1.5-7B-6bit, now running natively on Apple Silicon via MLX. The video is of UI-TARS-1.5-7B-6bit completing the prompt \"draw a line from the red circle to the green circle, then open reddit in a new tab\" running entirely on MacBook. The video is just a replay, during actual usage it took between 15s to 50s per turn with 720p screenshots (on avg its ~30s per turn), this was also with many apps open so it had to fight for memory at times. Built using c/ua : https://github.com/trycua/cua Join us making them here: https://discord.gg/4fuebBsAUj Kudos to the MLX community here on huggingface : mlx-community See translation", "url": "https://huggingface.co/posts/dhruv3006/692657412350660", "date_published": "2025-05-14T09:25:42.128886"}, {"id": "https://huggingface.co/posts/VirtualOasis/965866013655862", "image": "", "title": "Automatic Multi-Modal Research Agent", "content_text": "Automatic Multi-Modal Research Agent I am thinking of building an Automatic Research Agent that can boost creativity! Input: Topics or data sources Processing: Automated deep research Output: multimodal results (such as reports, videos, audio, diagrams) & multi-platform publishing. There is a three-stage process In the initial Stage, output for text-based content in markdown format allows for user review before transformation into various other formats, such as PDF or HTML. The second stage transforms the output into other modalities, like audio, video, diagrams, and translations into different languages. The final stage focuses on publishing multi-modal content across multiple platforms like X, GitHub, Hugging Face, YouTube, and podcasts, etc. See translation", "url": "https://huggingface.co/posts/VirtualOasis/965866013655862", "date_published": "2025-05-14T09:25:42.129193"}, {"id": "https://huggingface.co/posts/hesamation/756119536681094", "image": "", "title": "this book actually exists for free, \u201cthe little book of deep learning\u201d. best to refresh your mind about DL basics:", "content_text": "this book actually exists for free, \u201cthe little book of deep learning\u201d. best to refresh your mind about DL basics: > foundations of machine learning > how models train > common layers (dropout, pooling\u2026) > basic intro to LLMs actually optimized for mobile. Book: https://fleuret.org/public/lbdl.pdf See translation", "url": "https://huggingface.co/posts/hesamation/756119536681094", "date_published": "2025-05-14T09:25:42.129485"}, {"id": "https://huggingface.co/posts/DawnC/683112818630492", "image": "", "title": "\ud83d\ude80 VisionScout Now Speaks More Like Me \u2014 Thanks to LLMs!", "content_text": "\ud83d\ude80 VisionScout Now Speaks More Like Me \u2014 Thanks to LLMs! I'm thrilled to share a major update to VisionScout, my end-to-end vision system. Beyond robust object detection (YOLOv8) and semantic context (CLIP), VisionScout now features a powerful LLM-based scene narrator (Llama 3.2), improving the clarity, accuracy, and fluidity of scene understanding. This isn\u2019t about replacing the pipeline , it\u2019s about giving it a better voice. \u2728 \u2b50\ufe0f What the LLM Brings Fluent, Natural Descriptions: The LLM transforms structured outputs into human-readable narratives. Smarter Contextual Flow: It weaves lighting, objects, zones, and insights into a unified story. Grounded Expression: Carefully prompt-engineered to stay factual \u2014 it enhances, not hallucinates. Helpful Discrepancy Handling: When YOLO and CLIP diverge, the LLM adds clarity through reasoning. VisionScout Still Includes: \ud83d\uddbc\ufe0f YOLOv8-based detection (Nano / Medium / XLarge) \ud83d\udcca Real-time stats & confidence insights \ud83e\udde0 Scene understanding via...", "url": "https://huggingface.co/posts/DawnC/683112818630492", "date_published": "2025-05-14T09:25:42.130021"}]}