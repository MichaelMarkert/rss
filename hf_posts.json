{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Jaward/982484477481896", "image": "", "title": "made a few improvements on custom grpo trainer:", "content_text": "made a few improvements on custom grpo trainer: - added sequence similarity reward (seems to work) - improved vllm support (5x inference speed) - adjusted reward scores (this helped with format/accuracy) - can now push to hf hub (already pushed mine lol: Jaward/smollm2_360m_grpo_gsm8k_reasoner ) Code: https://github.com/Jaykef/ai-algorithms/blob/main/smollm2_360M_135M_grpo_gsm8k.ipynb See translation", "url": "https://huggingface.co/posts/Jaward/982484477481896", "date_published": "2025-03-02T17:16:54.924445"}, {"id": "https://huggingface.co/posts/prithivMLmods/305640045790864", "image": "", "title": "Dropping some of the custom fine-tunes based on SigLIP2,", "content_text": "Dropping some of the custom fine-tunes based on SigLIP2, with a single-label classification problem type! \ud83c\udf00\ud83e\udde4 - AI vs Deepfake vs Real : prithivMLmods/AI-vs-Deepfake-vs-Real-Siglip2 - Deepfake Detect : prithivMLmods/Deepfake-Detect-Siglip2 - Fire Detection : prithivMLmods/Fire-Detection-Siglip2 - Deepfake Quality Assess : prithivMLmods/Deepfake-Quality-Assess-Siglip2 - Guard Against Unsafe Content : prithivMLmods/Guard-Against-Unsafe-Content-Siglip2 \ud83c\udf20Collection : prithivMLmods/siglip2-custom-67bcdb2de8fe96b99fb4e19e See translation", "url": "https://huggingface.co/posts/prithivMLmods/305640045790864", "date_published": "2025-03-02T17:16:54.924808"}, {"id": "https://huggingface.co/posts/lingvanex-mt/205421793754165", "image": "", "title": "Dear HF Community!", "content_text": "Dear HF Community! Our company open-sourced machine translation models for 12 rare languages under MIT license. You can use them freely with OpenNMT translation framework. Each model is about 110 mb and has an excellent performance, ( about 40000 characters / s on Nvidia RTX 3090 ) Download models there https://huggingface.co/lingvanex You can test translation quality there: https://lingvanex.com/translate/ See translation", "url": "https://huggingface.co/posts/lingvanex-mt/205421793754165", "date_published": "2025-03-02T17:16:54.925090"}, {"id": "https://huggingface.co/posts/mkurman/255930071052172", "image": "", "title": "Introducing a new architecture, MedIT One \u2013 a single-token transformer with LSTM-like recurrence.", "content_text": "Introducing a new architecture, MedIT One \u2013 a single-token transformer with LSTM-like recurrence. It is extremely fast in training and inference, but we lack funding for large-scale training. Enjoy \ud83c\udf53 https://github.com/MedITSolutionsKurman/medit-one See translation", "url": "https://huggingface.co/posts/mkurman/255930071052172", "date_published": "2025-03-02T17:16:54.925379"}, {"id": "https://huggingface.co/posts/Bils/313728910739163", "image": "", "title": "create amazing audio ads in just a few steps", "content_text": "create amazing audio ads in just a few steps Bils/AIPromoStudio See translation", "url": "https://huggingface.co/posts/Bils/313728910739163", "date_published": "2025-03-02T17:16:54.925596"}, {"id": "https://huggingface.co/posts/davanstrien/119757489156561", "image": "", "title": "\ud83d\udcca Introducing \"Hugging Face Dataset Spotlight\" \ud83d\udcca", "content_text": "\ud83d\udcca Introducing \"Hugging Face Dataset Spotlight\" \ud83d\udcca I'm excited to share the first episode of our AI-generated podcast series focusing on nice datasets from the Hugging Face Hub! This first episode explores mathematical reasoning datasets: - SynthLabsAI/Big-Math-RL-Verified : Over 250,000 rigorously verified problems spanning multiple difficulty levels and mathematical domains - open-r1/OpenR1-Math-220k : 220,000 math problems with multiple reasoning traces, verified for accuracy using Math Verify and Llama-3.3-70B models. - facebook/natural_reasoning : 1.1 million general reasoning questions carefully deduplicated and decontaminated from existing benchmarks, showing superior scaling effects when training models like Llama3.1-8B-Instruct. Plus a bonus segment on bespokelabs/bespoke-manim ! https://www.youtube.com/watch?v=-TgmRq45tW4 See translation", "url": "https://huggingface.co/posts/davanstrien/119757489156561", "date_published": "2025-03-02T17:16:54.926000"}, {"id": "https://huggingface.co/posts/Kseniase/433849056207490", "image": "", "title": "9 types of \"Chain-of-...\" approaches:", "content_text": "9 types of \"Chain-of-...\" approaches: Chain-of-Thought (CoT) prompting enhances reasoning in AI models by breaking down complex problems into step-by-step logical sequences. It continues proving its effectiveness, especially in top-performing reasoning models. However, there are other similar methods, that expand CoT and can be used for different purposes. Here are 9 of them: 1. Chain-of-Action-Thought (COAT) -> Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search (2502.02508) Helps model decide when to keep thinking, double-check their work, or try a different approach, using special guiding tokens. 2. Chain of Draft (CoD) -> Chain of Draft: Thinking Faster by Writing Less (2502.18600) It helps model generate short but meaningful reasoning steps, cutting costs and making processing faster 3. Chain-of-Agents -> Chain of Agents: Large Language Models Collaborating on Long-Context Tasks (2406.02818) Uses multi-agent...", "url": "https://huggingface.co/posts/Kseniase/433849056207490", "date_published": "2025-03-02T17:16:54.926562"}, {"id": "https://huggingface.co/posts/nroggendorff/209736816535732", "image": "", "title": "We're using RLHF on diffusion models, right? Just making sure..", "content_text": "We're using RLHF on diffusion models, right? Just making sure.. See translation", "url": "https://huggingface.co/posts/nroggendorff/209736816535732", "date_published": "2025-03-02T17:16:54.926782"}, {"id": "https://huggingface.co/posts/nyuuzyou/326966093381385", "image": "", "title": "\ud83d\udcda Historical Russian Technical Journal Images Dataset -", "content_text": "\ud83d\udcda Historical Russian Technical Journal Images Dataset - nyuuzyou/journals \u0421ollection of digitized pages from vintage Russian technical journals featuring: - 7.47k high-quality images - Machine-generated descriptions in Russian - Valuable historical technical content for image-to-text applications Content descriptions are dedicated to the public domain under the CC0 1.0 license, allowing unrestricted use without attribution. See translation", "url": "https://huggingface.co/posts/nyuuzyou/326966093381385", "date_published": "2025-03-02T17:16:54.927085"}, {"id": "https://huggingface.co/posts/fdaudens/113970261627099", "image": "", "title": "What if AI becomes as ubiquitous as the internet, but runs locally and transparently on our devices?", "content_text": "What if AI becomes as ubiquitous as the internet, but runs locally and transparently on our devices? Fascinating TED talk by @ thomwolf on open source AI and its future impact. Imagine this for AI: instead of black box models running in distant data centers, we get transparent AI that runs locally on our phones and laptops, often without needing internet access. If the original team moves on? No problem - resilience is one of the beauties of open source. Anyone (companies, collectives, or individuals) can adapt and fix these models. This is a compelling vision of AI's future that solves many of today's concerns around AI transparency and centralized control. Watch the full talk here: https://www.ted.com/talks/thomas_wolf_what_if_ai_just_works See translation", "url": "https://huggingface.co/posts/fdaudens/113970261627099", "date_published": "2025-03-02T17:16:54.927426"}]}