{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/399941870774449", "image": "", "title": "Can AI models trained solely on 100% synthetic data achieve top-tier accuracy in real-world object detection?", "content_text": "Can AI models trained solely on 100% synthetic data achieve top-tier accuracy in real-world object detection? \ud83d\udc49 Sergio Sanz, PhD just proved it while winning Duality AI\u2019s Synthetic-to-Real Object Detection Challenge using Falcon-generated imagery. His model achieved perfect real-world detection accuracy without a single real image in the training loop. In this blog, Dr. Sanz walks us through his method, which includes the design and training of an advanced pipeline to achieve 100% detection accuracy. His full technical breakdown covers: \ud83d\udccd Synthetic-only training \ud83d\udccd Data augmentation with an ensemble learning approach for better generalization \ud83d\udccd Custom occlusion generation \ud83d\udccd A Faster R-CNN model fine-tuned with Falcon generated data \ud83d\udccd And much more! The results speak for themselves! \ud83d\udcd6 Read the blog here: https://www.duality.ai/blog/leveraging-synthetic-data-for-real-world-object-detection Congratulations Sergio! We can't wait to see what you do next. \ud83d\udd14 Ready to take on the next...", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/399941870774449", "date_published": "2025-06-15T17:19:41.743497"}, {"id": "https://huggingface.co/posts/a-r-r-o-w/709852031491261", "image": "", "title": "New diffusion model for text-to-image and video-to-world generation: Cosmos Predict-2 \ud83d\udc7d", "content_text": "New diffusion model for text-to-image and video-to-world generation: Cosmos Predict-2 \ud83d\udc7d Model collection: nvidia/cosmos-predict2-68028efc052239369a0f2959 Diffusers support: https://github.com/huggingface/diffusers/pull/11695 Documentation: https://huggingface.co/docs/diffusers/main/en/api/pipelines/cosmos These are results with the 2B param model. Imagine what you could do with the 14B version! Go check it out now! See translation", "url": "https://huggingface.co/posts/a-r-r-o-w/709852031491261", "date_published": "2025-06-15T17:19:41.743798"}, {"id": "https://huggingface.co/posts/seawolf2357/480409853177984", "image": "", "title": "\u26a1 FusionX Enhanced Wan 2.1 I2V (14B) \ud83c\udfac", "content_text": "\u26a1 FusionX Enhanced Wan 2.1 I2V (14B) \ud83c\udfac \ud83d\ude80 Revolutionary Image-to-Video Generation Model Generate cinematic-quality videos in just 8 steps! Heartsync/WAN2-1-fast-T2V-FusioniX \u2728 Key Features \ud83c\udfaf Ultra-Fast Generation: Premium quality in just 8-10 steps \ud83c\udfac Cinematic Quality: Smooth motion with detailed textures \ud83d\udd25 FusionX Technology: Enhanced with CausVid + MPS Rewards LoRA \ud83d\udcd0 Optimized Resolution: 576\u00d71024 default settings \u26a1 50% Speed Boost: Faster rendering compared to base models \ud83d\udee0\ufe0f Technical Stack Base Model: Wan2.1 I2V 14B Enhancement Technologies: \ud83d\udd17 CausVid LoRA (1.0 strength) - Motion modeling \ud83d\udd17 MPS Rewards LoRA (0.7 strength) - Detail optimization Scheduler: UniPC Multistep (flow_shift=8.0) Auto Prompt Enhancement: Automatic cinematic keyword injection \ud83c\udfa8 How to Use Upload Image - Select your starting image Enter Prompt - Describe desired motion and style Adjust Settings - 8 steps, 2-5 seconds recommended Generate - Complete in just minutes! \ud83d\udca1 Optimization Tips \u2705 Recommended Settings:...", "url": "https://huggingface.co/posts/seawolf2357/480409853177984", "date_published": "2025-06-15T17:19:41.744457"}, {"id": "https://huggingface.co/posts/ginipick/718905723783644", "image": "", "title": "\ud83c\udfac VEO3 Directors - All-in-One AI Video Creation Suite", "content_text": "\ud83c\udfac VEO3 Directors - All-in-One AI Video Creation Suite \ud83d\ude80 What is VEO3 Directors? VEO3 Directors is a revolutionary end-to-end AI video creation platform that transforms your ideas into cinematic reality. From story conception to final video with synchronized audio - all in one seamless workflow! \ud83d\udd17 Try It Now ginigen/VEO3-Directors ginigen/VEO3-Free ginigen/VEO3-Free-mirror \u2728 Key Features \ud83d\udcdd Story Seed Generator \ud83c\udfb2 Instantly generate creative story ideas across multiple genres \ud83c\udf0f Bilingual support (English/Korean) \ud83c\udfad Rich categories: Genre, Setting, Characters, and more \ud83c\udfa5 AI Script & Prompt Crafting \ud83d\udcac Powered by Friendli API for Hollywood-quality prompts \ud83e\udd16 AI Director writes detailed cinematography instructions \ud83c\udfac Professional elements: camera movements, lighting, VFX \ud83c\udfac Video + Audio Generation \ud83c\udfa8 Wan2.1-T2V-14B for stunning visual quality \u26a1 NAG 4-step inference - 10x faster generation \ud83c\udfb5 MMAudio auto-generates matching soundscapes \ud83c\udf9b\ufe0f Full control over resolution, duration, and style...", "url": "https://huggingface.co/posts/ginipick/718905723783644", "date_published": "2025-06-15T17:19:41.745043"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/453570470195139", "image": "", "title": "Beginner\u2019s Guide \u2014 Generate Videos With SwarmUI", "content_text": "Beginner\u2019s Guide \u2014 Generate Videos With SwarmUI Full article here please check out : https://huggingface.co/blog/MonsterMMORPG/beginners-guide-generate-videos-with-swarmui Proper ComfyUI backend then SwarmUI installation tutorial : https://youtu.be/fTzlQ0tjxj0 Proper ComfyUI backend then SwarmUI installation tutorial on RunPod : https://youtu.be/R02kPf9Y3_w See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/453570470195139", "date_published": "2025-06-15T17:19:41.745307"}, {"id": "https://huggingface.co/posts/FlameF0X/196232708808828", "image": "", "title": "I realised a small documentation on how to make your own LM architecture called [LM-From-Scratch](", "content_text": "I realised a small documentation on how to make your own LM architecture called [LM-From-Scratch]( https://github.com/FlameF0X/LM-From-Scratch ) See translation", "url": "https://huggingface.co/posts/FlameF0X/196232708808828", "date_published": "2025-06-15T17:19:41.745509"}, {"id": "https://huggingface.co/posts/codelion/378799954783125", "image": "", "title": "New Research: Theoretical Foundations for In-Context Learning in Transformers", "content_text": "New Research: Theoretical Foundations for In-Context Learning in Transformers I'm excited to share our latest theoretical work that formally proves an interesting property of large language models: base transformer models can approximate fine-tuned capabilities using only inference-time techniques like in-context learning. The core question we investigated: Can specialized behaviors typically acquired through expensive supervised fine-tuning be elicited from base models without any parameter updates? Our theoretical contribution: We provide a formal proof, grounded in the Turing completeness of transformers, showing that this is indeed possible under certain assumptions. The work establishes mathematical bounds on the minimal dataset sizes needed for approximation. Key theoretical results: - For text generation tasks: O(mV/\u03b5\u00b2) examples suffice (where m = number of contexts, V = vocabulary size, \u03b5 = error tolerance) - For linear classification: O(d/\u03b5) examples (where d = input...", "url": "https://huggingface.co/posts/codelion/378799954783125", "date_published": "2025-06-15T17:19:41.746047"}, {"id": "https://huggingface.co/posts/hesamation/842061188959684", "image": "", "title": "this repo is gold! a collection of LLM apps with multi-agents, MCP, RAG and so much more.", "content_text": "this repo is gold! a collection of LLM apps with multi-agents, MCP, RAG and so much more. the best way to learn is by building, and this repo provides the blueprint. Repo: https://github.com/Shubhamsaboo/awesome-llm-apps See translation", "url": "https://huggingface.co/posts/hesamation/842061188959684", "date_published": "2025-06-15T17:19:41.746269"}, {"id": "https://huggingface.co/posts/openfree/323475043338914", "image": "", "title": "\ud83c\udf0f Whisper-OCR Multilingual Translation Space \ud83d\ude80", "content_text": "\ud83c\udf0f Whisper-OCR Multilingual Translation Space \ud83d\ude80 Welcome! This Space takes English audio, video, images, and PDFs and instantly converts them into Chinese (ZH), Thai (TH), and Russian (RU)\u2014no other source language required. VIDraft/voice-trans \u2728 Key Features \ud83c\udfa4 Microphone \u2013 Record English speech \u2192 transcript + 3-language translation \ud83d\udd0a Audio File \u2013 Upload English audio \u2192 transcript + translation \ud83c\udfac Video File \u2013 Auto-extract audio with FFmpeg \u2192 transcript + translation \ud83d\uddbc\ufe0f Image \u2013 Nanonets-OCR pulls text \u2192 translation \ud83d\udcc4 PDF \u2013 Up to 50 pages of text & tables \u2192 translation \ud83d\udd04 Realtime Mode \u2013 Flush every 10-15 s; newest lines appear at the top \ud83d\udee0\ufe0f Quick Start Click \u201cDuplicate\u201d to fork, or launch directly. Pick a tab (\ud83c\udfa4/\ud83d\udd0a/\ud83c\udfac/\ud83d\uddbc\ufe0f/\ud83d\udcc4/\ud83d\udd04) and feed it English input. After a few seconds, see the \ud83d\udcdc original and \ud83c\udf10 3-language translation side by side. \u26a1 Tech Stack openai/whisper-large-v3-turbo \u2014 fast, high-accuracy ASR Nanonets-OCR-s (+ Flash Attention 2) \u2014 document/image OCR Gradio Blocks \u2014 clean tabbed UI...", "url": "https://huggingface.co/posts/openfree/323475043338914", "date_published": "2025-06-15T17:19:41.746776"}, {"id": "https://huggingface.co/posts/salma-remyx/831599737885559", "image": "", "title": "When multiple benchmarks yield conflicting model rankings, how do you know which model to trust?", "content_text": "When multiple benchmarks yield conflicting model rankings, how do you know which model to trust? In this substack, we explore that question in the context of spatial reasoning capabilities as seen from the perspective of 3 new benchmarks. Read more: https://remyxai.substack.com/p/benchmark-fusion See translation", "url": "https://huggingface.co/posts/salma-remyx/831599737885559", "date_published": "2025-06-15T17:19:41.747014"}]}