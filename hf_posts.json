{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/csabakecskemeti/277521964775652", "image": "", "title": "Christmas came early this year", "content_text": "Christmas came early this year See translation", "url": "https://huggingface.co/posts/csabakecskemeti/277521964775652", "date_published": "2025-10-20T13:32:44.046713"}, {"id": "https://huggingface.co/posts/ronantakizawa/523357349957866", "image": "", "title": "Introducing AWQ and GPTQ quantized versions of SmolVLM from Hugging Face!", "content_text": "Introducing AWQ and GPTQ quantized versions of SmolVLM from Hugging Face! These models only had their text models quantized, and had a 50% model size reduction (4GB~2GB) while keeping model degradation under 1% on the DocVQA benchmark. #huggingface #smolvlm #smollm ronantakizawa/SmolVLM-Instruct-awq ronantakizawa/SmolVLM-Instruct-gptq See translation", "url": "https://huggingface.co/posts/ronantakizawa/523357349957866", "date_published": "2025-10-20T13:32:44.046965"}, {"id": "https://huggingface.co/posts/appvoid/680866662633308", "image": "", "title": "today is going to be a great day for small models, are you ready?", "content_text": "today is going to be a great day for small models, are you ready? See translation", "url": "https://huggingface.co/posts/appvoid/680866662633308", "date_published": "2025-10-20T13:32:44.047154"}, {"id": "https://huggingface.co/posts/umarbutler/611899757397292", "image": "", "title": "I'm excited to announce the release of Kanon 2 Embedder, the world's best legal embedding model, ranked first on the Massive Legal Embedding Benchmark \ud83c\udf89", "content_text": "I'm excited to announce the release of Kanon 2 Embedder, the world's best legal embedding model, ranked first on the Massive Legal Embedding Benchmark \ud83c\udf89 This model is the product of quite literally months of painstaking work alongside @ abdurrahmanbutler collecting, cleaning, and processing terabytes of data as well as coming up with novel improvements to the standard embedder training recipe to push the limits of what's possible. Kanon 2 Embedder is my most advanced model to date. On MLEB, it benchmarks as 9% more accurate than OpenAI's best embedding model and 30% faster. Even when truncated from 1,792 to 768 dimensions, Kanon 2 Embedder continues to hold the number one spot on MLEB. Importantly, Kanon 2 Embedder is also privacy and security friendly \u2014 unlike Voyage, Cohere and Jina, none of your data is used to train our models by default. Kanon 2 Embedder can also be self-hosted for enterprises with heightened security or reliability requirements. You can read the full...", "url": "https://huggingface.co/posts/umarbutler/611899757397292", "date_published": "2025-10-20T13:32:44.047644"}, {"id": "https://huggingface.co/posts/Kseniase/152348317273822", "image": "", "title": "5 Lectures and keynotes defining AI right now", "content_text": "5 Lectures and keynotes defining AI right now If you want to understand the multifaceted AI landscape in 2025 and see where the field is heading \u2013 start with (or revisit) these legendary talks. They can help you capture what\u2019s happening in AI from multiple angles: 1. Andrej Karpathy: Software Is Changing (Again) \u2192 https://www.youtube.com/watch?v=LCEmiRjPEtQ Unveils Software 3.0 \u2013 a paradigm where LLMs are the new computers, programmed with prompts instead of code. The key: developers must now master coding, training, and prompting as AI becomes the heart of software building 2. Richard Sutton, The OaK Architecture: A Vision of SuperIntelligence from Experience \u2192 https://www.youtube.com/watch?v=gEbbGyNkR2U Unveils the OaK (Options and Knowledge) architecture \u2013 a model-based RL framework for continual intelligence, where every component learns, meta-learns & builds hierarchical abstractions 3. GTC March 2025 Keynote with NVIDIA CEO Jensen Huang \u2192...", "url": "https://huggingface.co/posts/Kseniase/152348317273822", "date_published": "2025-10-20T13:32:44.048279"}, {"id": "https://huggingface.co/posts/nroggendorff/141176924031866", "image": "", "title": "I love getting emails telling me when there's somebody else's active access token in one of my commit SHAs. HF should really only tell you if it is your token, otherwise I could just make a dataset with a bunch of random strings and wait for a valid token.", "content_text": "I love getting emails telling me when there's somebody else's active access token in one of my commit SHAs. HF should really only tell you if it is your token, otherwise I could just make a dataset with a bunch of random strings and wait for a valid token. permission,token write,hf_. .. finegrained,hf_. .. ... , ... ... Also, don't comment about how unlikely this is. I've gotten a warning email about a token I 'leaked' at least four times. In all cases, it has been in the digest hash. See translation", "url": "https://huggingface.co/posts/nroggendorff/141176924031866", "date_published": "2025-10-20T13:32:44.048572"}, {"id": "https://huggingface.co/posts/lamhieu/873520082917207", "image": "", "title": "\ud83d\ude80 Introducing the xLLMs Dataset Collection", "content_text": "\ud83d\ude80 Introducing the xLLMs Dataset Collection The xLLMs project is a growing suite of multilingual and multimodal dialogue datasets designed to train and evaluate advanced conversational LLMs. Each dataset focuses on a specific capability \u2014 from long-context reasoning and factual grounding to STEM explanations, math Q&A, and polite multilingual interaction. \ud83c\udf0d Explore the full collection on Hugging Face: \ud83d\udc49 lamhieu/xllms-66cdfe34307bb2edc8c6df7d \ud83d\udcac Highlight: xLLMs \u2013 Dialogue Pubs A large-scale multilingual dataset built from document-guided synthetic dialogues (Wikipedia, WikiHow, and technical sources). It\u2019s ideal for training models on long-context reasoning, multi-turn coherence, and tool-augmented dialogue across 9 languages. \ud83d\udc49 lamhieu/xllms_dialogue_pubs \ud83e\udde0 Designed for: - Long-context and reasoning models - Multilingual assistants - Tool-calling and structured response learning All datasets are open for research and development use \u2014 free, transparent, and carefully curated to...", "url": "https://huggingface.co/posts/lamhieu/873520082917207", "date_published": "2025-10-20T13:32:44.049013"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/558002389274434", "image": "", "title": "How to Install and Use ComfyUI and SwarmUI on Massed Compute and RunPod Private Cloud GPU Services :", "content_text": "How to Install and Use ComfyUI and SwarmUI on Massed Compute and RunPod Private Cloud GPU Services : https://youtu.be/bBxgtVD3ek4 https://youtu.be/bBxgtVD3ek4 If your GPU is not strong enough to run Generative AI models this is the tutorial that you need. Or you want to scale your generation speed by using multiple GPUs at the same time again this is excellent tutorial. In this tutorial I will show how to setup ComfyUI and SwarmUI literally 1-click on Massed Compute and RunPod and use your most liked best image and video generation models like Qwen, FLUX, Wan 2.2 and more. \ud83d\ude80 Unleash the full power of AI image and video generation on the cloud! This comprehensive tutorial is your step-by-step guide to installing and configuring SwarmUI and ComfyUI on two of the most popular cloud GPU platforms: Massed Compute and RunPod. Learn how to set up a powerful multi-GPU workflow to generate stunning, ultra-realistic images and videos at incredible speeds. We'll cover everything from deploying...", "url": "https://huggingface.co/posts/MonsterMMORPG/558002389274434", "date_published": "2025-10-20T13:32:44.049666"}, {"id": "https://huggingface.co/posts/AdinaY/411769057230600", "image": "", "title": "PaddleOCR VL\ud83d\udd25 0.9B Multilingual VLM by Baidu", "content_text": "PaddleOCR VL\ud83d\udd25 0.9B Multilingual VLM by Baidu PaddlePaddle/PaddleOCR-VL \u2728 Ultra-efficient NaViT + ERNIE-4.5 architecture \u2728 Supports 109 languages \ud83e\udd2f \u2728 Accurately recognizes text, tables, formulas & charts \u2728 Fast inference and lightweight for deployment See translation", "url": "https://huggingface.co/posts/AdinaY/411769057230600", "date_published": "2025-10-20T13:32:44.049916"}, {"id": "https://huggingface.co/posts/sergiopaniego/407632078462846", "image": "", "title": "New drop! \ud83d\udca5 The VLM Object Understanding Comparison Space now runs with Qwen3-VL-4B and moondream3.", "content_text": "New drop! \ud83d\udca5 The VLM Object Understanding Comparison Space now runs with Qwen3-VL-4B and moondream3. You can compare how models reason about images \ud83e\udde0 Bonus: thanks to @ ariG23498 , you now get auto-suggested prompts to explore faster. Let\u2019s gooo sergiopaniego/vlm_object_understanding See translation", "url": "https://huggingface.co/posts/sergiopaniego/407632078462846", "date_published": "2025-10-20T13:32:44.050168"}]}