{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/clem/522668354429256", "image": "", "title": "Today, we're unveiling two new open-source AI robots! HopeJR for $3,000 & Reachy Mini for $300 \ud83e\udd16\ud83e\udd16\ud83e\udd16", "content_text": "Today, we're unveiling two new open-source AI robots! HopeJR for $3,000 & Reachy Mini for $300 \ud83e\udd16\ud83e\udd16\ud83e\udd16 Let's go open-source AI robotics! See translation", "url": "https://huggingface.co/posts/clem/522668354429256", "date_published": "2025-05-30T13:33:47.530886"}, {"id": "https://huggingface.co/posts/AtAndDev/639250895656011", "image": "", "title": "deepseek-ai/DeepSeek-R1-0528", "content_text": "deepseek-ai/DeepSeek-R1-0528 This is the end See translation", "url": "https://huggingface.co/posts/AtAndDev/639250895656011", "date_published": "2025-05-30T13:33:47.531099"}, {"id": "https://huggingface.co/posts/DawnC/538322807718464", "image": "", "title": "VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration", "content_text": "VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration I'm excited to share significant improvements to VisionScout that substantially enhance accuracy and analytical capabilities. \u2b50\ufe0f Key Enhancements - CLIP Zero-Shot Landmark Detection: The system now identifies famous landmarks and architectural features without requiring specific training data, expanding scene understanding beyond generic object detection. - Places365 Environmental Classification: Integration of MIT's Places365 model provides robust scene baseline classification across 365 categories, significantly improving lighting analysis accuracy and overall scene identification precision. - Enhanced Multi-Modal Fusion: Advanced algorithms now dynamically combine insights from YOLOv8, CLIP, and Places365 to optimize accuracy across diverse scenarios. - Refined LLM Narratives: Llama 3.2 integration continues to transform analytical data into fluent, contextually rich descriptions while maintaining...", "url": "https://huggingface.co/posts/DawnC/538322807718464", "date_published": "2025-05-30T13:33:47.531630"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/279762403721253", "image": "", "title": "VEO 3 FLOW Full Tutorial - How To Use VEO3 in FLOW Guide :", "content_text": "VEO 3 FLOW Full Tutorial - How To Use VEO3 in FLOW Guide : https://youtu.be/AoEmQPU2gtg Tutorial link : https://youtu.be/AoEmQPU2gtg VEO 3 AI is rocking generative AI field right now. FLOW is the platform that lets you use VEO 3 with so many cool features. This is an official tutorial and guide made by Google team. I edited it slightly. I hope this be helpful. FLOW : https://labs.google/flow/about Veo 3 is Google DeepMind\u2019s most advanced video generation model to date. It allows users to create high-quality, cinematic video clips from simple text prompts, making it one of the most powerful AI tools for video creation. What sets Veo 3 apart is its ability to generate videos with native audio. This means that along with stunning visuals, Veo 3 can produce synchronized dialogue, ambient sounds, and background music\u2014all from a single prompt. For filmmakers, this is a significant leap forward, as it eliminates the need for separate audio generation or complex syncing processes. Veo 3...", "url": "https://huggingface.co/posts/MonsterMMORPG/279762403721253", "date_published": "2025-05-30T13:33:47.532238"}, {"id": "https://huggingface.co/posts/darkc0de/635443238112929", "image": "", "title": "\ud83e\udd17\ud83d\udc68\ud83c\udffb\u200d\ud83c\udf93", "content_text": "\ud83e\udd17\ud83d\udc68\ud83c\udffb\u200d\ud83c\udf93", "url": "https://huggingface.co/posts/darkc0de/635443238112929", "date_published": "2025-05-30T13:33:47.532418"}, {"id": "https://huggingface.co/posts/merve/470654136703534", "image": "", "title": "introducing: VLM vibe eval \ud83e\udead", "content_text": "introducing: VLM vibe eval \ud83e\udead visionLMsftw/VLMVibeEval vision LMs are saturated over benchmarks, so we built vibe eval \ud83d\udcac > compare different models with refreshed in-the-wild examples in different categories \ud83e\udd20 > submit your favorite model for eval no numbers -- just vibes! See translation", "url": "https://huggingface.co/posts/merve/470654136703534", "date_published": "2025-05-30T13:33:47.532670"}, {"id": "https://huggingface.co/posts/fdaudens/323840314242853", "image": "", "title": "\ud83c\udfb5 Dream come true for content creators! TIGER AI can extract voice, effects & music from ANY audio file \ud83e\udd2f", "content_text": "\ud83c\udfb5 Dream come true for content creators! TIGER AI can extract voice, effects & music from ANY audio file \ud83e\udd2f This lightweight model uses frequency band-split technology to separate speech like magic. Kudos to @ fffiloni for the amazing demo! fffiloni/TIGER-audio-extraction See translation", "url": "https://huggingface.co/posts/fdaudens/323840314242853", "date_published": "2025-05-30T13:33:47.532939"}, {"id": "https://huggingface.co/posts/hesamation/260011784391977", "image": "", "title": "I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book.", "content_text": "I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book. It gives an overview, then goes into detail for each stage, even providing best practices. It\u2019s 115 pages on arxiv, definitely worth a read. Check it out: https://arxiv.org/abs/2408.13296 See translation", "url": "https://huggingface.co/posts/hesamation/260011784391977", "date_published": "2025-05-30T13:33:47.533193"}, {"id": "https://huggingface.co/posts/jeffboudier/548880163054097", "image": "", "title": "\ud83d\udc4f Congrats", "content_text": "\ud83d\udc4f Congrats @ jinanz adding TimesFM times series forecasting to Transformers! Learn how to use TimesFM in this blog post by the Nutanix team: https://huggingface.co/blog/Nutanix/introducing-timesfm-for-time-series-forcasting See translation", "url": "https://huggingface.co/posts/jeffboudier/548880163054097", "date_published": "2025-05-30T13:33:47.533423"}, {"id": "https://huggingface.co/posts/AdinaY/228294422537840", "image": "", "title": "\ud83d\udd25 New benchmark & dataset for Subject-to-Video generation", "content_text": "\ud83d\udd25 New benchmark & dataset for Subject-to-Video generation OPENS2V-NEXUS by Pekin University \u2728 Fine-grained evaluation for subject consistency BestWishYsh/OpenS2V-Eval \u2728 5M-scale dataset: BestWishYsh/OpenS2V-5M \u2728 New metrics \u2013 automatic scores for identity, realism, and text match See translation", "url": "https://huggingface.co/posts/AdinaY/228294422537840", "date_published": "2025-05-30T13:33:47.533669"}]}