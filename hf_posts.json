{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/468043722468280", "image": "", "title": "11 Fascinating new Policy Optimization techniques", "content_text": "11 Fascinating new Policy Optimization techniques Policy optimization (PO) algorithms are central to training AI models with preference-based feedback. In recent weeks, numerous new PO methods have emerged that build on or replace the popular PPO and GRPO, solving their issues. Here are 11 of them: 1. BAlanced Policy Optimization (BAPO) \u2192 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping (2510.18927) Dynamically adjusting the clipping bounds in PPO-style updates to balance positive and negative gradients and prevent entropy collapse 2. Training-Free GRPO \u2192 Training-Free Group Relative Policy Optimization (2510.08191) Instead of using numeric rewards, it compares rollouts semantically to distill useful knowledge as a token prior, which is then applied during inference to guide the model\u2019s behavior 3. Asymmetric Importance Sampling Policy Optimization (ASPO) \u2192 ASPO: Asymmetric Importance Sampling Policy Optimization...", "url": "https://huggingface.co/posts/Kseniase/468043722468280", "date_published": "2025-11-04T09:27:20.910938"}, {"id": "https://huggingface.co/posts/abidlabs/941146046599374", "image": "", "title": "Why I think local, open-source models will eventually win.", "content_text": "Why I think local, open-source models will eventually win. The most useful AI applications are moving toward multi-turn agentic behavior: systems that take hundreds or even thousands of iterative steps to complete a task, e.g. Claude Code, computer-control agents that click, type, and test repeatedly. In these cases, the power of the model is not how smart it is per token, but in how quickly it can interact with its environment and tools across many steps. In that regime, model quality becomes secondary to latency. An open-source model that can call tools quickly, check that the right thing was clicked, or verify that a code change actually passes tests can easily outperform a slightly \u201csmarter\u201d closed model that has to make remote API calls for every move. Eventually, the balance tips: it becomes impractical for an agent to rely on remote inference for every micro-action. Just as no one would tolerate a keyboard that required a network request per keystroke, users won\u2019t accept...", "url": "https://huggingface.co/posts/abidlabs/941146046599374", "date_published": "2025-11-04T09:27:20.911448"}, {"id": "https://huggingface.co/posts/wang12390/323744389614625", "image": "", "title": "Experience the future of fashion with our AI-powered virtual try-on technology. See how clothes look on anyone instantly, create realistic outfit visualizations, and mix-and-match styles with unprecedented accuracy.", "content_text": "Experience the future of fashion with our AI-powered virtual try-on technology. See how clothes look on anyone instantly, create realistic outfit visualizations, and mix-and-match styles with unprecedented accuracy. https://miragic.ai/products/virtual-try-on See translation", "url": "https://huggingface.co/posts/wang12390/323744389614625", "date_published": "2025-11-04T09:27:20.911681"}, {"id": "https://huggingface.co/posts/Norod78/977626760436669", "image": "", "title": "Multilingual Tokenization Showdown", "content_text": "Multilingual Tokenization Showdown Analyzing 12 LLM Tokenizers Across 204 Languages. First, I've created a dataset with Wikipedia's \"Cat\" article text in 272 languages: Norod78/WikiCat-Multilingual For each language entry with at least 100 words, I tokenized the text using 12 tokenizers and calculated the \"Characters per token\" ratio and \"Word per token\" ratio. The higher this ratio is, the more information each token represents on average for that language (and perhaps allowing the llm to potentially learn more per-parameter if trained on a dataset of that language). You can see a slideshow summary of the results here: https://norod.github.io/wikicat-tokenizer-eval/tokenizer-slideshow.html I hope I interpreted the results correctly, I've made the code available on GitHub so you can re-create the raw results jsonl with this repo: https://github.com/Norod/wikicat-tokenizer-eval Post on X: https://x.com/Norod78/status/1984366900550266999 See translation", "url": "https://huggingface.co/posts/Norod78/977626760436669", "date_published": "2025-11-04T09:27:20.912026"}, {"id": "https://huggingface.co/posts/nouamanetazi/972464132222376", "image": "", "title": "After training \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25\ud835\udc0b\ud835\udc0c\ud835\udfd1 on \ud835\udfd1\ud835\udfd6\ud835\udfd2 \ud835\udc07\ud835\udfcf\ud835\udfce\ud835\udfce\ud835\udc2c for nearly a month, I've come to realize something most people overlook: \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc24\ud835\udc1e-\ud835\udc28\ud835\udc2b-\ud835\udc1b\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc24 \ud835\udc1f\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc22\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20. \ud83d\udd25", "content_text": "After training \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25\ud835\udc0b\ud835\udc0c\ud835\udfd1 on \ud835\udfd1\ud835\udfd6\ud835\udfd2 \ud835\udc07\ud835\udfcf\ud835\udfce\ud835\udfce\ud835\udc2c for nearly a month, I've come to realize something most people overlook: \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc24\ud835\udc1e-\ud835\udc28\ud835\udc2b-\ud835\udc1b\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc24 \ud835\udc1f\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc22\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20. \ud83d\udd25 Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious \ud835\udc0d\ud835\udc02\ud835\udc02\ud835\udc0b \ud835\udc1e\ud835\udc2b\ud835\udc2b\ud835\udc28\ud835\udc2b\ud835\udc2c, or when your expensive GPU cluster is running at \ud835\udfd4\ud835\udfce% \ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc32, the problem isn't your model. It's most probably a \ud835\udc26\ud835\udc22\ud835\udc2c\ud835\udc2e\ud835\udc2c\ud835\udc1e \ud835\udc28\ud835\udc1f \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc21\ud835\udc1a\ud835\udc2b\ud835\udc1d\ud835\udc30\ud835\udc1a\ud835\udc2b\ud835\udc1e. \ud83d\udee0\ufe0f Questions that seemed simple but had no clear answers: Why is \ud835\udc0c\ud835\udc28\ud835\udc04 \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc2c\ud835\udc25\ud835\udc28\ud835\udc30\ud835\udc1e\ud835\udc2b \ud835\udc2d\ud835\udc21\ud835\udc1a\ud835\udc27 \ud835\udc1d\ud835\udc1e\ud835\udc27\ud835\udc2c\ud835\udc1e \ud835\udc26\ud835\udc28\ud835\udc1d\ud835\udc1e\ud835\udc25\ud835\udc2c? Which \ud835\udc0d\ud835\udc02\ud835\udc02\ud835\udc0b \ud835\udc1f\ud835\udc25\ud835\udc1a\ud835\udc20\ud835\udc2c should we actually set? How often should we checkpoint without killing throughput? That's why we built \ud835\udc13\ud835\udc21\ud835\udc1e \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25 \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc0f\ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1b\ud835\udc28\ud835\udc28\ud835\udc24 \ud83d\udcd6: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1e\ud835\udc2b that most teams get wrong. We validated real vs...", "url": "https://huggingface.co/posts/nouamanetazi/972464132222376", "date_published": "2025-11-04T09:27:20.912614"}, {"id": "https://huggingface.co/posts/Shivansh000/941986646578616", "image": "", "title": "I am dedicating this weekend to practicing/reading the latest b(ook)log from hugging face. It is meant to be a guide for anyone trying to go from \u201cwe have a great dataset and GPUs\u201d to \u201cwe built a really strong model.\u201d Will share thoughts upon completion.", "content_text": "I am dedicating this weekend to practicing/reading the latest b(ook)log from hugging face. It is meant to be a guide for anyone trying to go from \u201cwe have a great dataset and GPUs\u201d to \u201cwe built a really strong model.\u201d Will share thoughts upon completion. Thanks for the treat @ eliebak @ ThomasWolf and HF team! HuggingFaceTB/smol-training-playbook See translation", "url": "https://huggingface.co/posts/Shivansh000/941986646578616", "date_published": "2025-11-04T09:27:20.912899"}, {"id": "https://huggingface.co/posts/unmodeled-tyler/973176037226952", "image": "", "title": "New Datasets Published:", "content_text": "New Datasets Published: vanta-research/poetic-imagery-small vanta-research/excitement-small We are open sourcing two of our datasets today, which were used in the training of Apollo Astralis 8B and 4B. The first dataset, poetic-imagery-small is designed to give the model's responses a bit of \"depth\" to them in order to encourage curiosity and thought from the user. Additionally, the excitement-small dataset is designed to teach the model how to use \"excited\" language conversationally. This dataset was used on both Apollo Astralis models, which effectively demonstrate general excitement during user interaction. VANTA Research is an AI safety project which aims to research and develop language models aligned for all types of thinking. These datasets were created aligned with that mission, in addition to rigorous AI safety standards. See translation", "url": "https://huggingface.co/posts/unmodeled-tyler/973176037226952", "date_published": "2025-11-04T09:27:20.913228"}, {"id": "https://huggingface.co/posts/flozi00/890663421107803", "image": "", "title": "Some weeks ago, i've just decide its time to leave LinkedIn for me.", "content_text": "Some weeks ago, i've just decide its time to leave LinkedIn for me. It got silent around my open source activities the last year, so i thought something has to change. That's why my focus will move to share experiences and insights about hardware, drivers, kernels and linux. I won't post about how to use models, built agents or do prompting. I want to share about some deeper layers the actual hypes are built on. I will start posting summarizations of my articles here on the hub. English version: https://flozi.net/en German translated version: https://flozi.net/de Feel free to reach me if you want to read something specific. See translation", "url": "https://huggingface.co/posts/flozi00/890663421107803", "date_published": "2025-11-04T09:27:20.913550"}, {"id": "https://huggingface.co/posts/codelion/338248148684079", "image": "", "title": "The 1 Billion Token Challenge: Finding the Perfect Pre-training Mix", "content_text": "The 1 Billion Token Challenge: Finding the Perfect Pre-training Mix We trained a GPT-2 model to 90%+ performance using just 1/10th the training data through 50+ systematic experiments on dataset mixing strategies. Key Finding: A static mix of 50% finePDFs + 30% DCLM-baseline + 20% FineWeb-Edu consistently outperforms complex curriculum learning approaches. Static mixing is simpler, faster, and avoids catastrophic failures from hard distribution shifts. Results: Our GPT-2-70M model (70M parameters, 1B tokens) scores 38.15% on benchmarks vs GPT-2's 39.13% - only 0.98 points behind despite 10x less data and 44% fewer parameters. It even beats GPT-2 on TruthfulQA (47.31% vs 40.69%). The takeaway: careful dataset curation matters more than total data volume. Model: codelion/gpt-2-70m Datasets: https://huggingface.co/collections/codelion/pre-training-dataset-samples Full blog: https://huggingface.co/blog/codelion/optimal-dataset-mixing See translation", "url": "https://huggingface.co/posts/codelion/338248148684079", "date_published": "2025-11-04T09:27:20.913892"}, {"id": "https://huggingface.co/posts/InezCornell/148411333647765", "image": "", "title": "Radar data for experimentation: Access real radar datasets and expert support to build AI/ML solutions using radar data.", "content_text": "Radar data for experimentation: Access real radar datasets and expert support to build AI/ML solutions using radar data. Ideal for companies looking to add radar capabilities to their services \u2013 access the data and guidance to help you get started quickly. Free access to: - Sample dataset as a CSV - Accompanying README file https://www.plextek.com/learn/radar-data-for-experimentation/ See translation", "url": "https://huggingface.co/posts/InezCornell/148411333647765", "date_published": "2025-11-04T09:27:20.914158"}]}