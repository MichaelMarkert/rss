{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/omarkamali/275991644251688", "image": "", "title": "Hello", "content_text": "Hello picomon ! AMD GPU Monitoring made easy Just run uvx picomon and behold: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 GPU 0 GFX 42 % UMC 21 % \u2502 \u2502 GPU 1 GFX 78 % UMC 66 % \u2502 \u2502 PWR 135 / 250 W (54%) VRAM 10.0 / 16.0 GB 62 % \u2502 \u2502 PWR 210 / 250 W (84%) VRAM 14.5 / 16.0 GB 90 % \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 GFX \u2581\u2582\u2582\u2583\u2584\u2584\u2585\u2586\u2586\u2587\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581 \u2502 \u2502 GFX \u2582\u2583\u2584\u2585\u2586\u2587\u2588\u2588\u2587\u2586\u2585\u2584\u2582\u2582\u2583\u2585\u2586 \u2502 \u2502 PWR \u2581\u2581\u2582\u2582\u2583\u2584\u2584\u2585\u2586\u2587\u2588\u2588\u2587\u2586\u2585\u2584\u2582\u2581 \u2502 \u2502 PWR \u2582\u2582\u2583\u2584\u2585\u2586\u2587\u2588\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2582\u2583 \u2502 \u2502 VRM \u2581\u2581\u2582\u2582\u2583\u2584\u2584\u2585\u2586\u2587\u2588\u2588\u2588\u2587\u2586\u2585\u2584\u2582 \u2502 \u2502 VRM \u2582\u2583\u2584\u2585\u2586\u2586\u2587\u2588\u2588\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2582 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Repo at https://github.com/omarkamali/picomon Or pypi at https://pypi.org/project/picomon", "url": "https://huggingface.co/posts/omarkamali/275991644251688", "date_published": "2025-12-08T17:24:38.209635"}, {"id": "https://huggingface.co/posts/sergiopaniego/364808323176425", "image": "", "title": "Want to get started with fine-tuning but don\u2019t know where to begin? \ud83e\udd13\u261d\ufe0f", "content_text": "Want to get started with fine-tuning but don\u2019t know where to begin? \ud83e\udd13\u261d\ufe0f We\u2019re expanding our collection of beginner-friendly free Colab notebooks so you can learn and fine-tune models using TRL at no cost \ud83d\udd2c Check out the full list of free notebooks: https://huggingface.co/docs/trl/main/en/example_overview#notebooks \ud83d\udd2c If you want more advanced content, we also have a lot to cover in the community tutorials: https://huggingface.co/docs/trl/community_tutorials And now the obvious question: what would you like us to add next? See translation", "url": "https://huggingface.co/posts/sergiopaniego/364808323176425", "date_published": "2025-12-08T17:24:38.210045"}, {"id": "https://huggingface.co/posts/codelion/632031761173923", "image": "", "title": "NotebookLM's infographics feature is amazing, it generates poster-type images from any text. Here is one I tried for my new HF article on ellora -", "content_text": "NotebookLM's infographics feature is amazing, it generates poster-type images from any text. Here is one I tried for my new HF article on ellora - https://huggingface.co/blog/codelion/ellora-lora-recipes See translation", "url": "https://huggingface.co/posts/codelion/632031761173923", "date_published": "2025-12-08T17:24:38.210323"}, {"id": "https://huggingface.co/posts/Kseniase/673116238988658", "image": "", "title": "15 Outstanding Research Papers from NeurIPS 2025", "content_text": "15 Outstanding Research Papers from NeurIPS 2025 NeurIPS 2025, as a premier annual event in machine learning and computational neuroscience, tackles major topics like the future of AI, current research, and the most difficult challenges. While we\u2019re not attending this year, we\u2019re closely following the updates and today we pull together a quick, easy-to-digest roundup of a few standout papers so you can jump in without getting overwhelmed. Here is a list of 15 papers from NeurIPS 2025, including 8 top research papers that received awards, along with 7 others that caught our attention: 1. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks \u2192 https://neurips.cc/virtual/2025/loc/san-diego/test-of-time/128328 Test of Time Award winner. Introduces the RPN, a small convnet that predicts objectness and boxes on shared features, enabling Faster R-CNN to share computation and run around 5 fps on a GPU 2. Artificial Hivemind: The Open-Ended Homogeneity of LMs (and...", "url": "https://huggingface.co/posts/Kseniase/673116238988658", "date_published": "2025-12-08T17:24:38.211073"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/781719759837719", "image": "", "title": "Don't forget to checkout our latest amazing training tutorial", "content_text": "Don't forget to checkout our latest amazing training tutorial Z-Image Turbo LoRA training with AI Toolkit and Z-Image ControlNet Full Tutorial for Highest Quality https://youtu.be/ezD6QO14kRc See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/781719759837719", "date_published": "2025-12-08T17:24:38.211370"}, {"id": "https://huggingface.co/posts/rajkumarrawal/605767913889349", "image": "", "title": "September(2025) LLM Core Knowledge & Reasoning Benchmarks Report By", "content_text": "September(2025) LLM Core Knowledge & Reasoning Benchmarks Report By AiParivartanResearchLab (AIPRL-LIR) Monthly LLM's Intelligence Reports for AI Decision Makers : Our \"aiprl-llm-intelligence-report\" repo to establishes (AIPRL-LIR) framework for Large Language Model overall evaluation and analysis through systematic monthly intelligence reports. Unlike typical AI research papers or commercial reports. It provides structured insights into AI model performance, benchmarking methodologies, Multi-hosting provider analysis, industry trends ... ( all in one monthly report ) Leading Models & Companies, 23 Benchmarks in 6 Categories, Global Hosting Providers, & Research Highlights Here\u2019s what you\u2019ll find inside this month\u2019s intelligence report:- Leading Models & Companies : openai , Anthropic , meta-llama , deepmind google , mistralai , Cohere , Qwen , deepseek-ai , MicrosoftResearch , amazonwebservices , nvidia , grokgpt-org and more. 23 Benchmarks in 6 Categories : With a special focus on...", "url": "https://huggingface.co/posts/rajkumarrawal/605767913889349", "date_published": "2025-12-08T17:24:38.211906"}, {"id": "https://huggingface.co/posts/codelion/196218932923903", "image": "", "title": "Recently, Essential AI released a new 8B base model", "content_text": "Recently, Essential AI released a new 8B base model EssentialAI/rnj-1 they highlighted the importance of data mix for pretraning - \"In the long run, we expect our methods to automatically represent, transform, and blend data to optimize measurable abilities in pre-training. Our work on modeling data taxonomies led to new approaches for jointly clustering and mixing data distributions under data repetition penalties. Many improvements in our STEM abilities can be traced back to this. \" This resonates with the recent work we did around optimal dataset mixing for pretraining where we saw have the right mix can increase the efficiency of training - https://huggingface.co/blog/codelion/optimal-dataset-mixing See translation", "url": "https://huggingface.co/posts/codelion/196218932923903", "date_published": "2025-12-08T17:24:38.212241"}, {"id": "https://huggingface.co/posts/prithivMLmods/334358996299299", "image": "", "title": "Try CUA GUI Operator \ud83d\udda5\ufe0f Space, the demo of some interesting multimodal ultra-compact Computer Use Agent (CUA) models in a single app, including Fara-7B, UI-TARS-1.5-7B, and Holo models, to perform GUI localization tasks.", "content_text": "Try CUA GUI Operator \ud83d\udda5\ufe0f Space, the demo of some interesting multimodal ultra-compact Computer Use Agent (CUA) models in a single app, including Fara-7B, UI-TARS-1.5-7B, and Holo models, to perform GUI localization tasks. \u25cf CUA-GUI-Operator [Demo]: prithivMLmods/CUA-GUI-Operator \u25cf Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations Other related multimodal spaces \u25cf Qwen3-VL: prithivMLmods/Qwen3-VL-HF-Demo \u25cf Multimodal-VLM-v1.0: prithivMLmods/Multimodal-VLM-v1.0 \u25cf Vision-to-VibeVoice-en: prithivMLmods/Vision-to-VibeVoice-en I have planned to add Chrome sandboxes to streamline it and turn it into a browser based CUA multimodal tool, which will be added to the same space soon. To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/334358996299299", "date_published": "2025-12-08T17:24:38.212690"}, {"id": "https://huggingface.co/posts/ovi054/581452329729774", "image": "", "title": "Anim Lab AI\u26a1", "content_text": "Anim Lab AI\u26a1 Turn any math concept or logic into a clear video explanation instantly using AI. This is my submission for the MCP 1st Birthday Hackathon, and it\u2019s already crossed 1,000 runs. \ud83d\udc49 Try it now: MCP-1st-Birthday/anim-lab-ai Demo outputs are attached \ud83d\udc47 See translation", "url": "https://huggingface.co/posts/ovi054/581452329729774", "date_published": "2025-12-08T17:24:38.212964"}, {"id": "https://huggingface.co/posts/prithivMLmods/612580119302031", "image": "", "title": "One speech model with seven voices, streamlined with multimodal capabilities for vision tasks. Performs vision(image-text) to audio inference with Qwen2.5-VL + VibeVoice-Realtime-0.5B. Vision to VibeVoice (EN) -  The demo is live.  \ud83d\udde3\ufe0f\ud83d\udd25", "content_text": "One speech model with seven voices, streamlined with multimodal capabilities for vision tasks. Performs vision(image-text) to audio inference with Qwen2.5-VL + VibeVoice-Realtime-0.5B. Vision to VibeVoice (EN) - The demo is live. \ud83d\udde3\ufe0f\ud83d\udd25 \ud83e\udd17 Vision-to-VibeVoice-en [Demo]: prithivMLmods/Vision-to-VibeVoice-en \u2728 Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations \u2728 Speech [VibeVoice-Realtime-0.5B]: microsoft/VibeVoice-Realtime-0.5B \u2728 Vision [Qwen2.5-VL]: Qwen/Qwen2.5-VL-7B-Instruct To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/612580119302031", "date_published": "2025-12-08T17:24:38.213307"}]}