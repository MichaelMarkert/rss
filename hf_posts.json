{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/mitkox/340947617819121", "image": "", "title": "I got 370 tokens/sec of Qwen3-30B-A3B 2507 on my desktop Z8 GPU workstation. My target is 400 t/s, and the last 10 % always tastes like victory!", "content_text": "I got 370 tokens/sec of Qwen3-30B-A3B 2507 on my desktop Z8 GPU workstation. My target is 400 t/s, and the last 10 % always tastes like victory! See translation", "url": "https://huggingface.co/posts/mitkox/340947617819121", "date_published": "2025-08-02T09:27:38.351661"}, {"id": "https://huggingface.co/posts/jsulz/651298897017923", "image": "", "title": "We've crossed 1 million repositories backed by Xet storage on Hugging Face! \ud83d\ude80\ud83d\ude80\ud83d\ude80", "content_text": "We've crossed 1 million repositories backed by Xet storage on Hugging Face! \ud83d\ude80\ud83d\ude80\ud83d\ude80 You can follow along our progress converting the Hub from Git LFS to Xet at jsulz/ready-xet-go We have a lot of repos left to migrate, which means I have plenty of time to add more animations \ud83e\udd2a See translation", "url": "https://huggingface.co/posts/jsulz/651298897017923", "date_published": "2025-08-02T09:27:38.351938"}, {"id": "https://huggingface.co/posts/YerbaPage/437092763804097", "image": "", "title": "Latest work on SWE-Bench \ud83d\udc1b", "content_text": "Latest work on SWE-Bench \ud83d\udc1b Our two new papers from the SJTU & Huawei: Powered by DeepSeek-V3, we've achieved a new SOTA on the SWE-Bench benchmark! We introduce two innovative approaches: \u2694\ufe0f SWE-Debate: AI agents compete and \"debate\" to generate the best code fix. \ud83e\udde0 SWE-Exp: An AI agent learns from past repair \"experience\" to solve new issues more efficiently. \ud83d\udc47 Explore the future of software development: SWE-Debate \ud83d\udcc4 Paper: https://arxiv.org/abs/2507.23348 \ud83d\udcbb Code: https://github.com/YerbaPage/SWE-Debate SWE-Exp \ud83d\udcc4 Paper: https://arxiv.org/abs/2507.23361 \ud83d\udcbb Code: https://github.com/YerbaPage/SWE-Exp See translation", "url": "https://huggingface.co/posts/YerbaPage/437092763804097", "date_published": "2025-08-02T09:27:38.352269"}, {"id": "https://huggingface.co/posts/chintankp/680096865746882", "image": "", "title": "We\u2019re excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100.", "content_text": "We\u2019re excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100. Super v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. Besides leading accuracy, it also delivers high throughput. Key features: - Leading accuracy on multi-step reasoning, math, coding, and function-calling - Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples - Fully transparent training data on HF (Nemotron-Post-Training-Dataset-v1) Try Super v1.5 on build.nvidia.com or download from Hugging Face See translation", "url": "https://huggingface.co/posts/chintankp/680096865746882", "date_published": "2025-08-02T09:27:38.352642"}, {"id": "https://huggingface.co/posts/IlyasMoutawwakil/721004690541036", "image": "", "title": "\ud83d\ude80 Optimum: The Last v1 Release \ud83d\ude80", "content_text": "\ud83d\ude80 Optimum: The Last v1 Release \ud83d\ude80 Optimum v1.27 marks the final major release in the v1 series. As we close this chapter, we're laying the groundwork for a more modular and community-driven future: - Optimum v2: A lightweight core package for porting Transformers, Diffusers, or Sentence-Transformers to specialized AI hardware/software/accelerators.. - Optimum\u2011ONNX: A dedicated package where the ONNX/ONNX Runtime ecosystem lives and evolves, faster-moving and decoupled from the Optimum core. \ud83c\udfaf Why this matters: - A clearer governance path for ONNX, fostering stronger community collaboration and improved developer experience.. - Enable innovation at a faster pace in a more modular, open-source environment. \ud83d\udca1 What this means: - More transparency, broader participation, and faster development driven by the community and key actors in the ONNX ecosystem (PyTorch, Microsoft, Joshua Lochner \ud83d\udc40, ...) - A cleaner, more maintainable core Optimum, focused on extending HF libraries to special AI...", "url": "https://huggingface.co/posts/IlyasMoutawwakil/721004690541036", "date_published": "2025-08-02T09:27:38.353249"}, {"id": "https://huggingface.co/posts/merve/770620321222949", "image": "", "title": "Cohere just dropped", "content_text": "Cohere just dropped CohereLabs/command-a-vision-07-2025 , a 112B (dense!) vision LM > based on SigLIP2 & Command-A > built for enterprise use cases \ud83d\udd25 > use with Inference Providers or transformers \ud83e\udd17 read their blog https://huggingface.co/blog/CohereLabs/introducing-command-a-vision-07-2025 See translation", "url": "https://huggingface.co/posts/merve/770620321222949", "date_published": "2025-08-02T09:27:38.353500"}, {"id": "https://huggingface.co/posts/AdinaY/564352975503737", "image": "", "title": "Qwen team did it again!!", "content_text": "Qwen team did it again!! They just released Qwen3-Coder-30B-A3B-Instruct on the hub\ud83d\udd25 Qwen/Qwen3-Coder-30B-A3B-Instruct \u2728 Apache 2.0 \u272830B total / 3.3B active (128 experts, 8 top-k) \u2728 Native 256K context, extendable to 1M via Yarn \u2728 Built for Agentic Coding See translation", "url": "https://huggingface.co/posts/AdinaY/564352975503737", "date_published": "2025-08-02T09:27:38.353760"}, {"id": "https://huggingface.co/posts/prithivMLmods/923940739727688", "image": "", "title": "Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, we\u2019ve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources.", "content_text": "Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, we\u2019ve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources. \u2728 Multimodal-VLM-Thinking : prithivMLmods/Multimodal-VLM-Thinking \u2728 Multimodal-VLM-OCR : prithivMLmods/Multimodal-VLM-OCR \u2726 Models used in these spaces: \u2728 Lumian-VLR-7B-Thinking : prithivMLmods/Lumian-VLR-7B-Thinking \u2728 Enesidaon-VLR-7B-no-Thinking : prithivMLmods/Enesidaon-VLR-7B-no-Thinking \u2728 GLM-4.1V-9B-Thinking : zai-org/GLM-4.1V-9B-Thinking \u2728 DREX-062225-exp : prithivMLmods/DREX-062225-exp & more ... \u2726 Multimodal Model Collections and Spaces: \u2728 Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a \u2728 Multimodal Spaces : prithivMLmods/multimodal-...", "url": "https://huggingface.co/posts/prithivMLmods/923940739727688", "date_published": "2025-08-02T09:27:38.354219"}, {"id": "https://huggingface.co/posts/AdinaY/263963364371914", "image": "", "title": "Qwen3-30B-A3B-Thinking-2507 \ud83d\udd25 latest step in scaling thinking capabilities from  Alibaba Qwen team.", "content_text": "Qwen3-30B-A3B-Thinking-2507 \ud83d\udd25 latest step in scaling thinking capabilities from Alibaba Qwen team. Qwen/Qwen3-30B-A3B-Thinking-2507-FP8 \u2728 30B total / 3B active - Apache 2.0 \u2728 Native 256K context \u2728 SOTA coding, alignment, agentic reasoning See translation", "url": "https://huggingface.co/posts/AdinaY/263963364371914", "date_published": "2025-08-02T09:27:38.354453"}, {"id": "https://huggingface.co/posts/merve/514822650680483", "image": "", "title": "past week in open AI was insane \ud83d\udd25 here's some of picks, find more here", "content_text": "past week in open AI was insane \ud83d\udd25 here's some of picks, find more here merve/releases-july-25-688768ca47fe3693407e02d1 \ud83d\udcac LLMs & VLMs > Qwen/Qwen3-235B-A22B-Thinking-2507 had a new update (OS) > Qwen/Qwen3-Coder-480B-A35B-Instruct is out with 480B total 35B active params \ud83e\udd2f (OS) > AllenAI dropped an update to allenai/olmOCR-7B-0725 \ud83d\udcdd > InternLM released internlm/Intern-S1 - 235B Qwen3 MoE + 6B InternViT encoder (OS) > OmniSVG/OmniSVG is a new SVG generation VLM (OS) \ud83d\uddbc\ufe0f image/video/3D generation > WanAI released Wan2.2 series - both T2V and I2V 14B models for high-quality video generation (OS) multimodalart/wan-22-688767e313337b434ed55112 > Tencent dropped tencent/HunyuanWorld-1 - image-to-3D scene generation model See translation", "url": "https://huggingface.co/posts/merve/514822650680483", "date_published": "2025-08-02T09:27:38.354825"}]}