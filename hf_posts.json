{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/m-ric/175050207181959", "image": "", "title": "STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! \ud83d\udea8", "content_text": "STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! \ud83d\udea8 A lone scientist just proposed Tiny Recursive Model (TRM), and it is literally the most impressive model that I've seen this year. \u27a1\ufe0f Tiny Recursive Model is 7M parameters \u27a1\ufe0f On ARC-AGI, it beats flagship models like Gemini-2.5-pro Consider how wild this is: Gemini-2.5-pro must be over 10,000x bigger and had 1,000 as many authors \ud83d\ude02 (Alexia is alone on the paper) What's this sorcery? In short: it's a very tiny Transformers, but it loops over itself at two different frequencies, updating two latent variables: one for the proposed answer and one for the reasoning. @ AlexiaJM started from the paper Hierarchical Reasoning Model, published a few months ago, that already showed breakthrough improvement on AGI for its small size (27M) Hierarchical Reasoning Model had introduced one main feature: \ud83d\udd0e Deep supervision In their model, one part (here one layer) would run at high frequency, and...", "url": "https://huggingface.co/posts/m-ric/175050207181959", "date_published": "2025-10-11T17:17:30.192184"}, {"id": "https://huggingface.co/posts/AdinaY/255762505303069", "image": "", "title": "At the close of the National Holiday\ud83c\udde8\ud83c\uddf3, Antgroup drops a new SoTA model.", "content_text": "At the close of the National Holiday\ud83c\udde8\ud83c\uddf3, Antgroup drops a new SoTA model. Ling-1T \ud83d\udd25 the trillion-parameter flagship of the Ling 2.0 series. inclusionAI/Ling-1T \u27281T total / 50B active params per token \u272820T+ reasoning-dense tokens (Evo-CoT) \u2728128K context via YaRN \u2728FP8 training: 15%+ faster, same precision as BF16 \u2728Hybrid Syntax-Function-Aesthetics reward for front-end & visual generation See translation", "url": "https://huggingface.co/posts/AdinaY/255762505303069", "date_published": "2025-10-11T17:17:30.192511"}, {"id": "https://huggingface.co/posts/Ethank01/441451500060564", "image": "", "title": "No invitation code needed \u2014 create AI videos with one click!", "content_text": "No invitation code needed \u2014 create AI videos with one click! Experience Sora 2, Veo 3, and Wan 2.2 all in one place on iMini. \ud83d\udc49 Try it here: https://imini.com/ See translation", "url": "https://huggingface.co/posts/Ethank01/441451500060564", "date_published": "2025-10-11T17:17:30.192737"}, {"id": "https://huggingface.co/posts/giadap/452837154929545", "image": "", "title": "\ud83c\udf0e AI ethics and sustainability are two sides of the same coin.", "content_text": "\ud83c\udf0e AI ethics and sustainability are two sides of the same coin. In our new blog post with Dr. Sasha Luccioni, we argue that separating them (as is too often the case) means missing the bigger picture of how AI systems impact both people and the planet. Ethical and sustainable AI development can\u2019t be pursued in isolation. The same choices that affect who benefits or is harmed by AI systems also determine how much energy and resources they consume. We explore how two key concepts, evaluation and transparency, can serve as bridges between these domains: \ud83d\udcca Evaluation, by moving beyond accuracy or performance metrics to include environmental and social costs, as we\u2019ve done with tools like the AI Energy Score. \ud83d\udd0d Transparency, by enabling reproducibility, accountability, and environmental reporting through open tools like the Environmental Transparency Space. AI systems mirror our priorities. If we separate ethics from sustainability, we risk building technologies that are efficient but...", "url": "https://huggingface.co/posts/giadap/452837154929545", "date_published": "2025-10-11T17:17:30.193193"}, {"id": "https://huggingface.co/posts/jwgu/145606397801534", "image": "", "title": "\ud83c\udf89 NEW RELEASES: Cosmos Predict 2.5 and Transfer 2.5", "content_text": "\ud83c\udf89 NEW RELEASES: Cosmos Predict 2.5 and Transfer 2.5 Cosmos Predict 2.5: - Combines Text2World, Image2World, and Video2World - Multimodal, future-state video prediction Cosmos Transfer 2.5: - High-fidelity multicontrol world simulations - Inputs: RGB, depth, segmentation\u2014blended seamlessly These updates boost development of autonomous vehicles, robotics, and video analytics. Don\u2019t miss Jensen Huang\u2019s keynote at NVIDIA GTC Washington, D.C. on 10/28 to hear the latest in physical AI. \ud83d\udcfa Watch live: https://nvda.ws/4pUjF4x \ud83d\udd17 Try Predict 2.5: https://nvda.ws/4otReZZ \ud83d\udd17 Try Transfer 2.5: https://nvda.ws/46GEx7T See translation", "url": "https://huggingface.co/posts/jwgu/145606397801534", "date_published": "2025-10-11T17:17:30.193543"}, {"id": "https://huggingface.co/posts/Severian/804699132791352", "image": "", "title": "MLX port of BDH (Baby Dragon Hatchling) is up!", "content_text": "MLX port of BDH (Baby Dragon Hatchling) is up! I\u2019ve ported the BDH ( https://github.com/pathwaycom/bdh ) model to MLX for Apple Silicon. It\u2019s a faithful conversion of the PyTorch version: same math, same architecture (byte-level vocab, shared weights across layers, ReLU sparsity, RoPE attention with Q=K), with MLX-friendly APIs and a detailed README explaining the few API-level differences and why results are equivalent. Code, docs, and training script are ready to use. You may need to adjust the training script a bit to fit your own custom dataset. Only tested on M4 so far, but should work perfect for any M1/M2/M3 users out there. I\u2019m currently training this MLX build on my Internal Knowledge Map (IKM) dataset Severian/Internal-Knowledge-Map Training\u2019s underway; expect a day or so before I publish weights. When it\u2019s done, I\u2019ll upload the checkpoint to Hugging Face for anyone to test. Repo: https://github.com/severian42/BDH-MLX HF model (coming soon): Severian/BDH-MLX If you try it...", "url": "https://huggingface.co/posts/Severian/804699132791352", "date_published": "2025-10-11T17:17:30.193977"}, {"id": "https://huggingface.co/posts/piercus/787328298619334", "image": "", "title": "We've just forked LBM to reproduce the LBM eraser results", "content_text": "We've just forked LBM to reproduce the LBM eraser results Our fork : https://github.com/finegrain-ai/LBM LBM paper: LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) LBM relighting demo : jasperai/LBM_relighting See translation", "url": "https://huggingface.co/posts/piercus/787328298619334", "date_published": "2025-10-11T17:17:30.194200"}, {"id": "https://huggingface.co/posts/hba123/315319549896319", "image": "", "title": "\ud83e\udd16 What if building your own robot arm costs less than \u00a3220?", "content_text": "\ud83e\udd16 What if building your own robot arm costs less than \u00a3220? For years, robotics has been locked behind high prices and complex systems. So we decided to change that. Today, we\u2019re open-sourcing Ark-Bot \u2014 a fully 3D-printed, 6-DOF robot arm that works seamlessly with our Python robotics library, Ark. And yes\u2026 It\u2019s only \u00a3215.86 to build. \ud83e\udde0ArkBot Specs \ud83e\udde0 1\ufe0f\u20e3 Reach: 1 meter 2\ufe0f\u20e3 Weight: 2.6 kg 3\ufe0f\u20e3 Payload: 1.8 kg \ud83d\udcaa 4\ufe0f\u20e3 DOF: 6 5\ufe0f\u20e3 Input Voltage: DC 12V \ud83e\udd1fFully 3D-printable & open-source \ud83e\udd1fIntegrated with Ark \u2014 no ROS required \ud83d\udcf9 We\u2019ve also released a video showing the full assembly process \u2014 because robotics should be something everyone can learn, build, and improve on. \ud83d\udc69\u200d\ud83c\udf93 With Ark-Bot, anyone \u2014 from students to AI researchers \u2014 can experiment with embodied AI, robot learning, and control algorithms on real hardware, affordably. If you could control a 1-meter robot arm from your laptop for under \u00a3220\u2026 \ud83d\udc49 What would you build first? \ud83d\udd17https://github.com/Robotics-Ark/ark_bot \ud83c\udfa5...", "url": "https://huggingface.co/posts/hba123/315319549896319", "date_published": "2025-10-11T17:17:30.194668"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/350754844731753", "image": "", "title": "Ovi is Local Version of VEO 3 & SORA 2 - The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer on Windows even with a 6GB GPUs - Full Tutorial for Windows, RunPod and Massed Compute - Gradio App", "content_text": "https://youtu.be/T00VmkMQRPQ Ovi is Local Version of VEO 3 & SORA 2 - The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer on Windows even with a 6GB GPUs - Full Tutorial for Windows, RunPod and Massed Compute - Gradio App Tutorial : https://youtu.be/T00VmkMQRPQ Forget waiting lists and expensive APIs. The era of closed-off, corporate-controlled AI video generation is soon over. This is Ovi : The first-ever public, open-source model that generates both VIDEO and synchronized AUDIO, and you can run it on your own computer\u2014even with a 6GB GPU! This isn't just a demo; it's a full, step-by-step revolution. Tutorial Info In this ultimate A-Z guide, I'll show you EVERYTHING you need to know to install and master this Sora 2 and VEO3 like AI. We'll go from zero to generating incredible talking videos from text or a single image. \ud83d\udd25 In This Tutorial, You Will Learn To: \ud83c\udf93 Master the Ultimate SORA 2 and VEO 3...", "url": "https://huggingface.co/posts/MonsterMMORPG/350754844731753", "date_published": "2025-10-11T17:17:30.195278"}, {"id": "https://huggingface.co/posts/kanaria007/689097395708339", "image": "", "title": "\u2705 New Article: *Envy \u2014 The Structural Law of Relative Evaluation*", "content_text": "\u2705 New Article: *Envy \u2014 The Structural Law of Relative Evaluation* Title: \ud83e\udded Envy, Comparison, and the Structural Law of Relative Evaluation: Why \u201cThey Got More\u201d Hurts \u2014 and How Protocols Contain It \ud83d\udd17 https://huggingface.co/blog/kanaria007/envy-structural-law-of-relative-evaluation --- Summary: Envy isn\u2019t a moral glitch \u2014 it\u2019s a *relative-evaluation loop*. When identity meets comparison, the mind computes *delta-to-others* (status, attention, resources). If that delta breaches a threshold without a repair path, the loop escalates into resentment. Structured Intelligence makes this computable \u2014 and containable. > Envy compares levels. > *Integrity compares trajectories.* --- Why It Matters: \u2022 Turns envy from shame into an *auditable signal* (where, when, and why comparison spikes) \u2022 Provides *de-escalation protocols* for individuals, teams, and platforms \u2022 Guides product/organization design to reduce perceived unfairness and burnout --- What\u2019s Inside: \u2022 The Envy Loop: *trigger \u2192...", "url": "https://huggingface.co/posts/kanaria007/689097395708339", "date_published": "2025-10-11T17:17:30.195845"}]}