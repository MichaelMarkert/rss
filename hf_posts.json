{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/openfree/102455854917725", "image": "", "title": "\ud83c\udf3e NH Prediction: AI System for Korean Agricultural Price Forecasting \ud83c\udf3e", "content_text": "\ud83c\udf3e NH Prediction: AI System for Korean Agricultural Price Forecasting \ud83c\udf3e \ud83d\udcca Project Introduction Price volatility in agricultural markets has significant impacts from producers to consumers! NH Prediction is an innovative system that utilizes cutting-edge AI technology to predict Korean agricultural wholesale prices based on extensive data spanning 40 years. \ud83d\ude80 VIDraft/NH-Prediction ginipick/NH-Korea \ud83e\udde0 VIDraft's 14 Enhanced Prediction Models The VIDraft research team has developed 14 advanced prediction models by reinforcing existing forecasting approaches: \ud83d\udd2e VID-SARIMA Series: Precisely models seasonality and trends (up to 99.99% accuracy) \u2696\ufe0f VID-ETS Series: Captures multiplicative/additive variation patterns \ud83d\udcc8 VID-Holt/Holt-Winters: Simultaneous analysis of linear trends and seasonality \ud83d\udcc9 VID-MovingAverage/WeightedMA: Noise removal and medium-term trend identification \ud83d\udd0d VID-Fourier+LR: Hybrid approach capturing complex periodicity \u2728 Key Features \ud83c\udf1f Item-Specific Optimization:...", "url": "https://huggingface.co/posts/openfree/102455854917725", "date_published": "2025-05-23T05:23:29.236176"}, {"id": "https://huggingface.co/posts/codelion/735622263233891", "image": "", "title": "\ud83e\uddec Hey everyone! Just released **OpenEvolve** - an open-source implementation of Google DeepMind's AlphaEvolve system.", "content_text": "\ud83e\uddec Hey everyone! Just released **OpenEvolve** - an open-source implementation of Google DeepMind's AlphaEvolve system. It's an evolutionary coding agent that uses LLMs to discover and optimize algorithms. I successfully replicated DeepMind's results on circle packing (99.97% match!) and evolved a random search into a simulated annealing algorithm. \u2728 Key features: - Evolves entire codebases (not just single functions) - Works with any OpenAI-compatible API - LLM ensemble approach for better results - Multi-objective optimization \ud83d\udc49 Check it out: GitHub: https://github.com/codelion/openevolve Blog post: https://huggingface.co/blog/codelion/openevolve Would love to hear your thoughts or answer any questions about it! See translation", "url": "https://huggingface.co/posts/codelion/735622263233891", "date_published": "2025-05-23T05:23:29.236574"}, {"id": "https://huggingface.co/posts/merve/523189303979360", "image": "", "title": "Bu post'u \u00e7evirebilirsiniz \ud83e\udd17\ud83d\udc97", "content_text": "Bu post'u \u00e7evirebilirsiniz \ud83e\udd17\ud83d\udc97 See translation", "url": "https://huggingface.co/posts/merve/523189303979360", "date_published": "2025-05-23T05:23:29.236825"}, {"id": "https://huggingface.co/posts/AdinaY/709690582361356", "image": "", "title": "ByteDance is absolutely cooking lately\ud83d\udd25", "content_text": "ByteDance is absolutely cooking lately\ud83d\udd25 BAGEL \ud83e\udd6f 7B active parameter open multimodal foundation model by Bytedance Seed team. ByteDance-Seed/BAGEL-7B-MoT \u2728 Apache 2.0 \u2728 Outperforms top VLMs (Qwen2.5-VL & InternVL-2.5) \u2728 Mixture-of-Transformer-Experts + dual encoders \u2728 Trained on trillions of interleaved tokens See translation", "url": "https://huggingface.co/posts/AdinaY/709690582361356", "date_published": "2025-05-23T05:23:29.237092"}, {"id": "https://huggingface.co/posts/jsulz/285043040409517", "image": "", "title": "Heyo", "content_text": "Heyo @ RichardErkhov the xet-team at Hugging face was wondering if you wanted to join the fun and jump over to Xet storage. \ud83e\udd17 We've been onboarding folks https://huggingface.co/blog/xet-on-the-hub know the backend can scale (Llama 4 and Qwen 3 are on Xet), is great for working with quants (see xet-team/quantization-dedup ), and we're pushing on inviting impactful orgs and users on the Hub. You fit the bill. We'd love to onboard you, get some feedback, and create some excitement \ud83c\udf89 The steps are pretty straightforward - join the waitlist at hf.co/join/xet and we'll take care of the rest. The system is fully backward compatible, so you shouldn't notice a thing. BUT to get the best experience when uploading/downloading, make sure you have hf_xet installed alongside the latest huggingface_hub What do you think? See translation", "url": "https://huggingface.co/posts/jsulz/285043040409517", "date_published": "2025-05-23T05:23:29.237513"}, {"id": "https://huggingface.co/posts/clem/670042306060895", "image": "", "title": "Playing with Veo3 this morning. Share your prompt if you want me to create videos for you (bonus point if they funnily reference HF/open-source). These videos are \"a cat on the moon rapping \"I love Hugging Face\"\"!", "content_text": "Playing with Veo3 this morning. Share your prompt if you want me to create videos for you (bonus point if they funnily reference HF/open-source). These videos are \"a cat on the moon rapping \"I love Hugging Face\"\"! See translation", "url": "https://huggingface.co/posts/clem/670042306060895", "date_published": "2025-05-23T05:23:29.237733"}, {"id": "https://huggingface.co/posts/pranavupadhyaya52/196223267192478", "image": "", "title": "Hello everyone. I've built a medical AI assistant application.", "content_text": "Hello everyone. I've built a medical AI assistant application. pranavupadhyaya52/MediWiki_Medical_Assistant It is a multimodal chatbot and can accept text, radiology images, prescription and lab reports (currently it only accepts one image per chat.) and audio files (wav and MP3 extension files). It is built on top of a finetuned Llama 3.2 11B vision instruct. It also uses a 41000 medically related question answer pair stored in the form of chromadb embedding for Retrieval Augmented Generation (RAG). Please let me know your thoughts on my project and how I can improve it further. Thank you. See translation", "url": "https://huggingface.co/posts/pranavupadhyaya52/196223267192478", "date_published": "2025-05-23T05:23:29.238019"}, {"id": "https://huggingface.co/posts/cfahlgren1/983652211807712", "image": "", "title": "Yesterday, we dropped a new conversational viewer for datasets on the hub! \ud83d\udcac", "content_text": "Yesterday, we dropped a new conversational viewer for datasets on the hub! \ud83d\udcac Actually being able to view and inspect your data is extremely important. This is a big step in making data more accessible and actionable for everyone. Here's some datasets you can try it out on: \u2022 mlabonne/FineTome-100k \u2022 Salesforce/APIGen-MT-5k \u2022 open-thoughts/OpenThoughts2-1M \u2022 allenai/tulu-3-sft-mixture Any other good ones? See translation", "url": "https://huggingface.co/posts/cfahlgren1/983652211807712", "date_published": "2025-05-23T05:23:29.238330"}, {"id": "https://huggingface.co/posts/sayakpaul/592772325865027", "image": "", "title": "Despite the emergence of combining LLM and DiT architectures for T2I synthesis, its design remains severely understudied.", "content_text": "Despite the emergence of combining LLM and DiT architectures for T2I synthesis, its design remains severely understudied. This was done long ago and got into CVPR25 -- super excited to finally share it now, along with the data and code \u2665\ufe0f We explore several architectural choices that affect this design. We provide an open & reproducible training recipe that works at scale. Works like Playground v3 have already explored a deep fusion between an LLM and a DiT, sharing their representations through layerwise attention. They exhibit excellent performance on T2I. Despite its compelling results and other performance virtues, it remains unexplored, which is what we want to improve in our work. Specifically, we take a pre-trained LLM (Gemma-2B) and trainable DiT, and set out to explore what makes a \"good deep fusion\" between the two for T2I. We explore several key questions in the work, such as: Q1: How should we do attention? We considered several alternatives. PixArt-Alpha like attention...", "url": "https://huggingface.co/posts/sayakpaul/592772325865027", "date_published": "2025-05-23T05:23:29.238926"}, {"id": "https://huggingface.co/posts/merve/870882250701193", "image": "", "title": "tis the year of any-to-any/omni models \ud83e\udd20", "content_text": "tis the year of any-to-any/omni models \ud83e\udd20 ByteDance-Seed/BAGEL-7B-MoT 7B native multimodal model that understands and generates both image + text it outperforms leading VLMs like Qwen 2.5-VL \ud83d\udc4f and has Apache 2.0 license \ud83d\ude31 See translation", "url": "https://huggingface.co/posts/merve/870882250701193", "date_published": "2025-05-23T05:23:29.239183"}]}