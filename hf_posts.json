{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/445000542637232", "image": "", "title": "10 awesome advanced LoRA approaches", "content_text": "10 awesome advanced LoRA approaches Low-Rank Adaptation (LoRA) is the go-to method for efficient model fine-tuning that adds small low-rank matrices instead of retraining full models. The field isn\u2019t standing still \u2013 new LoRA variants push the limits of efficiency, generalization, and personalization. So we\u2019re sharing 10 of the latest LoRA approaches you should know about: 1. Mixture-of-LoRA-experts \u2192 Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection (2509.13878) Adds multiple low-rank adapters (LoRA) into a model\u2019s layers, and a routing mechanism activates the most suitable ones for each input. This lets the model adapt better to new unseen conditions 2. Amortized Bayesian Meta-Learning for LoRA (ABMLL) \u2192 Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models (2508.14285) Balances global and task-specific parameters within a Bayesian framework to improve uncertainty calibration and generalization to new tasks without high...", "url": "https://huggingface.co/posts/Kseniase/445000542637232", "date_published": "2025-09-22T17:17:36.866354"}, {"id": "https://huggingface.co/posts/Ethank01/541492789102968", "image": "", "title": "Discussing  iMini Nano Banana \u2013 Practical AI Templates for Creative Workflows", "content_text": "Discussing iMini Nano Banana \u2013 Practical AI Templates for Creative Workflows \ud83c\udf99\ufe0fI\u2019ve been experimenting with imini nano banana recently, and the new batch of 100+ AI templates feels like a real shift in how we handle creative workflows. \ud83c\udfa8One example: the photo-to-hand-drawn sketch template. I tested it with a travel photo (a blonde girl in front of the Eiffel Tower) \u2192 the output was a clean, consistent hand-drawn sketch in seconds. Normally, this would take manual editing in Photoshop/Illustrator, but the AI template cut the process down to a single click. \ud83d\udca1From a workflow perspective, this could streamline tasks for: Designers: quick iterations on style variations Marketers: generating campaign visuals at scale Content creators: producing unique artwork without advanced editing skills \ud83d\udd8c\ufe0fIf anyone here has tested imini nano banana in professional projects, I\u2019d like to hear how it fits into your pipeline. Full details: https://imini.com/nano-banana See translation", "url": "https://huggingface.co/posts/Ethank01/541492789102968", "date_published": "2025-09-22T17:17:36.866803"}, {"id": "https://huggingface.co/posts/salma-remyx/977986978639462", "image": "", "title": "Trustworthy AI evals has been an industry challenge for the last few years, so what's missing?", "content_text": "Trustworthy AI evals has been an industry challenge for the last few years, so what's missing? Causal Reasoning. Model based eval frameworks can't tell you if your changes actually improved user outcomes - you need to take a systems level approach. At Remyx, we\u2019re building the intelligence layer for AI experimentation. Check out this example on how we start laying the scaffolding to launch controlled experiments to turn your hypotheses into insights on what drives performance for your application. Check out the latest at Remyx in our docs: https://docs.remyx.ai Try your first experiment today! https://engine.remyx.ai See translation", "url": "https://huggingface.co/posts/salma-remyx/977986978639462", "date_published": "2025-09-22T17:17:36.867143"}, {"id": "https://huggingface.co/posts/merve/604366247415617", "image": "", "title": "large AI labs open-sourced a ton of models last week \ud83d\udd25", "content_text": "large AI labs open-sourced a ton of models last week \ud83d\udd25 here's few picks, find even more here merve/sep-16-releases-68d13ea4c547f02f95842f05 \ud83e\udd1d > IBM released a new Docling model with 258M params based on Granite (A2.0) \ud83d\udcdd ibm-granite/granite-docling-258M > Xiaomi released 7B audio LM with base and instruct variants (MIT) XiaomiMiMo/mimo-audio-68cc7202692c27dae881cce0 > DecartAI released Lucy Edit, open Nano Banana \ud83c\udf4c (NC) decart-ai/Lucy-Edit-Dev > OpenGVLab released a family of agentic computer use models (3B/7B/32B) with the dataset \ud83d\udcbb OpenGVLab/scalecua-68c912cf56f7ff4c8e034003 > Meituan Longcat released thinking version of LongCat-Flash \ud83d\udcad meituan-longcat/LongCat-Flash-Thinking See translation", "url": "https://huggingface.co/posts/merve/604366247415617", "date_published": "2025-09-22T17:17:36.867494"}, {"id": "https://huggingface.co/posts/prithivMLmods/322831563234696", "image": "", "title": "Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.\ud83e\udd17\u2728", "content_text": "Dropping some experimental adapters for FLUX.1-Kontext-dev, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, and Monochrome-Pencil. These were trained under various settings with minimal image pairs to achieve optimal results. The dataset result sets end pairs were synthesized using Gemini-2.5-Flash-Image-Preview and others.\ud83e\udd17\u2728 \u26a1Demo : prithivMLmods/Photo-Mate-i2i prithivMLmods/PhotoCleanser-i2i : Remove objects while preserving the rest of the image. prithivMLmods/Photo-Restore-i2i : Restore old photos into moderately colorized, detailed images. prithivMLmods/Polaroid-Warm-i2i : Seamless vintage Polaroid-style images with warm, faded tones. prithivMLmods/Yarn-Photo-i2i : Convert images into yarn-stitched artwork while retaining key details. prithivMLmods/Monochrome-Pencil : Turn images into monochrome pencil sketches while keeping original features. \u2728Note: All the above models share the same auto-labeling multimodal VLM captioning model,...", "url": "https://huggingface.co/posts/prithivMLmods/322831563234696", "date_published": "2025-09-22T17:17:36.867983"}, {"id": "https://huggingface.co/posts/Tanaybh/360553614352968", "image": "", "title": "The Bias is YOU - LLMs Mirror Your Own Assumptions", "content_text": "The Bias is YOU - LLMs Mirror Your Own Assumptions AI doesn't just have bias - it reflects yours. When you ask a question with positive framing, you get positive answers. Ask with negative framing, you get negative answers. The AI becomes a mirror of your own assumptions. Your framing determines the answer - The same topic yields opposite responses based on how you ask AIs amplify your sentiment - Negative questions often get even MORE negative responses This affects everyone - From students doing research to professionals making decisions Why This Matters This isn't a technical glitch - it's fundamental to how these systems work. They're trained on human language, and humans frame things with bias. The AI learned to match that framing. Think about the implications: - Medical professionals seeking second opinions - Students researching controversial topics - Business leaders evaluating strategies - Anyone using AI for important decisions The Stochastic Mirror Effect Let's call this...", "url": "https://huggingface.co/posts/Tanaybh/360553614352968", "date_published": "2025-09-22T17:17:36.868395"}, {"id": "https://huggingface.co/posts/salma-remyx/953215218627820", "image": "", "title": "Rolling Benchmarks - Evaluating AI Agents on Unseen GitHub Repos", "content_text": "Rolling Benchmarks - Evaluating AI Agents on Unseen GitHub Repos Static benchmarks are prone to leaderboard hacking and training data contamination, so how about a dynamic/rolling benchmark? By limiting submissions to only freshly published code, we could evaluate based on consistency over time with rolling averages instead of finding agents overfit to a static benchmark. Can rolling benchmarks bring us closer to evaluating agents in a way more closely aligned with their real-world applications? Perhaps a new direction for agent evaluation? Would love to hear what you think about this! More on reddit: https://www.reddit.com/r/LocalLLaMA/comments/1nmvw7a/rolling_benchmarks_evaluating_ai_agents_on_unseen/ See translation", "url": "https://huggingface.co/posts/salma-remyx/953215218627820", "date_published": "2025-09-22T17:17:36.868705"}, {"id": "https://huggingface.co/posts/onekq/332042657908085", "image": "", "title": "Claude Opus 4.1 is slightly better than Opus 4, but still behind GPT-5", "content_text": "Claude Opus 4.1 is slightly better than Opus 4, but still behind GPT-5 onekq-ai/WebApp1K-models-leaderboard See translation", "url": "https://huggingface.co/posts/onekq/332042657908085", "date_published": "2025-09-22T17:17:36.868910"}, {"id": "https://huggingface.co/posts/YerbaPage/735409135497339", "image": "", "title": "Announcing our new work on \"Repository Level Question Answering\"! \ud83c\udf89", "content_text": "Announcing our new work on \"Repository Level Question Answering\"! \ud83c\udf89 We introduce SWE-QA, a repository-level code QA benchmark with 576 real-world questions that require a deep understanding of the entire codebase to answer. Curious about the limits of today's LLMs on complex codebases? Check out our paper and open-source data! \ud83d\udcbb\ud83e\udde0 Link \ud83d\udc47 https://arxiv.org/pdf/2509.14635 https://github.com/peng-weihan/SWE-QA-Bench #LLM #AI #SoftwareEngineering #Benchmark See translation", "url": "https://huggingface.co/posts/YerbaPage/735409135497339", "date_published": "2025-09-22T17:17:36.869191"}, {"id": "https://huggingface.co/posts/vikhyatk/428307575853218", "image": "", "title": "Just released a preview of Moondream 3!", "content_text": "Just released a preview of Moondream 3! moondream/moondream3-preview This is a 9B parameter, 2B active MoE VLM with state of the art visual reasoning capabilities. More details in the release blog post: https://moondream.ai/blog/moondream-3-preview See translation", "url": "https://huggingface.co/posts/vikhyatk/428307575853218", "date_published": "2025-09-22T17:17:36.869411"}]}