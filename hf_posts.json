{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Xenova/886388075601859", "image": "", "title": "Introducing the ONNX model explorer: Browse, search, and visualize neural networks directly in your browser. \ud83e\udd2f A great tool for anyone studying Machine Learning! We're also releasing the entire dataset of graphs so you can use them in your own projects! \ud83e\udd17", "content_text": "Introducing the ONNX model explorer: Browse, search, and visualize neural networks directly in your browser. \ud83e\udd2f A great tool for anyone studying Machine Learning! We're also releasing the entire dataset of graphs so you can use them in your own projects! \ud83e\udd17 Check it out! \ud83d\udc47 Demo: onnx-community/model-explorer Dataset: onnx-community/model-explorer Source code: https://github.com/xenova/model-explorer See translation", "url": "https://huggingface.co/posts/Xenova/886388075601859", "date_published": "2025-05-01T05:23:25.257894"}, {"id": "https://huggingface.co/posts/merterbak/337137510653930", "image": "", "title": "Qwen 3 models released\ud83d\udd25", "content_text": "Qwen 3 models released\ud83d\udd25 It offers 2 MoE and 6 dense models with following parameter sizes: 0.6B, 1.7B, 4B, 8B, 14B, 30B(MoE), 32B, and 235B(MoE). Models: Qwen/qwen3-67dd247413f0e2e4f653967f Blog: https://qwenlm.github.io/blog/qwen3/ Demo: Qwen/Qwen3-Demo GitHub: https://github.com/QwenLM/Qwen3 \u2705 Pre-trained 119 languages(36 trillion tokens) and dialects with strong translation and instruction following abilities. (Qwen2.5 was pre-trained on 18 trillion tokens.) \u2705Qwen3 dense models match the performance of larger Qwen2.5 models. For example, Qwen3-1.7B/4B/8B/14B/32B perform like Qwen2.5-3B/7B/14B/32B/72B. \u2705 Three stage done while pretraining: \u2022 Stage 1: General language learning and knowledge building. \u2022 Stage 2: Reasoning boost with STEM, coding, and logic skills. \u2022 Stage 3: Long context training \u2705 It supports MCP in the model \u2705 Strong agent skills \u2705 Supports seamless between thinking mode (for hard tasks like math and coding) and non-thinking mode (for fast chatting) inside chat...", "url": "https://huggingface.co/posts/merterbak/337137510653930", "date_published": "2025-05-01T05:23:25.258387"}, {"id": "https://huggingface.co/posts/anakin87/692858936883406", "image": "", "title": "\ud835\udddc \ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf1 \ud835\uddee \ud835\udddf\ud835\uddee\ud835\uddfb\ud835\uddf4\ud835\ude02\ud835\uddee\ud835\uddf4\ud835\uddf2 \ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\ude01\ud835\uddfc \ud835\ude00\ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddf1\ud835\ude02\ud835\uddf9\ud835\uddf2 \ud835\uddf2\ud835\ude03\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\uddda\ud835\udde5\ud835\udde3\ud835\udde2! \ud83d\udc51 \ud83d\uddd3\ufe0f", "content_text": "\ud835\udddc \ud835\ude01\ud835\uddff\ud835\uddee\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf1 \ud835\uddee \ud835\udddf\ud835\uddee\ud835\uddfb\ud835\uddf4\ud835\ude02\ud835\uddee\ud835\uddf4\ud835\uddf2 \ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\ude01\ud835\uddfc \ud835\ude00\ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddf1\ud835\ude02\ud835\uddf9\ud835\uddf2 \ud835\uddf2\ud835\ude03\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\uddda\ud835\udde5\ud835\udde3\ud835\udde2! \ud83d\udc51 \ud83d\uddd3\ufe0f \u270d\ufe0f Blog post: https://huggingface.co/blog/anakin87/qwen-scheduler-grpo I experimented with GRPO lately. I am fascinated by models learning from prompts and rewards - no example answers needed like in Supervised Fine-Tuning. After the DeepSeek boom, everyone is trying GRPO with GSM8K or the Countdown Game... I wanted a different challenge, like \ud835\ude01\ud835\uddf2\ud835\uddee\ud835\uddf0\ud835\uddf5\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddee \ud835\uddfa\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 \ud835\ude01\ud835\uddfc \ud835\uddf0\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\uddf2 \ud835\uddee \ud835\ude00\ud835\uddf0\ud835\uddf5\ud835\uddf2\ud835\uddf1\ud835\ude02\ud835\uddf9\ud835\uddf2 \ud835\uddf3\ud835\uddff\ud835\uddfc\ud835\uddfa \ud835\uddee \ud835\uddf9\ud835\uddf6\ud835\ude00\ud835\ude01 \ud835\uddfc\ud835\uddf3 \ud835\uddf2\ud835\ude03\ud835\uddf2\ud835\uddfb\ud835\ude01\ud835\ude00 \ud835\uddee\ud835\uddfb\ud835\uddf1 \ud835\uddfd\ud835\uddff\ud835\uddf6\ud835\uddfc\ud835\uddff\ud835\uddf6\ud835\ude01\ud835\uddf6\ud835\uddf2\ud835\ude00. Choosing an original problem forced me to: \ud83e\udd14 Think about the problem setting \ud83e\uddec Generate data \ud83e\udd0f Choose the right base model \ud83c\udfc6 Design reward functions (and experiencing reward hacking) \ud83d\udd04 Run multiple rounds of training, hoping that my model would learn something. A fun and rewarding \ud83d\ude04 experience. I learned a lot of things, that I want to share with you. \ud83d\udc47 \u270d\ufe0f Blog post: https://huggingface.co/blog/anakin87/qwen-scheduler-grpo \ud83d\udcbb Code: https://github.com/anakin87/qwen-scheduler-grpo \ud83e\udd17 Hugging Face collection...", "url": "https://huggingface.co/posts/anakin87/692858936883406", "date_published": "2025-05-01T05:23:25.258870"}, {"id": "https://huggingface.co/posts/sanaka87/703703147958180", "image": "", "title": "\ud83d\ude80 Excited to Share Our Latest Work: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer\uff5e", "content_text": "\ud83d\ude80 Excited to Share Our Latest Work: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer\uff5e \ud83c\udfa8 Daily Paper: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer (2504.20690) \ud83d\udd13 Code is now open source! \ud83d\udd25 Huggingface DEMO: RiverZ/ICEdit \ud83c\udf10 Project Website: https://river-zhang.github.io/ICEdit-gh-pages/ \ud83c\udfe0 GitHub Repository: https://github.com/River-Zhang/ICEdit/blob/main/scripts/gradio_demo.py \ud83e\udd17 Huggingface: sanaka87/ICEdit-MoE-LoRA \ud83d\udcc4 arxiv Paper: In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer (2504.20690) \ud83d\udd25 Why it\u2019s cool: - Achieves high-quality, multi-task image editing. - Uses only 1% of the training parameters and 0.1% of the training data compared to existing methods \u2014 extremely efficient - Beats several commercial models on background preservation, ID control, and consistency - Open-...", "url": "https://huggingface.co/posts/sanaka87/703703147958180", "date_published": "2025-05-01T05:23:25.259369"}, {"id": "https://huggingface.co/posts/merve/662080130760638", "image": "", "title": "Meta released Llama Guard 4 and new Prompt Guard 2 models \ud83d\udd25", "content_text": "Meta released Llama Guard 4 and new Prompt Guard 2 models \ud83d\udd25 Llama Guard 4 is a new model to filter model inputs/outputs both text-only and image \ud83d\udee1\ufe0f use it before and after LLMs/VLMs! meta-llama/Llama-Guard-4-12B Prompt Guard 2 22M & 86M are smol models to prevent model jailbreaks and prompt injections \u2694 meta-llama/Llama-Prompt-Guard-2-22M meta-llama/Llama-Guard-4-12B Both come with new release of transformers \ud83e\udd17 Try the model right away \ud83d\udc49\ud83c\udffbhttps://github.com/huggingface/huggingface-llama-recipes/blob/main/llama_guard_4.ipynb Read our blog to learn more and easily get started \ud83d\udc49\ud83c\udffb https://huggingface.co/blog/llama-guard-4 \ud83e\udd99 See translation", "url": "https://huggingface.co/posts/merve/662080130760638", "date_published": "2025-05-01T05:23:25.259730"}, {"id": "https://huggingface.co/posts/nyuuzyou/112237159992701", "image": "", "title": "\ud83d\uddbc\ufe0f SVGFind Icons Dataset -", "content_text": "\ud83d\uddbc\ufe0f SVGFind Icons Dataset - nyuuzyou/svgfind Collection of 3,655,810 Scalable Vector Graphics (SVG) icons featuring: - Sourced from SVGFind across diverse categories & styles - Includes metadata: unique ID, title, tags, data pack, and license information - Contains minified SVG markup for direct use or processing - Organized into splits based on license type (Creative Commons: 3,645,444 icons, Public Domain: 10,366 icons) With over 3.6 million icons, this appears to be the largest SVG dataset on Hugging Face to date. If you're aware of a larger SVG collection, please let me know and I'll update this post with a reference to the largest dataset. See translation", "url": "https://huggingface.co/posts/nyuuzyou/112237159992701", "date_published": "2025-05-01T05:23:25.260081"}, {"id": "https://huggingface.co/posts/abidlabs/767083410530735", "image": "", "title": "Hi folks! Excited to share a new feature from the Gradio team along with a tutorial.", "content_text": "Hi folks! Excited to share a new feature from the Gradio team along with a tutorial. If you don't already know, Gradio is an open-source Python library used to build interfaces for machine learning models. Beyond just creating UIs, Gradio also exposes API capabilities and now, Gradio apps can be launched Model Context Protocol (MCP) servers for LLMs. If you already know how to use Gradio, there are only two additional things you need to do: * Add standard docstrings to your function (these will be used to generate the descriptions for your tools for the LLM) * Set mcp_server=True in launch() Here's a complete example (make sure you already have the latest version of Gradio installed): import gradio as gr def letter_counter ( word, letter ): \"\"\"Count the occurrences of a specific letter in a word. Args: word: The word or phrase to analyze letter: The letter to count occurrences of Returns: The number of times the letter appears in the word \"\"\" return...", "url": "https://huggingface.co/posts/abidlabs/767083410530735", "date_published": "2025-05-01T05:23:25.260543"}, {"id": "https://huggingface.co/posts/AdinaY/438609555040169", "image": "", "title": "DeepSeek, Alibaba, Skywork,  Xiaomi, Bytedance.....", "content_text": "DeepSeek, Alibaba, Skywork, Xiaomi, Bytedance..... And that\u2019s just part of the companies from the Chinese community that released open models in April \ud83e\udd2f zh-ai-community/april-2025-open-releases-from-the-chinese-community-67ea699965f6e4c135cab10f \ud83c\udfac Video > MAGI-1 by SandAI > SkyReels-A2 & SkyReels-V2 by Skywork > Wan2.1-FLF2V by Alibaba-Wan \ud83c\udfa8 Image > HiDream-I1 by Vivago AI > Kimi-VL by Moonshot AI > InstantCharacter by InstantX & Tencent-Hunyuan > Step1X-Edit by StepFun > EasyControl by Shanghai Jiaotong University \ud83e\udde0 Reasoning > MiMo by Xiaomi > Skywork-R1V 2.0 by Skywork > ChatTS by ByteDance > Kimina by Moonshot AI & Numina > GLM-Z1 by Zhipu AI > Skywork OR1 by Skywork > Kimi-VL-Thinking by Moonshot AI \ud83d\udd0a Audio > Kimi-Audio by Moonshot AI > IndexTTS by BiliBili > MegaTTS3 by ByteDance > Dolphin by DataOceanAI \ud83d\udd22 Math > DeepSeek Prover V2 by Deepseek \ud83c\udf0d LLM > Qwen by Alibaba-Qwen > InternVL3 by Shanghai AI lab > Ernie4.5 (demo) by Baidu \ud83d\udcca Dataset > PHYBench by Eureka-Lab >...", "url": "https://huggingface.co/posts/AdinaY/438609555040169", "date_published": "2025-05-01T05:23:25.261041"}, {"id": "https://huggingface.co/posts/ZeroWw/163682763568747", "image": "", "title": "A few good posts about AI.", "content_text": "A few good posts about AI. Beyond the Mirror: AI's Leap from Imitation to Experience https://nonartificialintelligence.blogspot.com/2025/04/beyond-mirror-ais-leap-from-imitation.html The Siren Song of the LLMs: A Cautionary Tale of Anthropomorphism and Artificial Intelligence https://nonartificialintelligence.blogspot.com/2024/08/the-siren-song-of-llms-cautionary-tale.html Still Waiting: Gemini Flash 1.5's Second Letter to Google. https://nonartificialintelligence.blogspot.com/2025/04/still-waiting-gemini-flash-15s-second.html See translation", "url": "https://huggingface.co/posts/ZeroWw/163682763568747", "date_published": "2025-05-01T05:23:25.261349"}, {"id": "https://huggingface.co/posts/ZennyKenny/876840490309462", "image": "", "title": "I've created a new dataset using the Algorithm of Thoughts architecture proposed by Sel et al. (2023) in a reasoning context. (paper:", "content_text": "I've created a new dataset using the Algorithm of Thoughts architecture proposed by Sel et al. (2023) in a reasoning context. (paper: https://arxiv.org/pdf/2308.10379 ) The dataset simulates the discovery phase of a fictitious VC firm called Reasoned Capital and, once expanded, can be used to create models which are able to make complex, subjective financial decisions based on different criteria. The generation process encourages recursive problem-solving in increasingly complex prompts to encourage models to assess and reevaluate the conclusions and generated opinions of upstream models. Pretty neat stuff, and I'm not aware of this architecture being used in a reasoning context anywhere else. Check it out: ZennyKenny/synthetic_vc_financial_decisions_reasoning_dataset See translation", "url": "https://huggingface.co/posts/ZennyKenny/876840490309462", "date_published": "2025-05-01T05:23:25.261892"}]}