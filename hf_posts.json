{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/YatharthS/933040846295981", "image": "", "title": "Just uploaded a detailed blog about my findings in optimizing NeuTTS to generate 200 seconds of audio in a single second. Also went in depth in NeuTTS\u2019s architecture. Will be happy to answer any questions.", "content_text": "Just uploaded a detailed blog about my findings in optimizing NeuTTS to generate 200 seconds of audio in a single second. Also went in depth in NeuTTS\u2019s architecture. Will be happy to answer any questions. https://huggingface.co/blog/YatharthS/making-neutts-200x-realtime See translation", "url": "https://huggingface.co/posts/YatharthS/933040846295981", "date_published": "2025-11-23T09:23:01.797994"}, {"id": "https://huggingface.co/posts/unmodeled-tyler/762230736035210", "image": "", "title": "NEW Model Alert:", "content_text": "NEW Model Alert: vanta-research/atom-olmo3-7b We are excited at VANTA Research to release our atom-olmo3-7b model using the brand new Olmo3 architecture from Allen AI. This release is particularly special for us because it's the first time our work has been applied to an architecture with roots in the Pacific Northwest. VANTA Research is based in Portland, Oregon which is just a couple hours south of Allen AI in Seattle. Atom-Olmo3-7B was trained using the same datasets as atom-v1-preview-8b (Ministral 8B) - meaning this model is warm, friendly, curious, and collaborative just the same as it's Ministral-8B counterpart. Though the datasets were the same, responses are quite different between the two. Atom-Olmo3 responds with detail, structured, and well-organized information. Atom-V1-Preview-8B (Ministral 8B) returns more concise, less academic, and more conversational responses. Both models are native in human-AI collaboration and exploratory learning - though they each present it...", "url": "https://huggingface.co/posts/unmodeled-tyler/762230736035210", "date_published": "2025-11-23T09:23:01.798437"}, {"id": "https://huggingface.co/posts/branikita/234726599671172", "image": "", "title": "Publishing our research on dual-motor backlash compensation for STS3215 servos. To complete our arXiv submission, we need a quick endorsement from someone who has published in robotics (cs.RO/eess.SY).", "content_text": "Publishing our research on dual-motor backlash compensation for STS3215 servos. To complete our arXiv submission, we need a quick endorsement from someone who has published in robotics (cs.RO/eess.SY). If you can help, here\u2019s the code: L64QM3 Thank you! See translation", "url": "https://huggingface.co/posts/branikita/234726599671172", "date_published": "2025-11-23T09:23:01.798685"}, {"id": "https://huggingface.co/posts/mybbnae/820891401991666", "image": "", "title": "@Reubencf", "content_text": "@ Reubencf made an WebOS check it out MCP-1st-Birthday/Reuben_OS See translation", "url": "https://huggingface.co/posts/mybbnae/820891401991666", "date_published": "2025-11-23T09:23:01.798879"}, {"id": "https://huggingface.co/posts/aufklarer/147735626089995", "image": "", "title": "Couple months ago I fine\u2011tuned Qwen3 Embeddings with LoRA on the LSPC dataset. This time I went the opposite way: a small, task\u2011specific 80M encoder with bidirectional attention, trained end\u2011to\u2011end. It outperforms the Qwen3 LoRA baseline on the same data (0.9315 macro\u2011F1 vs 0.8360). Details and code:", "content_text": "Couple months ago I fine\u2011tuned Qwen3 Embeddings with LoRA on the LSPC dataset. This time I went the opposite way: a small, task\u2011specific 80M encoder with bidirectional attention, trained end\u2011to\u2011end. It outperforms the Qwen3 LoRA baseline on the same data (0.9315 macro\u2011F1 vs 0.8360). Details and code: https://blog.ivan.digital/beating-qwen3-lora-with-a-tiny-pytorch-encoder-on-the-large-scale-product-corpus-afe536de205f See translation", "url": "https://huggingface.co/posts/aufklarer/147735626089995", "date_published": "2025-11-23T09:23:01.799170"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/527488277094596", "image": "", "title": "\ud83d\udde3\ufe0f Introducing the Duality AI +  LunateAI Challenge- Geospatial Object Detection: Rural Buildings!", "content_text": "\ud83d\udde3\ufe0f Introducing the Duality AI + LunateAI Challenge- Geospatial Object Detection: Rural Buildings! Train a model to detect difficult detection instances, such as a low number of pixels or weak feature responses, in rural aerial imagery, to win \ud83c\udfc6PRIZES\ud83c\udfc6 and \ud83e\udd29RECOGNITION\ud83e\udd29. Sign up here: https://www.kaggle.com/competitions/duality-ai-lunate-ai-geospatial-object-detection/overview This is the first competition in the \ud83c\udf0eGeospatial Kaggle Challenge Series\ud83c\udf0f, which will explore how geospatial-based digital twins can train an AI model for real-world applications. Duality is excited to be partnering with LunateAI, a high-end advisory business founded by the award-winning, industry-recognized global leader Dr. Nadine Alameh to usher in a new era of geospatial impact in conjunction with advances in computing and AI. Lunate helps government and industry leaders \ud83e\udd14 rethink, \ud83d\udca1redesign, and \ud83d\udcdd execute transformative geospatial strategies using AI, cloud, and Lunate\u2019s unparalleled global expertise. Read...", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/527488277094596", "date_published": "2025-11-23T09:23:01.799781"}, {"id": "https://huggingface.co/posts/mike-ravkine/606116479128907", "image": "", "title": "Applying Hazard and Entropy Analysis to LLMs", "content_text": "Applying Hazard and Entropy Analysis to LLMs Here's an example of a model that behaves perfectly well up to 8k, smoothly increasing its entropy before going into a struggle zone, collapsing, seeing a region of recovery and finally falling down hard at the 16k wall. Is your model implementation behaving badly like this? Would you know if it was? \ud83d\udc40 See translation", "url": "https://huggingface.co/posts/mike-ravkine/606116479128907", "date_published": "2025-11-23T09:23:01.800063"}, {"id": "https://huggingface.co/posts/wang12390/249978853611053", "image": "", "title": "Speed Painting  is transforming image \u201cFloral Spirit: The Face of Nature\u201d into hand-written video. Please check the result.", "content_text": "Speed Painting is transforming image \u201cFloral Spirit: The Face of Nature\u201d into hand-written video. Please check the result. This is input image. This is output video. This is huggingface space: Miragic-AI/Miragic-Speed-Painting This is website. https://miragic.ai/products/speed-painting See translation", "url": "https://huggingface.co/posts/wang12390/249978853611053", "date_published": "2025-11-23T09:23:01.800320"}, {"id": "https://huggingface.co/posts/hiyouga/713531971190066", "image": "", "title": "\ud83d\ude80 We're excited to support the ERNIE AI Developer Challenge!", "content_text": "\ud83d\ude80 We're excited to support the ERNIE AI Developer Challenge! Fine-tune ERNIE with LLaMA-Factory and compete for $3,000 prizes by building the most impactful model \u2014 with submissions reviewed by the core developers of LLaMA-Factory. \ud83d\udc49 Join Now: https://baiduernieai.devpost.com/?utm_source=LLaMAFactory&utm_medium=partner&utm_campaign=ERNIE+AI+Developer+Challenge See translation", "url": "https://huggingface.co/posts/hiyouga/713531971190066", "date_published": "2025-11-23T09:23:01.800579"}, {"id": "https://huggingface.co/posts/onekq/122975793150084", "image": "", "title": "No SOTA from gpt5 codex", "content_text": "No SOTA from gpt5 codex onekq-ai/WebApp1K-models-leaderboard See translation", "url": "https://huggingface.co/posts/onekq/122975793150084", "date_published": "2025-11-23T09:23:01.800763"}]}