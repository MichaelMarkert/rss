{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/marksverdhei/554043140037847", "image": "", "title": "Poll: Will 2026 be the year of subquadratic attention?", "content_text": "Poll: Will 2026 be the year of subquadratic attention? The transformer architecture is cursed by its computational complexity. It is why you run out of tokens and have to compact. But some would argue that this is a feature not a bug and that this is also why these models are so good. We've been doing a lot of research on trying to make equally good models that are computationally cheaper, But so far, none of the approaches have stood the test of time. Or so it seems. Please vote, don't be shy. Remember that the Dunning-Kruger effect is very real, so the person who knows less about transformers than you is going to vote. We want everyone's opinion, no matter confidence. \ud83d\udc4d if you think at least one frontier model* will have no O(n^2) attention by the end of 2026 \ud83d\udd25 If you disagree * Frontier models - models that match / outperform the flagship claude, gemini or chatgpt at the time on multiple popular benchmarks See translation", "url": "https://huggingface.co/posts/marksverdhei/554043140037847", "date_published": "2026-02-10T06:12:19.792364"}, {"id": "https://huggingface.co/posts/mitkox/464155376106577", "image": "", "title": "I just pushed Claude Code Agent Swarm with 20 coding agents on my desktop GPU workstation.", "content_text": "I just pushed Claude Code Agent Swarm with 20 coding agents on my desktop GPU workstation. With local AI, I don\u2019t have /fast CC switch, but I have /absurdlyfast: - 100\u2019499 tokens/second read, yeah 100k, not a typo | 811 tok/sec generation - KV cache: 707\u2019200 tokens - Hardware: 5+ year old GPUs 4xA6K gen1; It\u2019s not the car. It\u2019s the driver. Qwen3 Coder Next AWQ with cache at BF16. Scores 82.1% in C# on 29-years-in-dev codebase vs Opus 4.5 at only 57.5%. When your codebase predates Stack Overflow, you don't need the biggest model; you need the one that actually remembers Windows 95. My current bottleneck is my 27\" monitor. Can't fit all 20 Theos on screen without squinting. See translation", "url": "https://huggingface.co/posts/mitkox/464155376106577", "date_published": "2026-02-10T06:12:19.792821"}, {"id": "https://huggingface.co/posts/melvindave/248709425212035", "image": "", "title": "I made my own avatar banner maker", "content_text": "I made my own avatar banner maker https://avatar.donvitocodes.com/ Using Claude Code and Opus 4.6 in a day I use it in my HF profile too See translation", "url": "https://huggingface.co/posts/melvindave/248709425212035", "date_published": "2026-02-10T06:12:19.793051"}, {"id": "https://huggingface.co/posts/DavidAU/768773942094149", "image": "", "title": "Tiny but mighty: LFM 1.2B - 11 Distill / Fine tunes : Exceeding all benchmarks at 300-700+ T/S on GPU, 60+ T/S CPU.", "content_text": "Tiny but mighty: LFM 1.2B - 11 Distill / Fine tunes : Exceeding all benchmarks at 300-700+ T/S on GPU, 60+ T/S CPU. Almost all exceed LFM 1.2B Benchmarks - which are already very impressive. All benchmarks posted. A specialized merge of multiple of these fine tunes by @ nightmedia FAR exceeds the benchmarks set by the already impressive LFM. (LFM2.5-1.2B-MEGABRAIN-Thinking-Polaris-ClaudeHOPUS-Deepseek-GLM) Included are GLM 4.7 Flash, DeepSeek, Claude, Kimi V2 and other distill fine tunes. Here is the collection ( Quants by MRadermarcher). https://huggingface.co/collections/DavidAU/lfm-12b-sota-400-700-t-s-enhanced-fine-tunes-distills See translation", "url": "https://huggingface.co/posts/DavidAU/768773942094149", "date_published": "2026-02-10T06:12:19.793377"}, {"id": "https://huggingface.co/posts/FreshmanD/759883646504275", "image": "", "title": "LoongFlow Big News!!!", "content_text": "LoongFlow Big News!!! @ all We\u2019ve put AI Agents into a production GPU cluster to handle GPU failure prediction. Not as a demo. Not as AutoML. But as an evolving system that designs and improves its own models. On two GPU types: \u2013 IT21HMDB01-B2: +30% prediction accuracy \u2013 H800: +25% prediction accuracy The resulting models already meet production standards and are being wired into the ops pipeline. How it works: \u2022 An ML agent designs the full ML pipeline from scratch \u2022 A Math agent performs targeted evolutionary optimization \u2022 The agents explore, discard, and iterate toward better modelsHumans don\u2019t hand-tune parameters. This is not offline analysis. GPU failure prediction means: \u2022 heavy assets \u2022 real incidents \u2022 real operational risk The agents now trigger maintenance before failures happen. This feels like an early signal: AI agents are starting to take responsibility for infrastructure-level engineering decisions in production systems. For ML Agent, you can check:...", "url": "https://huggingface.co/posts/FreshmanD/759883646504275", "date_published": "2026-02-10T06:12:19.793867"}, {"id": "https://huggingface.co/posts/MikeDoes/444764433252763", "image": "", "title": "You don't need a massive research lab to build a privacy-preserving AI tool thanks to open datasets. With the right ingredients, anyone can.", "content_text": "You don't need a massive research lab to build a privacy-preserving AI tool thanks to open datasets. With the right ingredients, anyone can. A fantastic new guide shows how the democratization of AI is helping to advance safety. It walks through how to use Google's new fine-tuning API to turn Gemini into a powerful tool for PII anonymization. This project was powered by two key components: An accessible platform from Google. High-quality, open-source training data. We are honored that the author chose the Ai4Privacy pii-masking-200k dataset to provide the crucial data foundation. Our dataset delivered the volume and structure needed to successfully teach a state-of-the-art model how to perform a critical privacy function. This is the future we're working towards: powerful platforms combined with open, safety-focused data to create tools that benefit everyone. Kudos to the author for showcasing what's possible! \ud83d\udd17 Read the full step-by-step guide:...", "url": "https://huggingface.co/posts/MikeDoes/444764433252763", "date_published": "2026-02-10T06:12:19.794443"}, {"id": "https://huggingface.co/posts/dhruv3006/610200840727549", "image": "", "title": "Voiden Blocks: Building APIs Like LEGO", "content_text": "Voiden Blocks: Building APIs Like LEGO At Voiden, we believe API development should feel like writing clean, reusable code, because it IS code. That\u2019s why everything in Voiden is a Block, the smallest, most flexible piece of your API world. Your endpoints, headers, query params, JSON bodies, even file attachments, all are individual Blocks you can add, remove, reorder, and reuse. Think of it as LEGO for HTTP: snap together Blocks to build clean, modular API requests that are easy to read, maintain, and share. But it gets better. With Reusable Blocks, you create a Block once and import it everywhere you need it, just like importing functions in your code. Update the Block once, and changes ripple through all your requests automatically. Why this matters: - Save time & energy, no more repeating the same thing over and over - Stay consistent, headers, params, and auth always match across your projects - Keep your workspace clean & focused, add only the Blocks you need - Collaborate...", "url": "https://huggingface.co/posts/dhruv3006/610200840727549", "date_published": "2026-02-10T06:12:19.794978"}, {"id": "https://huggingface.co/posts/prithivMLmods/280174075106284", "image": "", "title": "Introducing FLUX.2-Klein-LoRA-Studio, a demo for image editing using specialized LoRA adapters built for the FLUX.2-Klein-Distilled model. It features an edit-style gallery for multi-style image editing, including de-light, face swap, mannequin, and more. Try the demo below.", "content_text": "Introducing FLUX.2-Klein-LoRA-Studio, a demo for image editing using specialized LoRA adapters built for the FLUX.2-Klein-Distilled model. It features an edit-style gallery for multi-style image editing, including de-light, face swap, mannequin, and more. Try the demo below. \ud83e\udd17Demo: prithivMLmods/FLUX.2-Klein-LoRA-Studio \ud83e\udd17Collection: https://huggingface.co/collections/prithivMLmods/image-generation-apps-collection \ud83e\udd17GitHub: https://github.com/PRITHIVSAKTHIUR/FLUX.2-Klein-LoRA-Studio To learn more, visit the app page or the respective model pages. See translation", "url": "https://huggingface.co/posts/prithivMLmods/280174075106284", "date_published": "2026-02-10T06:12:19.795308"}, {"id": "https://huggingface.co/posts/kanaria007/577090857590798", "image": "", "title": "\u2705 New Article: *City OS under SI-Core* (v0.1)", "content_text": "\u2705 New Article: *City OS under SI-Core* (v0.1) Title: \ud83c\udfd9\ufe0f City OS under SI-Core: Governance-Grade Intelligence for Urban Systems \ud83d\udd17 https://huggingface.co/blog/kanaria007/city-os-under-si-core --- Summary: \u201cSmart city\u201d stacks usually optimize isolated KPIs\u2014traffic flow, response times, energy usage\u2014then discover the real failures later: unfair allocation, unsafe automation, opaque vendor decisions, and un-auditable incidents. This article sketches *City OS under SI-Core*: a governance-first urban runtime where every action is an *auditable Jump*, every actuator change is *RML-tracked*, every policy is *PoLB-mode-bound*, and every optimization is constrained by *ETH + role/principal identity*. > Cities don\u2019t need a chatbot. > They need *verifiable decision infrastructure*. --- Why It Matters: \u2022 Prevents \u201cautomation without accountability\u201d for high-stakes civic systems \u2022 Makes multi-stakeholder authority explicit: *citizen / operator / regulator / vendor* roles \u2022 Enables safe...", "url": "https://huggingface.co/posts/kanaria007/577090857590798", "date_published": "2026-02-10T06:12:19.795924"}, {"id": "https://huggingface.co/posts/MaziyarPanahi/461225711706465", "image": "", "title": "From Golden Gate Bridge to Broken JSON: Why Anthropic's SAE Steering Fails for Structured Output", "content_text": "From Golden Gate Bridge to Broken JSON: Why Anthropic's SAE Steering Fails for Structured Output I ran 6 experiments trying to use Anthropic's SAE steering for JSON generation. - Base model: 86.8% valid JSON - Steering only: 24.4% - Fine-tuned: 96.6% - FSM constrained: 100% Steering is for semantics, not syntax. https://huggingface.co/blog/MaziyarPanahi/sae-steering-json See translation", "url": "https://huggingface.co/posts/MaziyarPanahi/461225711706465", "date_published": "2026-02-10T06:12:19.796177"}]}