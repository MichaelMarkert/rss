{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/seawolf2357/796388354612946", "image": "", "title": "\ud83d\udd25 AgenticAI: The Ultimate Multimodal AI with 16 MBTI Girlfriend Personas! \ud83d\udd25", "content_text": "\ud83d\udd25 AgenticAI: The Ultimate Multimodal AI with 16 MBTI Girlfriend Personas! \ud83d\udd25 Hello AI community! Today, our team is thrilled to introduce AgenticAI, an innovative open-source AI assistant that combines deep technical capabilities with uniquely personalized interaction. \ud83d\udc98 \ud83d\udee0\ufe0f MBTI 16 Types SPACES Collections link seawolf2357/heartsync-mbti-67f793d752ef1fa542e16560 \u2728 16 MBTI Girlfriend Personas Complete MBTI Implementation: All 16 MBTI female personas modeled after iconic characters (Dana Scully, Lara Croft, etc.) Persona Depth: Customize age groups and thinking patterns for hyper-personalized AI interactions Personality Consistency: Each MBTI type demonstrates consistent problem-solving approaches, conversation patterns, and emotional expressions \ud83d\ude80 Cutting-Edge Multimodal Capabilities Integrated File Analysis: Deep analysis and cross-referencing of images, videos, CSV, PDF, and TXT files Advanced Image Understanding: Interprets complex diagrams, mathematical equations, charts, and...", "url": "https://huggingface.co/posts/seawolf2357/796388354612946", "date_published": "2025-04-13T09:23:00.871685"}, {"id": "https://huggingface.co/posts/jasoncorkill/225557458891562", "image": "", "title": "\ud83d\ude80 We tried something new!", "content_text": "\ud83d\ude80 We tried something new! We just published a dataset using a new (for us) preference modality: direct ranking based on aesthetic preference. We ranked a couple of thousand images from most to least preferred, all sampled from the Open Image Preferences v1 dataset by the amazing @ data-is-better-together team. \ud83d\udcca Check it out here: Rapidata/2k-ranked-images-open-image-preferences-v1 We're really curious to hear your thoughts! Is this kind of ranking interesting or useful to you? Let us know! \ud83d\udcac If it is, please consider leaving a \u2764\ufe0f and if we hit 30 \u2764\ufe0fs, we\u2019ll go ahead and rank the full 17k image dataset! See translation", "url": "https://huggingface.co/posts/jasoncorkill/225557458891562", "date_published": "2025-04-13T09:23:00.872095"}, {"id": "https://huggingface.co/posts/AdinaY/929657833669065", "image": "", "title": "Shanghai AI Lab - OpenGV team just released InternVL3 \ud83d\udd25", "content_text": "Shanghai AI Lab - OpenGV team just released InternVL3 \ud83d\udd25 OpenGVLab/internvl3-67f7f690be79c2fe9d74fe9d \u2728 1/2/8/9/14/38/28B with MIT license \u2728 Stronger perception & reasoning vs InternVL 2.5 \u2728 Native Multimodal Pre-Training for even better language performance See translation", "url": "https://huggingface.co/posts/AdinaY/929657833669065", "date_published": "2025-04-13T09:23:00.872372"}, {"id": "https://huggingface.co/posts/hesamation/789492772324435", "image": "", "title": "Google published a 69-page whitepaper on Prompt Engineering and its best practices, a must-read if you are using LLMs in production:", "content_text": "Google published a 69-page whitepaper on Prompt Engineering and its best practices, a must-read if you are using LLMs in production: > zero-shot, one-shot, few-shot > system prompting > chain-of-thought (CoT) > ReAct LINK: https://www.kaggle.com/whitepaper-prompt-engineering > code prompting > best practices See translation", "url": "https://huggingface.co/posts/hesamation/789492772324435", "date_published": "2025-04-13T09:23:00.872636"}, {"id": "https://huggingface.co/posts/nomadicsynth/772723579239178", "image": "", "title": "What if intelligence didn\u2019t belong to the rich?", "content_text": "What if intelligence didn\u2019t belong to the rich? What if insight, planning, and innovation were available to everyone? What if we actually leveled the playing field \u2014 not by force, but by toolset? See translation", "url": "https://huggingface.co/posts/nomadicsynth/772723579239178", "date_published": "2025-04-13T09:23:00.872886"}, {"id": "https://huggingface.co/posts/JLouisBiz/947336957437059", "image": "", "title": "**Video**:", "content_text": "**Video**: https://www.youtube.com/watch?v=jRKRsGsLfW0 **Integrating large language model with file manager to describe your illegally downloaded movies.** When you have a bunch of movies downloaded by Torrent, you maybe want a description and description is missing. This video shows how you can use the script to invoke the large language model. And then you get a description of a movie in a second or three. See translation", "url": "https://huggingface.co/posts/JLouisBiz/947336957437059", "date_published": "2025-04-13T09:23:00.873177"}, {"id": "https://huggingface.co/posts/odellus/648294233512756", "image": "", "title": "Super grateful to", "content_text": "Super grateful to @ marriola for the release of the block diffusion code and model. I'm generating text with diffusion locally! Couldn't be more pleased. See translation", "url": "https://huggingface.co/posts/odellus/648294233512756", "date_published": "2025-04-13T09:23:00.873404"}, {"id": "https://huggingface.co/posts/S-Dreamer/228566884248971", "image": "", "title": "PiFlash", "content_text": "PiFlash A simple web-based tool to flash Raspberry Pi OS images to your SD cards. No additional software required! S-Dreamer/piflash See translation", "url": "https://huggingface.co/posts/S-Dreamer/228566884248971", "date_published": "2025-04-13T09:23:00.873631"}, {"id": "https://huggingface.co/posts/etemiz/726325088198598", "image": "", "title": "It looks like Llama 4 team gamed the LMArena benchmarks by making their Maverick model output emojis, longer responses and ultra high enthusiasm! Is that ethical or not? They could certainly do a better job by working with teams like llama.cpp, just like Qwen team did with Qwen 3 before releasing the model.", "content_text": "It looks like Llama 4 team gamed the LMArena benchmarks by making their Maverick model output emojis, longer responses and ultra high enthusiasm! Is that ethical or not? They could certainly do a better job by working with teams like llama.cpp, just like Qwen team did with Qwen 3 before releasing the model. In 2024 I started playing with LLMs just before the release of Llama 3. I think Meta contributed a lot to this field and still contributing. Most LLM fine tuning tools are based on their models and also the inference tool llama.cpp has their name on it. The Llama 4 is fast and maybe not the greatest in real performance but still deserves respect. But my enthusiasm towards Llama models is probably because they rank highest on my AHA Leaderboard: https://sheet.zoho.com/sheet/open/mz41j09cc640a29ba47729fed784a263c1d08 Looks like they did a worse job compared to Llama 3.1 this time. Llama 3.1 has been on top for a while. Ranking high on my leaderboard is not correlated to...", "url": "https://huggingface.co/posts/etemiz/726325088198598", "date_published": "2025-04-13T09:23:00.874100"}, {"id": "https://huggingface.co/posts/Yehor/936075739202200", "image": "", "title": "I have made a Rust project with integration of the latest state-of-the-art model for object detection, it outperforms YOLO!", "content_text": "I have made a Rust project with integration of the latest state-of-the-art model for object detection, it outperforms YOLO! Check it out: https://github.com/egorsmkv/rf-detr-usls See translation", "url": "https://huggingface.co/posts/Yehor/936075739202200", "date_published": "2025-04-13T09:23:00.874334"}]}