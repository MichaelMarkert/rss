{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/aiqtech/202174985893140", "image": "", "title": "\u2728 High-Resolution Ghibli Style Image Generator \u2728", "content_text": "\u2728 High-Resolution Ghibli Style Image Generator \u2728 \ud83c\udf1f Introducing FLUX Ghibli LoRA Hello everyone! Today I'm excited to present a special LoRA model for FLUX Dev.1. This model leverages a LoRA trained on high-resolution Ghibli images for FLUX Dev.1 to easily create beautiful Ghibli-style images with stunning detail! \ud83c\udfa8 space: aiqtech/FLUX-Ghibli-Studio-LoRA model: openfree/flux-chatgpt-ghibli-lora \ud83d\udd2e Key Features Trained on High-Resolution Ghibli Images - Unlike other LoRAs, this one is trained on high-resolution images, delivering sharper and more beautiful results Powered by FLUX Dev.1 - Utilizing the latest FLUX model for faster generation and superior quality User-Friendly Interface - An intuitive UI that allows anyone to create Ghibli-style images with ease Diverse Creative Possibilities - Express various themes in Ghibli style, from futuristic worlds to fantasy elements \ud83d\uddbc\ufe0f Sample Images Include \"Ghibli style\" in your prompts Try combining nature, fantasy elements, futuristic...", "url": "https://huggingface.co/posts/aiqtech/202174985893140", "date_published": "2025-04-01T09:25:28.122217"}, {"id": "https://huggingface.co/posts/hanzla/237314499914963", "image": "", "title": "Hi all,", "content_text": "Hi all, Last week, I open sourced Free Search API. It allows sourcing results from top search engines (including google, bing) for free. It uses searxng instances for this purpose. I was overwhelmed by community's response and I am glad for all the support and suggestions. So today, I have pushed several improvements that make this API more stable. These improvements include 1) Parallel scrapping of search results for faster response 2) Markdown formatting of search results 3) Prioritizing SearXNG instances that have faster google response time 4) Update/Get endpoints for searxng instances. Github: https://github.com/HanzlaJavaid/Free-Search/tree/main Try the deployed version: https://freesearch.replit.app/docs I highly appreciate PRs, issues, stars, and any kind of feedback. Let's join hands, and make it real big! See translation", "url": "https://huggingface.co/posts/hanzla/237314499914963", "date_published": "2025-04-01T09:25:28.122620"}, {"id": "https://huggingface.co/posts/danielhanchen/465464088880734", "image": "", "title": "You can now run DeepSeek-V3-0324 on your own local device!", "content_text": "You can now run DeepSeek-V3-0324 on your own local device! Run our Dynamic 2.42 and 2.71-bit DeepSeek GGUFs: unsloth/DeepSeek-V3-0324-GGUF You can run them on llama.cpp and other inference engines. See our guide here: https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-v3-0324-locally See translation", "url": "https://huggingface.co/posts/danielhanchen/465464088880734", "date_published": "2025-04-01T09:25:28.122888"}, {"id": "https://huggingface.co/posts/thomwolf/431321353076398", "image": "", "title": "The new DeepSite space is really insane for vibe-coders", "content_text": "The new DeepSite space is really insane for vibe-coders enzostvs/deepsite With the wave of vibe-coding-optimized LLMs like the latest open-source DeepSeek model (version V3-0324), you can basically prompt out-of-the-box and create any app and game in one-shot. It feels so powerful to me, no more complex framework or under-the-hood prompt engineering to have a working text-to-app tool. AI is eating the world and *open-source* AI is eating AI itself! PS: and even more meta is that the DeepSite app and DeepSeek model are both fully open-source code => time to start recursively improve? PPS: you still need some inference hosting unless you're running the 600B param model at home, so check the very nice list of HF Inference Providers for this model: deepseek-ai/DeepSeek-V3-0324 See translation", "url": "https://huggingface.co/posts/thomwolf/431321353076398", "date_published": "2025-04-01T09:25:28.123246"}, {"id": "https://huggingface.co/posts/aiqtech/518766175001571", "image": "", "title": "\ud83e\udd17 Hug Contributors", "content_text": "\ud83e\udd17 Hug Contributors Hugging Face Contributor Dashboard \ud83d\udc68\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb aiqtech/Contributors-Leaderboard \ud83d\udcca Key Features Contributor Activity Tracking: Visualize yearly and monthly contributions through interactive calendars Top 100 Rankings: Provide rankings based on models, spaces, and dataset contributions Detailed Analysis: Analyze user-specific contribution patterns and influence Visualization: Understand contribution activities at a glance through intuitive charts and graphs \ud83c\udf1f Core Visualization Elements Contribution Calendar: Track activity patterns with GitHub-style heatmaps Radar Chart: Visualize balance between models, spaces, datasets, and activity levels Monthly Activity Graph: Identify most active months and patterns Distribution Pie Chart: Analyze proportion by contribution type \ud83c\udfc6 Ranking System Rankings based on overall contributions, spaces, and models Automatic badges for top 10, 30, and 100 contributors Ranking visualization to understand your position in the community \ud83d\udca1 How...", "url": "https://huggingface.co/posts/aiqtech/518766175001571", "date_published": "2025-04-01T09:25:28.123805"}, {"id": "https://huggingface.co/posts/stefan-it/513898057053383", "image": "", "title": "Wohoo \ud83e\udd73 I have finished my 2025 GPU workstation build and I am very excited to train new awesome open source models on it.", "content_text": "Wohoo \ud83e\udd73 I have finished my 2025 GPU workstation build and I am very excited to train new awesome open source models on it. I built my last GPU workstation 5 years ago featuring an AMD Ryzen 5900X, 64GB of G.SKILL Trident Z RGB on an ASRock X570 Taichi cooled by an Alphacool Eisb\u00e4r 420. GPU was a Zotac RTX 3090 AMP Extreme. Unfortunately, I was never satisfied with the case - some Fractal Define 7, as it is definitely too small, airflow is not optimal as I had to open the front door all the time and it also arrived with a partly damaged side panel. For my new build, I've used the following components: an outstanding new AMD Ryzen 9950X3D with 64GB of Corsair Dominator Titanium (what a name). As a huge Noctua fan - warm greetings to my Austrian neighbors - I am using the brand new Noctua NH-D15 G2 on an ASRock X870E Taichi in an amazing Lian Li LANCOOL III chassis. One joke that only NVIDIA Blackwell users will understand: you definitely need a tempered glass panel to check if your...", "url": "https://huggingface.co/posts/stefan-it/513898057053383", "date_published": "2025-04-01T09:25:28.124377"}, {"id": "https://huggingface.co/posts/openfree/214646053127729", "image": "", "title": "\ud83d\ude80 Gemma3-R1984-27B: Next Generation Agentic AI Platform", "content_text": "\ud83d\ude80 Gemma3-R1984-27B: Next Generation Agentic AI Platform Model Path: VIDraft/Gemma-3-R1984-27B Space: VIDraft/Gemma-3-R1984-27B git clone VIDraft/Gemma-3-R1984-27B \ud83d\udcab A New Frontier in AI Innovation Gemma3-R1984-27B is a powerful agentic AI platform built on Google's Gemma-3-27B model. It integrates state-of-the-art deep research via web search with multimodal file processing capabilities and handles long contexts up to 8,000 tokens. Designed for local deployment on independent servers using NVIDIA A100 GPUs, it provides high security and prevents data leakage. \ud83d\udd13 Uncensored and Unrestricted AI Experience Gemma3-R1984-27B comes with all censorship restrictions removed, allowing users to operate any persona without limitations. The model perfectly implements various roles and characters according to users' creative requests, providing unrestricted responses that transcend the boundaries of conventional AI. This unlimited interaction opens infinite possibilities across research, creative...", "url": "https://huggingface.co/posts/openfree/214646053127729", "date_published": "2025-04-01T09:25:28.125023"}, {"id": "https://huggingface.co/posts/Yehor/460907086720491", "image": "", "title": "Are you interesting in different runtimes for AI models?", "content_text": "Are you interesting in different runtimes for AI models? Check out IREE (iree.dev), it convert models to MLIR and then execute on different platforms. I have tested it in Rust on CPU and CUDA: https://github.com/egorsmkv/eerie-yolo11 See translation", "url": "https://huggingface.co/posts/Yehor/460907086720491", "date_published": "2025-04-01T09:25:28.125280"}, {"id": "https://huggingface.co/posts/AdinaY/252351292657061", "image": "", "title": "AReal-Boba \ud83d\udd25 a fully open RL Frameworks released by AntGroup, an affiliate company of Alibaba.", "content_text": "AReal-Boba \ud83d\udd25 a fully open RL Frameworks released by AntGroup, an affiliate company of Alibaba. inclusionAI/areal-boba-67e9f3fa5aeb74b76dcf5f0a \u2728 7B/32B - Apache2.0 \u2728 Outperform on math reasoning \u2728 Replicating QwQ-32B with 200 data under $200 \u2728 All-in-one: weights, datasets, code & tech report See translation", "url": "https://huggingface.co/posts/AdinaY/252351292657061", "date_published": "2025-04-01T09:25:28.125583"}, {"id": "https://huggingface.co/posts/Wauplin/747413191251683", "image": "", "title": "\u203c\ufe0f huggingface_hub's v0.30.0 is out with our biggest update of the past two years!", "content_text": "\u203c\ufe0f huggingface_hub's v0.30.0 is out with our biggest update of the past two years! Full release notes: https://github.com/huggingface/huggingface_hub/releases/tag/v0.30.0 . \ud83d\ude80 Ready. Xet. Go! Xet is a groundbreaking new protocol for storing large objects in Git repositories, designed to replace Git LFS. Unlike LFS, which deduplicates files, Xet operates at the chunk level\u2014making it a game-changer for AI builders collaborating on massive models and datasets. Our Python integration is powered by [xet-core]( https://github.com/huggingface/xet-core ), a Rust-based package that handles all the low-level details. You can start using Xet today by installing the optional dependency: pip install -U huggingface_hub[hf_xet] With that, you can seamlessly download files from Xet-enabled repositories! And don\u2019t worry\u2014everything remains fully backward-compatible if you\u2019re not ready to upgrade yet. Blog post: https://huggingface.co/blog/xet-on-the-hub Docs:...", "url": "https://huggingface.co/posts/Wauplin/747413191251683", "date_published": "2025-04-01T09:25:28.126217"}]}