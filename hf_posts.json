{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/mlabonne/713929804806596", "image": "", "title": "New family of 1B models just dropped!", "content_text": "New family of 1B models just dropped! > LiquidAI/LFM2.5-1.2B-Base : 10T \u2192 28T tokens > LiquidAI/LFM2.5-1.2B-Instruct : new large-scale multi-stage RL > LiquidAI/LFM2.5-1.2B-JP : our most polite model > LiquidAI/LFM2.5-VL-1.6B : multi-image multilingual > LiquidAI/LFM2.5-Audio-1.5B : 8x times faster, no quality loss Super proud of this release \ud83e\udd17 See translation", "url": "https://huggingface.co/posts/mlabonne/713929804806596", "date_published": "2026-01-07T05:30:00.952746"}, {"id": "https://huggingface.co/posts/tsungyi/839679910683551", "image": "", "title": "Big news from CES \u2014 Cosmos Reason 2 is here \u2014 our most advanced reasoning vision-language model for physical AI, now topping the Physical AI Bench leaderboard\ud83c\udfc6", "content_text": "Big news from CES \u2014 Cosmos Reason 2 is here \u2014 our most advanced reasoning vision-language model for physical AI, now topping the Physical AI Bench leaderboard\ud83c\udfc6 shi-labs/physical-ai-bench-leaderboard What\u2019s new: - Enhanced physical reasoning & spatio-temporal understanding - Flexible deployment with 2B & 8B model sizes - Long-context understanding (up to 256K tokens) - Object detection with 2D/3D point localizations and trajectory data - New Cosmos Cookbook Recipes for faster onboarding Read the full blog \ud83d\udcd6 https://huggingface.co/blog/nvidia/nvidia-cosmos-reason-2-brings-advanced-reasoning Download Cosmos Reason 2 \ud83d\udc49 nvidia/Cosmos-Reason2-8B On top of Cosmos Reason 2, we also rolled out other new updates, including: - Cosmos Predict 2.5 \u2013 Unified Text2World/Image2World/Video2World model for higher-quality synthetic video worlds - Cosmos Transfer 2.5-2B \u2013 Lightweight, high-fidelity world-to-world translation with stronger physics alignment - NVIDIA GR00T N1.6 \u2013 Open robot foundation...", "url": "https://huggingface.co/posts/tsungyi/839679910683551", "date_published": "2026-01-07T05:30:00.953312"}, {"id": "https://huggingface.co/posts/AdinaY/239073779898851", "image": "", "title": "Chinese open source AI in December 2025 was about the stack coming together: open, end to end, and ready to ship \ud83d\udd25", "content_text": "Chinese open source AI in December 2025 was about the stack coming together: open, end to end, and ready to ship \ud83d\udd25 https://huggingface.co/collections/zh-ai-community/december-2025-china-open-source-highlights \u2728 Big wave of foundation models: still scaling, but efficiency, reasoning, and deployment now matter more than size - DeepSeek-V3.2 - Z.ai GLM-4.7 - MiniMax-M2.1 - Xiaomi: MiMo-V2-Flash \u2728 Multimodal reasoning is now default - Z.ai GLM-4.6V - Z.ai AutoGLM-Phone 9B - Bytedance: Dolphin-v2 \u2728 Image & video: editable assets and real workflows - Qwen-Image-Layered / Image-2512 - Meituan: LongCat-Image & Image Edit - AIDC: Ovis-Image-7B - Live-Avatar / LongCat-Video-Avatar - HY-WorldPlay / RealVideo \u2728 Audio goes edge ready - GLM-ASR-Nano / Fun-ASR-Nano - GLM-TTS / VoxCPM1.5 - CosyVoice 0.5B \u2728 The quiet backbone: data & infrastructure - Finch (FinWorkBench) - Tencent ARC: TimeLens-100K - BIGAI: TongSIM-Asset - MiniMax: VTP-Base \u2728 Also congrats on Minimax and Z.ai announced their IPOs...", "url": "https://huggingface.co/posts/AdinaY/239073779898851", "date_published": "2026-01-07T05:30:00.953856"}, {"id": "https://huggingface.co/posts/Hellohal2064/482165788562535", "image": "", "title": "\ud83d\ude80 Excited to share: The vLLM container for NVIDIA DGX Spark!", "content_text": "\ud83d\ude80 Excited to share: The vLLM container for NVIDIA DGX Spark! I've been working on getting vLLM to run natively on the new DGX Spark with its GB10 Blackwell GPU (SM121 architecture). The results? 2.5x faster inference compared to llama.cpp! \ud83d\udcca Performance Highlights: \u2022 Qwen3-Coder-30B: 44 tok/s (vs 21 tok/s with llama.cpp) \u2022 Qwen3-Next-80B: 45 tok/s (vs 18 tok/s with llama.cpp) \ud83d\udd27 Technical Challenges Solved: \u2022 Built PyTorch nightly with CUDA 13.1 + SM121 support \u2022 Patched vLLM for Blackwell architecture \u2022 Created custom MoE expert configs for GB10 \u2022 Implemented TRITON_ATTN backend workaround \ud83d\udce6 Available now: \u2022 Docker Hub: docker pull hellohal2064/vllm-dgx-spark-gb10:latest \u2022 HuggingFace: huggingface.co/Hellohal2064/vllm-dgx-spark-gb10 The DGX Spark's 119GB unified memory opens up possibilities for running massive models locally. Happy to connect with others working on the DGX Spark Blackwell! See translation", "url": "https://huggingface.co/posts/Hellohal2064/482165788562535", "date_published": "2026-01-07T05:30:00.954317"}, {"id": "https://huggingface.co/posts/pcuenq/421927498996428", "image": "", "title": "\ud83d\udc49 What happened in AI in 2025? \ud83d\udc48", "content_text": "\ud83d\udc49 What happened in AI in 2025? \ud83d\udc48 We prepared the 2025 version of the HF AI Timeline Grid, highlighting open vs API-based model releases, and allowing you to browse and filter by access, modality, and release type! Play with it here: 2025-ai-timeline/2025-ai-timeline Here's my personal quarterly TL;DR: 1\ufe0f\u20e3 Q1 \u2014 Learning to Reason Deepseek not only releases a top-notch reasoning model, but shows how to train them and compete with closed frontier models. OpenAI debuts Deep Research. Significant milestones: DeepSeek R1 & R1-Zero, Qwen 2.5 VL, OpenAI Deep Research, Gemini 2.5 Pro (experimental) 2\ufe0f\u20e3 Q2 \u2014 Multimodality and Coding More LLMs embrace multimodality by default, and there's a surge in coding agents. Strong vision, audio, and generative models emerge. Significant milestones: Llama 4, Qwen 3, Imagen 4, OpenAI Codex, Google Jules, Claude 4 3\ufe0f\u20e3 Q3 \u2014 \"Gold\" rush, OpenAI opens up, the community goes bananas Flagship models get gold in Math olympiads and hard benchmarks. OpenAI...", "url": "https://huggingface.co/posts/pcuenq/421927498996428", "date_published": "2026-01-07T05:30:00.954999"}, {"id": "https://huggingface.co/posts/AdinaY/447609737740306", "image": "", "title": "2025.1  - DeepSeek entered the scene, backed by High Flyer Quant", "content_text": "2025.1 - DeepSeek entered the scene, backed by High Flyer Quant 2026.1 - IQuest enters the game, backed by Uniquant Quant \ud83d\udcc8 and launching IQuest-Coder on huggingface https://huggingface.co/collections/IQuestLab/iquest-coder \u2728 40B models: Instruct / Thinking / Loop \u2728 Loop = MoE-level performance with only ~5% extra training cost \u2728 Native 128K context See translation", "url": "https://huggingface.co/posts/AdinaY/447609737740306", "date_published": "2026-01-07T05:30:00.955307"}, {"id": "https://huggingface.co/posts/CRAFTFramework/385180561862993", "image": "", "title": "\ud83d\udd27 Behind the Scenes", "content_text": "\ud83d\udd27 Behind the Scenes CRAFT development: Sun-Mon only (full-time job Wed-Sat). 48 projects completed solo. The time constraint is the proof\u2014if the methodology works under pressure, it works. Beta: February 2026 \u2192 craftframework.ai See translation", "url": "https://huggingface.co/posts/CRAFTFramework/385180561862993", "date_published": "2026-01-07T05:30:00.955561"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/816784228667629", "image": "", "title": "Qwen Image 2512 & 2511 Training Results Are Next Level + 33 ComfyUI Presets for Image & Video Gen", "content_text": "Qwen Image 2512 & 2511 Training Results Are Next Level + 33 ComfyUI Presets for Image & Video Gen Full tutorial: https://www.youtube.com/watch?v=RcoXd9v1t_c Qwen Image 2512 Text to Image model is a massive upgrade in quality just as Qwen Image Edit 2511 was for image editing tasks based on commands. I have trained both Qwen Image older Base, Qwen Image 2512 new base, Qwen Image 2509 older edit model and Qwen Image 2511 newer edit model and compared them in this video. The results are astonishing. Moreover, we have converted all of our premium SwarmUI image and video generation presets into ComfyUI workflows. Just drag and drop the workflow and start using immediately. All models are downloaded with our premium 1-click to use model downloader app. ComfyUI and SwarmUI are also installed with 1-click installers and our ComfyUI fully supports Sage Attention, Flash Attention, xFormers, Triton and more with RTX 5000 series and all GPUs like 3000, 2000, 4000 series, etc for both Linux and...", "url": "https://huggingface.co/posts/MonsterMMORPG/816784228667629", "date_published": "2026-01-07T05:30:00.956221"}, {"id": "https://huggingface.co/posts/RakshitAralimatti/360718325875789", "image": "", "title": "One of the most practical and genuinely useful use cases of agentic systems is a research assistant.", "content_text": "One of the most practical and genuinely useful use cases of agentic systems is a research assistant. I built a Deep Research multi-agent system using NVIDIA\u2019s Nemotron-3-Nano-30B-A3B model and CrewAI. Try it out yourself \ud83d\udc47 \ud83d\udd17 GitHub: https://github.com/rakshit2020/Deep-Research-Agent-using-CrewAI What truly made this system feel next-level was powering it with NVIDIA Nemotron-3-Nano-30B-A3B, its built for real-world agentic applications. The agentic system I built: 1. First talks to you and clarifies what you actually want, removing ambiguity 2. Then creates a proper research plan based on that clarity 3. Performs deep research using web search and content extraction tools 4. Finally produces a well-structured research report grounded in sources See translation", "url": "https://huggingface.co/posts/RakshitAralimatti/360718325875789", "date_published": "2026-01-07T05:30:00.956604"}, {"id": "https://huggingface.co/posts/kanaria007/128065561044409", "image": "", "title": "\u2705 New Article: *Operating an SI-Core (v0.1)*", "content_text": "\u2705 New Article: *Operating an SI-Core (v0.1)* Title: \ud83d\udee0\ufe0f Operating SI-Core: Dashboards, Playbooks, and Human Loops \ud83d\udd17 https://huggingface.co/blog/kanaria007/operating-si-core --- Summary: Designing an SI-Core is only half the job \u2014 the other half is *running it safely at 03:00*. This guide is a *non-normative ops runbook* for SRE/Ops teams and governance owners: what to put on the *one-page dashboard*, how to wire *alerts \u2192 actions*, when to use *safe-mode*, and how to answer the question that always arrives after an incident: > \u201cWhy did the system do *that*?\u201d --- Why It Matters: \u2022 Turns \u201cauditable AI\u201d into *operational reality* (not a slide deck) \u2022 Makes *ethics + rollback* measurable, actionable, and drillable \u2022 Clarifies how humans stay in the loop without becoming the bottleneck \u2022 Provides templates for *postmortems, escalation, and regulator-grade explanations* --- What\u2019s Inside: *Core Ops Dashboard (1 page):* \u2022 Determinism/consistency, ethics/oversight, rollback/recovery,...", "url": "https://huggingface.co/posts/kanaria007/128065561044409", "date_published": "2026-01-07T05:30:00.957207"}]}