{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/nouamanetazi/972464132222376", "image": "", "title": "After training \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25\ud835\udc0b\ud835\udc0c\ud835\udfd1 on \ud835\udfd1\ud835\udfd6\ud835\udfd2 \ud835\udc07\ud835\udfcf\ud835\udfce\ud835\udfce\ud835\udc2c for nearly a month, I've come to realize something most people overlook: \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc24\ud835\udc1e-\ud835\udc28\ud835\udc2b-\ud835\udc1b\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc24 \ud835\udc1f\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc22\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20. \ud83d\udd25", "content_text": "After training \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25\ud835\udc0b\ud835\udc0c\ud835\udfd1 on \ud835\udfd1\ud835\udfd6\ud835\udfd2 \ud835\udc07\ud835\udfcf\ud835\udfce\ud835\udfce\ud835\udc2c for nearly a month, I've come to realize something most people overlook: \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc24\ud835\udc1e-\ud835\udc28\ud835\udc2b-\ud835\udc1b\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc24 \ud835\udc1f\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc22\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20. \ud83d\udd25 Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious \ud835\udc0d\ud835\udc02\ud835\udc02\ud835\udc0b \ud835\udc1e\ud835\udc2b\ud835\udc2b\ud835\udc28\ud835\udc2b\ud835\udc2c, or when your expensive GPU cluster is running at \ud835\udfd4\ud835\udfce% \ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc32, the problem isn't your model. It's most probably a \ud835\udc26\ud835\udc22\ud835\udc2c\ud835\udc2e\ud835\udc2c\ud835\udc1e \ud835\udc28\ud835\udc1f \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc21\ud835\udc1a\ud835\udc2b\ud835\udc1d\ud835\udc30\ud835\udc1a\ud835\udc2b\ud835\udc1e. \ud83d\udee0\ufe0f Questions that seemed simple but had no clear answers: Why is \ud835\udc0c\ud835\udc28\ud835\udc04 \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc2c\ud835\udc25\ud835\udc28\ud835\udc30\ud835\udc1e\ud835\udc2b \ud835\udc2d\ud835\udc21\ud835\udc1a\ud835\udc27 \ud835\udc1d\ud835\udc1e\ud835\udc27\ud835\udc2c\ud835\udc1e \ud835\udc26\ud835\udc28\ud835\udc1d\ud835\udc1e\ud835\udc25\ud835\udc2c? Which \ud835\udc0d\ud835\udc02\ud835\udc02\ud835\udc0b \ud835\udc1f\ud835\udc25\ud835\udc1a\ud835\udc20\ud835\udc2c should we actually set? How often should we checkpoint without killing throughput? That's why we built \ud835\udc13\ud835\udc21\ud835\udc1e \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25 \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc0f\ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1b\ud835\udc28\ud835\udc28\ud835\udc24 \ud83d\udcd6: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1e\ud835\udc2b that most teams get wrong. We validated real vs...", "url": "https://huggingface.co/posts/nouamanetazi/972464132222376", "date_published": "2025-11-01T09:21:58.304534"}, {"id": "https://huggingface.co/posts/piercus/167394123498038", "image": "", "title": "Starts erasing! \ud83c\udf89 \ud83c\udf89 \ud83c\udf89", "content_text": "Starts erasing! \ud83c\udf89 \ud83c\udf89 \ud83c\udf89 This is made with a one-step SD1.5 LBM [1] eraser ! Data is open. Data pipeline is open. Training code is open. On our LBM fork : https://github.com/finegrain-ai/LBM [1] LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) See translation", "url": "https://huggingface.co/posts/piercus/167394123498038", "date_published": "2025-11-01T09:21:58.304824"}, {"id": "https://huggingface.co/posts/meg/795374277994612", "image": "", "title": "\ud83e\udd16 Did you know your voice might be cloned without your consent from just *one sentence* of audio?", "content_text": "\ud83e\udd16 Did you know your voice might be cloned without your consent from just *one sentence* of audio? That's not great. So with @ frimelle , we brainstormed a new idea for developers who want to curb malicious use: \u2728The Voice Consent Gate.\u2728 Details, code, here: https://huggingface.co/blog/voice-consent-gate See translation", "url": "https://huggingface.co/posts/meg/795374277994612", "date_published": "2025-11-01T09:21:58.305108"}, {"id": "https://huggingface.co/posts/sergiopaniego/207791817757812", "image": "", "title": "Sharing the slides from yesterday's talk about \"Fine Tuning with TRL\" from the", "content_text": "Sharing the slides from yesterday's talk about \"Fine Tuning with TRL\" from the @ TogetherAgent x @ huggingface workshop we hosted in our Paris office \ud83c\udf83! Link: https://github.com/sergiopaniego/talks/blob/main/fine_tuning_with_trl/Fine%20tuning%20with%20TRL%20(Oct%2025).pdf See translation", "url": "https://huggingface.co/posts/sergiopaniego/207791817757812", "date_published": "2025-11-01T09:21:58.305366"}, {"id": "https://huggingface.co/posts/wang12390/946400201713761", "image": "", "title": "AI Speedpainting of a Tranquil Mountain Temple!", "content_text": "AI Speedpainting of a Tranquil Mountain Temple! Just upload one image then it will generate hand-drawn video. Please watch till the end, if you like the result, please upvote. See translation", "url": "https://huggingface.co/posts/wang12390/946400201713761", "date_published": "2025-11-01T09:21:58.305578"}, {"id": "https://huggingface.co/posts/DmitryRyumin/744756733617336", "image": "", "title": "\ud83d\ude80\ud83d\udc4c\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\udd0c\ud83d\ude80", "content_text": "\ud83d\ude80\ud83d\udc4c\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\udd0c\ud83d\ude80 \ud83d\udcc4 Title: Understanding Co-speech Gestures in-the-wild \ud83d\udd1d \ud83d\udcdd Description: JEGAL is a tri-modal model that learns from gestures, speech and text simultaneously, enabling devices to interpret co-speech gestures in the wild. \ud83d\udc65 Authors: @ sindhuhegde , K R Prajwal, Taein Kwon, and Andrew Zisserman \ud83d\udcc5 Conference: ICCV, 19 \u2013 23 Oct, 2025 | Honolulu, Hawai'i, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: Understanding Co-speech Gestures in-the-wild (2503.22668) \ud83c\udf10 Web Page: https://www.robots.ox.ac.uk/~vgg/research/jegal \ud83d\udcc1 Repository: https://github.com/Sindhu-Hegde/jegal \ud83d\udcfa Video: https://www.youtube.com/watch?v=TYFOLKfM-rM \ud83d\ude80 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers \ud83d\ude80 Added to the Human Modeling Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/human-modeling.md \ud83d\udcda More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers curated by @ DmitryRyumin \ud83d\udd0d Keywords:...", "url": "https://huggingface.co/posts/DmitryRyumin/744756733617336", "date_published": "2025-11-01T09:21:58.306040"}, {"id": "https://huggingface.co/posts/pagezyhf/128586778684407", "image": "", "title": "\ud83d\ude80 Big news for AI builders!", "content_text": "\ud83d\ude80 Big news for AI builders! We\u2019re thrilled to announce that the Qwen3-VL family of vision-language models is now available on Azure AI Foundry, thanks to our collaboration with Microsoft. We bring open-source innovation to enterprise-grade AI infrastructure, making it easier than ever for enterprise to deploy and scale the latest and greatest from models from hugging Face securely within Azure. \ud83d\udd0d Highlights: - Deploy Qwen3-VL instantly via managed endpoints - Built-in governance, telemetry, and lifecycle management - True multimodal reasoning \u2014 vision, language, and code understanding - State-of-the-art performance, outperforming closed-source models like Gemini 2.5 Pro and GPT-5 - Available in both *Instruct* and *Thinking* modes, across 24 model sizes \ud83d\udc49 Get started today: search for Qwen3-VL in the Hugging Face Collection on Azure AI Foundry. See translation", "url": "https://huggingface.co/posts/pagezyhf/128586778684407", "date_published": "2025-11-01T09:21:58.306438"}, {"id": "https://huggingface.co/posts/DmitryRyumin/213442382070723", "image": "", "title": "\ud83d\ude80\ud83d\udca1\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\ude84\ud83d\ude80", "content_text": "\ud83d\ude80\ud83d\udca1\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\ude84\ud83d\ude80 \ud83d\udcc4 Title: LoftUp: Learning a Coordinate-based Feature Upsampler for Vision Foundation Models \ud83d\udd1d \ud83d\udcdd Description: LoftUp is a coordinate-based transformer that upscales the low-resolution features of VFMs (e.g. DINOv2 and CLIP) using cross-attention and self-distilled pseudo-ground truth (pseudo-GT) from SAM. \ud83d\udc65 Authors: Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang \ud83d\udcc5 Conference: ICCV, 19 \u2013 23 Oct, 2025 | Honolulu, Hawai'i, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models (2504.14032) \ud83c\udf10 Github Page: https://andrehuang.github.io/loftup-site \ud83d\udcc1 Repository: https://github.com/andrehuang/loftup \ud83d\ude80 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers \ud83d\ude80 Added to the Foundation Models and Representation Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/foundation-models-and-representation-learning.md \ud83d\udcda...", "url": "https://huggingface.co/posts/DmitryRyumin/213442382070723", "date_published": "2025-11-01T09:21:58.306929"}, {"id": "https://huggingface.co/posts/onekq/456763679689481", "image": "", "title": "Kimi K2 is a bit disappointing by my expectations. It is on a par with Codex mini.", "content_text": "Kimi K2 is a bit disappointing by my expectations. It is on a par with Codex mini. onekq-ai/WebApp1K-models-leaderboard See translation", "url": "https://huggingface.co/posts/onekq/456763679689481", "date_published": "2025-11-01T09:21:58.307147"}, {"id": "https://huggingface.co/posts/prithivMLmods/710644146568512", "image": "", "title": "A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on \u2197\ufe0f", "content_text": "A small blog post titled - Hall of Multimodal OCR VLMs and Demonstrations has been published on \u2197\ufe0f https://huggingface.co/blog/prithivMLmods/multimodal-ocr-vlms on behalf of strangervisionhf It discusses the latest trends in OCR models, the multilingual support offered by modern OCR systems, their unique capabilities, OCR benchmark model comparisons, transformer-based implementations, and strategies for streamlining transformers compatibility. See translation", "url": "https://huggingface.co/posts/prithivMLmods/710644146568512", "date_published": "2025-11-01T09:21:58.307443"}]}