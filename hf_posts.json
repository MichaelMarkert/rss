{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/clem/238420842235482", "image": "", "title": "We just crossed 1,500,000 public models on Hugging Face (and 500k spaces, 330k datasets, 50k papers). One new repository is created every 15 seconds. Congratulations all!", "content_text": "We just crossed 1,500,000 public models on Hugging Face (and 500k spaces, 330k datasets, 50k papers). One new repository is created every 15 seconds. Congratulations all! See translation", "url": "https://huggingface.co/posts/clem/238420842235482", "date_published": "2025-03-16T05:19:20.599648"}, {"id": "https://huggingface.co/posts/openfree/269373246432749", "image": "", "title": "\ud83d\ude80 Idea Transformer:", "content_text": "\ud83d\ude80 Idea Transformer: Idea Transformer: Infinity is an innovative tool that unlocks infinite creativity by generating unique transformation ideas and design images from up to three keywords and a chosen category. Leveraging a state-of-the-art diffusion pipeline, real-time translation, and a powerful LLM, it delivers fresh ideas every time. \ud83c\udfa8\u2728 openfree/Idea-Transformer Key Features Diverse Ideas: Randomly selects creative variations from your keywords and category \u2014 the possibilities are nearly endless! \ud83c\udfb2 Unique Design Images: Your text prompt produces striking, varied design images via the diffusion model. \ud83d\uddbc\ufe0f Real-Time Translation & Expansion: Korean inputs are automatically translated and enriched using an advanced LLM for high-quality output. \ud83d\udd04 Dual-Language Support: Enjoy an intuitive Gradio interface with separate English and Korean tabs for a global audience. \ud83c\udf0d Explore a Wide Range of Categories: Sensor Functions \ud83d\udce1: Creative changes in sensor technologies. Size & Shape Change \ud83d\udccf:...", "url": "https://huggingface.co/posts/openfree/269373246432749", "date_published": "2025-03-16T05:19:20.600342"}, {"id": "https://huggingface.co/posts/KaiChen1998/260621109030490", "image": "", "title": "\ud83d\udce2 Our EMOVA paper has been accepted by CVPR 2025, and we are glad to release all resources, including code (training & inference), datasets (training & evaluation), and checkpoints (EMOVA-3B/7B/72B)!", "content_text": "\ud83d\udce2 Our EMOVA paper has been accepted by CVPR 2025, and we are glad to release all resources, including code (training & inference), datasets (training & evaluation), and checkpoints (EMOVA-3B/7B/72B)! \ud83e\udd17 EMOVA is a novel end-to-end omni-modal LLM that can see, hear and speak. Given omni-modal (i.e., textual, visual and speech) inputs, EMOVA can generate both textual and speech responses with vivid emotional controls by utilizing the speech decoder and a style controller. \u2728 EMOVA Highlights \u2705 State-of-the-art omni-modality: EMOVA achieves SoTA comparable results on both vision-language and speech benchmarks simultaneously. \u2705 Device adaptation: our codebase supports training/inference on both NVIDIA GPUs (e.g., A800 & H20) and Ascend NPUs (e.g., 910B3)! \u2705 Modular design: we integrate multiple implementations of vision encoder, vision projector, and language model. \ud83d\udd25 You are all welcome to try and star! - Project page: https://emova-ollm.github.io/ - Github: https://github.com/emova-...", "url": "https://huggingface.co/posts/KaiChen1998/260621109030490", "date_published": "2025-03-16T05:19:20.600811"}, {"id": "https://huggingface.co/posts/nroggendorff/181073331446889", "image": "", "title": "to the nvidia employee that won't respond to my emails: hear me now.", "content_text": "to the nvidia employee that won't respond to my emails: hear me now. you have made a semi-powerful to irrelevant enemy. you have been warned See translation", "url": "https://huggingface.co/posts/nroggendorff/181073331446889", "date_published": "2025-03-16T05:19:20.601058"}, {"id": "https://huggingface.co/posts/hanzla/186651630886849", "image": "", "title": "Hello community,", "content_text": "Hello community, I want to share my work of creating a reasoning mamba model I used GRPO over Falcon3 Mamba Instruct to make this model. It generates blazing fast response while building good logic to answer challenging questions. Give it a try: Model repo: hanzla/Falcon3-Mamba-R1-v0 Space: hanzla/Falcon3MambaReasoner Looking forward to community feedback. See translation", "url": "https://huggingface.co/posts/hanzla/186651630886849", "date_published": "2025-03-16T05:19:20.601322"}, {"id": "https://huggingface.co/posts/Jaward/782050409908004", "image": "", "title": "This is the most exciting of this week\u2019s release for me: Gemini Robotics - A SOTA generalist Vision-Language-Action model that brings intelligence to the physical world. It comes with a verifiable real-world knowledge Embodied Reasoning QA benchmark. Cool part is that the model can be specialized with fast adaptation to new tasks and have such adaptations transferred to new robot embodiment like humanoids. Looking forward to the model and data on hf, it\u2019s about time I go full physical:)", "content_text": "This is the most exciting of this week\u2019s release for me: Gemini Robotics - A SOTA generalist Vision-Language-Action model that brings intelligence to the physical world. It comes with a verifiable real-world knowledge Embodied Reasoning QA benchmark. Cool part is that the model can be specialized with fast adaptation to new tasks and have such adaptations transferred to new robot embodiment like humanoids. Looking forward to the model and data on hf, it\u2019s about time I go full physical:) Technical Report: https://storage.googleapis.com/deepmind-media/gemini-robotics/gemini_robotics_report.pdf See translation", "url": "https://huggingface.co/posts/Jaward/782050409908004", "date_published": "2025-03-16T05:19:20.601657"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/310882641508042", "image": "", "title": "Think building custom digital twins for AI training is hard? Let us show you how to make it easy!", "content_text": "Think building custom digital twins for AI training is hard? Let us show you how to make it easy! Next week, Duality AI is offering a free \"Creating Your Own 4-Wheeled Vehicle Digital Twins for AI Training with Falcon Editor\" live class. Sign up here: https://forms.gle/2U5xugMjvSkZdeaR8 What we'll cover: \ud83d\ude97 Import & Configure a rigged 4-wheeled vehicle and transform it into a controllable system twin using Blueprints. \ud83d\ude97 Enable Dynamic Control by exposing Python variables for real-time adjustments. \ud83d\ude97 Attach Sensors to capture valuable simulation data. \ud83d\ude97 Assemble & Run a Simulation Scenario to generate training data for AI & robotics applications. See how Falcon creates synthetic data for faster, easier, and more targeted AI training by creating a FREE account here: https://www.duality.ai/edu See translation", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/310882641508042", "date_published": "2025-03-16T05:19:20.602072"}, {"id": "https://huggingface.co/posts/MJannik/453920098995289", "image": "", "title": "I've published an article showing five ways to use \ud83e\udea2 Langfuse with \ud83e\udd17 Hugging Face.", "content_text": "I've published an article showing five ways to use \ud83e\udea2 Langfuse with \ud83e\udd17 Hugging Face. My personal favorite is Method #4: Using Hugging Face Datasets for Langfuse Dataset Experiments. This lets you benchmark your LLM app or AI agent with a dataset hosted on Hugging Face. In this example, I chose the GSM8K dataset ( openai/gsm8k ) to test the mathematical reasoning capabilities of my smolagent :) Link to the Article here on HF: https://huggingface.co/blog/MJannik/hugging-face-and-langfuse See translation", "url": "https://huggingface.co/posts/MJannik/453920098995289", "date_published": "2025-03-16T05:19:20.602405"}, {"id": "https://huggingface.co/posts/burtenshaw/988620578150041", "image": "", "title": "Still speed running Gemma 3 to think. Today I focused on setting up gpu poor hardware to run GRPO.", "content_text": "Still speed running Gemma 3 to think. Today I focused on setting up gpu poor hardware to run GRPO. This is a plain TRL and PEFT notebook which works on mac silicone or colab T4. This uses the 1b variant of Gemma 3 and a reasoning version of GSM8K dataset. \ud83e\uddd1\u200d\ud83c\udf73 There\u2019s more still in the oven like releasing models, an Unsloth version, and deeper tutorials, but hopefully this should bootstrap your projects. Here\u2019s a link to the 1b notebook: https://colab.research.google.com/drive/1mwCy5GQb9xJFSuwt2L_We3eKkVbx2qSt?usp=sharing See translation", "url": "https://huggingface.co/posts/burtenshaw/988620578150041", "date_published": "2025-03-16T05:19:20.602733"}, {"id": "https://huggingface.co/posts/m-ric/783111989290994", "image": "", "title": "smolagents now support vLLM! \ud83e\udd73", "content_text": "smolagents now support vLLM! \ud83e\udd73 As one of the most popular local inference solutions, the community had been asking us to integrate vLLM: after a heavy refactoring of our LLM classes, we've just released smolagents 1.11.0, with a brand new VLLMModel class. Go try it and tell us what you think! https://github.com/huggingface/smolagents/blob/45b2c86857b7f7657daaa74e4d17d347e9e2c4a4/src/smolagents/models.py#L497 See translation", "url": "https://huggingface.co/posts/m-ric/783111989290994", "date_published": "2025-03-16T05:19:20.603043"}]}