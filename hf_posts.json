{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/DawnC/810522223628641", "image": "", "title": "SceneWeaver \u2014 AI-Powered Background Generation & Image Composition \ud83c\udfa8\u2728", "content_text": "SceneWeaver \u2014 AI-Powered Background Generation & Image Composition \ud83c\udfa8\u2728 Transform ordinary portraits into professional studio shots with just one click! What can SceneWeaver do? - \ud83d\udcf8 Upload any portrait photo and instantly generate stunning, professional-quality backgrounds - \ud83c\udfad Smart Subject Detection \u2014 Automatically identifies and extracts people, pets, or objects from your photos, even handling tricky cases like dark clothing and cartoon characters. - \ud83c\udf04 Creative Scene Library \u2014 Choose from 24 professionally curated backgrounds spanning offices, nature landscapes, urban settings, artistic styles, and seasonal themes, or describe your own custom vision. - \u2699\ufe0f Professional Results \u2014 Delivers studio-quality compositions in seconds, saving hours of manual editing work while maintaining natural lighting and color harmony. What's next? \ud83c\udfac Enhanced context-aware generation \ud83c\udfa8 Batch processing for multiple style variations \ud83d\udd27 Higher resolution output support \ud83c\udf10 Accessible cloud deployment Current...", "url": "https://huggingface.co/posts/DawnC/810522223628641", "date_published": "2025-11-24T09:30:41.864163"}, {"id": "https://huggingface.co/posts/prithivMLmods/585236174284354", "image": "", "title": "Introducing the advanced sketch-board editor \"Nano-Banana-Pro-Sketch-Board\" powered by the Gemini 2.5 Flash Image and Gemini 3 Pro Preview Image models through the Gemini API. This version includes more features than the Nano-Banana-AIO app for drawing and prompt-based concept transformation of freestyle sketches. \ud83d\udd25\ud83c\udf4c", "content_text": "Introducing the advanced sketch-board editor \"Nano-Banana-Pro-Sketch-Board\" powered by the Gemini 2.5 Flash Image and Gemini 3 Pro Preview Image models through the Gemini API. This version includes more features than the Nano-Banana-AIO app for drawing and prompt-based concept transformation of freestyle sketches. \ud83d\udd25\ud83c\udf4c \u2728Nano-Banana-Pro-Sketch-Board: prithivMLmods/Nano-Banana-Pro-Sketch-Board \u2728Collection: https://huggingface.co/collections/prithivMLmods/image-generation-apps-collection \u2728Github: https://github.com/PRITHIVSAKTHIUR/Nano-Banana-Pro-Sketch-Board \u2728Model-Garden: https://tinyurl.com/4xxs9dvy Some Other Relevant Apps [OSS] \u2b50Qwen-Image-Edit-2509-LoRAs-Fast-Fusion: prithivMLmods/Qwen-Image-Edit-2509-LoRAs-Fast-Fusion \u2b50Qwen-Image-Edit-2509-LoRAs-Fast: prithivMLmods/Qwen-Image-Edit-2509-LoRAs-Fast \u2b50Photo-Mate-i2i: prithivMLmods/Photo-Mate-i2i \u2b50Kontext-Photo-Mate-v2: prithivMLmods/Kontext-Photo-Mate-v2 Note: The Nano-Banana-Pro-Sketch-Board demo requires a Gemini API key for the...", "url": "https://huggingface.co/posts/prithivMLmods/585236174284354", "date_published": "2025-11-24T09:30:41.864668"}, {"id": "https://huggingface.co/posts/mitkox/488545088120873", "image": "", "title": "I run 20 AI coding agents locally on my desktop workstation at 400+ tokens/sec with MiniMax-M2. It\u2019s a Sonnet drop-in replacement in my Cursor, Claude Code, Droid, Kilo and Cline peak at 11k tok/sec input and 433 tok/s output, can generate 1B+ tok/m.All with 196k context window. I'm running it for 6 days now with this config.", "content_text": "I run 20 AI coding agents locally on my desktop workstation at 400+ tokens/sec with MiniMax-M2. It\u2019s a Sonnet drop-in replacement in my Cursor, Claude Code, Droid, Kilo and Cline peak at 11k tok/sec input and 433 tok/s output, can generate 1B+ tok/m.All with 196k context window. I'm running it for 6 days now with this config. Today max performance was stable at 490.2 tokens/sec across 48 concurrent clients and MiniMax M2. Z8 Fury G5, Xeon 3455, 4xA6K. Aibrix 0.5.0, vLLM 0.11.2, See translation", "url": "https://huggingface.co/posts/mitkox/488545088120873", "date_published": "2025-11-24T09:30:41.864994"}, {"id": "https://huggingface.co/posts/YatharthS/933040846295981", "image": "", "title": "Just uploaded a detailed blog about my findings in optimizing NeuTTS to generate 200 seconds of audio in a single second. Also went in depth in NeuTTS\u2019s architecture. Will be happy to answer any questions.", "content_text": "Just uploaded a detailed blog about my findings in optimizing NeuTTS to generate 200 seconds of audio in a single second. Also went in depth in NeuTTS\u2019s architecture. Will be happy to answer any questions. https://huggingface.co/blog/YatharthS/making-neutts-200x-realtime See translation", "url": "https://huggingface.co/posts/YatharthS/933040846295981", "date_published": "2025-11-24T09:30:41.865248"}, {"id": "https://huggingface.co/posts/John1604/431236892313740", "image": "", "title": "Both cat and  dog has RL, vision, hearing abilities like a human.  And they acted - run and jump - better than Optimus. Why can human own  cat and dog? Maybe we have better LLM model in brain than that of cat and dog?", "content_text": "Both cat and dog has RL, vision, hearing abilities like a human. And they acted - run and jump - better than Optimus. Why can human own cat and dog? Maybe we have better LLM model in brain than that of cat and dog? \u732b\u548c\u72d7\u90fd\u62e5\u6709\u50cf\u4eba\u7c7b\u4e00\u6837\u7684\u5f3a\u5316\u5b66\u4e60\u80fd\u529b\u3001\u89c6\u89c9\u548c\u542c\u89c9\u3002\u800c\u4e14\u5b83\u4eec\u8dd1\u8df3\u7b49\u65b9\u9762\u7684\u8868\u73b0\u751a\u81f3\u6bd4\u64ce\u5929\u67f1\u8fd8\u8981\u51fa\u8272\u3002\u4e3a\u4ec0\u4e48\u4eba\u7c7b\u53ef\u4ee5\u9972\u517b\u732b\u548c\u72d7\u5462\uff1f\u6216\u8bb8\u6211\u4eec\u7684\u5927\u8111\u62e5\u6709\u6bd4\u732b\u72d7\u66f4\u4f18\u79c0\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff1f See translation", "url": "https://huggingface.co/posts/John1604/431236892313740", "date_published": "2025-11-24T09:30:41.865493"}, {"id": "https://huggingface.co/posts/onekq/122975793150084", "image": "", "title": "No SOTA from gpt5 codex", "content_text": "No SOTA from gpt5 codex onekq-ai/WebApp1K-models-leaderboard See translation", "url": "https://huggingface.co/posts/onekq/122975793150084", "date_published": "2025-11-24T09:30:41.865678"}, {"id": "https://huggingface.co/posts/samerzaher80/574430859110807", "image": "", "title": "Need Help Getting arXiv Endorsement for My AI Research Paper", "content_text": "Need Help Getting arXiv Endorsement for My AI Research Paper Hi everyone, I hope you're doing well. I\u2019m trying to publish my new AI research paper on arXiv under the cs.AI category, but I currently need an endorser who is already authorized for cs.AI submissions. If anyone here is registered as a cs.AI endorser and is willing to help, I would truly appreciate it. Here is the official arXiv endorsement request link: \ud83d\udd17 https://arxiv.org/auth/endorse?x=EZEMO7 (Backup: http://arxiv.org/auth/endorse.php \u2014 Code: EZEMO7) My research: It\u2019s part of the AetherMind project \u2014 a self-reflective NLI reasoning system inspired by human cognitive consistency and used also in Alzheimer\u2019s research. If needed, I can share the abstract or full PDF. Thank you so much to anyone who can support. \u2014 Sameer S.Najm See translation", "url": "https://huggingface.co/posts/samerzaher80/574430859110807", "date_published": "2025-11-24T09:30:41.866067"}, {"id": "https://huggingface.co/posts/kanaria007/443085216876424", "image": "", "title": "\u2705 New Reference: *Atlas of Structured Intelligence*", "content_text": "\u2705 New Reference: *Atlas of Structured Intelligence* Title: \ud83d\uddfa\ufe0f Atlas of Structured Intelligence \ud83d\udd17 https://huggingface.co/blog/kanaria007/atlas-of-structured-intelligence --- Summary: Across 40+ articles, PoCs (education, computing, space), specs, and the Cosmic Intelligence Model, this *Atlas* is the navigation layer: it locates each piece, shows recurring protocols, and connects micro (cognition) \u2192 meso (institutions) \u2192 macro (civilization) \u2192 cosmic (CIM). *It\u2019s the map of the series itself, not a new theory.* > From fragments to framework \u2014 > *the Atlas turns a library into a system.* --- Why It Matters: \u2022 Gives newcomers a clear entry path; gives experts cross-links and dependencies \u2022 Reveals protocol reuse across domains (not just \u201cthemes,\u201d but shared machinery) \u2022 Bridges qualitative ideas with quantitative indices and evaluation --- What\u2019s Inside: \u2022 *Macro Map:* Core theory \u2192 applied domains \u2192 PoC suite \u2192 cosmic series \u2022 *Protocol Matrix:* Where jump-generator, memory-loop,...", "url": "https://huggingface.co/posts/kanaria007/443085216876424", "date_published": "2025-11-24T09:30:41.866609"}, {"id": "https://huggingface.co/posts/prithivMLmods/294108395051728", "image": "", "title": "Try the demo of NVIDIA Nemotron Parse v1.1, NVIDIA's latest VLM for understanding document semantics and extracting text and table elements with spatial grounding. It is capable of comprehensive text understanding and document structure analysis in a given document, and can provide bounding boxes with coordinates.", "content_text": "Try the demo of NVIDIA Nemotron Parse v1.1, NVIDIA's latest VLM for understanding document semantics and extracting text and table elements with spatial grounding. It is capable of comprehensive text understanding and document structure analysis in a given document, and can provide bounding boxes with coordinates. \u2b50Space[Demo]: prithivMLmods/NVIDIA-Nemotron-Parse-v1.1 \u2b50Model: nvidia/NVIDIA-Nemotron-Parse-v1.1 \u2b50Multimodal-Spaces: https://huggingface.co/collections/prithivMLmods/multimodal-implementations Some relevant Spaces \u2b50DeepSeek-OCR-experimental [latest transformers]: prithivMLmods/DeepSeek-OCR-experimental \u2b50Qwen3-VL-Outpost: prithivMLmods/Qwen3-VL-Outpost \u2b50Multimodal-OCR3: prithivMLmods/Multimodal-OCR3 Check out the other spaces in the multimodal implementation collection. To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/294108395051728", "date_published": "2025-11-24T09:30:41.867011"}, {"id": "https://huggingface.co/posts/mybbnae/820891401991666", "image": "", "title": "@Reubencf", "content_text": "@ Reubencf made an WebOS check it out MCP-1st-Birthday/Reuben_OS See translation", "url": "https://huggingface.co/posts/mybbnae/820891401991666", "date_published": "2025-11-24T09:30:41.867205"}]}