{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/ovi054/792225323579087", "image": "", "title": "WAN 2.2 Text to Image \u26a1", "content_text": "WAN 2.2 Text to Image \u26a1 ovi054/wan2-2-text-to-image We all know that WAN 2.2 A14B is a video model. But It turns out this video model can also produce great image results with incredible prompt adherence! The image output is sharp, detailed, and sticks to the prompt better than most. \ud83d\udc49 Try it now: ovi054/wan2-2-text-to-image See translation", "url": "https://huggingface.co/posts/ovi054/792225323579087", "date_published": "2025-08-13T13:38:17.145004"}, {"id": "https://huggingface.co/posts/fdaudens/155578033980873", "image": "", "title": "OpenAI\u2019s GPT-OSS has sparked ~400 new models on Hugging Face and racked up 5M downloads in less than a week, already outpacing DeepSeek R1\u2019s first-week numbers.", "content_text": "OpenAI\u2019s GPT-OSS has sparked ~400 new models on Hugging Face and racked up 5M downloads in less than a week, already outpacing DeepSeek R1\u2019s first-week numbers. For comparison: when R1 launched, I tracked 550 derivatives (across 8 base models) in a week, with ~3M downloads. GPT-OSS is ahead on adoption and engagement. It\u2019s also the most-liked release of any major LLM this summer. The 20B and 120B versions quickly shot past Kimi K2, GLM 4.5, and others in likes. Most-downloaded GPT-OSS models include LM Studio and Unsloth AI versions: 1\ufe0f\u20e3 openai/gpt-oss-20b - 2.0M 2\ufe0f\u20e3 lmstudio-community/gpt-oss-20b-MLX-8bit - 750K 3\ufe0f\u20e3 openai/gpt-oss-120b - 430K 4\ufe0f\u20e3 unsloth/gpt-oss-20b-GGUF - 380K 5\ufe0f\u20e3 lmstudio-community/gpt-oss-20b-GGUF - 330K The 20B version is clearly finding its audience, showing the power of smaller, faster, more memory- and energy-efficient models. (These numbers don\u2019t include calls to the models via inference providers, so the real usage is likely even bigger, especially for the...", "url": "https://huggingface.co/posts/fdaudens/155578033980873", "date_published": "2025-08-13T13:38:17.145495"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/624409186500391", "image": "", "title": "Qwen Image is literally unchallenged at understanding complex prompts and writing amazing text on generated images. This model feels almost as if it\u2019s illegal to be open source and free. It is my new tool for generating thumbnail images. Even with low-effort prompting, the results are excellent.", "content_text": "https://youtu.be/R6h02YY6gUs Qwen Image is literally unchallenged at understanding complex prompts and writing amazing text on generated images. This model feels almost as if it\u2019s illegal to be open source and free. It is my new tool for generating thumbnail images. Even with low-effort prompting, the results are excellent. This tutorial literally shows how these images were generated with Gemini 2.5 Pro made prompts : Qwen Image Dominates Text-to-Image: 700+ Tests Reveal Why It\u2019s Better Than FLUX \u2014 Presets Published https://youtu.be/R6h02YY6gUs Gemini 2.5 Pro is freely available on Google Studio AI All images generated in easy to use SwarmUI and they are unmodified raw generations SwarmUI and ComfyUI install tutorial : Master Local AI Art & Video Generation with SwarmUI (ComfyUI Backend): The Ultimate 2025 Tutorial https://www.youtube.com/watch?v=fTzlQ0tjxj0 See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/624409186500391", "date_published": "2025-08-13T13:38:17.145918"}, {"id": "https://huggingface.co/posts/badaoui/360505768110002", "image": "", "title": "Is there a \"one-size-fits-all\" recipe for quantizing Large Language Models? \ud83e\udd14", "content_text": "Is there a \"one-size-fits-all\" recipe for quantizing Large Language Models? \ud83e\udd14 As part of my ongoing work in mixed-precision quantization, I've been exploring this question by measuring layer-by-layer sensitivity. The goal is to see if we can find universal rules for which layers can be quantized aggressively without impacting performance.The results are fascinating and reveal two key insights: 1\ufe0f\u20e3 Sensitivity profiles are like architectural \"fingerprints.\" Models from the same family share strikingly similar sensitivity patterns. As you can see in the charts below for the Gemma and SmolLM families, the ranking and relative sensitivity of the layers remain remarkably consistent. This suggests that the underlying architecture is a primary driver of a model's quantization behavior. 2\ufe0f\u20e3 A \"universal\" mixed-precision quantization strategy is challenging. While models within a family are similar, these \"fingerprints\" change dramatically when comparing different architectures like LLaMA,...", "url": "https://huggingface.co/posts/badaoui/360505768110002", "date_published": "2025-08-13T13:38:17.146548"}, {"id": "https://huggingface.co/posts/Akhil-Theerthala/336725802829969", "image": "", "title": "I'm excited to announce that I've just released the newest versions of my Kuvera models and the expanded Personal Finance Reasoning dataset on Hugging Face!", "content_text": "I'm excited to announce that I've just released the newest versions of my Kuvera models and the expanded Personal Finance Reasoning dataset on Hugging Face! What's new: I've expanded the Personal Finance Reasoning Dataset, which now includes 18.9k samples of real-world financial questions paired with detailed, empathetic answers. The previous generation pipeline was also streamlined with better psychological context and response validations. I've also released new Kuvera models trained on this improved dataset: - Kuvera-4B & 8B: These are my upgraded non-reasoning models, fine-tuned to provide practical financial advice. I've specifically trained the 8B model to better understand the user's emotional context. - Kuvera-12B: A first experimental reasoning model focused on the query resolution. As the sole person working on this project, this release is a noticeable step forward from my previous work, offering more powerful and nuanced tools for financial AI. I am actively looking to...", "url": "https://huggingface.co/posts/Akhil-Theerthala/336725802829969", "date_published": "2025-08-13T13:38:17.147015"}, {"id": "https://huggingface.co/posts/mrs83/441245217845502", "image": "", "title": "Introducing the Computer Says No Dataset:", "content_text": "Introducing the Computer Says No Dataset: ethicalabs/computer-says-no An LLM can do almost anything, but should it? This dataset provides clear examples of when LLMs should decline requests, such as: - Counting characters (e.g., \"number of 'r's in 'raspberry'\" \u2013 seriously, you\u2019ve got this) - Solving basic equations (like *5.9 = x + 5.11* \u2013 please, show that calculator some love) Inspired by Little Britain's iconic \"Computer Says No\" sketch, we address a critical issue in AI systems today: the waste of using a rocket launcher to swat flies (aka powerful models for trivial tasks). Goals: - Reduce waste by saving compute for tasks that actually need it - Guide users to better tools - Spark discussion about ethical AI This isn\u2019t a training set. It\u2019s a provocation: if we don\u2019t define AI's limits, who will? See translation", "url": "https://huggingface.co/posts/mrs83/441245217845502", "date_published": "2025-08-13T13:38:17.147412"}, {"id": "https://huggingface.co/posts/albertvillanova/486728535468014", "image": "", "title": "Latest smolagents release supports GPT-5: build agents that think, plan, and act.", "content_text": "Latest smolagents release supports GPT-5: build agents that think, plan, and act. \u26a1 Upgrade now and put GPT-5 to work! See translation", "url": "https://huggingface.co/posts/albertvillanova/486728535468014", "date_published": "2025-08-13T13:38:17.147618"}, {"id": "https://huggingface.co/posts/meg/538913816669876", "image": "", "title": "New work from my socially-minded colleagues at Hugging Face, creating some foundations for AI companionship behavior evaluation.", "content_text": "New work from my socially-minded colleagues at Hugging Face, creating some foundations for AI companionship behavior evaluation. Evaluation Dataset: AI-companionship/INTIMA Paper: AI-companionship/INTIMA Work from @ giadap , @ frimelle , @ yjernite . See translation", "url": "https://huggingface.co/posts/meg/538913816669876", "date_published": "2025-08-13T13:38:17.147868"}, {"id": "https://huggingface.co/posts/hba123/507696815739226", "image": "", "title": "After so many requests, I want to update everyone on the status of Ark, i.e., doing robotics in Python. First, thanks a lot for the amazing and impressive interest we got in it. We are now @ 1.13 K downloads, which is beyond my wildest expectations (", "content_text": "After so many requests, I want to update everyone on the status of Ark, i.e., doing robotics in Python. First, thanks a lot for the amazing and impressive interest we got in it. We are now @ 1.13 K downloads, which is beyond my wildest expectations ( https://pepy.tech/projects/ark-robotics?timeRange=threeMonths&category=version&includeCIDownloads=true&granularity=daily&viewType=line&versions=0.1.1%2C0.1%2C0.0.1 )! 0. First and foremost, we are pip installable ( https://pypi.org/project/ark-robotics/ ) 1. We are currently working on supporting more robots: G1 and Drones are in the works with a cool set of amazing, fantastic colleagues. Those are coming. 2. We have support for multiple sensors and interfaces ( https://github.com/Robotics-Ark/ark_interfaces ) 3. We also now have support for machine learning via diffusion policies ( https://github.com/Robotics-Ark/ark_diffusion_policies_on_franka ) 4. We have a set of tutorials that detail each step (...", "url": "https://huggingface.co/posts/hba123/507696815739226", "date_published": "2025-08-13T13:38:17.148266"}, {"id": "https://huggingface.co/posts/Kseniase/253421169111650", "image": "", "title": "6 Must-read books about AI and Machine Learning:", "content_text": "6 Must-read books about AI and Machine Learning: Sharing some free, useful resources for you. In this collection, we\u2019ve gathered the most recent books to give you up-to-date information on key fundamental topics. Hope this helps you master AI and machine learning: 1. Machine Learning Systems by Vijay Janapa Reddi \u2192 https://www.mlsysbook.ai/ Provides a framework for building effective ML solutions, covering data engineering, optimization, hardware-aware training, inference acceleration, architecture choice, and other key principles 2. Generative Diffusion Modeling: A Practical Handbook by Zihan Ding, Chi Jin \u2192 https://arxiv.org/abs/2412.17162 Offers a unified view of diffusion models: probabilistic, score-based, consistency, rectified flow, pre/post-training. It aligns notations with code to close the \u201cpaper-to-code\u201d gap. 3. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges \u2192 https://arxiv.org/abs/2104.13478 Explores unified geometric principles to analyze neural...", "url": "https://huggingface.co/posts/Kseniase/253421169111650", "date_published": "2025-08-13T13:38:17.148878"}]}