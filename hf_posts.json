{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/danielhanchen/824171868881117", "image": "", "title": "Qwen releases Qwen3-Coder-Next! \ud83d\udc9c Run the locally on 46GB RAM or less.", "content_text": "Qwen releases Qwen3-Coder-Next! \ud83d\udc9c Run the locally on 46GB RAM or less. Thhe model excels at agentic coding & local use. With 256K context, it delivers similar performance to models with 10-20\u00d7 more active parameters. GGUF: unsloth/Qwen3-Coder-Next-GGUF Guide: https://unsloth.ai/docs/models/qwen3-coder-next See translation", "url": "https://huggingface.co/posts/danielhanchen/824171868881117", "date_published": "2026-02-04T09:52:45.474152"}, {"id": "https://huggingface.co/posts/MikeDoes/512575404125311", "image": "", "title": "A single lock on a door isn't enough. Real security is about layers.", "content_text": "A single lock on a door isn't enough. Real security is about layers. The same is true for AI privacy. A new paper, \"Whispered Tuning\", offers a fantastic layered solution that aims to fortify LLMs against privacy infringements. We're proud that the first, essential layer, a high-precision PII redaction model was built on the foundation of the Ai4Privacy/pii-65k dataset. Our dataset provided the necessary training material for their initial anonymization step, which then enabled them to develop further innovations like differential privacy fine-tuning and output filtering. This is a win-win: our data helps create a solid base, and researchers build powerful, multi-stage privacy architectures on top of it. Together, we're making AI safer. \ud83d\udd17 Read the full paper to see how a strong foundation enables a complete privacy solution: https://www.scirp.org/journal/paperinformation?paperid=130659 \ud83d\ude80 Stay updated on the latest in privacy-preserving AI\u2014follow us on LinkedIn:...", "url": "https://huggingface.co/posts/MikeDoes/512575404125311", "date_published": "2026-02-04T09:52:45.474620"}, {"id": "https://huggingface.co/posts/rajkumarrawal/904260944141642", "image": "", "title": "I submitted a \"FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning\" Paper by Tanyu Chen, Tairan Chen, Kai shen , Zhenghua Bao, Zhihui Zhang, Man Yuan, Yi Shi From", "content_text": "I submitted a \"FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning\" Paper by Tanyu Chen, Tairan Chen, Kai shen , Zhenghua Bao, Zhihui Zhang, Man Yuan, Yi Shi From FlashLabs to Daily Papers on huggingface . Chroma 1.0 enables real time spoken dialogue with personalized voice cloning through discrete speech representations and interleaved text audio token scheduling. Chroma 1.0 , the world\u2019s first open source, real time speech to speech model with voice cloning. FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning (2601.11141) See translation", "url": "https://huggingface.co/posts/rajkumarrawal/904260944141642", "date_published": "2026-02-04T09:52:45.474989"}, {"id": "https://huggingface.co/posts/imnotkitty/790273915312125", "image": "", "title": "The 2025 Chinese LLM Showdown: Western Models Still Dominate Top 4, but China Leads the Open-Source Arena.", "content_text": "The 2025 Chinese LLM Showdown: Western Models Still Dominate Top 4, but China Leads the Open-Source Arena. \ud83c\udfc6 The Champions: Claude-Opus-4.5, Gemini-3-Pro, GPT-5.2, and Gemini-3-Flash sweep the top four spots. \ud83d\ude80 The Pursuers: Doubao and DeepSeek-V3.2 tie for first place among Chinese models; GLM-4.7, ERNIE-5.0, and Kimi secure their positions in the domestic top five. \ud83d\udd25 The Biggest Highlight: The top three spots on the open-source leaderboard are entirely held by Team China (DeepSeek, GLM, Kimi), outperforming the best western open-source models. See translation", "url": "https://huggingface.co/posts/imnotkitty/790273915312125", "date_published": "2026-02-04T09:52:45.475321"}, {"id": "https://huggingface.co/posts/alibidaran/992533889532684", "image": "", "title": "I\u2019m excited to share PlaiTO, a reasoning-focused language model built on LLaMA 3.1 (8B) and optimized for humanities and social sciences.", "content_text": "I\u2019m excited to share PlaiTO, a reasoning-focused language model built on LLaMA 3.1 (8B) and optimized for humanities and social sciences. PlaiTO is designed to go beyond surface-level text generation, emphasizing structured reasoning, conceptual clarity, and analytical depth\u2014especially in domains centered on human behavior and social systems. \ud83c\udfaf Focus Areas Psychology Management & Organizational Studies Sociology \ud83d\udcca MMLU Benchmark Results (100 samples per domain) Professional Psychology: 76% Management: 74% Sociology: 75% These results highlight PlaiTO\u2019s strong performance in abstract, theory-heavy, and reasoning-driven tasks. \ud83d\udca1 Why PlaiTO? Strong analytical and reasoning capabilities Better handling of complex human-centered problems Suitable for academic, educational, and research use cases Balanced performance across multiple humanities disciplines PlaiTO is ideal for conceptual analysis, case reasoning, academic discussion, and decision-support scenarios\u2014while still requiring...", "url": "https://huggingface.co/posts/alibidaran/992533889532684", "date_published": "2026-02-04T09:52:45.475787"}, {"id": "https://huggingface.co/posts/kanaria007/276204153153960", "image": "", "title": "\u2705 New Article: Policy Load Balancer", "content_text": "\u2705 New Article: Policy Load Balancer Title: \ud83e\udded Policy Load Balancer: Risk Modes, Degradation, and Kill-Switches \ud83d\udd17 https://huggingface.co/blog/kanaria007/policy-load-balancer --- Summary: Even if you already have *Jumps* (atomic moves), *RML* (effect glue), *EVAL* (experiments), and *ETH* (hard constraints), a real system still needs one practical answer: *What operational mode is the system allowed to run in *right now*\u2014for whom, and under which governance?* This article introduces the *Policy Load Balancer (PoLB)*: a first-class \u201cmode selector\u201d that turns risk signals + ETH/EVAL/ID context into an *active mode descriptor* (risk band, mode name, allowed jump types, allowed RML levels, experiments on/off, engine whitelist), plus *degradation* and *kill-switch* rules. > PoLB is the goal surface for *how allowed* the system is to act. --- Why It Matters: \u2022 Replaces scattered feature flags with *structured, auditable modes* (and traceable transitions) \u2022 Ensures *nothing effectful runs...", "url": "https://huggingface.co/posts/kanaria007/276204153153960", "date_published": "2026-02-04T09:52:45.476426"}, {"id": "https://huggingface.co/posts/eaddario/695215283251750", "image": "", "title": "Experimental global target bits\u2011per\u2011weight quantization of mistralai/Ministral-3-14B-Instruct-2512 and mistralai/Ministral-3-14B-Reasoning-2512", "content_text": "Experimental global target bits\u2011per\u2011weight quantization of mistralai/Ministral-3-14B-Instruct-2512 and mistralai/Ministral-3-14B-Reasoning-2512 Unlike standard llama.cpp quantizations that rely on fixed type heuristics (e.g., Q4_K_M), the Target BPW approach optimizes per-tensor precision where it matters the most, and produces high quality models that meet a precise global file size target. Key Advantages: - VRAM Maximization: Can generate high quality models sized exactly to fit hardware constraints (e.g., fitting the model into exactly 24GB VRAM). - Data-Driven Precision: Quantization mix is determined by actual weight error sensitivity rather than hardcoded rules, often yielding better PPL/KLD size trade-offs. Full benchmarks (PPL, KLD, ARC, MMLU, etc.) and methodology in the models' cards eaddario/Ministral-3-14B-Instruct-2512-GGUF eaddario/Ministral-3-14B-Reasoning-2512-GGUF See translation", "url": "https://huggingface.co/posts/eaddario/695215283251750", "date_published": "2026-02-04T09:52:45.476836"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/876855019351468", "image": "", "title": "SECourses Musubi Trainer upgraded to V27 and FLUX 2, FLUX Klein, Z-Image training added with demo configs - amazing VRAM optimized - read the news", "content_text": "SECourses Musubi Trainer upgraded to V27 and FLUX 2, FLUX Klein, Z-Image training added with demo configs - amazing VRAM optimized - read the news App is here : https://www.patreon.com/posts/137551634 Full tutorial how to use and train : https://youtu.be/DPX3eBTuO_Y See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/876855019351468", "date_published": "2026-02-04T09:52:45.477073"}, {"id": "https://huggingface.co/posts/ZennyKenny/621232711062929", "image": "", "title": "\ud83e\udd14 Do you have a Hugging Face Space that you wish you could programmatically restart to induce data refresh or some other behavior?", "content_text": "\ud83e\udd14 Do you have a Hugging Face Space that you wish you could programmatically restart to induce data refresh or some other behavior? \ud83d\udc49 Try Spaces Scheduler for this use case: https://github.com/kghamilton89/spaces-scheduler \u27a1\ufe0f Lightweight \u27a1\ufe0f Easy to setup \u27a1\ufe0f Just works \ud83d\ude0e Happy to share some tooling with the Hugging Face community that's given me so much. See translation", "url": "https://huggingface.co/posts/ZennyKenny/621232711062929", "date_published": "2026-02-04T09:52:45.477345"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/938156483514129", "image": "", "title": "\ud83d\udc40 While we're in between Kaggle competitions, users can work on earning this certificate from Duality AI! \ud83d\ude4c", "content_text": "\ud83d\udc40 While we're in between Kaggle competitions, users can work on earning this certificate from Duality AI! \ud83d\ude4c Although these competitions are over, you can still submit results, achieve above a threshold, and earn the title of \ud83d\udca5OBJECT DETECTION EXPERT\ud83d\udca5. \u23f3 No time limit \ud83c\udfc3 No competition \ud83e\udde0 Just you against your own brain Start here: https://falcon.duality.ai/secure/documentation/certificate-challenge-object-detection?utm_source=linkedin&utm_medium=post&utm_campaign=HF People who already participated in one or more competitions last year already have a leg up! Your previous scores count towards this certificate, and now you just have to collect them all \ud83d\ude3d As always, special thanks to our collaborators LunateAI and 3LC.AI \ud83e\udef0 See translation", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/938156483514129", "date_published": "2026-02-04T09:52:45.477712"}]}