{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/YerbaPage/620116280773759", "image": "", "title": "**Build Claude Code from Scratch Together**", "content_text": "**Build Claude Code from Scratch Together** I'm building a minimal, open-source AI agent that runs in the terminal to help with programming tasks. Think of a simplified, from-scratch version of the Claude Code agent \ud83d\udcbb. You can check out the basic framework I've already started here: [github.com/YerbaPage/Terminal-Agent]( https://github.com/YerbaPage/Terminal-Agent ) \ud83d\udd25 To be clear, the goal isn't to create a perfect, commercial-grade tool. This is a for-fun project geared towards research and exploration. The main idea is to build a **flexible and minimal framework** that helps us understand the modules and functionalities of sophisticated agents like Claude Code. It's a playground to explore how we can enable agents to handle more complex tasks effectively. I'm looking for a few people who'd be interested in exploring this together. We could experiment with ideas like adding a to-do list manager to improve planning or finding better ways for the agent to manage its memory like some...", "url": "https://huggingface.co/posts/YerbaPage/620116280773759", "date_published": "2025-07-09T05:28:21.524479"}, {"id": "https://huggingface.co/posts/chansung/585764915046887", "image": "", "title": "YAML engineering becomes more and more important than ever from infra provisioning to model training (recipes).", "content_text": "YAML engineering becomes more and more important than ever from infra provisioning to model training (recipes). Here, I built a simple editor first for @ dstackai , and I will share the live endpoint this week. Let me know what you think about this approach. Based on this approach, if people think this is useful, I am going to do the same thing for the LLM training recipes for popular frameworks such as Hugging Face open-r1, Axolotl, and so on. Let me hear. See translation", "url": "https://huggingface.co/posts/chansung/585764915046887", "date_published": "2025-07-09T05:28:21.524774"}, {"id": "https://huggingface.co/posts/nicolay-r/221275408085007", "image": "", "title": "\ud83d\ude80 For those who interested in summarization of the long textual reports in medical domain  \ud83d\udcdd\ud83e\ude7a,", "content_text": "\ud83d\ude80 For those who interested in summarization of the long textual reports in medical domain \ud83d\udcdd\ud83e\ude7a, @ Xiaolihai and I delighted to share that we experiment with distillation tuning adaptation for Qwen-2.5 0.5B. We use reports from the MultiClinSum dataset and pass it through 72B version to retrieve report explanations in order to initiate ditillation tuning for 0.5B model. We experiment with passages written in English, French, Portuguese, and Spanish. \ud83d\udd11 We find that using distil-technique results in 2-4% performance increment on fine-tuning and similar improvements for reports in English (non-official and official evaluation). For the other it results in systems that perform similar to the convential tuning (standard) (see result below). Dataset: https://zenodo.org/records/15459174 Competition: https://participants-area.bioasq.org/general_information/MultiClinSum/ Github: https://github.com/nicolay-r/distil-tuning-llm model: nicolay-r/qwen25-05b-multiclinsum-distil See translation", "url": "https://huggingface.co/posts/nicolay-r/221275408085007", "date_published": "2025-07-09T05:28:21.525189"}, {"id": "https://huggingface.co/posts/Parveshiiii/143893358011969", "image": "", "title": "\ud83e\udde0 Glimpses of AGI \u2014 A Vision for All Humanity", "content_text": "\ud83e\udde0 Glimpses of AGI \u2014 A Vision for All Humanity What if AGI wasn\u2019t just a distant dream\u2014but a blueprint already unfolding? I\u2019ve just published a deep dive called Glimpses of AGI, exploring how scalable intelligence, synthetic reasoning, and alignment strategies are paving a new path forward. This isn\u2019t your average tech commentary\u2014it\u2019s a bold vision for conscious AI systems that reason, align, and adapt beyond narrow tasks. \ud83d\udd0d Read it, upvote it if it sparks something, and let\u2019s ignite a collective conversation about the future of AGI. https://huggingface.co/blog/Parveshiiii/glimpses-of-agi See translation", "url": "https://huggingface.co/posts/Parveshiiii/143893358011969", "date_published": "2025-07-09T05:28:21.525534"}, {"id": "https://huggingface.co/posts/Kseniase/251572743842280", "image": "", "title": "13 Outstanding MCP Servers", "content_text": "13 Outstanding MCP Servers MCP is redefining how AI assistants connect to the world of data and tools, so no wonder MCP servers are in high demand now. That\u2019s why we\u2019ve curated 13 cool MCP servers to upgrade your workflow: 1. Hugging Face Official MCP Server -> https://github.com/evalstate/hf-mcp-server Provides an access and interaction with Hugging Face models, datasets, and Gradio Spaces for dynamic tool integration and configuration across environments. 2. Browser MCP -> https://browsermcp.io/ An MCP server +Chrome extension. It allows to automate your browser with AI apps like VS Code, Claude, Cursor, and Windsurf. 3. Bright Data MCP -> https://github.com/brightdata/brightdata-mcp This one is for working with data in real-time: searching the web, navigating websites, taking action and retrieving data. 4. JSON MCP -> https://github.com/VadimNastoyashchy/json-mcp Interact with JSON files: split, merge, find specific data, and validate content within them. 5. Octagon Deep Research...", "url": "https://huggingface.co/posts/Kseniase/251572743842280", "date_published": "2025-07-09T05:28:21.526092"}, {"id": "https://huggingface.co/posts/merve/482686790570915", "image": "", "title": "ByteDance released Tar 1.5B and 7B: image-text in image-text out models, fully open-source \ud83d\udc4f", "content_text": "ByteDance released Tar 1.5B and 7B: image-text in image-text out models, fully open-source \ud83d\udc4f ByteDance-Seed/tar-6864cf0d9fe59a3b91cc4260 They have an image tokenizer unified with text, and they de-tokenize using either of two models (LLM and diffusion) The model is actually a full LLM (Qwen2), the tokenizer converts image tokens \ud83e\udd2f See translation", "url": "https://huggingface.co/posts/merve/482686790570915", "date_published": "2025-07-09T05:28:21.526363"}, {"id": "https://huggingface.co/posts/ProCreations/956872455588724", "image": "", "title": "20 FOLLOWERS IN LESS THAN A DAY? YA'LL ARE CRAZY!", "content_text": "20 FOLLOWERS IN LESS THAN A DAY? YA'LL ARE CRAZY! We zoomed pass 70, 80, and 90 followers in a day. That is insane. As a thank you, ive published one of my largest and most expensive dataset (rip my 2 dollars, most of my datasets are free to make) to the hub. To be used for text summarization in english, check it out! ProCreations/simple-summaries See translation", "url": "https://huggingface.co/posts/ProCreations/956872455588724", "date_published": "2025-07-09T05:28:21.526614"}, {"id": "https://huggingface.co/posts/m-ric/129069007859083", "image": "", "title": "Diffusion LLMs are coming for autoregressive LLMs \u26a1\ufe0f\u26a1\ufe0f Inception Labs' new diffusion model demolishes all leading LLMs on generation speed, with equal quality !", "content_text": "Diffusion LLMs are coming for autoregressive LLMs \u26a1\ufe0f\u26a1\ufe0f Inception Labs' new diffusion model demolishes all leading LLMs on generation speed, with equal quality ! Inception Labs was founded a few months ago, and they're not sleeping: after dropping a code model, they just published Mercury chat, a diffusion-based chat model that reaches 1000 tokens / second on H100, i.e. 10x more than models of equivalent performance on the same hardware! What's the breakthrough? Well instead, of generating tokens left-to-right like the more common autoregressive LLMs, diffusion models generate their blocks of text all at once, and successive steps refine the whole text. Diffusion models being really fast at isn't new, we have had some promising results on this by Google already back in May with Gemini Diffusion, and Mercury themselves had already published their coding model a few months ago But being that good quality is new - and now Inception Labs just proved that their models work well in chat...", "url": "https://huggingface.co/posts/m-ric/129069007859083", "date_published": "2025-07-09T05:28:21.527075"}, {"id": "https://huggingface.co/posts/merve/826044037923465", "image": "", "title": "Huge drops in open AI past week!", "content_text": "Huge drops in open AI past week! Find more models, datasets, demos here merve/releases-july-4-686bcc54ed7c45c341fbf654 Some of our picks \ud83e\udee1 \u23ef\ufe0f BAAI/MTVCraft is a new Veo3-like text-to-video model, demo is here BAAI/MTVCraft \ud83e\uddd1\ud83c\udffb\u200d\ud83d\udcbb apple/diffucoder-6868139f56672ae046fe04e8 is a new family of diffusion LLMs (7B base and instruct) for coding \ud83d\udde3\ufe0f kyutai/tts-1.6b-en_fr is a new small TTS model for English and France \ud83d\udc40 aharley/alltracker is a new pixel tracking model by Stanford, demo is here aharley/alltracker \ud83d\udcd6 racineai/OGC_MEGA_MultiDomain_DocRetrieval is a new large visual document retrieval dataset See translation", "url": "https://huggingface.co/posts/merve/826044037923465", "date_published": "2025-07-09T05:28:21.527424"}, {"id": "https://huggingface.co/posts/ProCreations/317763668973922", "image": "", "title": "Transformers are getting old! It\u2019s been 8 whole years since the architecture that powers almost all language models.", "content_text": "Transformers are getting old! It\u2019s been 8 whole years since the architecture that powers almost all language models. Read here for newer alternatives and variants of transformers: https://huggingface.co/blog/ProCreations/transformers-are-getting-old See translation", "url": "https://huggingface.co/posts/ProCreations/317763668973922", "date_published": "2025-07-09T05:28:21.527656"}]}