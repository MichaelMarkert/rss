{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/570379597864718", "image": "", "title": "6 Essential Reads on core AI/ML topics:", "content_text": "6 Essential Reads on core AI/ML topics: Time to look at some free useful resources that can help you upgrade your knowledge of AI and machine learning! Today we offer you these 6 must-read surveys that can be your perfect guides to the major fields and techniques: 1. Foundations of Large Language Models by Tong Xiao and Jingbo Zhu \u2192 https://arxiv.org/abs/2501.09223 Many recommend this 270-page book as a good resource to focus on fundamental concepts, such as pre-training, generative models, prompting, alignment, and inference 2. Large Language Models Post-Training: Surveying Techniques from Alignment to Reasoning -> A Survey on Post-training of Large Language Models (2503.06072) Read this to master policy optimization (RLHF, DPO, GRPO), supervised and parameter-efficient fine-tuning, reasoning, integration, and adaptation techniques 3. Agentic Large Language Models, a survey by Leiden University \u2192 https://arxiv.org/abs/2503.23037 Surveys agentic LLMs across reasoning, tools, and...", "url": "https://huggingface.co/posts/Kseniase/570379597864718", "date_published": "2025-07-23T09:32:13.163045"}, {"id": "https://huggingface.co/posts/MaziyarPanahi/751516664507693", "image": "", "title": "\ud83e\uddec Breaking news in Clinical AI: Introducing the OpenMed NER Model Discovery App on Hugging Face \ud83d\udd2c", "content_text": "\ud83e\uddec Breaking news in Clinical AI: Introducing the OpenMed NER Model Discovery App on Hugging Face \ud83d\udd2c OpenMed is back! \ud83d\udd25 Finding the right biomedical NER model just became as precise as a PCR assay! I'm thrilled to unveil my comprehensive OpenMed Named Entity Recognition Model Discovery App that puts 384 specialized biomedical AI models at your fingertips. \ud83c\udfaf Why This Matters in Healthcare AI: Traditional clinical text mining required hours of manual model evaluation. My Discovery App instantly connects researchers, clinicians, and data scientists with the exact NER models they need for their biomedical entity extraction tasks. \ud83d\udd2c What You Can Discover: \u2705 Pharmacological Models - Extract \"chemical compounds\", \"drug interactions\", and \"pharmaceutical\" entities from clinical notes \u2705 Genomics & Proteomics - Identify \"DNA sequences\", \"RNA transcripts\", \"gene variants\", \"protein complexes\", and \"cell lines\" \u2705 Pathology & Disease Detection - Recognize \"pathological formations\", \"cancer types\",...", "url": "https://huggingface.co/posts/MaziyarPanahi/751516664507693", "date_published": "2025-07-23T09:32:13.163691"}, {"id": "https://huggingface.co/posts/eliebak/684400126414992", "image": "", "title": "Kimi K2 tech report is full of gems as always. Here are my notes on it:", "content_text": "Kimi K2 tech report is full of gems as always. Here are my notes on it: > MuonClip: Pretty crazy how after 70k the training stabilizes and the QK-clip is basically inactive. There is also no loss in perf with QK-clip which is not trivial at all (at small scale but with aggressive threshold). Also a cool explanation of why muon makes the logit explode in appendix E (tl;dr is that muon makes the singular value of the update matrix higher) > Sparsity scaling laws to justify their ratio, they have a very solid training infra that allows the model to be trained at this sparsity level, they could have increased even more but as sparsity increases the training becomes less efficient. > They diminish the number of attention heads to make it more efficient for long context since attention heads are a big bottleneck for long context. They also remove 2 of the 3 \"first dense\" layers in the dsv3 arch. With the sparsity and attention heads (divided by 2) they achieve 83% increased flops compared...", "url": "https://huggingface.co/posts/eliebak/684400126414992", "date_published": "2025-07-23T09:32:13.164257"}, {"id": "https://huggingface.co/posts/AdinaY/235205941898959", "image": "", "title": "Qwen3-Coder \ud83d\udcbb agentic code model by Alibaba Qwen team\ud83d\ude80", "content_text": "Qwen3-Coder \ud83d\udcbb agentic code model by Alibaba Qwen team\ud83d\ude80 Qwen/Qwen3-Coder-480B-A35B-Instruct \u2728 480B total, 35B activated MoE \u2728 Agentic Coding + Browser Use \u2192 Top code model performance \u2728 256K context (up to 1M via Yarn) for repo-scale understanding See translation", "url": "https://huggingface.co/posts/AdinaY/235205941898959", "date_published": "2025-07-23T09:32:13.164508"}, {"id": "https://huggingface.co/posts/merve/633613682232574", "image": "", "title": "Now it's possible to do RAG with any-to-any models \ud83d\udd25", "content_text": "Now it's possible to do RAG with any-to-any models \ud83d\udd25 Learn how to search in a video dataset and generate using Tevatron/OmniEmbed-v0.1-multivent an all modality retriever, and Qwen/Qwen2.5-Omni-7B , any-to-any model in this notebook \ud83e\udd1d merve/smol-vision See translation", "url": "https://huggingface.co/posts/merve/633613682232574", "date_published": "2025-07-23T09:32:13.164770"}, {"id": "https://huggingface.co/posts/etemiz/270501016622467", "image": "", "title": "All you need is curation", "content_text": "All you need is curation See translation", "url": "https://huggingface.co/posts/etemiz/270501016622467", "date_published": "2025-07-23T09:32:13.164948"}, {"id": "https://huggingface.co/posts/prithivMLmods/817042382276733", "image": "", "title": "Upgraded the step-by-step notebook for fine-tuning SigLIP2 on domain-specific image classification tasks. The notebook supports both datasets with predefined train/test splits and those with only a train split, making it suitable for low-resource, custom, and real-world classification scenarios. \ud83d\udce2\ud83d\udc49", "content_text": "Upgraded the step-by-step notebook for fine-tuning SigLIP2 on domain-specific image classification tasks. The notebook supports both datasets with predefined train/test splits and those with only a train split, making it suitable for low-resource, custom, and real-world classification scenarios. \ud83d\udce2\ud83d\udc49 \u27ba FineTuning-SigLIP2-Notebook : prithivMLmods/FineTuning-SigLIP2-Notebook \u27ba GitHub : https://github.com/PRITHIVSAKTHIUR/FineTuning-SigLIP-2 \u27ba In the first, datasets include predefined train and test splits, enabling conventional supervised learning and generalization evaluation : prithivMLmods/FineTuning-SigLIP2-Notebook (.ipynb) \u27ba In the second scenario, only a training split is available; in such cases, the training set is either partially reserved for validation or reused entirely for evaluation : prithivMLmods/FineTuning-SigLIP2-Notebook (.ipynb) This flexibility supports experimentation in constrained or domain-specific settings, where standard test annotations may not exist. See...", "url": "https://huggingface.co/posts/prithivMLmods/817042382276733", "date_published": "2025-07-23T09:32:13.165352"}, {"id": "https://huggingface.co/posts/dhruv3006/705437635458343", "image": "", "title": "Ever wish you could have someone watching your Github repo 24/7?", "content_text": "Ever wish you could have someone watching your Github repo 24/7? We built an agent that monitors your repo, finds who most recently starred it, and autonomously reaches out via email! Discord : https://discord.com/invite/ZYN7f7KPjS Get your API Key here : https://tally.so/r/nrYr4X See translation", "url": "https://huggingface.co/posts/dhruv3006/705437635458343", "date_published": "2025-07-23T09:32:13.165596"}, {"id": "https://huggingface.co/posts/VolodymyrPugachov/444265930926373", "image": "", "title": "Digital Heart Model: Initial Research Launch \ud83d\ude80", "content_text": "Digital Heart Model: Initial Research Launch \ud83d\ude80 I am excited to announce the launch of research on the Digital Heart Model (DHM), an AI-driven digital twin designed to transform personalized cardiovascular care. DHM integrates multimodal data, focusing initially on cardiac imaging, histopathological imaging, and ECG data, to predict patient outcomes and optimize interventions. Initial Model and Dataset Overview: Base Model: Multimodal AI foundation combining Convolutional Neural Networks (CNN), Vision Transformers (ViT), and Graph Neural Networks (GNN). Datasets: Cardiac MRI and CT imaging datasets, histopathological cardiac tissue images, and extensive ECG waveform data. Expected Results from First Iteration: Cardiac event prediction (e.g., myocardial infarction) accuracy: AUC \u2265 0.90 Arrhythmia detection and classification accuracy: AUC \u2265 0.88 Enhanced segmentation accuracy for cardiac imaging: Dice Score \u2265 0.85 \ud83d\udd0d Next Steps: Conducting initial retrospective validation. Preparing...", "url": "https://huggingface.co/posts/VolodymyrPugachov/444265930926373", "date_published": "2025-07-23T09:32:13.166031"}, {"id": "https://huggingface.co/posts/Jaward/640818601212021", "image": "", "title": "Towards batch sizes too small to meter\ud83c\udf89 beautiful work! And my personal favorite so far - I adore peak performance at small/nano scale. Everyone deserves to run/train AGI locally:) our data, our god model!", "content_text": "Towards batch sizes too small to meter\ud83c\udf89 beautiful work! And my personal favorite so far - I adore peak performance at small/nano scale. Everyone deserves to run/train AGI locally:) our data, our god model! They showed that: - you can train LLMs (upto 1B params) with as low as batch_size=1. This is unconventional given small batch sizes can lead to unstable/spiky training runs. - you can have a stable train run with just vanilla SGD(stochastic gradient descent), no momentum required\ud83e\udd2f - small batch sizes are more robust to hyperparameters (i.e no worries with initialization) - smaller batch sizes outperforms (\u201cbetter per-Flops performance\u201d) larger batch sizes. \u201cWe recommend that practitioners training large models in memory-constrained settings exploit the benefits of small batch sizes rather than trying to emulate the large batch size setting (e.g., through gradient accumulation) typically used in industry.\u201d I\u2019ve been doing this for ages - my mantra: all my experiments must scale on...", "url": "https://huggingface.co/posts/Jaward/640818601212021", "date_published": "2025-07-23T09:32:13.166481"}]}