{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/sergiopaniego/565991505089039", "image": "", "title": "we've just added several example scripts to TRL showing how to train models with GRPO using some of the new OpenEnv environments", "content_text": "we've just added several example scripts to TRL showing how to train models with GRPO using some of the new OpenEnv environments train a model to interact with a browser (\ud83c\udfae BrowserGym Env), play Wordle (\ud83c\udfae Wordle Env) and moooore! TRL (GRPO + vLLM) + OpenEnv! \u26a1\ufe0f \ud83d\udcdd go play with them: https://github.com/huggingface/trl/tree/main/examples/scripts/openenv \ud83d\udcdd examples list: https://huggingface.co/docs/trl/main/en/example_overview#scripts See translation", "url": "https://huggingface.co/posts/sergiopaniego/565991505089039", "date_published": "2025-11-21T17:21:17.459387"}, {"id": "https://huggingface.co/posts/onekq/568645222085642", "image": "", "title": "GLM 4.6 is on a par with Gemini 2", "content_text": "GLM 4.6 is on a par with Gemini 2 onekq-ai/WebApp1K-models-leaderboard See translation", "url": "https://huggingface.co/posts/onekq/568645222085642", "date_published": "2025-11-21T17:21:17.459608"}, {"id": "https://huggingface.co/posts/wang12390/386545539363465", "image": "", "title": "Transforming Ideas Into Art: My New AI Speed Painting Demo", "content_text": "Transforming Ideas Into Art: My New AI Speed Painting Demo I\u2019m excited to share my latest AI speed painting demonstration, showcasing how quickly and smoothly AI can transform a simple idea into a fully rendered artwork. This video highlights the power of real-time AI brushwork, dynamic color composition, and fluid scene construction \u2014 all generated using my custom Miragic Speed Painting engine. What This Demo Shows - Ultra-fast painting generation from start to finish - Smooth, natural brushstrokes that feel hand-drawn - Stable composition and color consistency - A cinematic visual style suitable for creative projects - No diffusion-style noise or randomness \u2014 just pure painting Speed painting is perfect for: - Content creators and video editors - Graphic designers and social media marketers - Artists exploring quick concepts - Businesses needing fast creative assets - Anyone who wants beautiful visuals\u2026 without waiting minutes or hours Watch the Video I\u2019ve attached the full speed...", "url": "https://huggingface.co/posts/wang12390/386545539363465", "date_published": "2025-11-21T17:21:17.460123"}, {"id": "https://huggingface.co/posts/branikita/874837305207313", "image": "", "title": "Proud to share the results of our engineering team\u2019s recent work at", "content_text": "Proud to share the results of our engineering team\u2019s recent work at Robonine : \u2022 Together, we applied advanced topology optimization to redesign critical brackets of the manipulator, achieving a 57\u201376% reduction in structural deflection. \u2022 Our updated model also demonstrated a major stress decrease \u2014 from 93 MPa down to 25 MPa \u2014 all while staying within the allowed weight increase. \u2022 Although we didn\u2019t fully reach the target tip deviation of 0.3 mm (best achieved: 0.41 mm), the project gave us valuable insights and a solid foundation for the next design iteration. See translation", "url": "https://huggingface.co/posts/branikita/874837305207313", "date_published": "2025-11-21T17:21:17.460468"}, {"id": "https://huggingface.co/posts/grimjim/803126534676334", "image": "", "title": "Going forward, I will be adopting the term Magnitude-Preserving Orthogonal Ablation (MPOA) for my recent work in mitigating model damage from abliteration. The technique potentially unlocks reasoning capacity previously occupied with safety refusal processing.", "content_text": "Going forward, I will be adopting the term Magnitude-Preserving Orthogonal Ablation (MPOA) for my recent work in mitigating model damage from abliteration. The technique potentially unlocks reasoning capacity previously occupied with safety refusal processing. For details, start here: https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration Showcase results: grimjim/gemma-3-12b-it-norm-preserved-biprojected-abliterated (outperforms base instruct on UGI Leaderboard NatInt) (The existing name, while technically accurate, was a bit of a mouthful.) See translation", "url": "https://huggingface.co/posts/grimjim/803126534676334", "date_published": "2025-11-21T17:21:17.460743"}, {"id": "https://huggingface.co/posts/YatharthS/337806794067446", "image": "", "title": "Just released a heavily optimized library for NeuTTS. It's over 200x realtime meaning it can generate over 200 seconds of audio in a single second using batching and supports voice cloning!!\ud83e\udd2f\ud83e\udd2f", "content_text": "Just released a heavily optimized library for NeuTTS. It's over 200x realtime meaning it can generate over 200 seconds of audio in a single second using batching and supports voice cloning!!\ud83e\udd2f\ud83e\udd2f Link: https://github.com/ysharma3501/FastNeuTTS See translation", "url": "https://huggingface.co/posts/YatharthS/337806794067446", "date_published": "2025-11-21T17:21:17.460987"}, {"id": "https://huggingface.co/posts/obsxrver/107938712743937", "image": "", "title": "[Version 1.0] Training  Wan 2.2 LoRAs has never been easier", "content_text": "[Version 1.0] Training Wan 2.2 LoRAs has never been easier ( https://github.com/obsxrver/wan22-lora-training ) If you\u2019ve been wanting to train your own Wan 2.2 Video LoRAs but are intimidated by the hardware requirements, parameter tweaking insanity, or the installation nightmare\u2014I built a solution that handles it all for you. This is currently the easiest, fastest, and cheapest way to get a high-quality training run done. Why this method? * Zero Setup: No installing Python, CUDA, or hunting for dependencies. You launch a pre-built [Vast.AI]( http://Vast.AI ) template, and it's ready in minutes. * Full WebUI: Drag-and-drop your videos/images, edit captions, and click \"Start.\" No terminal commands required. * Extremely Cheap: You can rent a dual RTX 5090 node, train a full LoRA in 2-3 hours, and auto-shutdown. Total cost is usually $3 or less. * Auto-Save: It automatically uploads your finished LoRA to your Cloud Storage (Google Drive/S3/Dropbox) and kills the instance so you don't...", "url": "https://huggingface.co/posts/obsxrver/107938712743937", "date_published": "2025-11-21T17:21:17.461551"}, {"id": "https://huggingface.co/posts/prithivMLmods/663896599381140", "image": "", "title": "Try the all-new trending Qwen-Image-Edit-2509 (Multi-Image-Edits) specialized adapter demos, including Cloth-Design-Fuse, Texture Edit, Guided-Objects-Patching, and more \u2014 all in a single Hugging Face Space. The demo link is provided below. \ud83e\udd17\ud83d\udd25", "content_text": "Try the all-new trending Qwen-Image-Edit-2509 (Multi-Image-Edits) specialized adapter demos, including Cloth-Design-Fuse, Texture Edit, Guided-Objects-Patching, and more \u2014 all in a single Hugging Face Space. The demo link is provided below. \ud83e\udd17\ud83d\udd25 \u2b9e Space[Demo]: prithivMLmods/Qwen-Image-Edit-2509-LoRAs-Fast-Fusion \u2b9e Collection: https://huggingface.co/collections/prithivMLmods/image-generation-apps-collection \u2b9e Base Model: Qwen/Qwen-Image-Edit-2509 Similar applications\u2197\ufe0f \u2b9e Kontext-Photo-Mate-v2: prithivMLmods/Kontext-Photo-Mate-v2 \u2b9e Photo-Mate-i2i: prithivMLmods/Photo-Mate-i2i \u2b9e Qwen-Image-Edit-2509-LoRAs-Fast: prithivMLmods/Qwen-Image-Edit-2509-LoRAs-Fast To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/663896599381140", "date_published": "2025-11-21T17:21:17.461927"}, {"id": "https://huggingface.co/posts/flozi00/635605102777732", "image": "", "title": "Running large language models efficiently is more than just raw GPU power. The latest guide breaks down the essential math to determine if your LLM workload is compute-bound or memory-bound.", "content_text": "Running large language models efficiently is more than just raw GPU power. The latest guide breaks down the essential math to determine if your LLM workload is compute-bound or memory-bound. We apply these principles to a real-world example: Qwen's 32B parameter model on the new NVIDIA RTX PRO 6000 Blackwell Edition. In this guide, you will learn how to: Calculate your GPU's operational intensity (Ops:Byte Ratio) Determine your model's arithmetic intensity Identify whether your workload is memory-bound or compute-bound Read the full guide here: https://flozi.net/en/guides/ai/llm-inference-math See translation", "url": "https://huggingface.co/posts/flozi00/635605102777732", "date_published": "2025-11-21T17:21:17.462232"}, {"id": "https://huggingface.co/posts/Kseniase/762937246285628", "image": "", "title": "12 Types of JEPA", "content_text": "12 Types of JEPA Since Yann LeCun together with Randall Balestriero released a new paper on JEPA (Joint-Embedding Predictive Architecture), laying out its theory and introducing an efficient practical version called LeJEPA, we figured you might need even more JEPA. Here are 7 recent JEPA variants plus 5 iconic ones: 1. LeJEPA \u2192 LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics (2511.08544) Explains a full theory for JEPAs, defining the \u201cideal\u201d JEPA embedding as an isotropic Gaussian, and proposes the SIGReg objective to push JEPA toward this ideal, resulting in practical LeJEPA 2. JEPA-T \u2192 JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation (2510.00974) A text-to-image model that tokenizes images and captions with a joint predictive Transformer, enhances fusion with cross-attention and text embeddings before training loss, and generates images by iteratively denoising visual tokens conditioned on text 3. Text-JEPA \u2192...", "url": "https://huggingface.co/posts/Kseniase/762937246285628", "date_published": "2025-11-21T17:21:17.462907"}]}