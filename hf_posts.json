{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/404Zen/731905286538257", "image": "", "title": "4 must-try AI video models in 2026 \u2014 all in one place on iMini! \ud83c\udfac\u2728", "content_text": "4 must-try AI video models in 2026 \u2014 all in one place on iMini! \ud83c\udfac\u2728 Featuring Sora 2, Veo 3, Wan 2.5, and Seedance 3.0 \u2014 no invite code, no watermark! Try it now \ud83d\udc49 https://imini.com/ See translation", "url": "https://huggingface.co/posts/404Zen/731905286538257", "date_published": "2025-10-15T05:22:25.335507"}, {"id": "https://huggingface.co/posts/YerbaPage/639392890035292", "image": "", "title": "How to compress long code context? \ud83d\udcda", "content_text": "How to compress long code context? \ud83d\udcda Check out our LongCodeZip! Paper just got accepted to ASE 2025. \ud83d\udd25 Code & Demo: https://github.com/YerbaPage/LongCodeZip Paper: LongCodeZip: Compress Long Context for Code Language Models (2510.00446) See translation", "url": "https://huggingface.co/posts/YerbaPage/639392890035292", "date_published": "2025-10-15T05:22:25.335764"}, {"id": "https://huggingface.co/posts/Monica997/870861781065582", "image": "", "title": "You think those playful puppies are real? \ud83d\udc36\u2728", "content_text": "You think those playful puppies are real? \ud83d\udc36\u2728 Nope! It\u2019s a video I created using iMini\u2019s newly integrated Sora 2 model \u2014 no invite code, no watermark, just one simple text prompt to generate dynamic videos in seconds! \ud83c\udfac Limited-time offer: members can create without using credits! \ud83d\udc49 Try it now: https://imini.com/ See translation", "url": "https://huggingface.co/posts/Monica997/870861781065582", "date_published": "2025-10-15T05:22:25.336039"}, {"id": "https://huggingface.co/posts/Kseniase/543365627154110", "image": "", "title": "9 Powerful AI Video Generation Tools", "content_text": "9 Powerful AI Video Generation Tools Since Sora 2 is on fire these weeks, reminding us what high-quality video generation should look like, we decided you really need this list of video generation tools \u2013 great alternatives or complements to it. 1. Sora 2 \u2192 https://openai.com/sora/ It needs no introduction, but this OpenAI\u2019s text-to-video model produces short, ultra-realistic clips across styles (cinematic, photorealistic, animated, etc.) with synced audio 2. Google Veo 3 (Gemini Video Generation) \u2192 https://aistudio.google.com/models/veo-3 Part of Gemini AI. Generates 8-second high-fidelity videos from text or images with native sound: background soundtracks and realistic voices with near-perfect lip sync 3. Runway (Gen-4 by Runway ML) \u2192 https://runwayml.com/ Text, image, or video-to-video generation with advanced editing like changing lighting, weather, camera angles or replacing objects. Popular in AI filmmaking 4. Pika Labs \u2192 https://pollo.ai/m/pika-ai Provides creative, often...", "url": "https://huggingface.co/posts/Kseniase/543365627154110", "date_published": "2025-10-15T05:22:25.336585"}, {"id": "https://huggingface.co/posts/s3nh/172255383269757", "image": "", "title": "Just tried to create an educational assistant for younger people who can struggle with visualsation of 'what is this sorcery all about'.", "content_text": "Just tried to create an educational assistant for younger people who can struggle with visualsation of 'what is this sorcery all about'. Its first step of my spare time projects, sft on Qwen3-8B, EduHelper is a child-friendly tutoring assistant fine-tuned from the Qwen3-8B base model using parameter-efficient fine-tuning (PEFT) with LoRA on the ajibawa-2023/Education-Young-Children dataset. s3nh/EduHelp-8B Glad to share my work, have a wonderful day! See translation", "url": "https://huggingface.co/posts/s3nh/172255383269757", "date_published": "2025-10-15T05:22:25.336854"}, {"id": "https://huggingface.co/posts/unmodeled-tyler/162043774477631", "image": "", "title": "vanta-research/apollo-astralis-8b", "content_text": "vanta-research/apollo-astralis-8b I ran the same prompt sequence on my model Apollo Astralis 8B and Hermes4 14B from Nous Research.. The raw chat logs were then given to 3 different architectures (DeepSeek 3.1, LLaMA 405B, GPT-OSS 120B). All 3 models were given the same, simple instructions to analyze the logs and determine which model performed better. All 3 independently chose Astralis 8B for stronger reasoning, alignment, transparency, and collaborative language. Astralis 8B is designed to keep you motivated by applying warm collaborative language mixed with rigorous logical reasoning and problem solving capabilities. Give Astralis a try! See translation", "url": "https://huggingface.co/posts/unmodeled-tyler/162043774477631", "date_published": "2025-10-15T05:22:25.337165"}, {"id": "https://huggingface.co/posts/prithivMLmods/844227545389355", "image": "", "title": "The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board \u2014 including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. \ud83e\udd17\ud83d\udd25", "content_text": "The demo of Qwen3-VL-30B-A3B-Instruct, the next-generation and powerful vision-language model in the Qwen series, delivers comprehensive upgrades across the board \u2014 including superior text understanding and generation, deeper visual perception and reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities. \ud83e\udd17\ud83d\udd25 \u26a1 Space / App: prithivMLmods/Qwen3-VL-HF-Demo The model\u2019s demo supports a wide range of tasks, including; Image Inference, Video Inference, PDF Inference, Image Captioning (VLA), GIF Inference. \u26a1 Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Thanks for granting the blazing-fast Zero GPU access, @ merve \ud83d\ude4f \u26a1 Other Pages > Github: https://github.com/prithivsakthiur/qwen3-vl-hf-demo > Multimodal VLMs July'25 : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 > VL caption \u2014 < Sep 15 \u201925 : prithivMLmods/vl-caption-sep-15-25-68c7f6d737985c63c13e2391 > Multimodal...", "url": "https://huggingface.co/posts/prithivMLmods/844227545389355", "date_published": "2025-10-15T05:22:25.337622"}, {"id": "https://huggingface.co/posts/andywu-kby/521155221047550", "image": "", "title": "\ud83d\udce2 Product Update: SalesPilot 1.2 Released!", "content_text": "\ud83d\udce2 Product Update: SalesPilot 1.2 Released! \ud83d\udd27 What\u2019s New: - Sales Forecasting, Sales Analysis using Excel - No technical skills required - Dashboard and Delete Functionality - Chatbot Application https://miragic.ai/products/sales-pilot Looking forward to your feedback! See translation", "url": "https://huggingface.co/posts/andywu-kby/521155221047550", "date_published": "2025-10-15T05:22:25.337871"}, {"id": "https://huggingface.co/posts/kanaria007/687620663929284", "image": "", "title": "\u2705 New Article: *Procrastination as a Structural Loop*", "content_text": "\u2705 New Article: *Procrastination as a Structural Loop* Title: \u23f3 Procrastination as a Structural Loop: Why \u201cI Know, But I Don\u2019t Act\u201d Persists \u2014 and How Protocols Contain It \ud83d\udd17 https://huggingface.co/blog/kanaria007/procrastination-as-structural-loop --- Summary: Procrastination isn\u2019t laziness \u2014 it\u2019s a *miswired loop*. When task threat and reward prediction skew the thresholds, the *jump-generator* routes to *delay*, *reflexia* amplifies avoidance, and the *memory-loop* reinforces \u201cnot now.\u201d Once we treat it as structure, we can *reindex costs, lower entry friction, and relink intention to execution*. > Motivation wavers. > *Loops can be rewired.* --- Why It Matters: \u2022 Reframes procrastination as *auditable mechanics*, not a moral flaw \u2022 Turns \u201cstuck\u201d into a stepwise *rollback \u2192 small jump \u2192 stable loop* \u2022 Applies from individuals to teams (deadlines, sprints, reviews) --- What\u2019s Inside: \u2022 The Procrastination Loop: trigger \u2192 aversion tag \u2192 avoidance jump \u2192 guilt overlay \u2192 recurrence \u2022...", "url": "https://huggingface.co/posts/kanaria007/687620663929284", "date_published": "2025-10-15T05:22:25.338425"}, {"id": "https://huggingface.co/posts/AbstractPhil/715642693504510", "image": "", "title": "David + Imagenet = high% val.", "content_text": "David + Imagenet = high% val. AbstractPhil/gated-david https://github.com/AbstractEyes/lattice_vocabulary/blob/master/src/geovocab2/train/model/core/david.py David's code has been released. I am currently setting up a trainer and will release the process on how to condition David to behave. This isn't the easiest process, but it's necessary to run David on a curriculum rather than simply feeding the model with cross-entropy and hoping for the best. David's internals involve a clock mechanism that allows direct control of David's freeze/unfreeze mechanisms at runtime - allowing for many opinions to be generated simultaneously. David is multiple models in one, not just one - and yet David is single-shot oriented. The prototype to the route of thought that led me to find the Cantor's Stairs positional encodings solution and the prototype to ViT-Zana, ViT-Beatrix, ViT-Beatrix-Dual-Block, and today the direct porting of David's complex architecture and the process to train David has...", "url": "https://huggingface.co/posts/AbstractPhil/715642693504510", "date_published": "2025-10-15T05:22:25.338912"}]}