{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/danielhanchen/766411311755038", "image": "", "title": "\ud83e\udda5  Introducing Unsloth Dynamic v2.0 GGUFs!", "content_text": "\ud83e\udda5 Introducing Unsloth Dynamic v2.0 GGUFs! Our v2.0 quants set new benchmarks on 5-shot MMLU and KL Divergence, meaning you can now run & fine-tune quantized LLMs while preserving as much accuracy as possible. Llama 4: unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF DeepSeek-R1: unsloth/DeepSeek-R1-GGUF-UD Gemma 3: unsloth/gemma-3-27b-it-GGUF We made selective layer quantization much smarter. Instead of modifying only a subset of layers, we now dynamically quantize all layers so every layer has a different bit. Now, our dynamic method can be applied to all LLM architectures, not just MoE's. Blog with Details: https://docs.unsloth.ai/basics/dynamic-v2.0 All our future GGUF uploads will leverage Dynamic 2.0 and our hand curated 300K\u20131.5M token calibration dataset to improve conversational chat performance. For accurate benchmarking, we built an evaluation framework to match the reported 5-shot MMLU scores of Llama 4 and Gemma 3. This allowed apples-to-apples comparisons between full-...", "url": "https://huggingface.co/posts/danielhanchen/766411311755038", "date_published": "2025-04-26T09:21:43.300584"}, {"id": "https://huggingface.co/posts/ginipick/721764114758575", "image": "", "title": "\ud83d\ude80 AI Blog Generator with Streamlit: The Ultimate Guide!", "content_text": "\ud83d\ude80 AI Blog Generator with Streamlit: The Ultimate Guide! ginigen/blogger Hello there! Today I'm excited to introduce you to a powerful AI blog creation tool called Ginigen Blog. This amazing app automatically generates high-quality blog content using Streamlit and the latest ChatGPT 4.1 API. And the best part? It's completely free to use! \ud83d\udc69\u200d\ud83d\udcbb\u2728 \ud83e\udde0 What Makes Ginigen Blog Special Ginigen Blog is not just a simple text generator! It offers these exceptional features: Multiple Blog Templates: SEO-optimized, tutorials, reviews, and more Web Search Integration: Creates accurate content based on the latest information File Upload Analysis: Automatically analyzes TXT, CSV, and PDF files to incorporate into blogs Automatic Image Generation: Creates images that match your blog topic Various Output Formats: Download in Markdown, HTML, and more Latest GPT-4.1 Model: Cutting-edge AI technology for higher quality blog creation Completely Free Service: Access high-quality content generation without...", "url": "https://huggingface.co/posts/ginipick/721764114758575", "date_published": "2025-04-26T09:21:43.301258"}, {"id": "https://huggingface.co/posts/julien-c/991263782380176", "image": "", "title": "BOOOOM: Today I'm dropping TINY AGENTS", "content_text": "BOOOOM: Today I'm dropping TINY AGENTS the 50 lines of code Agent in Javascript \ud83d\udd25 I spent the last few weeks working on this, so I hope you will like it. I've been diving into MCP (Model Context Protocol) to understand what the hype was all about. It is fairly simple, but still quite powerful: MCP is a standard API to expose sets of Tools that can be hooked to LLMs. But while doing that, came my second realization: Once you have a MCP Client, an Agent is literally just a while loop on top of it. \ud83e\udd2f \u27a1\ufe0f read it exclusively on the official HF blog: https://huggingface.co/blog/tiny-agents See translation", "url": "https://huggingface.co/posts/julien-c/991263782380176", "date_published": "2025-04-26T09:21:43.301625"}, {"id": "https://huggingface.co/posts/shekkizh/475018614737384", "image": "", "title": "\ud83d\ude4b\ud83c\udffd\u200d\u2642\ufe0f Is your \"multi agent\" system really multi agentic? Or is it just a modular setup with a bunch of different prompts? \ud83e\udd28", "content_text": "\ud83d\ude4b\ud83c\udffd\u200d\u2642\ufe0f Is your \"multi agent\" system really multi agentic? Or is it just a modular setup with a bunch of different prompts? \ud83e\udd28 I\u2019ve had this discussion way too often, so I finally wrote it all down. If you\u2019re building with agents, you need to read this. Here\u2019s the TLDR: \u2705 True multi agent systems require: \u2022 Persistent, private state per agent \u2022 Memory that impacts future decisions \u2022 Adaptation based on past experiences \u274c Just having modular components, function calls, or multiple LLMs doesn't cut it. That\u2019s not multi agentic. It\u2019s just pipelining. \ud83e\udd1d The magic is in evolving relationships, context retention, and behavioral shifts over time. \ud83e\udde0 If your agents aren\u2019t learning from each other or changing based on past experience\u2026 you are missing the point. What do you think? Curious what patterns you're experimenting with \ud83e\uddd0 \ud83d\udc49 Full post: https://shekkizh.github.io/posts/2025/04/multi-agents/ See translation", "url": "https://huggingface.co/posts/shekkizh/475018614737384", "date_published": "2025-04-26T09:21:43.302040"}, {"id": "https://huggingface.co/posts/JingzeShi/736642176876033", "image": "", "title": "@SmallDoge", "content_text": "@ SmallDoge SmallTalks( SmallDoge/SmallTalks ) is a synthetic dataset designed for supervised fine-tuning of language models. The dataset covers a variety of conversational content, including daily conversations, tool usage, Python programming, encyclopedia Q&A, exam problem-solving, logical reasoning, and more. Each task is provided in both English and Chinese versions. See translation", "url": "https://huggingface.co/posts/JingzeShi/736642176876033", "date_published": "2025-04-26T09:21:43.302293"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/129160122230232", "image": "", "title": "30 seconds hard test on FramePack - [0] a man talking , [5] a man crying , [10] a man smiling , [15] a man frowning , [20] a man sleepy , [25] a man going crazy - i think result is excellent when we consider how hard this test is - Generated with SECourses FramePack App V40", "content_text": "30 seconds hard test on FramePack - [0] a man talking , [5] a man crying , [10] a man smiling , [15] a man frowning , [20] a man sleepy , [25] a man going crazy - i think result is excellent when we consider how hard this test is - Generated with SECourses FramePack App V40 App link and 1-click installers for Windows, RunPod and Massed Compute here : https://www.patreon.com/posts/126855226 I got the prompt using idea from this pull request : https://github.com/lllyasviel/FramePack/pull/218/files Not exactly same implementation but i think pretty accurate when considering that it is a 30 second 30 fps video at 840p resolution See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/129160122230232", "date_published": "2025-04-26T09:21:43.302603"}, {"id": "https://huggingface.co/posts/orasul/511709219027283", "image": "", "title": "hi, it is deki, and now I am open sourced.", "content_text": "hi, it is deki, and now I am open sourced. An Android AI agent powered by open-source ML model, \ud835\uddf1\ud835\uddf2\ud835\uddf8\ud835\uddf6, was fully open-sourced. It understands what\u2019s on your screen and can perform tasks based on your voice or text commands. Some examples: * \"Write my friend \"some_name\" in WhatsApp that I'll be 15 minutes late\" * \"Open Twitter in the browser and write a post about something\" * \"Read my latest notifications\" * \"Write a linkedin post about something\" Currently, it works only on Android \u2014 but support for other OS is planned. The ML and backend codes were also fully open-sourced. Video prompt example: \"Open linkedin, tap post and write: hi, it is deki, and now I am open sourced. But don't send, just return\" License: GPLv3 You can find other AI agent demos or usage examples, like, code generation or object detection in github. Github: https://github.com/RasulOs/deki See translation", "url": "https://huggingface.co/posts/orasul/511709219027283", "date_published": "2025-04-26T09:21:43.303014"}, {"id": "https://huggingface.co/posts/onekq/992154552707771", "image": "", "title": "I've recently attended a panel on AI applications. The panelists are managers/directors of Fortune 500 companies. These people make things happen and own results, so their stories and pain points are fresh.", "content_text": "I've recently attended a panel on AI applications. The panelists are managers/directors of Fortune 500 companies. These people make things happen and own results, so their stories and pain points are fresh. (1) Models are used EVERYWHERE, customer facing and internal support, etc. (2) A successful application must improve one of the following: revenue (\ud83d\udcb5\ud83d\udcb5), cost (\ud83d\udcb5\ud83d\udcb5), CSAT (still \ud83d\udcb5\ud83d\udcb5) (3) They proactively search on \ud83e\udd17HF\ud83e\udd17 for models and use them. Open source models (especially small ones) can flexibly fit into their existing workflows/infras, which enable them to deliver, and fast. (4) The main barrier for adoption is license. A director told me they picked a model and finetuned it, then learned they would have to share enhancements. As a result, they dropped this model and the million dollar impact went to another model. So to fellow model builders: (1) celebrate that our work is useful and generate lots of values (2) make your license permissive if you want maximum impact See...", "url": "https://huggingface.co/posts/onekq/992154552707771", "date_published": "2025-04-26T09:21:43.303435"}, {"id": "https://huggingface.co/posts/shekkizh/738330695730383", "image": "", "title": "Think AGI is just around the corner? Not so fast.", "content_text": "Think AGI is just around the corner? Not so fast. When OpenAI released its Computer-Using Agent (CUA) API, I happened to be playing Wordle \ud83e\udde9 and thought, why not see how the model handles it? Spoiler: Wordle turned out to be a surprisingly effective benchmark. So Romain Cosentino Ph.D. and I dug in and analyzed the results of several hundred runs. \ud83d\udd11 Takeaways 1\ufe0f\u20e3 Even the best computer-using models struggle with simple, context-dependent tasks. 2\ufe0f\u20e3 Visual perception and reasoning remain major hurdles for multimodal agents. 3\ufe0f\u20e3 Real-world use cases reveal significant gaps between hype and reality. Perception accuracy drops to near zero by the last turn \ud83d\udcc9 \ud83d\udd17 Read our arxiv article for more details https://www.arxiv.org/abs/2504.15434 See translation", "url": "https://huggingface.co/posts/shekkizh/738330695730383", "date_published": "2025-04-26T09:21:43.303866"}, {"id": "https://huggingface.co/posts/merve/911581845573108", "image": "", "title": "Don't sleep on new AI at Meta Vision-Language release! \ud83d\udd25", "content_text": "Don't sleep on new AI at Meta Vision-Language release! \ud83d\udd25 facebook/perception-encoder-67f977c9a65ca5895a7f6ba1 facebook/perception-lm-67f9783f171948c383ee7498 Meta dropped swiss army knives for vision with A2.0 license \ud83d\udc4f > image/video encoders for vision language modelling and spatial understanding (object detection etc) \ud83d\udc4f > The vision LM outperforms InternVL3 and Qwen2.5VL \ud83d\udc4f > They also release gigantic video and image datasets The authors attempt to come up with single versatile vision encoder to align on diverse set of tasks. They trained Perception Encoder (PE) Core: a new state-of-the-art family of vision encoders that can be aligned for both vision-language and spatial tasks. For zero-shot image tasks, it outperforms latest sota SigLIP2 \ud83d\udc4f > Among fine-tuned ones, first one is PE-Spatial. It's a model to detect bounding boxes, segmentation, depth estimation and it outperforms all other models \ud83d\ude2e > Second one is PLM, Perception Language Model, where they combine PE-Core with...", "url": "https://huggingface.co/posts/merve/911581845573108", "date_published": "2025-04-26T09:21:43.304421"}]}