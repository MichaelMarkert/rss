{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/codelion/741062673202173", "image": "", "title": "I wanted to share a technique that's been working really well for recovering performance after INT4 quantization.", "content_text": "I wanted to share a technique that's been working really well for recovering performance after INT4 quantization. Typically, quantizing the LLM to INT4 (unlike say INT8) for inference can incur some accuracy loss. Instead of accepting the quality loss, we used the FP16 model as a teacher to train a tiny LoRA adapter (rank=16) for the quantized model. The cool part: the model generates its own training data using the Magpie technique so no external datasets needed. This is critical because we want to remain as much as possible in the distribution of the model's natural responses. Last year Apple's foundational models paper ( https://arxiv.org/pdf/2407.21075 ) had proposed a similar technique and found \"By using accuracy-recovery LoRA adapters with only rank 16, Alpaca win rate can be improved by 7-18%, GMS8K accuracy is boosted by 5-10%.\" (page 47). We saw similar results on Qwen3-0.6B: Perplexity: 2.40 \u2192 2.09 (only 5.7% degradation from FP16 baseline) Memory: Only 0.28GB vs 1.0GB...", "url": "https://huggingface.co/posts/codelion/741062673202173", "date_published": "2025-08-29T13:30:07.499370"}, {"id": "https://huggingface.co/posts/AdinaY/473496396970919", "image": "", "title": "MiniCPM-V 4.5 \ud83d\ude80 New MLLM for image, multi-image & video understanding, running even on your phone, released by OpenBMB", "content_text": "MiniCPM-V 4.5 \ud83d\ude80 New MLLM for image, multi-image & video understanding, running even on your phone, released by OpenBMB openbmb/MiniCPM-V-4_5 \u2728 SOTA vision language capability \u2728 96\u00d7 video token compression > high-FPS & long video reasoning \u2728 Switchable fast vs deep thinking modes \u2728 Strong OCR, document parsing, supports 30+ languages See translation", "url": "https://huggingface.co/posts/AdinaY/473496396970919", "date_published": "2025-08-29T13:30:07.499681"}, {"id": "https://huggingface.co/posts/ginipick/809439997973106", "image": "", "title": "\ud83c\udf89 Fashion Fit 360: The New Standard in AI Virtual Try-On!", "content_text": "\ud83c\udf89 Fashion Fit 360: The New Standard in AI Virtual Try-On! \ud83d\ude80 Now Live and Free to Use!Say goodbye to online shopping uncertainty - \"Will this look good on me?\" - with our revolutionary solution!Fashion Fit 360 is a cutting-edge AI-powered virtual fitting service that transforms your fashion shopping experience. LINK: ginigen/Fashion-Fit360 \u2728 Core Features \ud83d\udd04 360-Degree Multi-Pose Generation Transform a single front-facing photo into 6 different viewing angles! Front, side, and back views for complete visualization Experience a real fitting room mirror effect Check fit and style from every perspective \ud83d\udc57 15 Fashion Item Categories Apparel: Tops, bottoms, dresses Jewelry: Necklaces, earrings, rings, bracelets Accessories: Sunglasses, eyewear, hats, ties, bow ties, belts Essentials: Bags, shoes \ud83c\udfaf Perfect For: \ud83d\udecd\ufe0f Online Shopping Enthusiasts: Preview before purchase - zero return hassles! \ud83d\udc8d Jewelry Lovers: Virtually try expensive pieces before investing \ud83c\udf81 Thoughtful Gift-Givers: Test items...", "url": "https://huggingface.co/posts/ginipick/809439997973106", "date_published": "2025-08-29T13:30:07.500292"}, {"id": "https://huggingface.co/posts/ginipick/948503650396274", "image": "", "title": "\ud83c\udf4c Nano Banana + Video: AI Image Style Transfer & Video Generation Tool", "content_text": "\ud83c\udf4c Nano Banana + Video: AI Image Style Transfer & Video Generation Tool \ud83c\udfa8 Key Features 1\ufe0f\u20e3 Image Style Transfer ginigen/Nano-Banana-Video \ud83d\udcf8 Upload up to 2 images for style fusion \u2728 High-quality image generation with Google Nano Banana model \ud83c\udfad Apply desired styles with text prompts 2\ufe0f\u20e3 Video Generation \ud83c\udfac Convert generated images to videos \ud83d\udcd0 Maintain original aspect ratio option \u23f1\ufe0f Adjustable duration (1-4 seconds) \ud83d\ude80 How to Use Step-by-Step Guide Step 1: Image Generation \ud83d\uddbc\ufe0f Enter style description Upload 1-2 images (optional) Click \"Generate Magic \u2728\" Step 2: Video Creation \ud83d\udcf9 Send generated image to video tab Set animation style Generate video! \ud83d\udca1 Use Cases \ud83c\udfde\ufe0f Transform landscape photos into artistic masterpieces \ud83e\udd16 Bring static images to life \ud83c\udfa8 Mix styles from two different images \ud83d\udcf1 Create short videos for social media \u26a1 Tech Stack Google Nano Banana Stable Video Diffusion Gradio Replicate API #AIVideoGenerator #ImageToVideoConverter #StyleTransferAI #GoogleNanoBanana...", "url": "https://huggingface.co/posts/ginipick/948503650396274", "date_published": "2025-08-29T13:30:07.500804"}, {"id": "https://huggingface.co/posts/merve/292180054306518", "image": "", "title": "first vision language model built off", "content_text": "first vision language model built off openai/gpt-oss-20b just dropped! \ud83d\udd25 InternVL3.5 comes with 32 models \ud83e\udd2f pre-trained, fine-tuned, aligned in various sizes OpenGVLab/internvl35-68ac87bd52ebe953485927fb comes with gpt-oss or Qwen3 for LLM part \u2935\ufe0f See translation", "url": "https://huggingface.co/posts/merve/292180054306518", "date_published": "2025-08-29T13:30:07.501049"}, {"id": "https://huggingface.co/posts/dhruv3006/852628837357270", "image": "", "title": "Pair a vision grounding model with a reasoning LLM with Cua", "content_text": "Pair a vision grounding model with a reasoning LLM with Cua Cua just shipped v0.4 of the Cua Agent framework with Composite Agents - you can now pair a vision/grounding model with a reasoning LLM using a simple modelA+modelB syntax. Best clicks + best plans. The problem: every GUI model speaks a different dialect. \u2022 some want pixel coordinates \u2022 others want percentages \u2022 a few spit out cursed tokens like <|loc095|> We built a universal interface that works the same across Anthropic, OpenAI, Hugging Face, etc.: agent = ComputerAgent( model=\"anthropic/claude-3-5-sonnet-20241022\", tools=[computer] ) But here\u2019s the fun part: you can combine models by specialization. Grounding model (sees + clicks) + Planning model (reasons + decides) \u2192 agent = ComputerAgent( model=\"huggingface-local/HelloKKMe/GTA1-7B+openai/gpt-4o\", tools=[computer] ) This gives GUI skills to models that were never built for computer use. One handles the eyes/hands, the other the brain. Think driver + navigator working...", "url": "https://huggingface.co/posts/dhruv3006/852628837357270", "date_published": "2025-08-29T13:30:07.501528"}, {"id": "https://huggingface.co/posts/openfree/636576339128278", "image": "", "title": "\ud83c\udf4c Nano Banana: Google AI Completely Free!", "content_text": "\ud83c\udf4c Nano Banana: Google AI Completely Free! \ud83c\udf89 Finally, Google's Nano Banana AI is available for everyone - absolutely FREE! \ud83c\udfaf Choose Your Perfect Version! \ud83c\udf1f Free Nano Banana - For Everyone Transform images with AI - It's that simple! \ud83d\ude80 Start in 3 Seconds 1\ufe0f\u20e3 Click Here 2\ufe0f\u20e3 Upload Image 3\ufe0f\u20e3 Enter Style \u2192 Done! \u2728 No Sign-up \u274c | No Payment \u274c | No Ads \u274c | Just Free \u2b55 \ud83d\udcf8 Simple drag & drop upload \u270f\ufe0f Describe styles in any language \u26a1 Results in under 30 seconds \ud83c\udfa8 Perfect for SNS, blogs, presentations \ud83d\udc49 Start Now: openfree/Free-Nano-Banana \ud83d\udd0d Nano Banana Upscale - For Designers Professional high-resolution output when you need it! \ud83d\uddbc\ufe0f 4x resolution upscaling (Real-ESRGAN) \ud83c\udfaf Optimized for print & large displays \ud83d\udc8e Premium quality with preserved details \ud83d\udcd0 Professional quality without Photoshop \ud83d\udc49 Create in HD: openfree/Nano-Banana-Upscale \ud83d\udcbb Nano Banana API - For Developers Power your app with AI! \ud83d\udd27 Instant RESTful API integration \ud83d\udce6 Python, JS, Java code examples included \u2699\ufe0f Batch processing &...", "url": "https://huggingface.co/posts/openfree/636576339128278", "date_published": "2025-08-29T13:30:07.502127"}, {"id": "https://huggingface.co/posts/codelion/968650774475150", "image": "", "title": "I recently added a recipe in ellora to improve reasoning capabilities to Gemma-3-1B using self-supervised learning. Model now shows step-by-step thinking in <think> tags before answering.", "content_text": "I recently added a recipe in ellora to improve reasoning capabilities to Gemma-3-1B using self-supervised learning. Model now shows step-by-step thinking in <think> tags before answering. Logic puzzle accuracy: 61% \u2192 84%. 3 hours training on single GPU. \ud83e\udde0 Used GRPO where model generates multiple responses and learns to prefer better reasoning. Works surprisingly well for making smaller models more transparent. \ud83d\udd17 Colab: https://colab.research.google.com/github/codelion/ellora/blob/main/Ellora_Recipe_2_Reasoning_LoRA_with_Self-Rewarding_GRPO.ipynb \ud83e\udd17 Model: codelion/gemma-3-1b-it-reasoning-grpo-lora \ud83d\udcbb Code: https://github.com/codelion/ellora See translation", "url": "https://huggingface.co/posts/codelion/968650774475150", "date_published": "2025-08-29T13:30:07.502471"}, {"id": "https://huggingface.co/posts/takarajordan/290509757896054", "image": "", "title": "I'm currently looking into what makes a scientific paper more popular than others on a platform like Hugging Face. I conducted a huge array of tests, content length, time based information even semantic feature extraction to get to some sort of answer around...", "content_text": "I'm currently looking into what makes a scientific paper more popular than others on a platform like Hugging Face. I conducted a huge array of tests, content length, time based information even semantic feature extraction to get to some sort of answer around... What actually drives popularity of these papers, why do some papers get zero upvotes and why do some get thousands? The answer is absolutely nothing. Yes that's right. Nothing about the actual paper itself drives popularity, the paper's popularity is driven by external factors like it's authors, external marketing and others. So next time you see a research paper with a lot of upvotes, just remember it's not because of the efforts of the authors. Remain objective. See translation", "url": "https://huggingface.co/posts/takarajordan/290509757896054", "date_published": "2025-08-29T13:30:07.502783"}, {"id": "https://huggingface.co/posts/tsungyi/147340620272288", "image": "", "title": "Cosmos Reason just topped Physical Reasoning Leaderboard on Hugging Face. \ud83d\udc4f\ud83d\udd25", "content_text": "Cosmos Reason just topped Physical Reasoning Leaderboard on Hugging Face. \ud83d\udc4f\ud83d\udd25 Cosmos Reason is an open, customizable, commercial-ready 7B-parameter, reasoning vision language model (VLM) for physical AI and robotics. The VLM empowers robots and vision AI agents to reason like humans, leveraging prior knowledge, physics understanding, and common sense to understand and operate intelligently in the real world. This model unlocks advanced capabilities for robotics, autonomous vehicles, and real-world operations\u2014from cities to high-tech factories. Key use cases include: Data curation & annotation: Automate high-quality dataset curation and annotation at scale. Robot planning & reasoning: Serve as the \"brain\" for deliberate, methodical decision-making with vision language action (VLA) models. Video analytics AI agents: Extract actionable insights and perform root-cause analysis on massive video datasets. Ready to build the next generation of physical AI? Get started \ud83d\udc49 nvidia/Cosmos-...", "url": "https://huggingface.co/posts/tsungyi/147340620272288", "date_published": "2025-08-29T13:30:07.503210"}]}