{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Wauplin/921235032674409", "image": "", "title": "Say hello to", "content_text": "Say hello to hf : a faster, friendlier Hugging Face CLI \u2728 We are glad to announce a long-awaited quality-of-life improvement: the Hugging Face CLI has been officially renamed from huggingface-cli to hf! So... why this change? Typing huggingface-cli constantly gets old fast. More importantly, the CLI\u2019s command structure became messy as new features were added over time (upload, download, cache management, repo management, etc.). Renaming the CLI is a chance to reorganize commands into a clearer, more consistent format. We decided not to reinvent the wheel and instead follow a well-known CLI pattern: hf <resource> <action>. Isn't hf auth login easier to type and remember? The full rationale, implementation details, and migration notes are in the blog post: https://huggingface.co/blog/hf-cli See translation", "url": "https://huggingface.co/posts/Wauplin/921235032674409", "date_published": "2025-07-26T17:23:35.598462"}, {"id": "https://huggingface.co/posts/mitkox/940300076081193", "image": "", "title": "I run Qwen3-Coder 480B locally on my Z8, with a 1-million token context window. It\u2019s the equivalent of parallel-parking a Nimitz-class carrier in a kiddie pool. Thanks to whatever dark pact the llama.cpp, CUDA, and kernel folks signed, hybrid inferencing + VRAM\u2194RAM offload let me stream the model\u2019s synapses across Xeon, RAM, and four lonely A6000s without summoning either the OOM killer or a small house fire.", "content_text": "I run Qwen3-Coder 480B locally on my Z8, with a 1-million token context window. It\u2019s the equivalent of parallel-parking a Nimitz-class carrier in a kiddie pool. Thanks to whatever dark pact the llama.cpp, CUDA, and kernel folks signed, hybrid inferencing + VRAM\u2194RAM offload let me stream the model\u2019s synapses across Xeon, RAM, and four lonely A6000s without summoning either the OOM killer or a small house fire. See translation", "url": "https://huggingface.co/posts/mitkox/940300076081193", "date_published": "2025-07-26T17:23:35.598823"}, {"id": "https://huggingface.co/posts/Xenova/793837995432659", "image": "", "title": "Introducing Voxtral WebGPU: State-of-the-art audio transcription directly in your browser! \ud83e\udd2f", "content_text": "Introducing Voxtral WebGPU: State-of-the-art audio transcription directly in your browser! \ud83e\udd2f \ud83d\udde3\ufe0f Transcribe videos, meeting notes, songs and more \ud83d\udd10 Runs on-device, meaning no data is sent to a server \ud83c\udf0e Multilingual (8 languages) \ud83e\udd17 Completely free (forever) & open source That's right, we're running Mistral's new Voxtral-Mini-3B model 100% locally in-browser on WebGPU, powered by Transformers.js and ONNX Runtime Web! \ud83d\udd25 Try it out yourself! \ud83d\udc47 webml-community/Voxtral-WebGPU See translation", "url": "https://huggingface.co/posts/Xenova/793837995432659", "date_published": "2025-07-26T17:23:35.599147"}, {"id": "https://huggingface.co/posts/hesamation/830297477341251", "image": "", "title": "longer context doesn't generate better responses. it can even hurt your llm/agent.  1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it.", "content_text": "longer context doesn't generate better responses. it can even hurt your llm/agent. 1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it. here are 4 types of context failure and why each one happens: 1. context poisoning: if hallucination finds its way into your context, the agent will rely on that false information to make its future moves. for example if the agent hallucinates about the \"task description\", all of its planning to solve the task would also be corrupt. 2. context distraction: when the context becomes too bloated, the model focuses too much on it rather than come up with novel ideas or to follow what it has learned during training. as Gemini 2.5 Pro technical report points out, as context grows significantly from 100K tokens, \"the agent showed a tendency toward favoring repeating actions from its vast history rather than synthesizing novel plans\". 3. context confusion: everyone lost it when MCPs became popular, it...", "url": "https://huggingface.co/posts/hesamation/830297477341251", "date_published": "2025-07-26T17:23:35.599668"}, {"id": "https://huggingface.co/posts/Blazgo/275361717193984", "image": "", "title": "\ud83d\ude80 Deca 3 Ultra Alpha is coming in the next 72 hours! \ud83d\ude80", "content_text": "\ud83d\ude80 Deca 3 Ultra Alpha is coming in the next 72 hours! \ud83d\ude80 We're on the verge of something monumental. Right now, we're in the final stages of testing, and we're about to drop a game-changing milestone in the open-source AI community. \ud83c\udf89 In just two weeks, we've managed to almost 4x the size of the largest open-source LLM at that time (and we are still 2.6x bigger than the largest LLM). This is unprecedented and a testament to the power of collaboration, innovation, and the relentless pursuit of pushing AI to its limits. The future of open-source AI is now. Stay tuned for the release \u2013 we\u2019re just getting started. - Model testing finishes: 24hrs from now - Model gets uploaded: 30hrs from now - Related code/inference stack gets published: 70-90hrs from now See translation", "url": "https://huggingface.co/posts/Blazgo/275361717193984", "date_published": "2025-07-26T17:23:35.600056"}, {"id": "https://huggingface.co/posts/AdinaY/243707766122533", "image": "", "title": "Big respect to the Qwen team! They just dropped another model\ud83d\udd25", "content_text": "Big respect to the Qwen team! They just dropped another model\ud83d\udd25 Qwen3-235B-A22B-Thinking-2507 \ud83e\udde0 new reasoning model by Qwen Qwen/Qwen3-235B-A22B-Thinking-2507 \u2728 235B total / 22B active (8 experts) \u2728 256K context window \u2728 Agent-ready with tool use & <think> reasoning mode Hope the team gets some well-deserved rest this weekend after all the massive releases \ud83d\ude4c See translation", "url": "https://huggingface.co/posts/AdinaY/243707766122533", "date_published": "2025-07-26T17:23:35.600349"}, {"id": "https://huggingface.co/posts/prithivMLmods/432897219160306", "image": "", "title": "Excited to introduce the new experimental model \"Qwen2.5-VL-7B-Abliterated-Caption-it\", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.\ud83e\uddea\ud83e\udd17", "content_text": "Excited to introduce the new experimental model \"Qwen2.5-VL-7B-Abliterated-Caption-it\", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.\ud83e\uddea\ud83e\udd17 \u2728 Try the demo here : prithivMLmods/Qwen2.5-VL \u2728 Qwen2.5-VL-7B-Abliterated-Caption-it : prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it \u2728 Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 \u2728 Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See translation", "url": "https://huggingface.co/posts/prithivMLmods/432897219160306", "date_published": "2025-07-26T17:23:35.600746"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/822376760397275", "image": "", "title": "NEW ARTICLE: \"Detecting Beyond Sight: Building AI-Enabled SAR Intelligence with Synthetic Data\"", "content_text": "NEW ARTICLE: \"Detecting Beyond Sight: Building AI-Enabled SAR Intelligence with Synthetic Data\" Synthetic Aperture Radar (SAR) reveals what optical sensors can\u2019t. AI can turn that information into actionable intelligence\u2014but only with the right training data. In our latest blog, we explore how Falcon\u2019s new virtual SAR sensor solves the SAR data bottleneck for AI development. As the newest addition to Falcon\u2019s sensor library, it models radar returns with precision\u2014including azimuth, range resolution, signal intensity, and noise. This Falcon-specific, GPU-accelerated raytraced SAR model is exposed via Falcon\u2019s Python API, giving teams precise, control over radar wave propagation and enabling physically grounded, highly customizable, and user-friendly SAR simulation. The result? High-fidelity, automatically labeled synthetic SAR imagery from any scenario\u2014on demand. No custom setup. No external workflows. Just mission-ready data for building AI models across defense, disaster response,...", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/822376760397275", "date_published": "2025-07-26T17:23:35.601194"}, {"id": "https://huggingface.co/posts/AdinaY/225572236442446", "image": "", "title": "Qwen is on fire this week \ud83d\udd25", "content_text": "Qwen is on fire this week \ud83d\udd25 They just released Qwen3-MT \ud83c\udf0d a translation model supports 92 languages. Demo is available on the hub. Qwen/Qwen3-MT-Demo \u2728 Highly Customizable: Supports custom terms, domain prompts, and translation memory for accurate, context-aware results. \u2728 Fast and affordable: $0.5 per million tokens. See translation", "url": "https://huggingface.co/posts/AdinaY/225572236442446", "date_published": "2025-07-26T17:23:35.601470"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/878234498759324", "image": "", "title": "Excuse the lag, it's from the real-time inference from the webcam \ud83d\udc40 . Did you know that YOLOv11 added Streamlit for live object detection straight from your webcam?", "content_text": "Excuse the lag, it's from the real-time inference from the webcam \ud83d\udc40 . Did you know that YOLOv11 added Streamlit for live object detection straight from your webcam? \ud83d\udcf8 Learn about live inference with YOLOv11 and the Streamlit Application - https://docs.ultralytics.com/guides/streamlit-live-inference/ \ud83e\udd6b Join the mentioned Kaggle competition here - https://www.kaggle.com/competitions/multi-class-object-detection-challenge/overview \ud83d\udc40 Checkout Duality AI - https://www.duality.ai/?utm_source=hf&utm_medium=post&utm_campaign=video \ud83e\udee1 Checkout 3LC.AI - https://3lc.ai/ See translation", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/878234498759324", "date_published": "2025-07-26T17:23:35.601773"}]}