{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/jsulz/651298897017923", "image": "", "title": "We've crossed 1 million repositories backed by Xet storage on Hugging Face! \ud83d\ude80\ud83d\ude80\ud83d\ude80", "content_text": "We've crossed 1 million repositories backed by Xet storage on Hugging Face! \ud83d\ude80\ud83d\ude80\ud83d\ude80 You can follow along our progress converting the Hub from Git LFS to Xet at jsulz/ready-xet-go We have a lot of repos left to migrate, which means I have plenty of time to add more animations \ud83e\udd2a See translation", "url": "https://huggingface.co/posts/jsulz/651298897017923", "date_published": "2025-08-02T13:33:49.254860"}, {"id": "https://huggingface.co/posts/mitkox/340947617819121", "image": "", "title": "I got 370 tokens/sec of Qwen3-30B-A3B 2507 on my desktop Z8 GPU workstation. My target is 400 t/s, and the last 10 % always tastes like victory!", "content_text": "I got 370 tokens/sec of Qwen3-30B-A3B 2507 on my desktop Z8 GPU workstation. My target is 400 t/s, and the last 10 % always tastes like victory! See translation", "url": "https://huggingface.co/posts/mitkox/340947617819121", "date_published": "2025-08-02T13:33:49.255088"}, {"id": "https://huggingface.co/posts/YerbaPage/437092763804097", "image": "", "title": "Latest work on SWE-Bench \ud83d\udc1b", "content_text": "Latest work on SWE-Bench \ud83d\udc1b Our two new papers from the SJTU & Huawei: Powered by DeepSeek-V3, we've achieved a new SOTA on the SWE-Bench benchmark! We introduce two innovative approaches: \u2694\ufe0f SWE-Debate: AI agents compete and \"debate\" to generate the best code fix. \ud83e\udde0 SWE-Exp: An AI agent learns from past repair \"experience\" to solve new issues more efficiently. \ud83d\udc47 Explore the future of software development: SWE-Debate \ud83d\udcc4 Paper: https://arxiv.org/abs/2507.23348 \ud83d\udcbb Code: https://github.com/YerbaPage/SWE-Debate SWE-Exp \ud83d\udcc4 Paper: https://arxiv.org/abs/2507.23361 \ud83d\udcbb Code: https://github.com/YerbaPage/SWE-Exp See translation", "url": "https://huggingface.co/posts/YerbaPage/437092763804097", "date_published": "2025-08-02T13:33:49.255434"}, {"id": "https://huggingface.co/posts/chintankp/680096865746882", "image": "", "title": "We\u2019re excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100.", "content_text": "We\u2019re excited to share that Llama Nemotron Super v1.5 -- our latest open reasoning model -- is leading the Artificial Analysis Intelligence Index - a leaderboard that spans advanced math, science, and agentic tasks, for models running on a single NVIDIA H100. Super v1.5 is trained with high-quality reasoning synthetic data generated from models like Qwen3-235B and DeepSeek R1. Besides leading accuracy, it also delivers high throughput. Key features: - Leading accuracy on multi-step reasoning, math, coding, and function-calling - Post-trained using RPO, DPO, and RLVR across 26M+ synthetic examples - Fully transparent training data on HF (Nemotron-Post-Training-Dataset-v1) Try Super v1.5 on build.nvidia.com or download from Hugging Face See translation", "url": "https://huggingface.co/posts/chintankp/680096865746882", "date_published": "2025-08-02T13:33:49.255797"}, {"id": "https://huggingface.co/posts/IlyasMoutawwakil/721004690541036", "image": "", "title": "\ud83d\ude80 Optimum: The Last v1 Release \ud83d\ude80", "content_text": "\ud83d\ude80 Optimum: The Last v1 Release \ud83d\ude80 Optimum v1.27 marks the final major release in the v1 series. As we close this chapter, we're laying the groundwork for a more modular and community-driven future: - Optimum v2: A lightweight core package for porting Transformers, Diffusers, or Sentence-Transformers to specialized AI hardware/software/accelerators.. - Optimum\u2011ONNX: A dedicated package where the ONNX/ONNX Runtime ecosystem lives and evolves, faster-moving and decoupled from the Optimum core. \ud83c\udfaf Why this matters: - A clearer governance path for ONNX, fostering stronger community collaboration and improved developer experience.. - Enable innovation at a faster pace in a more modular, open-source environment. \ud83d\udca1 What this means: - More transparency, broader participation, and faster development driven by the community and key actors in the ONNX ecosystem (PyTorch, Microsoft, Joshua Lochner \ud83d\udc40, ...) - A cleaner, more maintainable core Optimum, focused on extending HF libraries to special AI...", "url": "https://huggingface.co/posts/IlyasMoutawwakil/721004690541036", "date_published": "2025-08-02T13:33:49.256369"}, {"id": "https://huggingface.co/posts/merve/770620321222949", "image": "", "title": "Cohere just dropped", "content_text": "Cohere just dropped CohereLabs/command-a-vision-07-2025 , a 112B (dense!) vision LM > based on SigLIP2 & Command-A > built for enterprise use cases \ud83d\udd25 > use with Inference Providers or transformers \ud83e\udd17 read their blog https://huggingface.co/blog/CohereLabs/introducing-command-a-vision-07-2025 See translation", "url": "https://huggingface.co/posts/merve/770620321222949", "date_published": "2025-08-02T13:33:49.256628"}, {"id": "https://huggingface.co/posts/AdinaY/564352975503737", "image": "", "title": "Qwen team did it again!!", "content_text": "Qwen team did it again!! They just released Qwen3-Coder-30B-A3B-Instruct on the hub\ud83d\udd25 Qwen/Qwen3-Coder-30B-A3B-Instruct \u2728 Apache 2.0 \u272830B total / 3.3B active (128 experts, 8 top-k) \u2728 Native 256K context, extendable to 1M via Yarn \u2728 Built for Agentic Coding See translation", "url": "https://huggingface.co/posts/AdinaY/564352975503737", "date_published": "2025-08-02T13:33:49.256870"}, {"id": "https://huggingface.co/posts/prithivMLmods/923940739727688", "image": "", "title": "Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, we\u2019ve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources.", "content_text": "Exciting to bring the explicitly grounded experimental reasoning model, Lumian-VLR-7B-Thinking, built on top of Qwen2.5-VL, featuring reasoning-aware trajectories with enhanced spatial perception. Along with this, we\u2019ve also added a demo for the model while bringing some of the latest and most interesting models available on the hub to make full use of the remaining resources. \u2728 Multimodal-VLM-Thinking : prithivMLmods/Multimodal-VLM-Thinking \u2728 Multimodal-VLM-OCR : prithivMLmods/Multimodal-VLM-OCR \u2726 Models used in these spaces: \u2728 Lumian-VLR-7B-Thinking : prithivMLmods/Lumian-VLR-7B-Thinking \u2728 Enesidaon-VLR-7B-no-Thinking : prithivMLmods/Enesidaon-VLR-7B-no-Thinking \u2728 GLM-4.1V-9B-Thinking : zai-org/GLM-4.1V-9B-Thinking \u2728 DREX-062225-exp : prithivMLmods/DREX-062225-exp & more ... \u2726 Multimodal Model Collections and Spaces: \u2728 Vision-Language (VLr) : prithivMLmods/vision-language-for-reasoning-vlr-6889b3f45917352b5e3a6f7a \u2728 Multimodal Spaces : prithivMLmods/multimodal-...", "url": "https://huggingface.co/posts/prithivMLmods/923940739727688", "date_published": "2025-08-02T13:33:49.257343"}, {"id": "https://huggingface.co/posts/AdinaY/263963364371914", "image": "", "title": "Qwen3-30B-A3B-Thinking-2507 \ud83d\udd25 latest step in scaling thinking capabilities from  Alibaba Qwen team.", "content_text": "Qwen3-30B-A3B-Thinking-2507 \ud83d\udd25 latest step in scaling thinking capabilities from Alibaba Qwen team. Qwen/Qwen3-30B-A3B-Thinking-2507-FP8 \u2728 30B total / 3B active - Apache 2.0 \u2728 Native 256K context \u2728 SOTA coding, alignment, agentic reasoning See translation", "url": "https://huggingface.co/posts/AdinaY/263963364371914", "date_published": "2025-08-02T13:33:49.257581"}, {"id": "https://huggingface.co/posts/mitkox/598805408500117", "image": "", "title": "We\u2019ve reached a point where on device AI coding that is free, offline, and capable isn\u2019t just a theoretical possibility; it\u2019s sitting on my lap, barely warming my thighs.", "content_text": "We\u2019ve reached a point where on device AI coding that is free, offline, and capable isn\u2019t just a theoretical possibility; it\u2019s sitting on my lap, barely warming my thighs. My local MacBook Air setup includes a Qwen3 Coder Flash with a 1M context, Cline in a VSCode IDE. No internet, no cloud, no ID verification- this is the forbidden tech. Current stats: All agentic tools work great local, sandboxed, and MCP OK model output precision 17 tokens/sec. Not great, not terrible 65K tokens context, the model can do 1M, but let\u2019s be real, my MacBook Air would probably achieve fusion before hitting that smoothly Standard backend and cache off for the test All inference and function calling happen locally, offline, untethered. The cloud didn\u2019t even get a memo. See translation", "url": "https://huggingface.co/posts/mitkox/598805408500117", "date_published": "2025-08-02T13:33:49.257938"}]}