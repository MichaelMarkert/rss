{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/etemiz/710778843328598", "image": "", "title": "gpt-oss-120B scored 28 (one of the lowest) on AHA leaderboard. not very human aligned model.", "content_text": "gpt-oss-120B scored 28 (one of the lowest) on AHA leaderboard. not very human aligned model. these kind of models are not really \"free\": they are costing you your freedom if you know what i mean. See translation", "url": "https://huggingface.co/posts/etemiz/710778843328598", "date_published": "2025-08-16T13:29:58.266513"}, {"id": "https://huggingface.co/posts/fdaudens/770107969696647", "image": "", "title": "Want to learn to build an AI Agent? I put together a cookbook for creating your own news research agent with OpenAI GPT-OSS:", "content_text": "Want to learn to build an AI Agent? I put together a cookbook for creating your own news research agent with OpenAI GPT-OSS: - Searches headlines & specific sites - Pulls full articles when you need depth - Summarizes with clickable sources - Runs in a simple Gradio chat UI - No GPU, no local setup \u2014 just open-weight GPT-OSS models via Hugging Face If you\u2019ve been wanting to try agents but weren\u2019t sure where to start, this is an end-to-end example you can fork, run, and adapt. Full guide + code https://huggingface.co/blog/fdaudens/openai-gpt-oss-agent-inference-providers See translation", "url": "https://huggingface.co/posts/fdaudens/770107969696647", "date_published": "2025-08-16T13:29:58.266868"}, {"id": "https://huggingface.co/posts/nroggendorff/812423234168314", "image": "", "title": "No, I did not create those bots that just got banned today.", "content_text": "No, I did not create those bots that just got banned today. See translation", "url": "https://huggingface.co/posts/nroggendorff/812423234168314", "date_published": "2025-08-16T13:29:58.267063"}, {"id": "https://huggingface.co/posts/ovi054/459497213356295", "image": "", "title": "Update on", "content_text": "Update on ovi054/Qwen-Image-LORA \u26a1 You can now load a Qwen LoRA in this space as follows: 1. Model ID: flymy-ai/qwen-image-realism-lora 2. Model link: https://huggingface.co /flymy-ai/ qwen-image-realism-lora 3. Specific file link: https://huggingface.co /flymy-ai/ qwen-image-realism-lora /blob/m ain/flymy_realism.safetensors 4. Direct download link: https://huggingface.co /flymy-ai/ qwen-image-realism-lora /resolve/m ain/flymy_realism.safetensors You can also use an external .safetensors download link (if Hugging Face doesn\u2019t block it). It is useful if a model repository contains multiple weights and you want to load a specific one. \ud83d\udc49 Try it now: ovi054/Qwen-Image-LORA See translation", "url": "https://huggingface.co/posts/ovi054/459497213356295", "date_published": "2025-08-16T13:29:58.267446"}, {"id": "https://huggingface.co/posts/ovi054/657358125503535", "image": "", "title": "Image-to-Prompt\u26a1", "content_text": "Image-to-Prompt\u26a1 ovi054/image-to-prompt Extract text prompt from image. And you can reuse the prompt to generate similar images! Useful for prompt engineering, studying image-to-text alignment, making training datasets, or recreating similar outputs. Powered by: Gradio, Florence 2 \ud83d\udc49 Try it now: ovi054/image-to-prompt See translation", "url": "https://huggingface.co/posts/ovi054/657358125503535", "date_published": "2025-08-16T13:29:58.267713"}, {"id": "https://huggingface.co/posts/mlabonne/575026837446793", "image": "", "title": "Liquid just released two 450M and 1.6B param VLMs!", "content_text": "Liquid just released two 450M and 1.6B param VLMs! They're super fast and leverage SigLIP2 NaFlex encoders to handle native resolutions without distortion. It's ideal for on-device deployment in constrained environments like phones. It's available today on Hugging Face, with an inference and a fine-tuning Colab notebooks. LiquidAI/LFM2-VL-450M LiquidAI/LFM2-VL-1.6B See translation", "url": "https://huggingface.co/posts/mlabonne/575026837446793", "date_published": "2025-08-16T13:29:58.267962"}, {"id": "https://huggingface.co/posts/anakin87/751707976654130", "image": "", "title": "Want to quickly try Gemma 3 270m? \ud83d\udc8e\ud83d\udcac", "content_text": "Want to quickly try Gemma 3 270m? \ud83d\udc8e\ud83d\udcac I made a simple Space to do that: anakin87/gemma-3-270m-it \u26a1 Fast: Flash Attention, Zero GPU \u2699\ufe0f Configurable See translation", "url": "https://huggingface.co/posts/anakin87/751707976654130", "date_published": "2025-08-16T13:29:58.268184"}, {"id": "https://huggingface.co/posts/space-sue/968839932309229", "image": "", "title": "\ud83d\udd25 Wild Fire Tracker with Gradio MCP", "content_text": "\ud83d\udd25 Wild Fire Tracker with Gradio MCP Just finished this fire/smoke detection space that watches video feeds and alerts when it spots danger. Uses computer vision to analyze frames every 10 seconds. Agents-MCP-Hackathon/wild-fire-tracker What it does: -Detects fire & smoke in real-time -Works with webcam, video files, or RTSP streams -Shows confidence scores + color-coded alerts - Has both web UI and API integration Looking for feedback on: -Detection accuracy -What features would be most useful -Real-world use cases you'd want Would love to hear thoughts from anyone working in fire monitoring, emergency response, or just interested in computer vision projects! Try it out and let me know what you think. See translation", "url": "https://huggingface.co/posts/space-sue/968839932309229", "date_published": "2025-08-16T13:29:58.268574"}, {"id": "https://huggingface.co/posts/kanaria007/826776352207938", "image": "", "title": "\u2705 New Article: *Philosophy of Science as Structured Cognition*", "content_text": "\u2705 New Article: *Philosophy of Science as Structured Cognition* Title: \ud83d\udd2c Epistemic Architecture: Structured Intelligence and the Meta-Structure of Scientific Reasoning \ud83d\udd17 https://huggingface.co/blog/kanaria007/structured-philosophy-of-science --- Summary: Science is often described as *method* or *accumulated knowledge*. Structured Intelligence reframes it as *epistemic architecture*: * Hypotheses as *jumpable structures* * Experiments as *self\u2011auditing loops* * Theories as *reusable, ethically constrained models* > Science isn\u2019t just discovery \u2014 > *it\u2019s the structured evolution of understanding.* --- Why It Matters: \u2022 Makes *reasoning paths explicit and auditable* \u2022 Connects *scientific method with cognitive architecture* \u2022 Enables *AI systems to model knowledge growth transparently* --- What\u2019s Inside: \u2022 Science as *structured jumps and self\u2011corrections* \u2022 Comparison of *linear method vs recursive architecture* \u2022 Structural conditions for *reproducibility and paradigm shifts* \u2022...", "url": "https://huggingface.co/posts/kanaria007/826776352207938", "date_published": "2025-08-16T13:29:58.269121"}, {"id": "https://huggingface.co/posts/ezgikorkmaz/616130439361444", "image": "", "title": "If you are interested safety and adversarial perspective in deep reinforcement learning, I will leave this repository here:", "content_text": "If you are interested safety and adversarial perspective in deep reinforcement learning, I will leave this repository here: Link: https://github.com/EzgiKorkmaz/adversarial-reinforcement-learning See translation", "url": "https://huggingface.co/posts/ezgikorkmaz/616130439361444", "date_published": "2025-08-16T13:29:58.269344"}]}