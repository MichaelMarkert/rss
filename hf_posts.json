{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/MonsterMMORPG/482948371636786", "image": "", "title": "Qwen Image Models Training - 0 to Hero Level Tutorial - LoRA & Fine Tuning - Base & Edit Model -", "content_text": "Qwen Image Models Training - 0 to Hero Level Tutorial - LoRA & Fine Tuning - Base & Edit Model - https://youtu.be/DPX3eBTuO_Y This is a full comprehensive step-by-step tutorial for how to train Qwen Image models. This tutorial covers how to do LoRA training and full Fine-Tuning / DreamBooth training on Qwen Image models. It covers both the Qwen Image base model and the Qwen Image Edit Plus 2509 model. This tutorial is the product of 21 days of full R&D, costing over $800 in cloud services to find the best configurations for training. Furthermore, we have developed an amazing, ultra-easy-to-use Gradio app to use the legendary Kohya Musubi Tuner trainer with ease. You will be able to train locally on your Windows computer with GPUs with as little as 6 GB of VRAM for both LoRA and Fine-Tuning. Furthermore, I have shown how to train a character (person), a product (perfume) and a style (GTA5 artworks). Tutorial Link : https://youtu.be/DPX3eBTuO_Y See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/482948371636786", "date_published": "2025-11-07T13:31:19.300656"}, {"id": "https://huggingface.co/posts/codelion/382097318111878", "image": "", "title": "On this day in 2019, OpenAI released the final GPT-2 model as part of their staged release. I still remember that November well - so much was happening, but GPT-2's release felt like a watershed moment for the field. It showed us what was possible with carefully trained language models.", "content_text": "On this day in 2019, OpenAI released the final GPT-2 model as part of their staged release. I still remember that November well - so much was happening, but GPT-2's release felt like a watershed moment for the field. It showed us what was possible with carefully trained language models. To recreate some of that GPT-2 magic, I recently tackled an interesting challenge: can you pretrain a language model with just 1 billion tokens - roughly 1/10th of what GPT-2 used - and still get comparable performance? After 50+ systematic experiments testing different dataset mixtures, the answer is yes. The result is codelion/gpt-2-70m , which achieves over 90% of GPT-2's benchmark performance despite being trained on 10x less data. The key was finding the optimal dataset composition: 50% high-quality textbook PDFs, 30% filtered web content, and 20% educational resources. It even beats GPT-2 on TruthfulQA (47.31% vs 40.69%). If you're interested in the full story of how we discovered this optimal...", "url": "https://huggingface.co/posts/codelion/382097318111878", "date_published": "2025-11-07T13:31:19.301080"}, {"id": "https://huggingface.co/posts/DawnC/897621817995080", "image": "", "title": "Pixcribe \u2014 AI-Powered Social Media Caption Generator \ud83d\udcf8\u2728", "content_text": "Pixcribe \u2014 AI-Powered Social Media Caption Generator \ud83d\udcf8\u2728 Transform your images into compelling stories with intelligent multi-model analysis! What can Pixcribe do? \ud83d\udcf8 Upload photos (up to 10) to get instant AI-generated captions in Traditional Chinese and English - \ud83c\udff7\ufe0f Brand Recognition \u2014 Detects logos and brand elements through visual detection, semantic analysis, and OCR verification. - \ud83c\udfa8 Scene Understanding \u2014 Analyzes composition, lighting conditions, and visual aesthetics to capture your image's mood and context. - \ud83d\udd0d Smart Text Extraction \u2014 Identifies and incorporates text from your images into captions seamlessly. - \u26a1 Multi-Model Intelligence \u2014 Combines YOLOv11 object detection, OpenCLIP semantic understanding, EasyOCR text recognition, U2-Net saliency detection, and Qwen2.5-VL-7B caption generation. What's next? \ud83c\udfac Video processing capabilities \ud83c\udf10 Enhanced multilingual support \ud83c\udfaf Interactive caption refinement with user feedback \u26a1 Real-time processing optimizations - Current Status:...", "url": "https://huggingface.co/posts/DawnC/897621817995080", "date_published": "2025-11-07T13:31:19.301626"}, {"id": "https://huggingface.co/posts/AdinaY/911192969719025", "image": "", "title": "Kimi K2 Thinking is now live on the hub \ud83d\udd25", "content_text": "Kimi K2 Thinking is now live on the hub \ud83d\udd25 moonshotai/Kimi-K2-Thinking \u2728 1T MoE for deep reasoning & tool use \u2728 Native INT4 quantization = 2\u00d7 faster inference \u2728 256K context window \u2728 Modified MIT license See translation", "url": "https://huggingface.co/posts/AdinaY/911192969719025", "date_published": "2025-11-07T13:31:19.301871"}, {"id": "https://huggingface.co/posts/sergiopaniego/990279445625588", "image": "", "title": "fine-tuning a 14B model with TRL + SFT on a free Colab (T4 GPU)?", "content_text": "fine-tuning a 14B model with TRL + SFT on a free Colab (T4 GPU)? thanks to the latest TRL optimizations, you actually can! sharing a new notebook showing how to do it \ud83d\ude0e colab: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_trl_lora_qlora.ipynb notebooks in TRL: https://github.com/huggingface/trl/tree/main/examples/notebooks See translation", "url": "https://huggingface.co/posts/sergiopaniego/990279445625588", "date_published": "2025-11-07T13:31:19.302159"}, {"id": "https://huggingface.co/posts/mike-ravkine/305096078349013", "image": "", "title": "There is no anxiety quite like powering up 2KW of basement compute after rewiring it all. Small bit of trouble with the horizontal 3090 because I misread my motherboard manual, but otherwise so far so good.. Next we see if I've built up enough cooling to hit my target TDP on those 3-slot nvlinked cards especially.  The 4-slot bridges are much easier to work with but their prices went bananas and I couldn't acquire a second, so gotta get a little creative with intakes.", "content_text": "There is no anxiety quite like powering up 2KW of basement compute after rewiring it all. Small bit of trouble with the horizontal 3090 because I misread my motherboard manual, but otherwise so far so good.. Next we see if I've built up enough cooling to hit my target TDP on those 3-slot nvlinked cards especially. The 4-slot bridges are much easier to work with but their prices went bananas and I couldn't acquire a second, so gotta get a little creative with intakes. See translation", "url": "https://huggingface.co/posts/mike-ravkine/305096078349013", "date_published": "2025-11-07T13:31:19.302431"}, {"id": "https://huggingface.co/posts/branikita/234092651364485", "image": "", "title": "Load test conducted on the Feetech STS3250 servo motor.", "content_text": "Load test conducted on the Feetech STS3250 servo motor. With a 2 kg load on a 100 mm arm, the motor operated near its limit. At higher acceleration settings, lifting performance decreased noticeably. The temperature increased from 40 \u00b0C to 70 \u00b0C within 8 minutes. The test highlights the torque and thermal constraints under sustained load conditions. #Robotics #Engineering #ServoMotor #Testing #Feetech #Automation #Mechatronics #Hardware See translation", "url": "https://huggingface.co/posts/branikita/234092651364485", "date_published": "2025-11-07T13:31:19.302712"}, {"id": "https://huggingface.co/posts/wang12390/704280754748835", "image": "", "title": "Totally Free Image Generator, which is the best quality.", "content_text": "Totally Free Image Generator, which is the best quality. Transform your imagination into breathtaking visuals with our advanced AI technology. No skills required\u2014just describe your vision and watch the magic happen Huggingface Demo: Miragic-AI/Miragic-AI-Image-Generator Website: https://miragic.ai/products/image-generator See translation", "url": "https://huggingface.co/posts/wang12390/704280754748835", "date_published": "2025-11-07T13:31:19.302988"}, {"id": "https://huggingface.co/posts/atasoglu/710262131050494", "image": "", "title": "Introducing ToolsGen \ud83d\udee0\ufe0f", "content_text": "Introducing ToolsGen \ud83d\udee0\ufe0f I built a tool to solve a problem I kept running into: creating quality datasets for training LLMs to use tools. ToolsGen takes your JSON tool definitions and automatically generates realistic user requests, corresponding tool calls, and evaluates them using an LLM-as-a-judge pipeline. It outputs datasets ready to use with Hugging Face. What makes it useful: - Generates realistic user requests + tool calls from JSON definitions - LLM-as-a-judge quality scoring with multi-dimensional rubrics - Multiple sampling strategies (random, parameter-aware, semantic) - OpenAI-compatible API support - Outputs JSONL with train/val splits Still early days (API isn't stable yet), but it's already helping me generate tool-calling datasets much faster. Check it out: https://github.com/atasoglu/toolsgen Happy to hear feedback or ideas! See translation", "url": "https://huggingface.co/posts/atasoglu/710262131050494", "date_published": "2025-11-07T13:31:19.303389"}, {"id": "https://huggingface.co/posts/abidlabs/941146046599374", "image": "", "title": "Why I think local, open-source models will eventually win.", "content_text": "Why I think local, open-source models will eventually win. The most useful AI applications are moving toward multi-turn agentic behavior: systems that take hundreds or even thousands of iterative steps to complete a task, e.g. Claude Code, computer-control agents that click, type, and test repeatedly. In these cases, the power of the model is not how smart it is per token, but in how quickly it can interact with its environment and tools across many steps. In that regime, model quality becomes secondary to latency. An open-source model that can call tools quickly, check that the right thing was clicked, or verify that a code change actually passes tests can easily outperform a slightly \u201csmarter\u201d closed model that has to make remote API calls for every move. Eventually, the balance tips: it becomes impractical for an agent to rely on remote inference for every micro-action. Just as no one would tolerate a keyboard that required a network request per keystroke, users won\u2019t accept...", "url": "https://huggingface.co/posts/abidlabs/941146046599374", "date_published": "2025-11-07T13:31:19.303852"}]}