{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/daavoo/629869039060445", "image": "", "title": "2025: The Year of Agents.", "content_text": "2025: The Year of Agents. 2026: The Year of Local Agents? Relying on cloud-hosted LLMs is often overkill. While frontier models still lead in complex coding, local models are now more than capable of handling many agentic workflows\u2014with zero latency and total privacy. To help bridge the gap between local inference and usable agents, I\u2019m releasing agent.cpp: https://github.com/mozilla-ai/agent.cpp It provides minimal, high-performance building blocks for agents in C++, built directly around the awesome llama.cpp ecosystem. Stop sending your data to a remote API. Start building and running agents on your own hardware. See translation", "url": "https://huggingface.co/posts/daavoo/629869039060445", "date_published": "2025-12-21T13:31:21.854890"}, {"id": "https://huggingface.co/posts/prithivMLmods/787095126804028", "image": "", "title": "Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... \ud83e\udd17\ud83d\udd25", "content_text": "Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... \ud83e\udd17\ud83d\udd25 \u2728 TRELLIS.2-Text-to-3D [Demo]: prithivMLmods/TRELLIS.2-Text-to-3D \u2728 Multimodal Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations \u2728 Github: https://github.com/PRITHIVSAKTHIUR/TRELLIS.2-Text-to-3D To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/787095126804028", "date_published": "2025-12-21T13:31:21.855276"}, {"id": "https://huggingface.co/posts/branikita/131682304514806", "image": "", "title": "Next week, we will release full documentation for the SO ARM 101 with a parallel gripper, featuring leader and follower arms and support for widely used stereo cameras.", "content_text": "Next week, we will release full documentation for the SO ARM 101 with a parallel gripper, featuring leader and follower arms and support for widely used stereo cameras. See translation", "url": "https://huggingface.co/posts/branikita/131682304514806", "date_published": "2025-12-21T13:31:21.855494"}, {"id": "https://huggingface.co/posts/etemiz/592416201142301", "image": "", "title": "looks like the best way to incorporate truth in AI is to use some kind of RAG.", "content_text": "looks like the best way to incorporate truth in AI is to use some kind of RAG. what are the state of the art ways to consume knowledge graphs? and what is the best way to build a knowledge graph using AI? See translation", "url": "https://huggingface.co/posts/etemiz/592416201142301", "date_published": "2025-12-21T13:31:21.855708"}, {"id": "https://huggingface.co/posts/danielhanchen/264398594064230", "image": "", "title": "Google releases FunctionGemma, a new 270M parameter model that runs on just 0.5 GB RAM.\u2728", "content_text": "Google releases FunctionGemma, a new 270M parameter model that runs on just 0.5 GB RAM.\u2728 Built for tool-calling, run locally on your phone at 50+ tokens/s, or fine-tune with Unsloth & deploy to your phone. GGUF: unsloth/functiongemma-270m-it-GGUF Docs + Notebook: https://docs.unsloth.ai/models/functiongemma See translation", "url": "https://huggingface.co/posts/danielhanchen/264398594064230", "date_published": "2025-12-21T13:31:21.855959"}, {"id": "https://huggingface.co/posts/prithivMLmods/761836377624422", "image": "", "title": "Introducing demos for new SOTA models from AI2: SAGE-MM (Smart Any-Horizon Agents for Long-Video Reasoning) and Molmo-2, an open vision-language model that supports multi-image (QA and pointing) and video (QA, pointing, and tracking). The respective demo-related collections are listed below. \ud83c\udf83\ud83d\udd25", "content_text": "Introducing demos for new SOTA models from AI2: SAGE-MM (Smart Any-Horizon Agents for Long-Video Reasoning) and Molmo-2, an open vision-language model that supports multi-image (QA and pointing) and video (QA, pointing, and tracking). The respective demo-related collections are listed below. \ud83c\udf83\ud83d\udd25 \u2728 SAGE-MM [Video-Reasoning]: prithivMLmods/SAGE-MM-Video-Reasoning \u2728 Molmo2 [Demo]: prithivMLmods/Molmo2-HF-Demo \ud83c\udf83 GitHub[SAGE-MM]: https://github.com/PRITHIVSAKTHIUR/SAGE-MM-Video-Reasoning \ud83c\udf83 GitHub[Molmo2]: https://github.com/PRITHIVSAKTHIUR/Molmo2-HF-Demo \ud83c\udf83 Multimodal Implementations: https://huggingface.co/collections/prithivMLmods/multimodal-implementations To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/761836377624422", "date_published": "2025-12-21T13:31:21.856322"}, {"id": "https://huggingface.co/posts/John1604/712509372180068", "image": "", "title": "\u6211\u5373\u5c06\u8fbe\u5230\u516c\u5171\u5b58\u50a8\u7a7a\u95f4\u4e0a\u9650\u3002\u6211\u53d1\u73b0\u6211\u7684\u4ed3\u5e93 John1604/Kimi-K2-Thinking-q6K-gguf \u6ca1\u6709\u83b7\u5f97\u8db3\u591f\u7684\u4e0b\u8f7d\u91cf\uff0c\u51e0\u4e4e\u5360\u7528\u4e86 1T \u5b58\u50a8\u7a7a\u95f4\u3002\u5c3d\u7ba1\u6211\u559c\u7231 Kimi K2 \u7684\u601d\u8003\u65b9\u5f0f\uff0c\u4f46\u53ef\u80fd\u4e0d\u5f97\u4e0d\u5220\u9664\u8fd9\u4e2a\u6a21\u578b\u3002\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u771f\u6b63\u7684\u5f00\u6e90 1T LLM\uff0c\u4e0e\u4efb\u4f55\u524d\u6cbf\u7684 LLM \u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u5728 AI \u7ade\u4e89\u4e2d\uff0c\u7f8e\u56fd\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1axAI,  OpenAI, \u8c37\u6b4c\u548cAnthropologie\u3002\u4e2d\u56fd\u4e5f\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1a\u963f\u91cc\u5df4\u5df4, Kimi, DeepSeek\u548cGLM\u3002\u76ee\u524d\u53cc\u65b9\u52bf\u5747\u529b\u654c\u3002", "content_text": "\u6211\u5373\u5c06\u8fbe\u5230\u516c\u5171\u5b58\u50a8\u7a7a\u95f4\u4e0a\u9650\u3002\u6211\u53d1\u73b0\u6211\u7684\u4ed3\u5e93 John1604/Kimi-K2-Thinking-q6K-gguf \u6ca1\u6709\u83b7\u5f97\u8db3\u591f\u7684\u4e0b\u8f7d\u91cf\uff0c\u51e0\u4e4e\u5360\u7528\u4e86 1T \u5b58\u50a8\u7a7a\u95f4\u3002\u5c3d\u7ba1\u6211\u559c\u7231 Kimi K2 \u7684\u601d\u8003\u65b9\u5f0f\uff0c\u4f46\u53ef\u80fd\u4e0d\u5f97\u4e0d\u5220\u9664\u8fd9\u4e2a\u6a21\u578b\u3002\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u771f\u6b63\u7684\u5f00\u6e90 1T LLM\uff0c\u4e0e\u4efb\u4f55\u524d\u6cbf\u7684 LLM \u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u5728 AI \u7ade\u4e89\u4e2d\uff0c\u7f8e\u56fd\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1axAI, OpenAI, \u8c37\u6b4c\u548cAnthropologie\u3002\u4e2d\u56fd\u4e5f\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1a\u963f\u91cc\u5df4\u5df4, Kimi, DeepSeek\u548cGLM\u3002\u76ee\u524d\u53cc\u65b9\u52bf\u5747\u529b\u654c\u3002 I'm about to reach my public storage limit. I've discovered that my repository John1604/Kimi-K2-Thinking-q6K-gguf isn't getting enough downloads and is nearly consuming 1TB of storage. While I love Kimi K2's way of thinking, I have to delete this model because it's a true open-source 1TB LLM, comparable to any cutting-edge LLM model. In the AI \u200b\u200brace, four US companies have 1TB+ models: xAI, OpenAI, Google, and Anthropic. China also has four companies with 1TB+ models: Alibaba, Kimi, DeepSeek, and GLM. Currently, the two sides are evenly matched. Only American team and Chinese team have LLM with 1T+ parameters. Let's cheer for them to reach AGI in next 5 to 10 years. Maybe a 64T chinese model will do it -- Human and cat brain...", "url": "https://huggingface.co/posts/John1604/712509372180068", "date_published": "2025-12-21T13:31:21.856757"}, {"id": "https://huggingface.co/posts/projectlosangeles/440962027896970", "image": "", "title": "\ud83d\udd25Check out Project Los Angeles new SOTA searchable MIDI dataset! \ud83d\udd25", "content_text": "\ud83d\udd25Check out Project Los Angeles new SOTA searchable MIDI dataset! \ud83d\udd25 projectlosangeles/Discover-MIDI-Dataset The dataset features over 6.74M+ unique searchable MIDIs and is tailored for MIDI music discovery and symbolic music AI! If you like the dataset, please\u2764\ufe0f Sincerely, Alex Project Los Angeles Tegridy Code 2025 See translation", "url": "https://huggingface.co/posts/projectlosangeles/440962027896970", "date_published": "2025-12-21T13:31:21.857021"}, {"id": "https://huggingface.co/posts/unmodeled-tyler/429910399522120", "image": "", "title": "NEW MODEL:", "content_text": "NEW MODEL: vanta-research/scout-8b VANTA Research is excited to share our new model, Scout-8B! This iteration of Scout is based on the RNJ-1 Instruct architecture from Essential AI, and not only improves but expands on the capabilities from vanta-research/scout-4b Scout is specifically designed for: Tactical Intelligence Analysis - Systematic problem decomposition - Structured reconnaissance approach - Data-driven assessment methodology Operational Planning - Multi-phase operation planning - Risk assessment and mitigation - Resource allocation guidance Technical Assessment - Architecture evaluation and analysis - Performance optimization recommendations - Security perimeter assessment This model is great for anyone that works in security, IT, DevOps, or anyone looking for a unique, but functional AI collaborator. Check it out! See translation", "url": "https://huggingface.co/posts/unmodeled-tyler/429910399522120", "date_published": "2025-12-21T13:31:21.857359"}, {"id": "https://huggingface.co/posts/codelion/327583349449116", "image": "", "title": "Introducing PTS Visualizer - an interactive tool for exploring how language models reason!", "content_text": "Introducing PTS Visualizer - an interactive tool for exploring how language models reason! Visualize pivotal tokens, thought anchors, and reasoning circuits. See which tokens and sentences significantly impact success probability, explore embedding clusters, and trace reasoning step-by-step. Try it: codelion/pts-visualizer Explore PTS datasets: - Qwen3-0.6B: codelion/Qwen3-0.6B-pts - DeepSeek-R1: codelion/DeepSeek-R1-Distill-Qwen-1.5B-pts Or upload your own JSONL files! GitHub: https://github.com/codelion/pts See translation", "url": "https://huggingface.co/posts/codelion/327583349449116", "date_published": "2025-12-21T13:31:21.857629"}]}