{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/prithivMLmods/319015417669347", "image": "", "title": "It's really interesting about the deployment of a new state of matter in Majorana 1: the world\u2019s first quantum processor powered by topological qubits. If you missed this news this week, here are some links for you:", "content_text": "It's really interesting about the deployment of a new state of matter in Majorana 1: the world\u2019s first quantum processor powered by topological qubits. If you missed this news this week, here are some links for you: \ud83d\udcd6 Read the story: https://news.microsoft.com/source/features/innovation/microsofts-majorana-1-chip-carves-new-path-for-quantum-computing/ \u269b\ufe0f Quantum Blog: https://azure.microsoft.com/en-us/blog/quantum/2025/02/19/microsoft-unveils-majorana-1-the-worlds-first-quantum-processor-powered-by-topological-qubits/ \ud83c\udd71\ufe0fTopological qubit arrays: https://arxiv.org/abs/2502.12252 \ud83d\udcdd Majorana 1 Intro: https://youtu.be/Q4xCR20Dh1E?si=Z51DbEYnZFp_88Xp \ud83c\udf00The Path to a Million Qubits: https://youtu.be/wSHmygPQukQ?si=TS80EhI62oWiMSHK See translation", "url": "https://huggingface.co/posts/prithivMLmods/319015417669347", "date_published": "2025-02-23T17:17:30.562128"}, {"id": "https://huggingface.co/posts/lysandre/966361810633890", "image": "", "title": "SmolVLM-2 and SigLIP-2 are now part of", "content_text": "SmolVLM-2 and SigLIP-2 are now part of transformers in dedicated releases! They're added on top of the v4.49.0 release, and can be installed from the following tags: v4.49.0-SmolVLM-2 and v4.49.0-SigLIP-2 . This marks a new beginning for the release process of transformers. For the past five years, we've been doing monthly releases featuring many models (v4.49.0, the latest release, features 9 new architectures). Starting with SmolVLM-2 & SigLIP2, we'll now additionally release tags supporting new models on a stable branch. These models are therefore directly available for use by installing from the tag itself. These tags will continue to be updated with fixes applied to these models. Going forward, continue expecting software releases following semantic versioning: v4.50.0 will have ~10 new architectures compared to v4.49.0, as well as a myriad of new features, improvements and bug fixes. Accompanying these software releases, we'll release tags offering brand new models as fast as...", "url": "https://huggingface.co/posts/lysandre/966361810633890", "date_published": "2025-02-23T17:17:30.562535"}, {"id": "https://huggingface.co/posts/fdaudens/422173269922572", "image": "", "title": "Trying something new to keep you ahead of the curve: The 5 AI stories of the week - a weekly curation of the most important AI news you need to know. Do you like it?", "content_text": "Trying something new to keep you ahead of the curve: The 5 AI stories of the week - a weekly curation of the most important AI news you need to know. Do you like it? For more AI stories and deeper analysis, check out my newsletter: https://open.substack.com/pub/fdaudens/p/ai-competition-heats-up-grok-3-iphone See translation", "url": "https://huggingface.co/posts/fdaudens/422173269922572", "date_published": "2025-02-23T17:17:30.562792"}, {"id": "https://huggingface.co/posts/jsulz/911431940353906", "image": "", "title": "Time flies!", "content_text": "Time flies! Six months after joining Hugging Face the Xet team is kicking off the first migrations from LFS to our storage for a number of repositories on the Hub. More on the nitty gritty details behind the migration soon, but here are the big takeaways: \ud83e\udd16 We've successfully completed the first migrations from LFS -> Xet to test the infrastructure and prepare for a wider release \u2705 No action on your part needed - you can work with a Xet-backed repo like any other repo on the Hub (for now - major improvements on their way!) \ud83d\udc40 Keep an eye out for the Xet logo to see if a repo you know is on our infra! See the screenshots below to spot the difference \ud83d\udc47 \u23e9 \u23e9 \u23e9 Blazing uploads and downloads coming soon. W\u2019re gearing up for a full integration with the Hub's Python library that will make building on the Hub faster than ever - special thanks to @ celinah and @ Wauplin for their assistance. \ud83c\udf89 Want Early Access? If you\u2019re curious and want to test it out the bleeding edge that will power the...", "url": "https://huggingface.co/posts/jsulz/911431940353906", "date_published": "2025-02-23T17:17:30.563350"}, {"id": "https://huggingface.co/posts/JingzeShi/354750943862398", "image": "", "title": "\ud83e\udd17Welcome to the Doge Edge Device Small language Model.", "content_text": "\ud83e\udd17Welcome to the Doge Edge Device Small language Model. SmallDoge/Doge-160M-Instruct See translation", "url": "https://huggingface.co/posts/JingzeShi/354750943862398", "date_published": "2025-02-23T17:17:30.563573"}, {"id": "https://huggingface.co/posts/mmhamdy/257109472269745", "image": "", "title": "\ud83c\udf89 We're excited to introduce MemoryCode, a novel synthetic dataset designed to rigorously evaluate LLMs' ability to track and execute coding instructions across multiple sessions. MemoryCode simulates realistic workplace scenarios where a mentee (the LLM) receives coding instructions from a mentor amidst a stream of both relevant and irrelevant information.", "content_text": "\ud83c\udf89 We're excited to introduce MemoryCode, a novel synthetic dataset designed to rigorously evaluate LLMs' ability to track and execute coding instructions across multiple sessions. MemoryCode simulates realistic workplace scenarios where a mentee (the LLM) receives coding instructions from a mentor amidst a stream of both relevant and irrelevant information. \ud83d\udca1 But what makes MemoryCode unique?! The combination of the following: \u2705 Multi-Session Dialogue Histories: MemoryCode consists of chronological sequences of dialogues between a mentor and a mentee, mirroring real-world interactions between coworkers. \u2705 Interspersed Irrelevant Information: Critical instructions are deliberately interspersed with unrelated content, replicating the information overload common in office environments. \u2705 Instruction Updates: Coding rules and conventions can be updated multiple times throughout the dialogue history, requiring LLMs to track and apply the most recent information. \u2705 Prospective Memory:...", "url": "https://huggingface.co/posts/mmhamdy/257109472269745", "date_published": "2025-02-23T17:17:30.564203"}, {"id": "https://huggingface.co/posts/stas/738116252437953", "image": "", "title": "Do you want ArcticTraining at", "content_text": "Do you want ArcticTraining at @ SnowflakeDB to add an ability to post-train DeepSeek V3/R1 models with DPO using just a few GPU nodes? Please vote here and tell others about it: https://github.com/snowflakedb/ArcticTraining/discussions/58 ArcticTraining is an open-source, easy to use post-training framework for NVIDIA GPUs built on top of DeepSpeed. See translation", "url": "https://huggingface.co/posts/stas/738116252437953", "date_published": "2025-02-23T17:17:30.564470"}, {"id": "https://huggingface.co/posts/DmitryRyumin/189065722993769", "image": "", "title": "\ud83d\ude80\ud83c\udfad\ud83c\udf1f New Research Alert - WACV 2025 (Avatars Collection)! \ud83c\udf1f\ud83c\udfad\ud83d\ude80", "content_text": "\ud83d\ude80\ud83c\udfad\ud83c\udf1f New Research Alert - WACV 2025 (Avatars Collection)! \ud83c\udf1f\ud83c\udfad\ud83d\ude80 \ud83d\udcc4 Title: EmoVOCA: Speech-Driven Emotional 3D Talking Heads \ud83d\udd1d \ud83d\udcdd Description: EmoVOCA is a data-driven method for generating emotional 3D talking heads by combining speech-driven lip movements with expressive facial dynamics. This method has been developed to overcome the limitations of corpora and to achieve state-of-the-art animation quality. \ud83d\udc65 Authors: @ FedeNoce , Claudio Ferrari, and Stefano Berretti \ud83d\udcc5 Conference: WACV, 28 Feb \u2013 4 Mar, 2025 | Arizona, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: https://arxiv.org/abs/2403.12886 \ud83c\udf10 Github Page: https://fedenoce.github.io/emovoca/ \ud83d\udcc1 Repository: https://github.com/miccunifi/EmoVOCA \ud83d\ude80 CVPR-2023-24-Papers: https://github.com/DmitryRyumin/CVPR-2023-24-Papers \ud83d\ude80 WACV-2024-Papers: https://github.com/DmitryRyumin/WACV-2024-Papers \ud83d\ude80 ICCV-2023-Papers: https://github.com/DmitryRyumin/ICCV-2023-Papers \ud83d\udcda More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers...", "url": "https://huggingface.co/posts/DmitryRyumin/189065722993769", "date_published": "2025-02-23T17:17:30.564965"}, {"id": "https://huggingface.co/posts/nicolay-r/986619870856670", "image": "", "title": "\ud83d\udce2 If you're looking for translating massive dataset of JSON-lines / CSV data with various set of source fields, then the following update would be relevant. So far and experimenting with adapting language specific Sentiment Analysis model, got a change to reforge and relaese bulk-translate 0.25.2.", "content_text": "\ud83d\udce2 If you're looking for translating massive dataset of JSON-lines / CSV data with various set of source fields, then the following update would be relevant. So far and experimenting with adapting language specific Sentiment Analysis model, got a change to reforge and relaese bulk-translate 0.25.2. \u2b50\ufe0f https://github.com/nicolay-r/bulk-translate/releases/tag/0.25.2 The update has the following major features - Supporting schemas: all the columns to be translated are now could be declared within the same prompt-style format. using json this automatically allows to map them onto output fields - The related updates for shell execution mode: schema parameter is now available alongside with just a prompt usage before. Benefit is that your output is invariant. You can extend and stack various translators with separated shell laucnhes. Screenshot below is the application of the google-translate engine in manual batching mode. \ud83d\ude80 Performance: 2.5 it / sec (in the case of a single field...", "url": "https://huggingface.co/posts/nicolay-r/986619870856670", "date_published": "2025-02-23T17:17:30.565443"}, {"id": "https://huggingface.co/posts/tegridydev/603902346938209", "image": "", "title": "Open Source AI Agents | Github/Repo List | [2025]", "content_text": "Open Source AI Agents | Github/Repo List | [2025] https://huggingface.co/blog/tegridydev/open-source-ai-agents-directory Check out the article & Follow, bookmark, save the tab as I will be updating it <3 (using it as my own notepad & decided i might keep it up to date if i post it here, instead of making the 15th_version of it and not saving it with a name i can remember on my desktop lol) See translation", "url": "https://huggingface.co/posts/tegridydev/603902346938209", "date_published": "2025-02-23T17:17:30.565708"}]}