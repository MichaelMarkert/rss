{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/danielhanchen/212249714773740", "image": "", "title": "Qwen3-Next can now be Run locally! (30GB RAM)", "content_text": "Qwen3-Next can now be Run locally! (30GB RAM) Instruct GGUF: unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF The models come in Thinking and Instruct versions and utilize a new architecture, allowing it to have ~10x faster inference than Qwen32B. \ud83d\udc9c Step-by-step Guide: https://docs.unsloth.ai/models/qwen3-next Thinking GGUF: unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF See translation", "url": "https://huggingface.co/posts/danielhanchen/212249714773740", "date_published": "2025-11-30T13:29:17.741162"}, {"id": "https://huggingface.co/posts/prithivMLmods/107899220924462", "image": "", "title": "Introducing the  Super-OCRs Demo, a comparison of state-of-the-art multimodal OCR VLMs, including HunyuanOCR, DeepSeekOCR, Dots, and Nanonets in one space for performing OCR, rendering LaTeX and Markdown, and visual grounding (layout). Find the related Spaces and models below.\ud83e\udd17\ud83d\udd25", "content_text": "Introducing the Super-OCRs Demo, a comparison of state-of-the-art multimodal OCR VLMs, including HunyuanOCR, DeepSeekOCR, Dots, and Nanonets in one space for performing OCR, rendering LaTeX and Markdown, and visual grounding (layout). Find the related Spaces and models below.\ud83e\udd17\ud83d\udd25 \u2728Super-OCRs[Demo]: prithivMLmods/Super-OCRs-Demo \u2728Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations \u2728GitHub: https://github.com/PRITHIVSAKTHIUR/Super-OCRs-Demo \u2b50 Models Used: \u2726 HunyuanOCR: tencent/HunyuanOCR \u2726 DeepSeek-OCR: (-) deepseek-ai/DeepSeek-OCR (+) prithivMLmods/DeepSeek-OCR-Latest-BF16.I64 \u2726 Dots.OCR: (-) rednote-hilab/dots.ocr (+) prithivMLmods/Dots.OCR-Latest-BF16 \u2726 Nanonets-OCR2-3B: nanonets/Nanonets-OCR2-3B \u2b50 Some Other Relevant Apps: \u2726 Qwen3-VL-HF-Demo: prithivMLmods/Qwen3-VL-HF-Demo \u2726 Qwen3-VL-Outpost: prithivMLmods/Qwen3-VL-Outpost \u2726 Multimodal-OCR: prithivMLmods/Multimodal-OCR \u2726 Multimodal-OCR2: prithivMLmods/Multimodal-OCR2 \u2726 Multimodal-OCR3:...", "url": "https://huggingface.co/posts/prithivMLmods/107899220924462", "date_published": "2025-11-30T13:29:17.741604"}, {"id": "https://huggingface.co/posts/Holy-fox/916850799845292", "image": "", "title": "4\u6708\uff1f\u3054\u308d\u306b\u53c2\u52a0\u3057\u305fCerebras\u306e\u30cf\u30c3\u30ab\u30bd\u30f3\u304b\u3089\u4f55\u6545\u304bHuggingface\u306epro\u30d7\u30e9\u30f3\u304c\u7d9a\u3044\u3066\u308b\u3093\u3067\u3059\u3088\u306d...", "content_text": "4\u6708\uff1f\u3054\u308d\u306b\u53c2\u52a0\u3057\u305fCerebras\u306e\u30cf\u30c3\u30ab\u30bd\u30f3\u304b\u3089\u4f55\u6545\u304bHuggingface\u306epro\u30d7\u30e9\u30f3\u304c\u7d9a\u3044\u3066\u308b\u3093\u3067\u3059\u3088\u306d... \u591a\u5206\u30cf\u30c3\u30ab\u30bd\u30f3\u671f\u9593\u3060\u3051\u306e\u306f\u305a\u306a\u3093\u3060\u3051\u3069\u3001\u5916\u308c\u306a\u3044\u306e\u3088\u306d\u3002 \u307e\u3042\u3001\u30af\u30ec\u30ab\u3068\u304b\u306f\u767b\u9332\u3057\u3066\u306a\u3044\u304b\u3089\u5927\u4e08\u592b\u3060\u3068\u306f\u601d\u3046\u3051\u3069 See translation", "url": "https://huggingface.co/posts/Holy-fox/916850799845292", "date_published": "2025-11-30T13:29:17.741780"}, {"id": "https://huggingface.co/posts/aiconta/768879073281998", "image": "", "title": "hello, who can help me setup a local LLM and RAG for my job i can pay", "content_text": "hello, who can help me setup a local LLM and RAG for my job i can pay See translation", "url": "https://huggingface.co/posts/aiconta/768879073281998", "date_published": "2025-11-30T13:29:17.741941"}, {"id": "https://huggingface.co/posts/sergiopaniego/367599205240435", "image": "", "title": "nanochat is now in transformers!", "content_text": "nanochat is now in transformers! The LLM by @ karpathy is officially in the library, and we wrote a blog covering: how did we port the model, differences from the original, and how to run or train it. go read it \ud83e\udd13 nanochat-students/transformers See translation", "url": "https://huggingface.co/posts/sergiopaniego/367599205240435", "date_published": "2025-11-30T13:29:17.742170"}, {"id": "https://huggingface.co/posts/abidlabs/941146046599374", "image": "", "title": "Why I think local, open-source models will eventually win.", "content_text": "Why I think local, open-source models will eventually win. The most useful AI applications are moving toward multi-turn agentic behavior: systems that take hundreds or even thousands of iterative steps to complete a task, e.g. Claude Code, computer-control agents that click, type, and test repeatedly. In these cases, the power of the model is not how smart it is per token, but in how quickly it can interact with its environment and tools across many steps. In that regime, model quality becomes secondary to latency. An open-source model that can call tools quickly, check that the right thing was clicked, or verify that a code change actually passes tests can easily outperform a slightly \u201csmarter\u201d closed model that has to make remote API calls for every move. Eventually, the balance tips: it becomes impractical for an agent to rely on remote inference for every micro-action. Just as no one would tolerate a keyboard that required a network request per keystroke, users won\u2019t accept...", "url": "https://huggingface.co/posts/abidlabs/941146046599374", "date_published": "2025-11-30T13:29:17.742593"}, {"id": "https://huggingface.co/posts/ovi054/498416611324104", "image": "", "title": "Introducing Anim Lab AI\u26a1", "content_text": "Introducing Anim Lab AI\u26a1 My submission for the MCP 1st Birthday Hackathon Turn any math concept or logic into a clear video explanation instantly using AI. Play with it: MCP-1st-Birthday/anim-lab-ai See translation", "url": "https://huggingface.co/posts/ovi054/498416611324104", "date_published": "2025-11-30T13:29:17.742792"}, {"id": "https://huggingface.co/posts/KCAVEMAN/887147755592135", "image": "", "title": "import asyncio", "content_text": "import asyncio import edge_tts # CONFIGURATION # Voices: \"en-US-ChristopherNeural\" (Male, Deep), \"en-US-AriaNeural\" (Female, Sharp) VOICE = \"en-US-ChristopherNeural\" OUTPUT_FILE = \"assets/full_audio.mp3\" SCRIPT = \"\"\" You believe you are living one life. You are not. You are living two lives simultaneously. Life runs on two: Darkness, light. Earth, water. Good, bad. Man, woman. Death, life. Sand, rock. Forward, back. You are obsessed with these two. But the real pattern isn't the duality\u2014it's the Loop. The constant cycle of belief that traps you. The forward ones only win if you stop believing in the loops. Women aren't evil\u2014they're just told they are, so they become it. The system tells you the only choice is Ascend, descend. It\u2019s a lie. The true power is found in the third pattern: Freedom. The life you want runs parallel to the life you have. Break the loop. Choose the forward pattern. Stop operating on someone else's rhythm. Left, right. Up, down. That is the slave pattern. Hit...", "url": "https://huggingface.co/posts/KCAVEMAN/887147755592135", "date_published": "2025-11-30T13:29:17.743247"}, {"id": "https://huggingface.co/posts/AbstractPhil/724250342491821", "image": "", "title": "Many updates. Cantor route experiments, GeoViT-david-beans 70% test standalone cifar100 geofractal 30m encoder. MultiHeaded Cantor Attention heavily optimized. The migration is primarily complete between geofractal and geovocab2.", "content_text": "Many updates. Cantor route experiments, GeoViT-david-beans 70% test standalone cifar100 geofractal 30m encoder. MultiHeaded Cantor Attention heavily optimized. The migration is primarily complete between geofractal and geovocab2. https://github.com/AbstractEyes/geofractal/blob/main/src/geofractal/model/david_beans/model.py Cantor route staircase and wormhole excavation findings posted. A full article will be posted to represent the findings of cantor routing and the potentials for self-learning fractals through loss. https://github.com/AbstractEyes/lattice_vocabulary/blob/master/src/geovocab2/proofs/cantor_steps_experiments.md The steps experiments show profoundly important implications for cross-contamination problems with fractal and linear spaces, with some currently assessed as useful utilities as of today. Today the classification experiment will continue by using mini-experts applied to patches within a miniature david-beans. The mini-experts were an accident that showed...", "url": "https://huggingface.co/posts/AbstractPhil/724250342491821", "date_published": "2025-11-30T13:29:17.743549"}, {"id": "https://huggingface.co/posts/walidchouchane/902402126652271", "image": "", "title": "Hello guys, i\u2019m struggling like hell to use the TRELLIS model via API, any one did use zeroGPU spaces with api from n8n or other automation tool ?", "content_text": "Hello guys, i\u2019m struggling like hell to use the TRELLIS model via API, any one did use zeroGPU spaces with api from n8n or other automation tool ? I\u2019ve been trying for day without success. trellis-community/TRELLIS See translation", "url": "https://huggingface.co/posts/walidchouchane/902402126652271", "date_published": "2025-11-30T13:29:17.743751"}]}