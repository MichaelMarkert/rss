{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Abhaykoul/404767027882987", "image": "", "title": "\ud83c\udf89 Dhanishtha 2.0 Preview is Now Open Source!", "content_text": "\ud83c\udf89 Dhanishtha 2.0 Preview is Now Open Source! The world's first Intermediate Thinking Model is now available to everyone! Dhanishtha 2.0 Preview brings revolutionary intermediate thinking capabilities to the open-source community. Unlike traditional reasoning models that think once, Dhanishtha can think, answer, rethink, answer again, and continue rethinking as needed using multiple blocks between responses. \ud83d\ude80 Key Features - Intermediate thinking: Think \u2192 Answer \u2192 Rethink \u2192 Answer \u2192 Rethink if needed... - Token efficient: Uses up to 79% fewer tokens than DeepSeek R1 on similar queries - Transparent thinking: See the model's reasoning process in real-time - Open source: Freely available for research and development HelpingAI/Dhanishtha-2.0-preview https://helpingai.co/chat See translation", "url": "https://huggingface.co/posts/Abhaykoul/404767027882987", "date_published": "2025-07-03T13:35:11.420643"}, {"id": "https://huggingface.co/posts/tomaarsen/190568030432786", "image": "", "title": "\u203c\ufe0fSentence Transformers v5.0 is out! The biggest update yet introduces Sparse Embedding models, encode methods improvements, Router module for asymmetric models & much more. Sparse + Dense = \ud83d\udd25 hybrid search performance! Details:", "content_text": "\u203c\ufe0fSentence Transformers v5.0 is out! The biggest update yet introduces Sparse Embedding models, encode methods improvements, Router module for asymmetric models & much more. Sparse + Dense = \ud83d\udd25 hybrid search performance! Details: 1\ufe0f\u20e3 Sparse Encoder Models Brand new support for sparse embedding models that generate high-dimensional embeddings (30,000+ dims) where <1% are non-zero: - Full SPLADE, Inference-free SPLADE, and CSR architecture support - 4 new modules, 12 new losses, 9 new evaluators - Integration with @ elastic-co , @ opensearch-project , @ NAVER LABS Europe, @ qdrant , @ IBM , etc. - Decode interpretable embeddings to understand token importance - Hybrid search integration to get the best of both worlds 2\ufe0f\u20e3 Enhanced Encode Methods & Multi-Processing - Introduce encode_query & encode_document automatically use predefined prompts - No more manual pool management - just pass device list directly to encode() - Much cleaner and easier to use than the old multi-process approach...", "url": "https://huggingface.co/posts/tomaarsen/190568030432786", "date_published": "2025-07-03T13:35:11.421338"}, {"id": "https://huggingface.co/posts/burtenshaw/697123415535373", "image": "", "title": "Inference for generative ai models looks like a mine field, but there\u2019s a simple protocol for picking the best inference:", "content_text": "Inference for generative ai models looks like a mine field, but there\u2019s a simple protocol for picking the best inference: \ud83c\udf0d 95% of users >> If you\u2019re using open (large) models and need fast online inference, then use Inference providers on auto mode, and let it choose the best provider for the model. https://huggingface.co/docs/inference-providers/index \ud83d\udc77 fine-tuners/ bespoke >> If you\u2019ve got custom setups, use Inference Endpoints to define a configuration from AWS, Azure, GCP. https://endpoints.huggingface.co/ \ud83e\uddab Locals >> If you\u2019re trying to stretch everything you can out of a server or local machine, use Llama.cpp, Jan, LMStudio or vLLM. https://huggingface.co/settings/local-apps#local-apps \ud83e\ude9f Browsers >> If you need open models running right here in the browser, use transformers.js. https://github.com/huggingface/transformers.js Let me know what you\u2019re using, and if you think it\u2019s more complex than this. See translation", "url": "https://huggingface.co/posts/burtenshaw/697123415535373", "date_published": "2025-07-03T13:35:11.421781"}, {"id": "https://huggingface.co/posts/Jaward/639375924369190", "image": "", "title": "I played around with the new RXTX paper (XX^T) and was able to train nanogpt with 4x4 RXTX matmuls in both attention layer and optimizer\ud83e\udd15", "content_text": "I played around with the new RXTX paper (XX^T) and was able to train nanogpt with 4x4 RXTX matmuls in both attention layer and optimizer\ud83e\udd15 It just works (well I had to add some guardrails) but still saves 5% of memory usage: The Patch: - Computes attention scores with a 4x4 blockwise RXTX matmuls (no pytorch dot prod) - Handles arbitrary sequence lengths by padding to the nearest multiple of 4. - An RXTX variant of shampoo with params reshaped into 4x4 blocks during each optimizer step. - Uses 5% less ops Code: https://github.com/Jaykef/ai-algorithms/blob/main/nanogpt-rxtx.ipynb Paper: https://arxiv.org/pdf/2505.09814 See translation", "url": "https://huggingface.co/posts/Jaward/639375924369190", "date_published": "2025-07-03T13:35:11.422136"}, {"id": "https://huggingface.co/posts/danielhanchen/374907101016508", "image": "", "title": "Gemma 3n finetuning is now 1.5x faster and uses 50% less VRAM in Unsloth!", "content_text": "Gemma 3n finetuning is now 1.5x faster and uses 50% less VRAM in Unsloth! Click \"Use this model\" and click \"Google Colab\"! unsloth/gemma-3n-E4B-it unsloth/gemma-3n-E2B-it https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_(4B)-Conversational.ipynb See translation", "url": "https://huggingface.co/posts/danielhanchen/374907101016508", "date_published": "2025-07-03T13:35:11.422375"}, {"id": "https://huggingface.co/posts/arthurbresnu/914099787091289", "image": "", "title": "\u203c\ufe0fSentence Transformers v5.0 is out! The biggest update yet introduces Sparse Embedding models, encode methods improvements, Router module & much more. Sparse + Dense = \ud83d\udd25 hybrid search performance!", "content_text": "\u203c\ufe0fSentence Transformers v5.0 is out! The biggest update yet introduces Sparse Embedding models, encode methods improvements, Router module & much more. Sparse + Dense = \ud83d\udd25 hybrid search performance! 1\ufe0f\u20e3 Sparse Encoder Models - New support for sparse embeddings (30k+ dims, <1% non-zero) * Full SPLADE, Inference-free SPLADE, CSR support * 4 new modules, 12 losses, 9 evaluators * Integration with elastic, opensearch-project, Qdrant, ibm-granite * Decode interpretable embeddings * Hybrid search integration 2\ufe0f\u20e3 Enhanced Encode Methods * encode_query & encode_document with auto prompts * Direct device list passing to encode() * Cleaner multi-processing 3\ufe0f\u20e3 Router Module & Training * Different paths for queries vs documents * Custom learning rates per parameter group * Composite loss logging * Perfect for two-tower architectures 4\ufe0f\u20e3 Documentation & Training * New Training/Loss Overview docs * 6 training example pages * Search engine integration examples Read the comprehensive blogpost about...", "url": "https://huggingface.co/posts/arthurbresnu/914099787091289", "date_published": "2025-07-03T13:35:11.422973"}, {"id": "https://huggingface.co/posts/AdinaY/923543336919000", "image": "", "title": "\ud83d\udd25 June highlights from China\u2019s open source ecosystem.", "content_text": "\ud83d\udd25 June highlights from China\u2019s open source ecosystem. zh-ai-community/june-2025-open-works-from-the-chinese-community-683d66c188f782dc5570ba15 \u2728Baidu & MiniMax both launched open foundation models - Baidu: Ernie 4.5 ( from 0.3B -424B ) \ud83e\udd2f - MiniMax: MiniMax -M1 ( Hybrid MoE reasoning model ) \u2728Multimodal AI is moving from fusion to full-stack reasoning: unified Any-to-Any pipelines across text, vision, audio, and 3D - Baidu: ERNIE-4.5-VL-424B - Moonshot AI: Kimi-VL-A3B - Alibaba: Ovis-U1 - BAAI: Video-XL-2/OmniGen2 - AntGroup: Ming-Lite-Omni - Chinese Academy of Science: Stream-Omni - Bytedance: SeedVR2-3B - Tencent: Hunyuan 3D 2.1/ SongGeneration - FishAudio: Openaudio-s1-mini \u2728Domain specific models are rapidly emerging - Alibaba DAMO: Lingshu-7B (medical MLLM) - BAAI: RoboBrain (Robotics) \u2728 So many small models! - OpenBMB: MiciCPM4 ( on device ) - Qwen: Embedding/Reranker (0.6B) - Alibaba: Ovis-U1-3B - Moonshot AI: Kimi-VL-A3B - Bytedance: SeedVR2-3B See translation", "url": "https://huggingface.co/posts/AdinaY/923543336919000", "date_published": "2025-07-03T13:35:11.423420"}, {"id": "https://huggingface.co/posts/m-ric/145810386001131", "image": "", "title": "If you're using any HF libraries, you should enable the Hub MCP in your agentic coding tool!", "content_text": "If you're using any HF libraries, you should enable the Hub MCP in your agentic coding tool! The brand new Docs Semantic Search tool is intravenous caffeine supply for Cursor, enables to correct API errors in a few seconds, gj @ mishig \u26a1\ufe0f\u26a1\ufe0f \ud83d\udc49 To enable Hub MCP, head to your account setting, under MCP, and it will give you everything you need! See translation", "url": "https://huggingface.co/posts/m-ric/145810386001131", "date_published": "2025-07-03T13:35:11.423723"}, {"id": "https://huggingface.co/posts/asigalov61/301808424415801", "image": "", "title": "Check out new symbolic music AI front end and CLI training app", "content_text": "Check out new symbolic music AI front end and CLI training app https://webchatappai.github.io/midi-gen/ https://github.com/WebChatAppAi/Orpheus-Midi-Model-Maker @ Timzoid @ Csplk @ not-lain @ victor @ bartowski @ John6666 See translation", "url": "https://huggingface.co/posts/asigalov61/301808424415801", "date_published": "2025-07-03T13:35:11.423962"}, {"id": "https://huggingface.co/posts/anakin87/460502915743038", "image": "", "title": "\ud83e\uddf0 Free up space on the Hub with", "content_text": "\ud83e\uddf0 Free up space on the Hub with super_squash_history \ud83e\uddf9 As you may know, Hugging Face Hub has storage limits on private repos (100 GB for free users, 1 TB for PROs). This weekend I did some cleanup on my private repos I went 1.58 TB down to 1 GB. \ud83d\ude05 Besides deleting old, unused models, the main tool I used was a lesser-known command: super_squash_history . When you train a model, you often push multiple checkpoints to the Hub. Each checkpoint = a commit. A 2.6B model in BF16 is ~5 GB. So 10 checkpoints = 50 GB. That adds up fast. While full commit history can be useful for rollbacks, it's often unnecessary for older experiments where only the final model matters. In these cases, you can use super_squash_history : it reduces your entire repo history to a single commit. https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.super_squash_history \u26a0\ufe0f super_squash_history is a non-revertible operation. Once squashed, the commit history cannot be...", "url": "https://huggingface.co/posts/anakin87/460502915743038", "date_published": "2025-07-03T13:35:11.424403"}]}