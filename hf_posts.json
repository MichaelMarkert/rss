{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/clem/522668354429256", "image": "", "title": "Today, we're unveiling two new open-source AI robots! HopeJR for $3,000 & Reachy Mini for $300 \ud83e\udd16\ud83e\udd16\ud83e\udd16", "content_text": "Today, we're unveiling two new open-source AI robots! HopeJR for $3,000 & Reachy Mini for $300 \ud83e\udd16\ud83e\udd16\ud83e\udd16 Let's go open-source AI robotics! See translation", "url": "https://huggingface.co/posts/clem/522668354429256", "date_published": "2025-05-30T09:25:15.514895"}, {"id": "https://huggingface.co/posts/DawnC/538322807718464", "image": "", "title": "VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration", "content_text": "VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration I'm excited to share significant improvements to VisionScout that substantially enhance accuracy and analytical capabilities. \u2b50\ufe0f Key Enhancements - CLIP Zero-Shot Landmark Detection: The system now identifies famous landmarks and architectural features without requiring specific training data, expanding scene understanding beyond generic object detection. - Places365 Environmental Classification: Integration of MIT's Places365 model provides robust scene baseline classification across 365 categories, significantly improving lighting analysis accuracy and overall scene identification precision. - Enhanced Multi-Modal Fusion: Advanced algorithms now dynamically combine insights from YOLOv8, CLIP, and Places365 to optimize accuracy across diverse scenarios. - Refined LLM Narratives: Llama 3.2 integration continues to transform analytical data into fluent, contextually rich descriptions while maintaining...", "url": "https://huggingface.co/posts/DawnC/538322807718464", "date_published": "2025-05-30T09:25:15.515427"}, {"id": "https://huggingface.co/posts/darkc0de/635443238112929", "image": "", "title": "\ud83e\udd17\ud83d\udc68\ud83c\udffb\u200d\ud83c\udf93", "content_text": "\ud83e\udd17\ud83d\udc68\ud83c\udffb\u200d\ud83c\udf93", "url": "https://huggingface.co/posts/darkc0de/635443238112929", "date_published": "2025-05-30T09:25:15.515605"}, {"id": "https://huggingface.co/posts/AtAndDev/639250895656011", "image": "", "title": "deepseek-ai/DeepSeek-R1-0528", "content_text": "deepseek-ai/DeepSeek-R1-0528 This is the end See translation", "url": "https://huggingface.co/posts/AtAndDev/639250895656011", "date_published": "2025-05-30T09:25:15.515808"}, {"id": "https://huggingface.co/posts/fdaudens/323840314242853", "image": "", "title": "\ud83c\udfb5 Dream come true for content creators! TIGER AI can extract voice, effects & music from ANY audio file \ud83e\udd2f", "content_text": "\ud83c\udfb5 Dream come true for content creators! TIGER AI can extract voice, effects & music from ANY audio file \ud83e\udd2f This lightweight model uses frequency band-split technology to separate speech like magic. Kudos to @ fffiloni for the amazing demo! fffiloni/TIGER-audio-extraction See translation", "url": "https://huggingface.co/posts/fdaudens/323840314242853", "date_published": "2025-05-30T09:25:15.516056"}, {"id": "https://huggingface.co/posts/AdinaY/228294422537840", "image": "", "title": "\ud83d\udd25 New benchmark & dataset for Subject-to-Video generation", "content_text": "\ud83d\udd25 New benchmark & dataset for Subject-to-Video generation OPENS2V-NEXUS by Pekin University \u2728 Fine-grained evaluation for subject consistency BestWishYsh/OpenS2V-Eval \u2728 5M-scale dataset: BestWishYsh/OpenS2V-5M \u2728 New metrics \u2013 automatic scores for identity, realism, and text match See translation", "url": "https://huggingface.co/posts/AdinaY/228294422537840", "date_published": "2025-05-30T09:25:15.516307"}, {"id": "https://huggingface.co/posts/openfree/518842941778923", "image": "", "title": "\ud83e\udde0  AI Brand Naming with 15 Specialized Theories", "content_text": "\ud83e\udde0 AI Brand Naming with 15 Specialized Theories \ud83c\udfaf Core Features 15 Expert Theories for professional brand naming Bilingual Support Korean/English for global brands Unified Evaluation System creativity/memorability/relevance scores Real-time Visualization theory-specific custom designs openfree/Naming \ud83d\udd2c Applied Theories Cognitive Theories (4) \ud83d\udfe6 Square Theory - Semantic square structure with 4-word relationships \ud83d\udd0a Sound Symbolism - Psychological connections between phonemes and meaning \ud83e\udde0 Cognitive Load - Minimized processing for instant recognition \ud83d\udc41\ufe0f Gestalt Theory - Perceptual principles where whole exceeds parts Creative Theories (3) \ud83d\udd00 Conceptual Blending - Merging concepts to create new meanings \ud83d\udd27 SCAMPER Method - 7 creative transformation techniques \ud83c\udf3f Biomimicry - Nature-inspired wisdom from 3.8 billion years of evolution Strategic Theories (2) \u2705 Jobs-to-be-Done - Customer-centric problem-solving focus \ud83d\udcad Design Thinking - Human-centered innovation methodology Cultural Theories (3)...", "url": "https://huggingface.co/posts/openfree/518842941778923", "date_published": "2025-05-30T09:25:15.516939"}, {"id": "https://huggingface.co/posts/lukmanaj/495766537273785", "image": "", "title": "I am so happy  to share to all that I\u2019ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I\u2019m doing my best to keep up. Excited for what\u2019s ahead!", "content_text": "I am so happy to share to all that I\u2019ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I\u2019m doing my best to keep up. Excited for what\u2019s ahead! See translation", "url": "https://huggingface.co/posts/lukmanaj/495766537273785", "date_published": "2025-05-30T09:25:15.517197"}, {"id": "https://huggingface.co/posts/hesamation/260011784391977", "image": "", "title": "I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book.", "content_text": "I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book. It gives an overview, then goes into detail for each stage, even providing best practices. It\u2019s 115 pages on arxiv, definitely worth a read. Check it out: https://arxiv.org/abs/2408.13296 See translation", "url": "https://huggingface.co/posts/hesamation/260011784391977", "date_published": "2025-05-30T09:25:15.517441"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/279762403721253", "image": "", "title": "VEO 3 FLOW Full Tutorial - How To Use VEO3 in FLOW Guide :", "content_text": "VEO 3 FLOW Full Tutorial - How To Use VEO3 in FLOW Guide : https://youtu.be/AoEmQPU2gtg Tutorial link : https://youtu.be/AoEmQPU2gtg VEO 3 AI is rocking generative AI field right now. FLOW is the platform that lets you use VEO 3 with so many cool features. This is an official tutorial and guide made by Google team. I edited it slightly. I hope this be helpful. FLOW : https://labs.google/flow/about Veo 3 is Google DeepMind\u2019s most advanced video generation model to date. It allows users to create high-quality, cinematic video clips from simple text prompts, making it one of the most powerful AI tools for video creation. What sets Veo 3 apart is its ability to generate videos with native audio. This means that along with stunning visuals, Veo 3 can produce synchronized dialogue, ambient sounds, and background music\u2014all from a single prompt. For filmmakers, this is a significant leap forward, as it eliminates the need for separate audio generation or complex syncing processes. Veo 3...", "url": "https://huggingface.co/posts/MonsterMMORPG/279762403721253", "date_published": "2025-05-30T09:25:15.518029"}]}