{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/codelion/260857666290873", "image": "", "title": "Reverse Engineering a $500M Mystery: From HashHop to Memory-Augmented Language Models", "content_text": "Reverse Engineering a $500M Mystery: From HashHop to Memory-Augmented Language Models I wrote a deep dive into how Magic AI's 100M token context window might work, starting from their HashHop benchmark and building up to MALM - a Memory-Augmented Language Model. Key insight: treating each key as a single token enables perfect retrieval at unlimited context lengths. The article covers: - How HashHop works and why its perfect accuracy is suspicious - Building a tokenized solver that achieves 100% accuracy - Scaling to MALM for real code search tasks - Why this approach could handle 100M+ tokens Read the full article: https://huggingface.co/blog/codelion/reverse-engineering-magic-hashhop Try the model: codelion/malm-165m Code: https://github.com/codelion/hash-hop See translation", "url": "https://huggingface.co/posts/codelion/260857666290873", "date_published": "2026-01-25T17:22:24.178985"}, {"id": "https://huggingface.co/posts/consome2/284818675921006", "image": "", "title": "We\u2019ve released two conversational speech datasets from oto on Hugging Face \ud83e\udd17", "content_text": "We\u2019ve released two conversational speech datasets from oto on Hugging Face \ud83e\udd17 Both are based on real, casual, full-duplex conversations, but with slightly different focuses. Dataset 1: Processed / curated subset otoearth/otoSpeech-full-duplex-processed-141h * Full-duplex, spontaneous multi-speaker conversations * Participants filtered for high audio quality * PII removal and audio enhancement applied * Designed for training and benchmarking S2S or dialogue models Dataset 2: Larger raw(er) release otoearth/otoSpeech-full-duplex-280h * Same collection pipeline, with broader coverage * More diversity in speakers, accents, and conversation styles * Useful for analysis, filtering, or custom preprocessing experiments We intentionally split the release to support different research workflows: clean and ready-to-use vs. more exploratory and research-oriented use. The datasets are currently private, but we\u2019re happy to approve access requests \u2014 feel free to request access if you\u2019re interested....", "url": "https://huggingface.co/posts/consome2/284818675921006", "date_published": "2026-01-25T17:22:24.179575"}, {"id": "https://huggingface.co/posts/raincandy-u/660562661454335", "image": "", "title": "\ud83e\udd17 Just released Rain-100M, an experimental ~97M-parameter Qwen3-style language model trained from random initialization.", "content_text": "\ud83e\udd17 Just released Rain-100M, an experimental ~97M-parameter Qwen3-style language model trained from random initialization. Repo: raincandy-u/Rain-100M Data: HuggingFaceFW/fineweb-edu , ~3B tokens, English only Tokenizer: custom 16k BPE, context length 4096 Architecture: 12 Transformer layers, hidden size 768, 12 heads, MLP 2048, SiLU, bf16 Rain-100M is a raw base model (not instruction-tuned or safety-aligned), aimed at small-scale research, debugging training pipelines, and CPU/edge experiments. If you run evaluations, finetunes, or visualizations with it, I would be very interested in your results! See translation", "url": "https://huggingface.co/posts/raincandy-u/660562661454335", "date_published": "2026-01-25T17:22:24.179955"}, {"id": "https://huggingface.co/posts/DavidAU/879154559678091", "image": "", "title": "Uncensored, Heretic GGUF quants of GLM 4.7 (30B-A3B) with correct Llamacpp and all updates ; NEO-CODE Imatrix W 16 bit OTs.", "content_text": "Uncensored, Heretic GGUF quants of GLM 4.7 (30B-A3B) with correct Llamacpp and all updates ; NEO-CODE Imatrix W 16 bit OTs. Also specialized quants (balanced for this model), and all quants are NEO-CODE Imatrix W 16 bit output tensor. DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF \"Reg quants, non-heretic\" : Also 16 bit ot, NEO-CODE Imatrix and specialized: DavidAU/GLM-4.7-Flash-NEO-CODE-Imatrix-MAX-GGUF See translation", "url": "https://huggingface.co/posts/DavidAU/879154559678091", "date_published": "2026-01-25T17:22:24.180224"}, {"id": "https://huggingface.co/posts/prithivMLmods/575874119029258", "image": "", "title": "Introducing QIE-2511-Zoom-Master for highlight-guided area zoom-in, enabling lossless zooming within a drawn square area, and QIE-2511-Object-Remover-v2 for precise object or highlight-guided area cleanup. These experimental adapters are trained based on QIE-2511. Find the adapters below.", "content_text": "Introducing QIE-2511-Zoom-Master for highlight-guided area zoom-in, enabling lossless zooming within a drawn square area, and QIE-2511-Object-Remover-v2 for precise object or highlight-guided area cleanup. These experimental adapters are trained based on QIE-2511. Find the adapters below. \ud83d\udd79\ufe0fQIE-2511-Zoom-Master : prithivMLmods/QIE-2511-Zoom-Master \ud83d\udd79\ufe0fQIE-2511-Object-Remover-v2: prithivMLmods/QIE-2511-Object-Remover-v2 \ud83e\udd17Demo: prithivMLmods/Qwen-Image-Edit-Object-Manipulator \ud83d\udcc2Collection: https://huggingface.co/collections/prithivMLmods/qwen-image-edit-exps To learn more, visit the app page or the respective model pages. See translation", "url": "https://huggingface.co/posts/prithivMLmods/575874119029258", "date_published": "2026-01-25T17:22:24.180557"}, {"id": "https://huggingface.co/posts/IlyasMoutawwakil/703555138750194", "image": "", "title": "After 2 months of refinement, I'm happy to announce that a lot of Transformers' modeling code is now significantly more torch-compile & export-friendly \ud83d\udd25", "content_text": "After 2 months of refinement, I'm happy to announce that a lot of Transformers' modeling code is now significantly more torch-compile & export-friendly \ud83d\udd25 Why it had to be done \ud83d\udc47 PyTorch's Dynamo compiler is increasingly becoming the default interoperability layer for ML systems. Anything that relies on torch.export or torch.compile, from model optimization to cross-framework integrations, benefits directly when models can be captured as a single dynamo-traced graph ! Transformers models are now easier to: \u2699\ufe0f Compile end-to-end with torch.compile backends \ud83d\udce6 Export reliably via torch.export and torch.onnx.export \ud83d\ude80 Deploy to ONNX / ONNX Runtime, Intel Corporation's OpenVINO, NVIDIA AutoDeploy (TRT-LLM), AMD's Quark, Meta's Executorch and more hardware-specific runtimes. This work aims at unblocking entire TorchDynamo-based toolchains that rely on exporting Transformers across runtimes and accelerators. We are doubling down on Transformers commitment to be a first-class citizen of the...", "url": "https://huggingface.co/posts/IlyasMoutawwakil/703555138750194", "date_published": "2026-01-25T17:22:24.181064"}, {"id": "https://huggingface.co/posts/kanaria007/389082518772877", "image": "", "title": "\u2705 New Article: *Jumps as Atomic Moves* (v0.1)", "content_text": "\u2705 New Article: *Jumps as Atomic Moves* (v0.1) Title: \ud83e\udde0 Jumps: Atomic Moves in Structured Intelligence (and How to Make Them Safe) \ud83d\udd17 https://huggingface.co/blog/kanaria007/jumps-atomic-moves-in-si --- Summary: In SI-Core, a *Jump* is the smallest *effectful* unit of reasoning+action: a move that consumes observations, proposes/chooses an action, and (optionally) commits results + memory updates. This article makes Jumps operational: *what a Jump must declare*, how it is gated (OBS/ETH/RML), how it produces auditable traces, and how to keep it safe under uncertainty\u2014without collapsing into \u201cjust prompt chaining.\u201d > If you can\u2019t name the Jump, you can\u2019t audit it. > If you can\u2019t gate it, you can\u2019t ship it. --- Why It Matters: \u2022 Stops hidden behavior: every effectful move becomes *declared + inspectable* \u2022 Prevents \u201cjumping in the dark\u201d via *OBS gating + sandbox-only paths* \u2022 Makes policy enforceable: ETH overlay can *allow/modify/block/escalate* per Jump type \u2022 Improves rollback...", "url": "https://huggingface.co/posts/kanaria007/389082518772877", "date_published": "2026-01-25T17:22:24.181680"}, {"id": "https://huggingface.co/posts/danielhanchen/579968620456275", "image": "", "title": "You can now fine-tune embedding models in our free Unsloth notebook! \ud83e\udd17", "content_text": "You can now fine-tune embedding models in our free Unsloth notebook! \ud83e\udd17 Fine-tuning embedding models improves retrieval & RAG by aligning vectors to your domain-specific notion of similarity, improving search, clustering, and recommendations on your data. \u2b50 Blog + Notebooks: https://unsloth.ai/docs/new/embedding-finetuning Unsloth trains embedding models 1.8-3.3x faster with 20% less VRAM, 2x longer context & no accuracy loss vs. FA2 setups. We'd like to thank Hugging Face and Unsloth contributor: electroglyph for making this possible! See translation", "url": "https://huggingface.co/posts/danielhanchen/579968620456275", "date_published": "2026-01-25T17:22:24.182033"}, {"id": "https://huggingface.co/posts/kanaria007/334353113368335", "image": "", "title": "\u2705 New Article: *Genius Replay Protocol* (v0.1)", "content_text": "\u2705 New Article: *Genius Replay Protocol* (v0.1) Title: \ud83e\udde0 Genius Replay Protocol: Capturing and Replaying High-Value Jumps \ud83d\udd17 https://huggingface.co/blog/kanaria007/genius-replay-protocol --- Summary: \u201cGenius\u201d isn\u2019t magic\u2014it\u2019s a **high-value Jump (or short Jump sequence)** that reliably produces outsized GCS gains with strong robustness and reuse potential. This article defines the **Genius Replay Protocol (GRP)**: a safe way to **capture, validate, store, and replay** those exceptional moves as reusable macro-intelligence\u2014without turning them into copy-pasted folklore. At the core is a **GeniusTrace** bundle: *ContextSignature* (where it applies) + *JumpSequence* (what was done) + *EvalSummary* (why it worked) + *EthicsTrace* (why it\u2019s allowed). > Don\u2019t replay outcomes. > Replay **structure**, under today\u2019s constraints. --- Why It Matters: \u2022 Turns \u201cit worked once\u201d into a **reproducible asset** \u2022 Prevents unsafe cargo-culting by replaying **structure**, not brittle outputs \u2022 Ensures...", "url": "https://huggingface.co/posts/kanaria007/334353113368335", "date_published": "2026-01-25T17:22:24.182627"}, {"id": "https://huggingface.co/posts/AbstractPhil/216348621155018", "image": "", "title": "Meet FluxLailah;", "content_text": "Meet FluxLailah; AbstractPhil/tiny-flux-deep ; 220m Flux variant currently pretraining at BF16. She is experimental, does not produce solid images yet - and yet she is producing. There is both an EMA and a raw weights pair producing different images. The EMA is particularly interesting at times. Lailah uses flan-t5-base, clip-vit-l-14, and BlackForestLabs Flux1s VAE. SEQ limit 128, images 512x512 for now. Lailah's early form is based on three variants. TinyFlux's weights were carefully planted into a deeper structure and trained yet again - dubbed TinyFlux-Deep. This variant has 15 dual-stream blocks and 25 single-stream blocks, nearly identical weight code as Flux with a similar attention mechanism - but intentionally deviant and compacted with careful consideration to scaling and purpose of mechanisms. She went through quite a few growing pains with her earlier attention mechanism which required a reimagining today and careful consideration of the consequences, and now I present...", "url": "https://huggingface.co/posts/AbstractPhil/216348621155018", "date_published": "2026-01-25T17:22:24.183045"}]}