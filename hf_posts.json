{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/IlyasMoutawwakil/848772925772411", "image": "", "title": "Transformers v5 just landed! \ud83d\ude80", "content_text": "Transformers v5 just landed! \ud83d\ude80 It significantly unifies and reduces modeling code across architectures, while opening the door to a whole new class of performance optimizations. My favorite new feature? \ud83e\udd14 The new dynamic weight loader + converter. Here\u2019s why \ud83d\udc47 Over the last few months, the core Transformers maintainers built an incredibly fast weight loader, capable of converting tensors on the fly while loading them in parallel threads. This means we\u2019re no longer constrained by how parameters are laid out inside the safetensors weight files. In practice, this unlocks two big things: - Much more modular modeling code. You can now clearly see how architectures build on top of each other (DeepSeek v2 \u2192 v3, Qwen v2 \u2192 v3 \u2192 MoE, etc.). This makes shared bottlenecks obvious and lets us optimize the right building blocks once, for all model families. - Performance optimizations beyond what torch.compile can do alone. torch.compile operates on the computation graph, but it can\u2019t change...", "url": "https://huggingface.co/posts/IlyasMoutawwakil/848772925772411", "date_published": "2026-01-29T05:49:43.973340"}, {"id": "https://huggingface.co/posts/danielhanchen/602916263790271", "image": "", "title": "You can now run Kimi K2.5 locally! \ud83d\udd25", "content_text": "You can now run Kimi K2.5 locally! \ud83d\udd25 We shrank the 1T model to 240GB (-60%) via Dynamic 1-bit. Get >40 tok/s on 242GB or 622GB VRAM/RAM for near full precision. GGUF: unsloth/Kimi-K2.5-GGUF Guide: https://unsloth.ai/docs/models/kimi-k2.5 See translation", "url": "https://huggingface.co/posts/danielhanchen/602916263790271", "date_published": "2026-01-29T05:49:43.973648"}, {"id": "https://huggingface.co/posts/ovi054/176731931119322", "image": "", "title": "ovi054/LTX-2-19b-Squish-LoRA", "content_text": "ovi054/LTX-2-19b-Squish-LoRA \u26a1 I trained a Squish LoRA for LTX-2. Upload an image and give prompt \"squish it\" to get the squish video. Demo output videos are attached. \ud83d\udc49Try it now: ovi054/LTX-2-19b-Squish-LoRA ovi054/ltx-2-Audio-to-Video See translation", "url": "https://huggingface.co/posts/ovi054/176731931119322", "date_published": "2026-01-29T05:49:43.973946"}, {"id": "https://huggingface.co/posts/RakshitAralimatti/297622038982343", "image": "", "title": "Just built my entire AI Engineer portfolio by pasting 2 links (GitHub and LinkedIn)  into", "content_text": "Just built my entire AI Engineer portfolio by pasting 2 links (GitHub and LinkedIn) into moonshotai Kimi 2.5. That's it. That's the workflow. Zero coding. Zero iteration. Zero \"make the button bigger.\" See for yourself: https://rakshit2020.github.io/rakshitaralimatti.github.io/ The model: \u2705 Scraped my GitHub repos automatically \u2705 Pulled my experience from LinkedIn \u2705 Designed an Aurora Glass theme \u2705 Mapped every skill to projects \u2705 Added animations I'd never code myself See translation", "url": "https://huggingface.co/posts/RakshitAralimatti/297622038982343", "date_published": "2026-01-29T05:49:43.974276"}, {"id": "https://huggingface.co/posts/imnotkitty/264000680649196", "image": "", "title": "\ud83d\udcccSame day, Two Releases.", "content_text": "\ud83d\udcccSame day, Two Releases. Jan 27th just got interesting on Open-source AI modles. \u2705Kimi K2.5: How to make models \"think\" across text and vision natively? moonshotai/Kimi-K2.5 \u2705DeepSeek-OCR 2: How to make models \"see\" more like humans, not scanners? deepseek-ai/DeepSeek-OCR-2 One focuses on depth of reasoning, the other on precision of vision. What's the key differentiator for a multimodal model in your view: raw power or computational elegance? See translation", "url": "https://huggingface.co/posts/imnotkitty/264000680649196", "date_published": "2026-01-29T05:49:43.974597"}, {"id": "https://huggingface.co/posts/AdinaY/365964215058126", "image": "", "title": "Big day in open source AI!!", "content_text": "Big day in open source AI!! \u2728 DeepSeek released OCR2 \ud83d\udca5 deepseek-ai/DeepSeek-OCR-2 \u2728 Kimi K2.5 just landed \ud83d\udd25 moonshotai/Kimi-K2.5 With the Chinese Spring Festival 3 weeks away, what\u2019s coming next?\ud83d\udc40 See translation", "url": "https://huggingface.co/posts/AdinaY/365964215058126", "date_published": "2026-01-29T05:49:43.974876"}, {"id": "https://huggingface.co/posts/kanaria007/711602614783831", "image": "", "title": "\u2705 New Article: *Post-Transformer Decision Cores* (v0.1)", "content_text": "\u2705 New Article: *Post-Transformer Decision Cores* (v0.1) Title: \ud83d\ude80 Post-Transformer Decision Cores: Goal-Native Engines Beyond LLMs \ud83d\udd17 https://huggingface.co/blog/kanaria007/post-tranformer-decision-cores --- Summary: Transformers are powerful\u2014but in SI-Core they\u2019re *not the essence of intelligence*. A *Decision Core* is anything that satisfies the *Jump contracts* (OBS/ETH/MEM/ID/EVAL + RML), and those contracts don\u2019t require next-token prediction. This article sketches what \u201cpost-Transformer\u201d looks like in practice: *goal-native, structure-aware controllers* that may use LLMs as tools\u2014but don\u2019t depend on them as the runtime brain. > Don\u2019t relax the contracts. > Replace the engine behind them. --- Why It Matters: \u2022 Makes LLMs *optional*: shift them to \u201cgenesis / exploration / explanation,\u201d while routine high-stakes Jumps run on structured cores \u2022 Improves boring-but-critical properties: *determinism (CAS), fewer inconsistencies (SCI), fewer ETH violations (EAI), better rollback...", "url": "https://huggingface.co/posts/kanaria007/711602614783831", "date_published": "2026-01-29T05:49:43.975537"}, {"id": "https://huggingface.co/posts/tegridydev/651013007690920", "image": "", "title": "Introducing OpenMALx", "content_text": "Introducing OpenMALx openmalx Repository for Infosec and Machine Learning Resources OpenMALx is an organization focused on the development of datasets and models for security analysis. The project objective is to provide structured data for training and evaluating large language models in a security context. --- Technical Focus **Dataset Formatting:** Processing raw security tool logs into instruction/response pairs for model training. **Local Execution:** Optimizing models for local hardware to ensure data remains on-premises. **Response Logic:** Developing structured formats for explaining security vulnerabilities and remediation steps. Active Projects **infosec-tool-output:** A dataset mapping static and dynamic analysis tool outputs to technical summaries. openmalx/infosec-tool-output **open-malsec:** A collection of text-based security threats, including phishing and social engineering samples, for classification tasks. openmalx/open-malsec See translation", "url": "https://huggingface.co/posts/tegridydev/651013007690920", "date_published": "2026-01-29T05:49:43.975969"}, {"id": "https://huggingface.co/posts/sergiopaniego/425017162094241", "image": "", "title": "Date idea: read the entire Transformers v5.0.0 release notes", "content_text": "Date idea: read the entire Transformers v5.0.0 release notes Officially stable now: https://github.com/huggingface/transformers/releases/tag/v5.0.0 See translation", "url": "https://huggingface.co/posts/sergiopaniego/425017162094241", "date_published": "2026-01-29T05:49:43.976204"}, {"id": "https://huggingface.co/posts/Parveshiiii/254137909606959", "image": "", "title": "\ud83d\ude80 Wanna train your own AI Model or Tokenizer from scratch?", "content_text": "\ud83d\ude80 Wanna train your own AI Model or Tokenizer from scratch? Building models isn\u2019t just for big labs anymore \u2014 with the right data, compute, and workflow, you can create **custom AI models** and **tokenizers** tailored to any domain. Whether it\u2019s NLP, domain\u2011specific datasets, or experimental architectures, training from scratch gives you full control over vocabulary, embeddings, and performance. \u2728 Why train your own? - Full control over vocabulary & tokenization - Domain\u2011specific optimization (medical, legal, technical, etc.) - Better performance on niche datasets - Freedom to experiment with architectures \u26a1 The best part? - Tokenizer training (TikToken / BPE) can be done in **just 3 lines of code**. - Model training runs smoothly on **Google Colab notebooks** \u2014 no expensive hardware required. \ud83d\udcc2 Try out my work: - \ud83d\udd17 https://github.com/OE-Void/Tokenizer-from_scratch - \ud83d\udd17 https://github.com/OE-Void/GPT See translation", "url": "https://huggingface.co/posts/Parveshiiii/254137909606959", "date_published": "2026-01-29T05:49:43.976618"}]}