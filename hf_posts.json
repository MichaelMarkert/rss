{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/384482543815919", "image": "", "title": "13 New types of LoRA", "content_text": "13 New types of LoRA LoRA (Low-Rank Adaptation) is a popular lightweight method for fine-tuning AI models. It doesn't update the full model, it adds small trainable components, low-rank matrices, while keeping the original weights frozen. Only these adapters are trained. Recently, many interesting new LoRA variations came out, so it\u2019s a great time to take a look at these 13 clever approaches: 1. T-LoRA \u2192 T-LoRA: Single Image Diffusion Model Customization Without Overfitting (2507.05964) A timestep-dependent LoRA method for adapting diffusion models with a single image. It dynamically adjusts updates and uses orthogonal initialization to reduce overlap, achieving better fidelity\u2013alignment balance than standard LoRA 2. SingLoRA \u2192 SingLoRA: Low Rank Adaptation Using a Single Matrix (2507.05566) Simplifies LoRA by using only one small matrix instead of usual two, and multiplying it by its own transpose (like A \u00d7 A\u1d40). It uses half the parameters of LoRA and avoids scale mismatch between...", "url": "https://huggingface.co/posts/Kseniase/384482543815919", "date_published": "2025-07-15T05:28:45.104510"}, {"id": "https://huggingface.co/posts/Tonic/414083244384754", "image": "", "title": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f Normalize adding compute & runtime traces to your model cards", "content_text": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f Normalize adding compute & runtime traces to your model cards See translation", "url": "https://huggingface.co/posts/Tonic/414083244384754", "date_published": "2025-07-15T05:28:45.104729"}, {"id": "https://huggingface.co/posts/prithivMLmods/700925755780035", "image": "", "title": "Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. \ud83e\udd17\ud83d\udcc4", "content_text": "Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. \ud83e\udd17\ud83d\udcc4 Spaces & Models : > Doc-VLMs-OCR : prithivMLmods/Doc-VLMs-OCR > core-OCR : prithivMLmods/core-OCR > Megalodon-OCR (3B) : prithivMLmods/Megalodon-OCR-Sync-0713 > Perseus-Doc-vl (7B): prithivMLmods/Perseus-Doc-vl-0712 Datasets Caption Mix : > Corvus-OCR-Caption-Mix : prithivMLmods/Corvus-OCR-Caption-Mix > Corvus-OCR-Caption-Mini-Mix : prithivMLmods/Corvus-OCR-Caption-Mini-Mix Collections : > Corvus OCR Caption Mix: prithivMLmods/corvus-ocr-caption-mix-687349bfaceffbd10976f0cc > Captioning / OCR / DocTable : prithivMLmods/captioning-ocr-doctable-687382e1da822008bb5c06f2 GitHub : > OCR-ReportLab :...", "url": "https://huggingface.co/posts/prithivMLmods/700925755780035", "date_published": "2025-07-15T05:28:45.105267"}, {"id": "https://huggingface.co/posts/hesamation/850768471232119", "image": "", "title": "in case you didn\u2019t know, Claude now has a developer training course with certificates,", "content_text": "in case you didn\u2019t know, Claude now has a developer training course with certificates, this is better than anything you can find on Coursera. covers Claude Code, MCP and its advanced topics and even more: https://www.anthropic.com/learn/build-with-claude See translation", "url": "https://huggingface.co/posts/hesamation/850768471232119", "date_published": "2025-07-15T05:28:45.105525"}, {"id": "https://huggingface.co/posts/AdinaY/423045666935241", "image": "", "title": "Kimi-K2 is now available on the hub\ud83d\udd25\ud83d\ude80", "content_text": "Kimi-K2 is now available on the hub\ud83d\udd25\ud83d\ude80 This is a trillion-parameter MoE model focused on long context, code, reasoning, and agentic behavior. moonshotai/kimi-k2-6871243b990f2af5ba60617d \u2728 Base & Instruct \u2728 1T total / 32B active - Modified MIT License \u2728 128K context length \u2728 Muon optimizer for stable trillion-scale training See translation", "url": "https://huggingface.co/posts/AdinaY/423045666935241", "date_published": "2025-07-15T05:28:45.105793"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/377698831818015", "image": "", "title": "\ud83d\udce2 Generate your own data in simulation using two new free and customizable data-generating Scenarios on Duality's FalconCloud service.", "content_text": "\ud83d\udce2 Generate your own data in simulation using two new free and customizable data-generating Scenarios on Duality's FalconCloud service. \ud83d\ude4c These multi-class Scenarios are designed to target model weaknesses for our recent Kaggle competition, but they are free to anyone for non-commercial use! \ud83d\udcf8 Control object and camera posing \ud83d\udc49 Select random variable ranges \ud83d\uddbc\ufe0f Set post-processing effects \u2795 and more to create a robust dataset for strong model training. Access the 2 Scenarios here: \ud83d\udca0 https://falcon.duality.ai/secure/scenarios/edit/9e90e036-8af9-41e4-8af0-1343b8e8f467?utm_source=Kaggle&utm_medium=post&utm_campaign=competition_4 \ud83d\udca0 https://falcon.duality.ai/secure/scenarios/edit/e3294c19-49d4-4f64-9ca8-8373876c2c94?utm_source=Kaggle&utm_medium=post&utm_campaign=competition_4 See translation", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/377698831818015", "date_published": "2025-07-15T05:28:45.106144"}, {"id": "https://huggingface.co/posts/hba123/150628355666585", "image": "", "title": "In our latest paper, Bourbaki (7b), we show how one can achieve state-of-the-art 7B theorem provers on PutnamBench by applying MCTS to what we call self-generated and goal-conditioned MDPs. I started a series of Blogs on this!", "content_text": "In our latest paper, Bourbaki (7b), we show how one can achieve state-of-the-art 7B theorem provers on PutnamBench by applying MCTS to what we call self-generated and goal-conditioned MDPs. I started a series of Blogs on this! Why a series of Blogs \ud83d\ude1d? I want to try to make everyone understand what Bourbaki (7b) is and what it does. I don't want to just give you a ChatGPT summary with some result hype. I think there are many things to improve, and I am hoping with more exposure to this, beyond experiments and codes, some people would be interested and help us improve it! In this first blog, we will be talking basics: 1) MCTS and why it should be applied to LLMs so that the whole world is not just fine-tuning a 100000000000000000000000 b model on 10 data points (not that i have not done it before \ud83e\udd2a\ud83e\udd2a), 2) the basics of MDPs, and 3) the Vanilla MCTS algorithm. Check it out: https://huggingface.co/blog/hba123/bourbaki7b If you find it useful, consider upvoting and sharing this post and...", "url": "https://huggingface.co/posts/hba123/150628355666585", "date_published": "2025-07-15T05:28:45.106610"}, {"id": "https://huggingface.co/posts/DawnC/760622875415705", "image": "", "title": "\ud83c\udfaf Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science!", "content_text": "\ud83c\udfaf Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science! This isn't just another computer vision project. VisionScout represents a fundamental shift from simple object detection to genuine scene understanding, where four specialized AI models work together to interpret what's actually happening in an image. \ud83c\udfd7\ufe0f Part 1: Architecture Foundation How careful system design transforms independent models into collaborative intelligence through proper layering and coordination strategies. \u2699\ufe0f Part 2: Deep Technical Implementation The five core algorithms powering the system: dynamic weight adjustment, attention mechanisms, statistical methods, lighting analysis, and CLIP's zero-shot learning. \ud83c\udf0d Part 3: Real-World Validation Concrete case studies from indoor spaces to cultural landmarks, demonstrating how integrated systems deliver insights no single model could achieve. What makes this valuable:...", "url": "https://huggingface.co/posts/DawnC/760622875415705", "date_published": "2025-07-15T05:28:45.107164"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/139363304280237", "image": "", "title": "MultiTalk Levelled Up - Way Better Animation Compared to Before with New Workflows - Image to Video >", "content_text": "MultiTalk Levelled Up - Way Better Animation Compared to Before with New Workflows - Image to Video > https://youtu.be/wgCtUeog41g MultiTalk is greatly upgraded. After doing more than 1 day more research with MultiTalk by using 8x A6000 48 GB GPUs, I have significantly improved the MultiTalk workflows and now I am sharing 4 different category workflows with you. VRAM usages and speeds are same but just better quality and animation. Moreover I am introducing a new app which is image and video comparison sliders. Ultra fast and lightweight. Runs as a html app and no GPU is required. https://youtu.be/wgCtUeog41g MultiTalk Full Tutorial With 1-Click Installer - Make Talking and Singing Videos From Static Images > https://youtu.be/8cMIwS9qo4M By using MeiGen MultiTalk you can generate amazing fully animated real-like videos from given audio input. Not only talking but also animating the body movements is possible. In this video I will show you how to install ComfyUI on Windows and...", "url": "https://huggingface.co/posts/MonsterMMORPG/139363304280237", "date_published": "2025-07-15T05:28:45.107599"}, {"id": "https://huggingface.co/posts/sequelbox/729247272054764", "image": "", "title": "Some new releases:", "content_text": "Some new releases: - brought the new Shining Valiant 3 series (science-reasoning, AI-reasoning, general chat) to Qwen 3 4B: ValiantLabs/Qwen3-4B-ShiningValiant3 - merged models for Shining Valiant 3 and Esper 3, combining their technical expertise and reasoning skills: 4b: sequelbox/Qwen3-4B-PlumEsper 8b: sequelbox/Qwen3-8B-PlumEsper coming up we'll have some experimental reasoning releases - datasets and models will be out soon! also will be bringing SV3 and Esper 3 to more models. lets keep working for open source :) love, allegra See translation", "url": "https://huggingface.co/posts/sequelbox/729247272054764", "date_published": "2025-07-15T05:28:45.107882"}]}