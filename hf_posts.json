{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/gokaygokay/758462412009896", "image": "", "title": "FlashPack: Lightning-Fast Model Loading for PyTorch", "content_text": "FlashPack: Lightning-Fast Model Loading for PyTorch https://github.com/fal-ai/flashpack FlashPack \u2014 a new, high-throughput file format and loading mechanism for PyTorch that makes model checkpoint I/O blazingly fast, even on systems without access to GPU Direct Storage (GDS). With FlashPack, loading any model can be 3\u20136\u00d7 faster than with the current state-of-the-art methods like accelerate or the standard load_state_dict() and to() flow \u2014 all wrapped in a lightweight, pure-Python package that works anywhere. See translation", "url": "https://huggingface.co/posts/gokaygokay/758462412009896", "date_published": "2025-10-27T17:20:27.790119"}, {"id": "https://huggingface.co/posts/DmitryRyumin/275169930033420", "image": "", "title": "\ud83d\ude80\ud83d\udc41\ufe0f\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83d\udc41\ufe0f\ud83d\ude80", "content_text": "\ud83d\ude80\ud83d\udc41\ufe0f\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83d\udc41\ufe0f\ud83d\ude80 \ud83d\udcc4 Title: Token Activation Map to Visually Explain Multimodal LLMs \ud83d\udd1d \ud83d\udcdd Description: The Token Activation Map (TAM) is an advanced explainability method for multimodal LLMs. Using causal inference and a Rank Gaussian Filter, TAM reveals token-level interactions and eliminates redundant activations. The result is clearer, high-quality visualizations that enhance understanding of object localization, reasoning and multimodal alignment across models. \ud83d\udc65 Authors: Yi Li, Hualiang Wang, Xinpeng Ding, Haonan Wang, and Xiaomeng Li \ud83d\udcc5 Conference: ICCV, 19 \u2013 23 Oct, 2025 | Honolulu, Hawai'i, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: Token Activation Map to Visually Explain Multimodal LLMs (2506.23270) \ud83d\udcc1 Repository: https://github.com/xmed-lab/TAM \ud83d\ude80 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers \ud83d\ude80 Added to the Multi-Modal Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/multi-modal-learning.md \ud83d\udcda More...", "url": "https://huggingface.co/posts/DmitryRyumin/275169930033420", "date_published": "2025-10-27T17:20:27.790624"}, {"id": "https://huggingface.co/posts/DmitryRyumin/256377930602220", "image": "", "title": "\ud83d\ude80\ud83e\udd16\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\udd16\ud83d\ude80", "content_text": "\ud83d\ude80\ud83e\udd16\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\udd16\ud83d\ude80 \ud83d\udcc4 Title: Variance-based Pruning for Accelerating and Compressing Trained Networks \ud83d\udd1d \ud83d\udcdd Description: The one-shot pruning method efficiently compresses networks, reducing computation and memory usage while retaining almost full performance and requiring minimal fine-tuning. \ud83d\udc65 Authors: Uranik Berisha, Jens Mehnert, and Alexandru Paul Condurache \ud83d\udcc5 Conference: ICCV, 19 \u2013 23 Oct, 2025 | Honolulu, Hawai'i, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: Variance-Based Pruning for Accelerating and Compressing Trained Networks (2507.12988) \ud83d\ude80 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers \ud83d\ude80 Added to the Efficient Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/efficient-learning.md \ud83d\udcda More Papers: more cutting-edge research presented at other conferences in the DmitryRyumin/NewEraAI-Papers curated by @ DmitryRyumin \ud83d\udd0d Keywords: #VarianceBasedPruning #NetworkCompression #ModelAcceleration...", "url": "https://huggingface.co/posts/DmitryRyumin/256377930602220", "date_published": "2025-10-27T17:20:27.791062"}, {"id": "https://huggingface.co/posts/mike-ravkine/114687779852365", "image": "", "title": "Spatial reasoning is a domain where LLMs struggle surprisingly hard.", "content_text": "Spatial reasoning is a domain where LLMs struggle surprisingly hard. A new paper, \"Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models\" compares performance on a handful of spacial reasoning tasks and finds all SOTA LLMs breaking down and hallucinating their faces off when the grids get large. https://arxiv.org/html/2510.20198v1 The word search task is especially revealing: notice the bias towards detecting \"horizontal\" while struggling with \"vertical\" - LLMs only understand simple, linear relationships.. add a stride for 2D and it's basically over. See translation", "url": "https://huggingface.co/posts/mike-ravkine/114687779852365", "date_published": "2025-10-27T17:20:27.791352"}, {"id": "https://huggingface.co/posts/onekq/651277400400501", "image": "", "title": "Context rot is such a catchy phrase, but the problem has been identified 2+ years ago, called attention decay.", "content_text": "Context rot is such a catchy phrase, but the problem has been identified 2+ years ago, called attention decay. Lost in the Middle: How Language Models Use Long Contexts (2307.03172) I spotted the same problem in coding tasks, and documented in my book ( https://www.amazon.com/dp/9999331130 ). Why did this problem become hot again? This is because many of us thought the problem has been solved by long context models, which is not true. Here we were misled by benchmarks. Most long-context benchmarks build around the QA scenario, i.e. \"finding needle in haystack\". But in agentic scenarios, the model needs to find EVERYTHING in the haystack, and just can't afford enough attention for this challenge. See translation", "url": "https://huggingface.co/posts/onekq/651277400400501", "date_published": "2025-10-27T17:20:27.791666"}, {"id": "https://huggingface.co/posts/onekq/981925262392459", "image": "", "title": "I am on the model layer and focus on atomic tasks, so I don't get involved in product discussions. But this provocative article provoked the community quite a bit. The case in point is Claude Code, which happens to be my biggest productivity revolution since ChatGPT.", "content_text": "I am on the model layer and focus on atomic tasks, so I don't get involved in product discussions. But this provocative article provoked the community quite a bit. The case in point is Claude Code, which happens to be my biggest productivity revolution since ChatGPT. RAG predated TUI and agents. So to be fair it's quite an achievement to survive the AI evolution. But I feel it is overshadowed by context engineering in the agent era. How does everyone feel about this? https://www.nicolasbustamante.com/p/the-rag-obituary-killed-by-agents See translation", "url": "https://huggingface.co/posts/onekq/981925262392459", "date_published": "2025-10-27T17:20:27.791942"}, {"id": "https://huggingface.co/posts/mitkox/949505421454376", "image": "", "title": "I\u2019m just reading that Ryzen AI 395 has to be 30% slower than DGX Spark in LLM inferencing\u2026 and only 96GB GPU RAM\u2026 good I haven\u2019t RTFM upfront, so I made the AMD faster with 128GB unified RAM \ud83e\udee1", "content_text": "I\u2019m just reading that Ryzen AI 395 has to be 30% slower than DGX Spark in LLM inferencing\u2026 and only 96GB GPU RAM\u2026 good I haven\u2019t RTFM upfront, so I made the AMD faster with 128GB unified RAM \ud83e\udee1 Z2 mini G1a can run Qwen3 Coder 30B BF16 at 26.8 tok/sec in ~60GB GPU RAM See translation", "url": "https://huggingface.co/posts/mitkox/949505421454376", "date_published": "2025-10-27T17:20:27.792213"}, {"id": "https://huggingface.co/posts/nroggendorff/194600363127405", "image": "", "title": "Is it hot in here, or is it just me?", "content_text": "Is it hot in here, or is it just me? See translation", "url": "https://huggingface.co/posts/nroggendorff/194600363127405", "date_published": "2025-10-27T17:20:27.792402"}, {"id": "https://huggingface.co/posts/SelmaNajih001/973945902491280", "image": "", "title": "How Financial News Can Be Used to Train Good Financial Models \ud83d\udcf0", "content_text": "How Financial News Can Be Used to Train Good Financial Models \ud83d\udcf0 Numbers tell you what happened, but news tells you why. I\u2019ve written an article explaining how news can be used to train AI models for sentiment analysis and better forecasting. Hope you find it interesting! Read it here: https://huggingface.co/blog/SelmaNajih001/llms-applied-to-finance I would love to read your opinions! I\u2019m open to suggestions on how to improve the methodology and the training See translation", "url": "https://huggingface.co/posts/SelmaNajih001/973945902491280", "date_published": "2025-10-27T17:20:27.792690"}, {"id": "https://huggingface.co/posts/DmitryRyumin/687304943131343", "image": "", "title": "\ud83d\ude80\ud83c\udff7\ufe0f\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\udde9\ud83d\ude80", "content_text": "\ud83d\ude80\ud83c\udff7\ufe0f\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\udde9\ud83d\ude80 \ud83d\udcc4 Title: Heavy Labels Out! Dataset Distillation with Label Space Lightening \ud83d\udd1d \ud83d\udcdd Description: The HeLlO framework is a new corpus distillation method that removes the need for large soft labels. It uses a lightweight, online image-to-label projector based on CLIP. This projector has been adapted using LoRA-style, parameter-efficient tuning. It has also been initialized with text embeddings. \ud83d\udc65 Authors: @ roseannelexie , @ Huage001 , Zigeng Chen, Jingwen Ye, and Xinchao Wang \ud83d\udcc5 Conference: ICCV, 19 \u2013 23 Oct, 2025 | Honolulu, Hawai'i, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: Heavy Labels Out! Dataset Distillation with Label Space Lightening (2408.08201) \ud83d\udcfa Video: https://www.youtube.com/watch?v=kAyK_3wskgA \ud83d\ude80 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers \ud83d\ude80 Added to the Efficient Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/efficient-learning.md \ud83d\udcda More Papers: more cutting-edge research...", "url": "https://huggingface.co/posts/DmitryRyumin/687304943131343", "date_published": "2025-10-27T17:20:27.793158"}]}