{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/abidlabs/941146046599374", "image": "", "title": "Why I think local, open-source models will eventually win.", "content_text": "Why I think local, open-source models will eventually win. The most useful AI applications are moving toward multi-turn agentic behavior: systems that take hundreds or even thousands of iterative steps to complete a task, e.g. Claude Code, computer-control agents that click, type, and test repeatedly. In these cases, the power of the model is not how smart it is per token, but in how quickly it can interact with its environment and tools across many steps. In that regime, model quality becomes secondary to latency. An open-source model that can call tools quickly, check that the right thing was clicked, or verify that a code change actually passes tests can easily outperform a slightly \u201csmarter\u201d closed model that has to make remote API calls for every move. Eventually, the balance tips: it becomes impractical for an agent to rely on remote inference for every micro-action. Just as no one would tolerate a keyboard that required a network request per keystroke, users won\u2019t accept...", "url": "https://huggingface.co/posts/abidlabs/941146046599374", "date_published": "2025-11-05T17:23:08.086879"}, {"id": "https://huggingface.co/posts/sergiopaniego/990279445625588", "image": "", "title": "fine-tuning a 14B model with TRL + SFT on a free Colab (T4 GPU)?", "content_text": "fine-tuning a 14B model with TRL + SFT on a free Colab (T4 GPU)? thanks to the latest TRL optimizations, you actually can! sharing a new notebook showing how to do it \ud83d\ude0e colab: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_trl_lora_qlora.ipynb notebooks in TRL: https://github.com/huggingface/trl/tree/main/examples/notebooks See translation", "url": "https://huggingface.co/posts/sergiopaniego/990279445625588", "date_published": "2025-11-05T17:23:08.087184"}, {"id": "https://huggingface.co/posts/wang12390/323744389614625", "image": "", "title": "Experience the future of fashion with our AI-powered virtual try-on technology. See how clothes look on anyone instantly, create realistic outfit visualizations, and mix-and-match styles with unprecedented accuracy.", "content_text": "Experience the future of fashion with our AI-powered virtual try-on technology. See how clothes look on anyone instantly, create realistic outfit visualizations, and mix-and-match styles with unprecedented accuracy. https://miragic.ai/products/virtual-try-on See translation", "url": "https://huggingface.co/posts/wang12390/323744389614625", "date_published": "2025-11-05T17:23:08.087411"}, {"id": "https://huggingface.co/posts/flozi00/890663421107803", "image": "", "title": "Some weeks ago, i've just decide its time to leave LinkedIn for me.", "content_text": "Some weeks ago, i've just decide its time to leave LinkedIn for me. It got silent around my open source activities the last year, so i thought something has to change. That's why my focus will move to share experiences and insights about hardware, drivers, kernels and linux. I won't post about how to use models, built agents or do prompting. I want to share about some deeper layers the actual hypes are built on. I will start posting summarizations of my articles here on the hub. English version: https://flozi.net/en German translated version: https://flozi.net/de Feel free to reach me if you want to read something specific. See translation", "url": "https://huggingface.co/posts/flozi00/890663421107803", "date_published": "2025-11-05T17:23:08.087704"}, {"id": "https://huggingface.co/posts/DawnC/897621817995080", "image": "", "title": "Pixcribe \u2014 AI-Powered Social Media Caption Generator \ud83d\udcf8\u2728", "content_text": "Pixcribe \u2014 AI-Powered Social Media Caption Generator \ud83d\udcf8\u2728 Transform your images into compelling stories with intelligent multi-model analysis! What can Pixcribe do? \ud83d\udcf8 Upload a photo to get instant AI-generated captions in Traditional Chinese and English. - \ud83c\udff7\ufe0f Brand Recognition \u2014 Detects logos and brand elements through visual detection, semantic analysis, and OCR verification. - \ud83c\udfa8 Scene Understanding \u2014 Analyzes composition, lighting conditions, and visual aesthetics to capture your image's mood and context. - \ud83d\udd0d Smart Text Extraction \u2014 Identifies and incorporates text from your images into captions seamlessly. - \u26a1 Multi-Model Intelligence \u2014 Combines YOLOv11 object detection, OpenCLIP semantic understanding, EasyOCR text recognition, U2-Net saliency detection, and Qwen2.5-VL-7B caption generation. What's next? \ud83c\udfac Video processing capabilities \ud83c\udf10 Enhanced multilingual support \ud83c\udfaf Interactive caption refinement with user feedback \u26a1 Real-time processing optimizations - Current Status: Under...", "url": "https://huggingface.co/posts/DawnC/897621817995080", "date_published": "2025-11-05T17:23:08.088253"}, {"id": "https://huggingface.co/posts/DheemanthReddy/522487783365937", "image": "", "title": "We just released Maya-1-Voice, an open source voice AI model with voice design and emotions.", "content_text": "We just released Maya-1-Voice, an open source voice AI model with voice design and emotions. Describe voices in natural language. Add 20+ emotions like <laugh>, <cry>, <whisper> inline. 3B parameters, production-ready, runs on single GPU with vLLM. Apache 2.0. Built on Llama backbone, predicts SNAC codec tokens for real-time streaming. Model: https://huggingface.co/maya-research/maya-1-voice See translation", "url": "https://huggingface.co/posts/DheemanthReddy/522487783365937", "date_published": "2025-11-05T17:23:08.088505"}, {"id": "https://huggingface.co/posts/Kseniase/468043722468280", "image": "", "title": "11 Fascinating new Policy Optimization techniques", "content_text": "11 Fascinating new Policy Optimization techniques Policy optimization (PO) algorithms are central to training AI models with preference-based feedback. In recent weeks, numerous new PO methods have emerged that build on or replace the popular PPO and GRPO, solving their issues. Here are 11 of them: 1. BAlanced Policy Optimization (BAPO) \u2192 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping (2510.18927) Dynamically adjusting the clipping bounds in PPO-style updates to balance positive and negative gradients and prevent entropy collapse 2. Training-Free GRPO \u2192 Training-Free Group Relative Policy Optimization (2510.08191) Instead of using numeric rewards, it compares rollouts semantically to distill useful knowledge as a token prior, which is then applied during inference to guide the model\u2019s behavior 3. Asymmetric Importance Sampling Policy Optimization (ASPO) \u2192 ASPO: Asymmetric Importance Sampling Policy Optimization...", "url": "https://huggingface.co/posts/Kseniase/468043722468280", "date_published": "2025-11-05T17:23:08.089183"}, {"id": "https://huggingface.co/posts/codelion/382097318111878", "image": "", "title": "On this day in 2019, OpenAI released the final GPT-2 model as part of their staged release. I still remember that November well - so much was happening, but GPT-2's release felt like a watershed moment for the field. It showed us what was possible with carefully trained language models.", "content_text": "On this day in 2019, OpenAI released the final GPT-2 model as part of their staged release. I still remember that November well - so much was happening, but GPT-2's release felt like a watershed moment for the field. It showed us what was possible with carefully trained language models. To recreate some of that GPT-2 magic, I recently tackled an interesting challenge: can you pretrain a language model with just 1 billion tokens - roughly 1/10th of what GPT-2 used - and still get comparable performance? After 50+ systematic experiments testing different dataset mixtures, the answer is yes. The result is **codelion/gpt-2-70m** ( codelion/gpt-2-70m ), which achieves over 90% of GPT-2's benchmark performance despite being trained on 10x less data. The key was finding the optimal dataset composition: 50% high-quality textbook PDFs, 30% filtered web content, and 20% educational resources. It even beats GPT-2 on TruthfulQA (47.31% vs 40.69%). If you're interested in the full story of how...", "url": "https://huggingface.co/posts/codelion/382097318111878", "date_published": "2025-11-05T17:23:08.089588"}, {"id": "https://huggingface.co/posts/nouamanetazi/972464132222376", "image": "", "title": "After training \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25\ud835\udc0b\ud835\udc0c\ud835\udfd1 on \ud835\udfd1\ud835\udfd6\ud835\udfd2 \ud835\udc07\ud835\udfcf\ud835\udfce\ud835\udfce\ud835\udc2c for nearly a month, I've come to realize something most people overlook: \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc24\ud835\udc1e-\ud835\udc28\ud835\udc2b-\ud835\udc1b\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc24 \ud835\udc1f\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc22\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20. \ud83d\udd25", "content_text": "After training \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25\ud835\udc0b\ud835\udc0c\ud835\udfd1 on \ud835\udfd1\ud835\udfd6\ud835\udfd2 \ud835\udc07\ud835\udfcf\ud835\udfce\ud835\udfce\ud835\udc2c for nearly a month, I've come to realize something most people overlook: \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc24\ud835\udc1e-\ud835\udc28\ud835\udc2b-\ud835\udc1b\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc24 \ud835\udc1f\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc22\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20. \ud83d\udd25 Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious \ud835\udc0d\ud835\udc02\ud835\udc02\ud835\udc0b \ud835\udc1e\ud835\udc2b\ud835\udc2b\ud835\udc28\ud835\udc2b\ud835\udc2c, or when your expensive GPU cluster is running at \ud835\udfd4\ud835\udfce% \ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc32, the problem isn't your model. It's most probably a \ud835\udc26\ud835\udc22\ud835\udc2c\ud835\udc2e\ud835\udc2c\ud835\udc1e \ud835\udc28\ud835\udc1f \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc21\ud835\udc1a\ud835\udc2b\ud835\udc1d\ud835\udc30\ud835\udc1a\ud835\udc2b\ud835\udc1e. \ud83d\udee0\ufe0f Questions that seemed simple but had no clear answers: Why is \ud835\udc0c\ud835\udc28\ud835\udc04 \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc2c\ud835\udc25\ud835\udc28\ud835\udc30\ud835\udc1e\ud835\udc2b \ud835\udc2d\ud835\udc21\ud835\udc1a\ud835\udc27 \ud835\udc1d\ud835\udc1e\ud835\udc27\ud835\udc2c\ud835\udc1e \ud835\udc26\ud835\udc28\ud835\udc1d\ud835\udc1e\ud835\udc25\ud835\udc2c? Which \ud835\udc0d\ud835\udc02\ud835\udc02\ud835\udc0b \ud835\udc1f\ud835\udc25\ud835\udc1a\ud835\udc20\ud835\udc2c should we actually set? How often should we checkpoint without killing throughput? That's why we built \ud835\udc13\ud835\udc21\ud835\udc1e \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25 \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc0f\ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1b\ud835\udc28\ud835\udc28\ud835\udc24 \ud83d\udcd6: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1e\ud835\udc2b that most teams get wrong. We validated real vs...", "url": "https://huggingface.co/posts/nouamanetazi/972464132222376", "date_published": "2025-11-05T17:23:08.090167"}, {"id": "https://huggingface.co/posts/DmitryRyumin/716491468051168", "image": "", "title": "\ud83d\ude80\ud83d\udc41\ufe0f\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83d\udc41\ufe0f\ud83d\ude80", "content_text": "\ud83d\ude80\ud83d\udc41\ufe0f\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83d\udc41\ufe0f\ud83d\ude80 \ud83d\udcc4 Title: Diving into the Fusion of Monocular Priors for Generalized Stereo Matching \ud83d\udd1d \ud83d\udcdd Description: The proposed method enhances stereo matching by efficiently combining unbiased monocular priors from vision foundation models. This method addresses misalignment and local optima issues using a binary local ordering map and pixel-wise linear regression. \ud83d\udc65 Authors: Chengtang Yao, Lidong Yu, Zhidan Liu, Jiaxi Zeng, Yuwei Wu, and Yunde Jia \ud83d\udcc5 Conference: ICCV, 19 \u2013 23 Oct, 2025 | Honolulu, Hawai'i, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: Diving into the Fusion of Monocular Priors for Generalized Stereo Matching (2505.14414) \ud83d\udcc1 Repository: https://github.com/YaoChengTang/Diving-into-the-Fusion-of-Monocular-Priors-for-Generalized-Stereo-Matching \ud83d\ude80 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers \ud83d\ude80 Added to the 3D Pose Understanding Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/3d-pose-...", "url": "https://huggingface.co/posts/DmitryRyumin/716491468051168", "date_published": "2025-11-05T17:23:08.090650"}]}