{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/danielhanchen/750340203924335", "image": "", "title": "You can now run MiniMax-2.5 locally! \ud83d\ude80", "content_text": "You can now run MiniMax-2.5 locally! \ud83d\ude80 At 230B parameters, MiniMax-2.5 is the strongest LLM under 700B params, delivering SOTA agentic coding & chat. Run Dynamic 3/4-bit on a 128GB Mac for 20 tokens/s. Guide: https://unsloth.ai/docs/models/minimax-2.5 GGUF: unsloth/MiniMax-M2.5-GGUF See translation", "url": "https://huggingface.co/posts/danielhanchen/750340203924335", "date_published": "2026-02-17T09:55:32.999585"}, {"id": "https://huggingface.co/posts/kostakoff/584269728210158", "image": "", "title": "My home lab for AI models - llmlaba v1", "content_text": "My home lab for AI models - llmlaba v1 After I began learning MLOps I realized that I needed some kind of home lab, there are a lot of GPUs that I need to learn how to set up and test. So I spent some time to do a researching which platform I could buy or build. My requirements ware: - Limited budget - Power supply 1 kW or higher - Few PCIe slots to be able to install more than one gpu - Zero maintenance cost, I don't want spend a lot of time or money to maintain lab hardware, except for the GPUs I chose the Intel Mac Pro 7.1: - Prices on eBay acceptable - Excelent cooling - 1.4 kW power supply - 7 PCIe slots - Zero maintenance: I don't need to do anything with the Mac Pro hardware; it just works - Classic UEFI boot loader It requires a bit of OS preparation: 1. Install Ubuntu 24.04 (it works with the general PC ISO image) 2. Set up T2 drivers sudo apt install -y dkms linux-headers-$( uname -r) applesmc-t2 apple-bce lm-sensors 3. Install t2fanrd to manually manage fans...", "url": "https://huggingface.co/posts/kostakoff/584269728210158", "date_published": "2026-02-17T09:55:33.000139"}, {"id": "https://huggingface.co/posts/danielhanchen/198614576061440", "image": "", "title": "You can now run Qwen3.5 locally! \ud83d\udc9c", "content_text": "You can now run Qwen3.5 locally! \ud83d\udc9c Qwen3.5-397B-A17B is an open MoE vision reasoning LLM for agentic coding & chat. It performs on par with Gemini 3 Pro, Claude Opus 4.5 & GPT-5.2. GGUF: unsloth/Qwen3.5-397B-A17B-GGUF Run Dynamic 3-bit on a 192GB Mac for 20 tokens/s. Guide: https://unsloth.ai/docs/models/qwen3.5 See translation", "url": "https://huggingface.co/posts/danielhanchen/198614576061440", "date_published": "2026-02-17T09:55:33.000425"}, {"id": "https://huggingface.co/posts/Janady07/852502523222465", "image": "", "title": "Here is one of the equations that make up the worlds first Artificial General Intelligence. Remember when building Artificial Intelligence or anything on a device it all starts out binary. Everything starts out with data flow physics and mathmatics", "content_text": "Here is one of the equations that make up the worlds first Artificial General Intelligence. Remember when building Artificial Intelligence or anything on a device it all starts out binary. Everything starts out with data flow physics and mathmatics See translation", "url": "https://huggingface.co/posts/Janady07/852502523222465", "date_published": "2026-02-17T09:55:33.000675"}, {"id": "https://huggingface.co/posts/AdinaY/578564678362048", "image": "", "title": "MiniMax M2.5 is now available on the hub \ud83d\ude80", "content_text": "MiniMax M2.5 is now available on the hub \ud83d\ude80 MiniMaxAI/MiniMax-M2.5 \u2728 229B - Modified MIT license \u272837% faster than M2.1 \u2728 ~$1/hour at 100 TPS See translation", "url": "https://huggingface.co/posts/AdinaY/578564678362048", "date_published": "2026-02-17T09:55:33.000915"}, {"id": "https://huggingface.co/posts/DavidAU/658014279530502", "image": "", "title": "Gemma 3 (1b, 4b, 12b and 27b) - Uncensored full Reasoning/Thinking models fine tuned using top distill datasets.", "content_text": "Gemma 3 (1b, 4b, 12b and 27b) - Uncensored full Reasoning/Thinking models fine tuned using top distill datasets. 20 Gemma 3 models 1B, 4B, 12B and 27B with full reasoning using GLM 4.7 Flash, GPT, Claude and Gemini datasets and more fully fine tuned using Unsloth. Most models are Heretic'ed (uncensored) first, and tuned second. This vastly improves the model. Models are also bench marked and in almost all cases exceed org model metrics - and in some cases by a lot. Enjoy the freedom and more powerful THINKING/REASONING and UNCENSORED Gemma 3s ! https://huggingface.co/collections/DavidAU/gemma-3-reasoning-thinking-models-incl-uncensored See translation", "url": "https://huggingface.co/posts/DavidAU/658014279530502", "date_published": "2026-02-17T09:55:33.001208"}, {"id": "https://huggingface.co/posts/ajibawa-2023/127700440427437", "image": "", "title": "Java-Code-Large (", "content_text": "Java-Code-Large ( ajibawa-2023/Java-Code-Large ) Java-Code-Large is a large-scale corpus of publicly available Java source code comprising more than 15 million java codes. The dataset is designed to support research in large language model (LLM) pretraining, code intelligence, software engineering automation, and program analysis. By providing a high-volume, language-specific corpus, Java-Code-Large enables systematic experimentation in Java-focused model training, domain adaptation, and downstream code understanding tasks. See translation", "url": "https://huggingface.co/posts/ajibawa-2023/127700440427437", "date_published": "2026-02-17T09:55:33.001474"}, {"id": "https://huggingface.co/posts/prithivMLmods/560324615932995", "image": "", "title": "Dropping the Qwen3 VL Series of Unredacted MAX-VL models. These models have undergone multi-stage training to minimize refusal rates through continuous abliterated optimization. You can find the models in BF16, FP8-Dynamic, and GGUF formats at the links below.\ud83d\udd25\ud83d\ude80", "content_text": "Dropping the Qwen3 VL Series of Unredacted MAX-VL models. These models have undergone multi-stage training to minimize refusal rates through continuous abliterated optimization. You can find the models in BF16, FP8-Dynamic, and GGUF formats at the links below.\ud83d\udd25\ud83d\ude80 Unredacted MAX - VL: \u279c prithivMLmods/Qwen3-VL-4B-Instruct-Unredacted-MAX \u279c prithivMLmods/Qwen3-VL-4B-Thinking-Unredacted-MAX \u279c prithivMLmods/Qwen3-VL-8B-Instruct-Unredacted-MAX \u279c prithivMLmods/Qwen3-VL-8B-Thinking-Unredacted-MAX Unredacted MAX - VL [FP8] \u279c prithivMLmods/Qwen3-VL-4B-Instruct-Unredacted-MAX-FP8 \u279c prithivMLmods/Qwen3-VL-4B-Thinking-Unredacted-MAX-FP8 \u279c prithivMLmods/Qwen3-VL-8B-Instruct-Unredacted-MAX-FP8 \u279c prithivMLmods/Qwen3-VL-8B-Thinking-Unredacted-MAX-FP8 Unredacted MAX - VL [GGUF] \u279c prithivMLmods/Qwen3-VL-4B-Instruct-Unredacted-MAX-GGUF \u279c prithivMLmods/Qwen3-VL-4B-Thinking-Unredacted-MAX-GGUF \u279c prithivMLmods/Qwen3-VL-8B-Instruct-Unredacted-MAX-GGUF \u279c prithivMLmods/Qwen3-VL-8B-Thinking-Unredacted-MAX-GGUF...", "url": "https://huggingface.co/posts/prithivMLmods/560324615932995", "date_published": "2026-02-17T09:55:33.001969"}, {"id": "https://huggingface.co/posts/demirytu/616984337971296", "image": "", "title": "is the chat feature down? recently I can not have success to get responses for my prompts.", "content_text": "is the chat feature down? recently I can not have success to get responses for my prompts. See translation", "url": "https://huggingface.co/posts/demirytu/616984337971296", "date_published": "2026-02-17T09:55:33.002163"}, {"id": "https://huggingface.co/posts/krisbailey/322212397790634", "image": "", "title": "While doing various projects I kept running into situations where I wanted to be able to have representative samples of some of the current large SOTA datasets that were smaller so I didn't need to worry about slicing or anything else at runtime.  So, I created sub datasets making sure to keep the same ratios of data sources.  Each dataset card provides info for what's in it.", "content_text": "While doing various projects I kept running into situations where I wanted to be able to have representative samples of some of the current large SOTA datasets that were smaller so I didn't need to worry about slicing or anything else at runtime. So, I created sub datasets making sure to keep the same ratios of data sources. Each dataset card provides info for what's in it. 100M token datasets: RedPajama v2 100M Falcon RefinedWeb 100M Cosmopedia 100M 1B token datasets: Fineweb-edu 1B RedPajama v1 1B RedPajama v2 1B (use this one) Cosmopedia 1B 10B token datasets: RedPajama v1 10B Cosmopedia 10B Collection here: https://huggingface.co/collections/krisbailey/bite-size-data See translation", "url": "https://huggingface.co/posts/krisbailey/322212397790634", "date_published": "2026-02-17T09:55:33.002459"}]}