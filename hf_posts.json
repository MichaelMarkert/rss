{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Xenova/927328273503233", "image": "", "title": "NEW: Real-time conversational AI models can now run 100% locally in your browser! \ud83e\udd2f", "content_text": "NEW: Real-time conversational AI models can now run 100% locally in your browser! \ud83e\udd2f \ud83d\udd10 Privacy by design (no data leaves your device) \ud83d\udcb0 Completely free... forever \ud83d\udce6 Zero installation required, just visit a website \u26a1\ufe0f Blazingly-fast WebGPU-accelerated inference Try it out: webml-community/conversational-webgpu For those interested, here's how it works: - Silero VAD for voice activity detection - Whisper for speech recognition - SmolLM2-1.7B for text generation - Kokoro for text to speech Powered by Transformers.js and ONNX Runtime Web! \ud83e\udd17 I hope you like it! See translation", "url": "https://huggingface.co/posts/Xenova/927328273503233", "date_published": "2025-06-06T09:26:00.197975"}, {"id": "https://huggingface.co/posts/danaaubakirova/558502564618988", "image": "", "title": "We just dropped SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics!", "content_text": "We just dropped SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics! check out the blog: https://huggingface.co/blog/smolvla read the technical report: SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics (2506.01844) access the model weights: lerobot/smolvla_base See translation", "url": "https://huggingface.co/posts/danaaubakirova/558502564618988", "date_published": "2025-06-06T09:26:00.198242"}, {"id": "https://huggingface.co/posts/MJannik/975422002507458", "image": "", "title": "Hi everyone, we\u2019ve got big news! Starting today, all Langfuse product features are available as free OSS (MIT license).", "content_text": "Hi everyone, we\u2019ve got big news! Starting today, all Langfuse product features are available as free OSS (MIT license). You can now upgrade your self-hosted Langfuse to access features like: - Managed LLM-as-a-Judge evaluations - Annotation queues - Prompt experiments - LLM playground We\u2019re incredibly grateful for the support of this amazing community and can\u2019t wait to hear your feedback on the new features! More on this change here: https://langfuse.com/blog/2025-06-04-open-sourcing-langfuse-product See translation", "url": "https://huggingface.co/posts/MJannik/975422002507458", "date_published": "2025-06-06T09:26:00.198556"}, {"id": "https://huggingface.co/posts/danieldk/385505075920135", "image": "", "title": "We have been working on a project called", "content_text": "We have been working on a project called kernels . kernels makes it possible to load compute kernels directly from the Hub! \ud83d\ude80 We plan to give kernels a more proper introduction soon. But for those who have been following along, we are happy to announce a new release: - New layer API with torch.compile support. - Experimental support for loading Apple Silicon Metal \ud83e\udd18 Kernels. - Generate wheels from Hub kernels for legacy deployments. Full release notes here: https://github.com/huggingface/kernels/releases/tag/v0.6.0 See translation", "url": "https://huggingface.co/posts/danieldk/385505075920135", "date_published": "2025-06-06T09:26:00.198907"}, {"id": "https://huggingface.co/posts/merve/599865137438975", "image": "", "title": "Past week was insanely packed for open AI! \ud83d\ude31", "content_text": "Past week was insanely packed for open AI! \ud83d\ude31 Luckily we picked some highlights for you \u2764\ufe0f lfg! \ud83d\udcac LLMs/VLMs > Deepseek \ud83d\udc33 released deepseek-ai/DeepSeek-R1-0528 , 38B model, only 0.2 and 1.4 points behind o3 in AIME 24/25 \ud83e\udd2f they also released an 8B distilled version based on Qwen3 (OS) deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d > Xiaomi released MiMo-7B-RL (LLM for code and math) and MiMo-VL-7B-RL (VLM for visual reasoning, GUI agentic task and general use) (OS) \ud83d\ude0d XiaomiMiMo/mimo-vl-68382ccacc7c2875500cd212 > NVIDIA released , new reasoning model nvidia/Nemotron-Research-Reasoning-Qwen-1.5B > DS: MiniMax released https://huggingface.co/MiniMaxAI/SynLogic , new 49k logical reasoning examples across 35 tasks including solving cipher, sudoku and more! \ud83d\uddbc\ufe0f Image/Video Generation > tencent released tencent/HunyuanPortrait , a new model for consistent portrait generation with SVD Research license. They also released tencent/HunyuanVideo-Avatar , audio driven avatar generation (OS) >...", "url": "https://huggingface.co/posts/merve/599865137438975", "date_published": "2025-06-06T09:26:00.199499"}, {"id": "https://huggingface.co/posts/merve/820895577634325", "image": "", "title": "Yesterday was the day of vision language action models (VLAs)!", "content_text": "Yesterday was the day of vision language action models (VLAs)! > SmolVLA: open-source small VLA for robotics by Hugging Face LeRobot team \ud83e\udd16 Blog: https://huggingface.co/blog/smolvla Model: lerobot/smolvla_base > Holo-1: 3B & 7B web/computer use agentic VLAs by H Company \ud83d\udcbb Model family: Hcompany/holo1-683dd1eece7eb077b96d0cbd Demo: https://huggingface.co/spaces/multimodalart/Holo1 Blog: https://huggingface.co/blog/Hcompany/holo1 super exciting times!! See translation", "url": "https://huggingface.co/posts/merve/820895577634325", "date_published": "2025-06-06T09:26:00.199815"}, {"id": "https://huggingface.co/posts/merve/361903268457703", "image": "", "title": "Qwen2.5-Omni is soooo good that people build multimodal reasoning models off of it \ud83e\udd79", "content_text": "Qwen2.5-Omni is soooo good that people build multimodal reasoning models off of it \ud83e\udd79 > KE-Team/Ke-Omni-R-3B is open-source audio reasoning model sota on average of benchmarks, based on Qwen/Qwen2.5-Omni-3B \ud83d\udde3\ufe0f > Haoz0206/Omni-R1 is a video reasoning model with pixel level grounding (see below) and it's super competitive \u23ef\ufe0f based on Qwen/Qwen2.5-Omni-7B See translation", "url": "https://huggingface.co/posts/merve/361903268457703", "date_published": "2025-06-06T09:26:00.200088"}, {"id": "https://huggingface.co/posts/AdinaY/854108171347548", "image": "", "title": "OpenAudio S1-mini \ud83d\udd0a a new OPEN multilingual TTS model trained on 2M+ hours of data, by FishAudio", "content_text": "OpenAudio S1-mini \ud83d\udd0a a new OPEN multilingual TTS model trained on 2M+ hours of data, by FishAudio fishaudio/openaudio-s1-mini \u2728 Supports 14 languages \u2728 50+ emotions & tones \u2728 RLHF-optimized \u2728 Special effects: laughing, crying, shouting, etc. See translation", "url": "https://huggingface.co/posts/AdinaY/854108171347548", "date_published": "2025-06-06T09:26:00.200329"}, {"id": "https://huggingface.co/posts/m-ric/239772998134713", "image": "", "title": "If you didn't yet, you should read the technical report for SmolVLA, published yesterday by the Hugging Face robotics team!", "content_text": "If you didn't yet, you should read the technical report for SmolVLA, published yesterday by the Hugging Face robotics team! \u27a1\ufe0f Amongst other ideas, it introduces \"Async inference\" to boost their robot actions. Robots have a problem: performing the actions takes time (Unlike agents where action executions are near-instant!) Most often, robots wait until they've finished performing actions to start thinking about hte next steps. This is a huge latency cost! So the team decided to have the PolicyServer (aka the\"thinking\" part) restart early : instead of waiting for the n observations they just sent to be completed, they gather the observations after k < n steps, and start preparing the next actions based on that while the steps are running until n, to directly send their next steps. \u27a1\ufe0f This boosted robot throughput by ~30%! (nearly 2\u00d7 tasks per time window). gg @ cadene and team! \ud83d\udc4f Report here: SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics (2506.01844)...", "url": "https://huggingface.co/posts/m-ric/239772998134713", "date_published": "2025-06-06T09:26:00.200771"}, {"id": "https://huggingface.co/posts/azettl/543079037838951", "image": "", "title": "Agents & MCP Hackathon Day 2", "content_text": "Agents & MCP Hackathon Day 2 Again, a short night, but here are some updates from my Hackathon projects before starting night #3. I managed to get the first version of both submissions (custom Gradio component and MCP server) online! You can check the roundtable MCP where multiple AIs discuss your question and try to reach consensus: azettl/consilium_mcp . The Gradio component is here: azettl/gradio_consilium_roundtable . I placed my API keys in the env variables, so you can test without needing your own keys, but I will remove them soon as I did not find a limit setting in Sambanova. Still, you can check them by adding your own keys in the config tab. Looking forward to your feedback, there are still many days I can and will improve this. See translation", "url": "https://huggingface.co/posts/azettl/543079037838951", "date_published": "2025-06-06T09:26:00.201100"}]}