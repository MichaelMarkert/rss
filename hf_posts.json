{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/yeonseok-zeticai/752870941871415", "image": "", "title": "\ud83c\udfaf RetinaFace On-Device Deployment Study: NPU Acceleration Breakthrough!", "content_text": "\ud83c\udfaf RetinaFace On-Device Deployment Study: NPU Acceleration Breakthrough! (Check details at :https://mlange.zetic.ai/p/Steve/RetinaFace) TL;DR: Successfully deployed RetinaFace with ZETIC.MLange achieving 1.43ms inference on mobile NPU! \ud83d\udd0d Complete Performance Analysis: Latency Comparison: - NPU: 1.43ms (Winner! \ud83c\udfc6) - GPU: 3.75ms - CPU: 21.42ms Accuracy Metrics - SNR: - FP16: 56.98 dB - Integer Quantized: 48.03 dB (Precision-Performance: Excellent trade-off maintained) Memory Footprint: - Model Size: 2.00 MB (highly compressed) - Runtime Memory: 14.58 MB peak - Deployment Ready: \u2705 Production optimized \ud83d\udee0 Technical Implementation: (Runnable with Copy & Paste at the MLange link!) \ud83d\udcca Device Compatibility Matrix: Tested on 50+ devices including Samsung Galaxy series, Google Pixel lineup, and Xiaomi devices, iPhones and iPads. Consistent sub-5ms performance across the board! \ud83d\ude80 Applications Unlocked: - Real-time AR/VR face tracking - Privacy-preserving edge authentication - Live video...", "url": "https://huggingface.co/posts/yeonseok-zeticai/752870941871415", "date_published": "2025-09-26T09:24:50.486639"}, {"id": "https://huggingface.co/posts/Monica997/874620000877286", "image": "", "title": "AI Just Made My Cat the King of Emojis \ud83d\udc51\ud83d\udc31\ud83d\ude02", "content_text": "AI Just Made My Cat the King of Emojis \ud83d\udc51\ud83d\udc31\ud83d\ude02 Never thought I\u2019d see this \u2014 but with iMini\u2019s nano banana model, my cat is now a full emoji + sticker pack \ud83c\udfa8\u2728 Used the 9-grid meme template + cartoon sticker generator, and in just ONE click \ud83d\udc49 my ordinary cat photo turned into a hilarious, cute, and super shareable set of stickers \ud83d\udcac\ud83d\udd25 No need to master complicated nano banana prompts \u2014 iMini handles everything. Perfect for chats, socials, or just showing off your pet\u2019s new \u201cdigital identity.\u201d \ud83d\udc49 Try it here: https://imini.com/nano-banana Who else wants their pet to be the next emoji star? \ud83c\udf1f See translation", "url": "https://huggingface.co/posts/Monica997/874620000877286", "date_published": "2025-09-26T09:24:50.487012"}, {"id": "https://huggingface.co/posts/sergiopaniego/566597314485869", "image": "", "title": "\ud83d\udca5 Tons of new material just landed in the smol-course! \ud83e\uddd1\u200d\ud83d\udcbb", "content_text": "\ud83d\udca5 Tons of new material just landed in the smol-course! \ud83e\uddd1\u200d\ud83d\udcbb > evaluation > alignment > VLMs > quizzes > assignments! > certificates!\ud83d\udc69\u200d\ud83c\udf93 go learn! \ud83d\udc49 https://huggingface.co/learn/smol-course/unit0/1 See translation", "url": "https://huggingface.co/posts/sergiopaniego/566597314485869", "date_published": "2025-09-26T09:24:50.487251"}, {"id": "https://huggingface.co/posts/nroggendorff/916862110503909", "image": "", "title": "I'm sorry, what?", "content_text": "I'm sorry, what?", "url": "https://huggingface.co/posts/nroggendorff/916862110503909", "date_published": "2025-09-26T09:24:50.487429"}, {"id": "https://huggingface.co/posts/tsungyi/865184416328763", "image": "", "title": "We\u2019re excited to share that Cosmos Reason has surpassed 1 million downloads on Hugging Face!", "content_text": "We\u2019re excited to share that Cosmos Reason has surpassed 1 million downloads on Hugging Face! Cosmos Reason is an open, customizable, commercial-ready 7B-parameter reasoning vision language model (VLM) designed for physical AI. By combining physics understanding, prior knowledge, and common sense reasoning, Cosmos Reason empowers AI agents and robots to operate intelligently in real-world environments. Key applications already unlocked include: \u2705 Automating large-scale dataset curation and annotation \ud83e\udd16 Powering robot planning and vision-language action (VLA) decision-making \ud83d\udcca Driving advanced video analytics and actionable insight generation We\u2019re proud to see a global community of developers using Cosmos Reason to teach robots to think like humans\u2014and we\u2019re just getting started. \u26a1 Get started with Cosmos Reason 1 NIM, an easy-to-use microservice for AI model deployment: https://catalog.ngc.nvidia.com/orgs/nim/teams/nvidia/containers/cosmos-reason1-7b?version=1 \ud83d\udcc8 See the leaderboard:...", "url": "https://huggingface.co/posts/tsungyi/865184416328763", "date_published": "2025-09-26T09:24:50.487865"}, {"id": "https://huggingface.co/posts/hba123/904232449612527", "image": "", "title": "Hello, amazing robotics people \ud83d\ude0d \ud83d\ude0d \ud83d\ude0d We have FINALLY delivered on your major request! Ark just got a major upgrade:", "content_text": "Hello, amazing robotics people \ud83d\ude0d \ud83d\ude0d \ud83d\ude0d We have FINALLY delivered on your major request! Ark just got a major upgrade: We\u2019ve now integrated Vision-Language-Action Models (VLAs) into Ark \ud83c\udf89 VLAs = models that connect vision + language \u2192 robot actions (see image) What does this mean? \ud83d\udde3\ufe0f Give robots natural language instructions \u2192 they act \ud83d\udc40 Combine perception + language for real-world control \ud83e\uddbe Powered by pi0 pretrained models for fast prototyping \u26a1 Supports easy data collection and fine-tuning within Ark within a couple of lines of code Next, we plan to go into the world of designing worlds \ud83d\ude09 Who knows, maybe those video models are actually zero-shot learners and reasoners? Check it out here \ud83d\udc49 https://github.com/Robotics-Ark/ark_framework Check out the tutorial \ud83d\udc49 https://arkrobotics.notion.site/VLA-Pi0-with-Ark-279e053d9c6f800ab0a2d498835dd96b \u2b50 Star the repo, try it with your robots, and let us together make robots great (again?)! See translation", "url": "https://huggingface.co/posts/hba123/904232449612527", "date_published": "2025-09-26T09:24:50.488279"}, {"id": "https://huggingface.co/posts/AnSungJae3489/522189375080147", "image": "", "title": "ShareGPT? How about ShareGPT-X?", "content_text": "ShareGPT? How about ShareGPT-X? We release **92K** Human with LLM conversations as a refresh and update over the original ShareGPT Dataset. DSULT-Core/ShareGPT-X See translation", "url": "https://huggingface.co/posts/AnSungJae3489/522189375080147", "date_published": "2025-09-26T09:24:50.488485"}, {"id": "https://huggingface.co/posts/omarkamali/930832276115275", "image": "", "title": "**Wikipedia Monthly's September edition is now live \ud83c\udf89**", "content_text": "**Wikipedia Monthly's September edition is now live \ud83c\udf89** Highlights of this edition: \u00b7 \ud83d\udde3\ufe0f 341 languages \u00b7 \ud83d\udcda 63.1M articles \u00b7 \ud83d\udce6 86.5GB of data This update also solves upload issues in the August edition where some languages had missing parts. Happy data engineering! omarkamali/wikipedia-monthly See translation", "url": "https://huggingface.co/posts/omarkamali/930832276115275", "date_published": "2025-09-26T09:24:50.488767"}, {"id": "https://huggingface.co/posts/prithivMLmods/355225487543965", "image": "", "title": "Photo-Mate-i2i \u2013 a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages!", "content_text": "Photo-Mate-i2i \u2013 a space for experimenting with adapters for image manipulation using Kontext adapters, including Photo-Restore-i2i, PhotoCleanser-i2i, Polaroid-Warm-i2i, Yarn-Photo-i2i, Monochrome-Pencil, and more. Try out the demo, and to learn more, visit the app page or the respective model pages! \u26a1Demo: prithivMLmods/Photo-Mate-i2i \u2699\ufe0fHow to Use: prithivMLmods/Photo-Mate-i2i#2 \ud83d\udc68\u200d\ud83d\udd27i2i-Kontext(Experimental LoRAs): prithivMLmods/i2i-kontext-exp-68ce573b5c0623476b636ec7 See translation", "url": "https://huggingface.co/posts/prithivMLmods/355225487543965", "date_published": "2025-09-26T09:24:50.489060"}, {"id": "https://huggingface.co/posts/dmoxy/581267174720703", "image": "", "title": "\ud83c\udf1e Our Summer of Workflows finale! New release Embedding Generation for Videos.", "content_text": "\ud83c\udf1e Our Summer of Workflows finale! New release Embedding Generation for Videos. With our latest ApertureDB AI workflow, you can now generate embeddings for video frames & clips and store them directly in a multimodal database\u2014ready for semantic search, RAG, or agentic use cases. \ud83c\udfac See It In Action: https://youtu.be/X2ZXE0EEAkk \ud83d\udd0e Example use cases: Natural language search across video libraries Highlight reel creation & scene retrieval Safer content moderation Lecture indexing + video-to-text alignment Multimodal RAG (text + images + video) \u2728 Check out the demo notebook to see how you can: Import videos from S3 Generate embeddings per frame/clip Query videos with natural language (\u201cshow me a baby\u201d) \ud83d\udc49 Try it here: https://shorturl.at/jSNtu Turn your video library into a fully searchable knowledge base. No scrubbing\u2014just instant, semantic video results. We\u2019d love your feedback and ideas on how you would use it. See translation", "url": "https://huggingface.co/posts/dmoxy/581267174720703", "date_published": "2025-09-26T09:24:50.489472"}]}