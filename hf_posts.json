{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/danielhanchen/824171868881117", "image": "", "title": "Qwen releases Qwen3-Coder-Next! \ud83d\udc9c Run the locally on 46GB RAM or less.", "content_text": "Qwen releases Qwen3-Coder-Next! \ud83d\udc9c Run the locally on 46GB RAM or less. Thhe model excels at agentic coding & local use. With 256K context, it delivers similar performance to models with 10-20\u00d7 more active parameters. GGUF: unsloth/Qwen3-Coder-Next-GGUF Guide: https://unsloth.ai/docs/models/qwen3-coder-next See translation", "url": "https://huggingface.co/posts/danielhanchen/824171868881117", "date_published": "2026-02-05T05:59:12.713149"}, {"id": "https://huggingface.co/posts/alibidaran/992533889532684", "image": "", "title": "I\u2019m excited to share PlaiTO, a reasoning-focused language model built on LLaMA 3.1 (8B) and optimized for humanities and social sciences.", "content_text": "I\u2019m excited to share PlaiTO, a reasoning-focused language model built on LLaMA 3.1 (8B) and optimized for humanities and social sciences. PlaiTO is designed to go beyond surface-level text generation, emphasizing structured reasoning, conceptual clarity, and analytical depth\u2014especially in domains centered on human behavior and social systems. \ud83c\udfaf Focus Areas Psychology Management & Organizational Studies Sociology \ud83d\udcca MMLU Benchmark Results (100 samples per domain) Professional Psychology: 76% Management: 74% Sociology: 75% These results highlight PlaiTO\u2019s strong performance in abstract, theory-heavy, and reasoning-driven tasks. \ud83d\udca1 Why PlaiTO? Strong analytical and reasoning capabilities Better handling of complex human-centered problems Suitable for academic, educational, and research use cases Balanced performance across multiple humanities disciplines PlaiTO is ideal for conceptual analysis, case reasoning, academic discussion, and decision-support scenarios\u2014while still requiring...", "url": "https://huggingface.co/posts/alibidaran/992533889532684", "date_published": "2026-02-05T05:59:12.713703"}, {"id": "https://huggingface.co/posts/MikeDoes/512575404125311", "image": "", "title": "A single lock on a door isn't enough. Real security is about layers.", "content_text": "A single lock on a door isn't enough. Real security is about layers. The same is true for AI privacy. A new paper, \"Whispered Tuning\", offers a fantastic layered solution that aims to fortify LLMs against privacy infringements. We're proud that the first, essential layer, a high-precision PII redaction model was built on the foundation of the Ai4Privacy/pii-65k dataset. Our dataset provided the necessary training material for their initial anonymization step, which then enabled them to develop further innovations like differential privacy fine-tuning and output filtering. This is a win-win: our data helps create a solid base, and researchers build powerful, multi-stage privacy architectures on top of it. Together, we're making AI safer. \ud83d\udd17 Read the full paper to see how a strong foundation enables a complete privacy solution: https://www.scirp.org/journal/paperinformation?paperid=130659 \ud83d\ude80 Stay updated on the latest in privacy-preserving AI\u2014follow us on LinkedIn:...", "url": "https://huggingface.co/posts/MikeDoes/512575404125311", "date_published": "2026-02-05T05:59:12.714197"}, {"id": "https://huggingface.co/posts/prithivMLmods/212829837698801", "image": "", "title": "Introducing the Qwen-Image-Edit-3D-Lighting-Control app, featuring 8\u00d7 horizontal and 3\u00d7 elevational lighting positions for precise 3D lighting control. It enables studio-level lighting using fast Qwen Image Edit fast inference, paired with Multi-Angle-Lighting adapters. \ud83d\udd26", "content_text": "Introducing the Qwen-Image-Edit-3D-Lighting-Control app, featuring 8\u00d7 horizontal and 3\u00d7 elevational lighting positions for precise 3D lighting control. It enables studio-level lighting using fast Qwen Image Edit fast inference, paired with Multi-Angle-Lighting adapters. \ud83d\udd26 \ud83d\udd25 Space: prithivMLmods/Qwen-Image-Edit-3D-Lighting-Control \u2705 Collection: https://huggingface.co/collections/prithivMLmods/image-generation-apps-collection \ud83d\udcc2 GitHub: https://github.com/PRITHIVSAKTHIUR/Qwen-Image-Edit-3D-Lighting-Control See translation", "url": "https://huggingface.co/posts/prithivMLmods/212829837698801", "date_published": "2026-02-05T05:59:12.714533"}, {"id": "https://huggingface.co/posts/AIPreplabs/635199649838795", "image": "", "title": "We\u2019ve all had that moment where we watch a tutorial, nod along, but then realize we can\u2019t actually do it ourselves because watching is just passive. At AIPrep, we are fixing this \"watch and forget\" cycle by building a foundational Generative Explanatory Model (GEM). GEM doesn't just give you a video or a wall of text; it builds an interactive lesson that asks you questions, catches your mistakes in real time, and adapts to your pace. We have just finished preparing our specialized datasets for this interactive logic, and you can already check them out on our profile to see how we are structuring this step-by-step reasoning. Training for the foundational model starts very soon, so stay in touch because something revolutionary is coming to the world of AI education. You can see our progress at aiprep.in.", "content_text": "We\u2019ve all had that moment where we watch a tutorial, nod along, but then realize we can\u2019t actually do it ourselves because watching is just passive. At AIPrep, we are fixing this \"watch and forget\" cycle by building a foundational Generative Explanatory Model (GEM). GEM doesn't just give you a video or a wall of text; it builds an interactive lesson that asks you questions, catches your mistakes in real time, and adapts to your pace. We have just finished preparing our specialized datasets for this interactive logic, and you can already check them out on our profile to see how we are structuring this step-by-step reasoning. Training for the foundational model starts very soon, so stay in touch because something revolutionary is coming to the world of AI education. You can see our progress at aiprep.in. See translation", "url": "https://huggingface.co/posts/AIPreplabs/635199649838795", "date_published": "2026-02-05T05:59:12.714948"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/876855019351468", "image": "", "title": "SECourses Musubi Trainer upgraded to V27 and FLUX 2, FLUX Klein, Z-Image training added with demo configs - amazing VRAM optimized - read the news", "content_text": "SECourses Musubi Trainer upgraded to V27 and FLUX 2, FLUX Klein, Z-Image training added with demo configs - amazing VRAM optimized - read the news App is here : https://www.patreon.com/posts/137551634 Full tutorial how to use and train : https://youtu.be/DPX3eBTuO_Y See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/876855019351468", "date_published": "2026-02-05T05:59:12.715213"}, {"id": "https://huggingface.co/posts/rajkumarrawal/904260944141642", "image": "", "title": "I submitted a \"FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning\" Paper by Tanyu Chen, Tairan Chen, Kai shen , Zhenghua Bao, Zhihui Zhang, Man Yuan, Yi Shi From", "content_text": "I submitted a \"FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning\" Paper by Tanyu Chen, Tairan Chen, Kai shen , Zhenghua Bao, Zhihui Zhang, Man Yuan, Yi Shi From FlashLabs to Daily Papers on huggingface . Chroma 1.0 enables real time spoken dialogue with personalized voice cloning through discrete speech representations and interleaved text audio token scheduling. Chroma 1.0 , the world\u2019s first open source, real time speech to speech model with voice cloning. FlashLabs Chroma 1.0: A Real-Time End-to-End Spoken Dialogue Model with Personalized Voice Cloning (2601.11141) See translation", "url": "https://huggingface.co/posts/rajkumarrawal/904260944141642", "date_published": "2026-02-05T05:59:12.715578"}, {"id": "https://huggingface.co/posts/AdinaY/629082711714950", "image": "", "title": "AI for science is moving fast\ud83d\ude80", "content_text": "AI for science is moving fast\ud83d\ude80 Intern-S1-Pro \ud83d\udd2c a MoE multimodal scientific reasoning model from Shanghai AI Lab internlm/Intern-S1-Pro \u2728 1T total / 22B active \u2728 Apache 2.0 \u2728 SoTA scientific reasoning performance \u2728 FoPE enables scalable modeling of long physical time series (10\u2070\u201310\u2076) See translation", "url": "https://huggingface.co/posts/AdinaY/629082711714950", "date_published": "2026-02-05T05:59:12.715869"}, {"id": "https://huggingface.co/posts/Javedalam/705319100384927", "image": "", "title": "GLM-OCR: A Tiny 0.9B-Parameter Model That Punches Far Above Its Weight", "content_text": "GLM-OCR: A Tiny 0.9B-Parameter Model That Punches Far Above Its Weight Released today by Z.ai, GLM-OCR is a compact vision-language model designed specifically for document understanding. At just 0.9 billion parameters, it belongs to a new generation of lightweight AI systems proving that raw model size is no longer the only path to high performance. Despite its small footprint, GLM-OCR posts exceptionally strong results across major document benchmarks. It scores 94.6 on OmniDocBench, 94.0 on OCRBench, and an impressive 96.5 on UniMERNet for formula recognition\u2014numbers that place it alongside, and in some cases ahead of, significantly larger specialized OCR models. The takeaway is clear: efficiency is rapidly becoming a defining feature of modern AI design. Developed by Z.ai, a research group focused on advancing multimodal foundation models, GLM-OCR reflects a broader shift toward highly optimized architectures that deliver serious capability without requiring massive compute...", "url": "https://huggingface.co/posts/Javedalam/705319100384927", "date_published": "2026-02-05T05:59:12.716485"}, {"id": "https://huggingface.co/posts/imnotkitty/790273915312125", "image": "", "title": "The 2025 Chinese LLM Showdown: Western Models Still Dominate Top 4, but China Leads the Open-Source Arena.", "content_text": "The 2025 Chinese LLM Showdown: Western Models Still Dominate Top 4, but China Leads the Open-Source Arena. \ud83c\udfc6 The Champions: Claude-Opus-4.5, Gemini-3-Pro, GPT-5.2, and Gemini-3-Flash sweep the top four spots. \ud83d\ude80 The Pursuers: Doubao and DeepSeek-V3.2 tie for first place among Chinese models; GLM-4.7, ERNIE-5.0, and Kimi secure their positions in the domestic top five. \ud83d\udd25 The Biggest Highlight: The top three spots on the open-source leaderboard are entirely held by Team China (DeepSeek, GLM, Kimi), outperforming the best western open-source models. See translation", "url": "https://huggingface.co/posts/imnotkitty/790273915312125", "date_published": "2026-02-05T05:59:12.716813"}]}