{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/csabakecskemeti/762115035937109", "image": "", "title": "Has anyone ever backed up a model to a sequential tape drive, or I'm the world first? :D", "content_text": "Has anyone ever backed up a model to a sequential tape drive, or I'm the world first? :D Just played around with my retro PC that has got a tape drive\u2014did it just because I can. See translation", "url": "https://huggingface.co/posts/csabakecskemeti/762115035937109", "date_published": "2025-06-11T17:21:24.287593"}, {"id": "https://huggingface.co/posts/zamal/148346638153657", "image": "", "title": "\ud83d\ude80 Videoxity is live on Hugging Face! \ud83c\udf9e\ufe0f", "content_text": "\ud83d\ude80 Videoxity is live on Hugging Face! \ud83c\udf9e\ufe0f A powerful, modular toolkit for intelligent video manipulation and scene editing. With Videoxity, you can: \ud83d\uddbc\ufe0f Auto-caption keyframes with BLIP \ud83e\udde0 Filter scenes using natural language (e.g. \u201cremove dog scenes\u201d) \u2702\ufe0f Seamlessly trim videos with FFmpeg \ud83d\udcca Generate frame-based summaries Powered by Groq LLM + LangChain, OpenCV, BLIP, and SentenceTransformers, Videoxity bridges vision and language to give developers full control over video content. \ud83d\udd27 Built for developers. Feedback welcome! \ud83d\udc49 Try it out here fau/videoxity See translation", "url": "https://huggingface.co/posts/zamal/148346638153657", "date_published": "2025-06-11T17:21:24.287939"}, {"id": "https://huggingface.co/posts/cbensimon/565026286160860", "image": "", "title": "\ud83d\ude80 ZeroGPU now supports PyTorch native quantization via", "content_text": "\ud83d\ude80 ZeroGPU now supports PyTorch native quantization via torchao While it hasn\u2019t been battle-tested yet, Int8WeightOnlyConfig is already working flawlessly in our tests. Let us know if you run into any issues \u2014 and we\u2019re excited to see what the community will build! import spaces from diffusers import FluxPipeline from torchao.quantization.quant_api import Int8WeightOnlyConfig, quantize_ pipeline = FluxPipeline.from_pretrained(...).to( 'cuda' ) quantize_(pipeline.transformer, Int8WeightOnlyConfig()) # Or any other component(s) @spaces.GPU def generate ( prompt: str ): return pipeline(prompt).images[ 0 ] See translation", "url": "https://huggingface.co/posts/cbensimon/565026286160860", "date_published": "2025-06-11T17:21:24.288284"}, {"id": "https://huggingface.co/posts/Kseniase/795992300839975", "image": "", "title": "12 Foundational AI Model Types", "content_text": "12 Foundational AI Model Types Let\u2019s refresh some fundamentals today to stay fluent in the what we all work with. Here are some of the most popular model types that shape the vast world of AI (with examples in the brackets): 1. LLM - Large Language Model (GPT, LLaMA) -> Large Language Models: A Survey (2402.06196) + history of LLMs: https://www.turingpost.com/t/The%20History%20of%20LLMs It's trained on massive text datasets to understand and generate human language. They are mostly build on Transformer architecture, predicting the next token. LLMs scale by increasing overall parameter count across all components (layers, attention heads, MLPs, etc.) 2. SLM - Small Language Model (TinyLLaMA, Phi models, SmolLM) A Survey of Small Language Models (2410.20011) Lightweight LM optimized for efficiency, low memory use, fast inference, and edge use. SLMs work using the same principles as LLMs 3. VLM - Vision-Language Model (CLIP, Flamingo) -> An Introduction to Vision-Language Modeling...", "url": "https://huggingface.co/posts/Kseniase/795992300839975", "date_published": "2025-06-11T17:21:24.288948"}, {"id": "https://huggingface.co/posts/drwlf/878228510592624", "image": "", "title": "Having an insanely good medical LLM is pointless if it won\u2019t answer your questions!", "content_text": "Having an insanely good medical LLM is pointless if it won\u2019t answer your questions! So we\u2019ve made 2 notebook for abliterating any model in order to achieve a good model that will actually help you! The notebooks are made using @ mlabonne \u2018s abliteration logic and datasets! Feel free to use them and happy training \ud83d\ude0a https://github.com/dralexlup/LLM-Abliteration See translation", "url": "https://huggingface.co/posts/drwlf/878228510592624", "date_published": "2025-06-11T17:21:24.289226"}, {"id": "https://huggingface.co/posts/jasoncorkill/871941197791232", "image": "", "title": "Imagine you could have an Image Arena score equivalent at each checkpoint during training. We released the first version of just that:", "content_text": "Imagine you could have an Image Arena score equivalent at each checkpoint during training. We released the first version of just that: Crowd-Eval Add one line of code to your training loop and you will have a new real human loss curve in your W&B dashboard. Thousands of real humans from around the world rating your model in real time at the cost of a few dollars per checkpoint is a game changer. Check it out here: https://github.com/RapidataAI/crowd-eval First 5 people to put it in their loop get 100'000 human responses for free! (ping me) See translation", "url": "https://huggingface.co/posts/jasoncorkill/871941197791232", "date_published": "2025-06-11T17:21:24.289527"}, {"id": "https://huggingface.co/posts/dvilasuero/324662497616161", "image": "", "title": "Super excited to launch Hugging Face Sheets: Spreadsheets meet AI and unstructured data.", "content_text": "Super excited to launch Hugging Face Sheets: Spreadsheets meet AI and unstructured data. A few months ago, we started imagining new ways to build and transform datasets with the latest open-source models. Today, I'm thrilled to introduce our first step in this direction. In a nutshell: \ud83d\udcc1 Effortlessly run prompts and models over your data. \ud83c\udf10 Agentic search for accuracy and real-time information. \ud83d\uddbc\ufe0f Familiar, minimalistic interface for interacting with data. \ud83c\udfaf Human feedback 2.0: Your input directly improves generated data. \ud83d\udcaf Access hundreds of open models and leading inference providers. Go to this space to try it out! aisheets/sheets Leave your questions below, we're just getting started! See translation", "url": "https://huggingface.co/posts/dvilasuero/324662497616161", "date_published": "2025-06-11T17:21:24.289886"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/946731855039439", "image": "", "title": "As part of Duality AI\u2019s recent Kaggle competition, we\u2019ve released a free, fully customizable cloud scenario designed to help you create targeted datasets with YOLO-compatible labels.", "content_text": "As part of Duality AI\u2019s recent Kaggle competition, we\u2019ve released a free, fully customizable cloud scenario designed to help you create targeted datasets with YOLO-compatible labels. The cloud simulation lets you customize the: \ud83d\udcf8 camera distance \ud83c\udf9e\ufe0f film grain variation \ud83d\uddbc\ufe0fbackground objects, \u2795 and more! Create the dataset that you need by following this link: https://falcon.duality.ai/secure/scenarios/edit/cca0bc47-265a-4f67-843f-a434b63271b3?utm_source=huggingface&utm_medium=social&utm_campaign=general I\u2019ve attached an instructional video we used for the competition, but this feature is free for anyone who has an account. https://vimeo.com/1091271731?share=copy See translation", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/946731855039439", "date_published": "2025-06-11T17:21:24.290223"}, {"id": "https://huggingface.co/posts/AdinaY/444223242188874", "image": "", "title": "RoboBrain 2.0\ud83d\udd25 OPEN embedded brain model by BAAIBeijing", "content_text": "RoboBrain 2.0\ud83d\udd25 OPEN embedded brain model by BAAIBeijing BAAI/RoboBrain2.0-7B \u2728 7B - Apache 2.0 / 32B coming soon \u2728 Supports multiple images, long videos, and high-resolution visuals \u2728 Spatial + temporal reasoning \u2728 Real-time memory & scene graphs See translation", "url": "https://huggingface.co/posts/AdinaY/444223242188874", "date_published": "2025-06-11T17:21:24.291121"}, {"id": "https://huggingface.co/posts/fdaudens/888164728339934", "image": "", "title": "MCP just hit a tipping point:", "content_text": "MCP just hit a tipping point: - @ hf .co made it dead simple: just type \"hf.co/mcp\" in your chat. No JSON wrestling, no config files. - Meanwhile, OpenAI, Google, and Microsoft all adopted it as their standard. https://huggingface.co/blog/fdaudens/mcp-ai-industry-standard See translation", "url": "https://huggingface.co/posts/fdaudens/888164728339934", "date_published": "2025-06-11T17:21:24.291348"}]}