{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/anakin87/201202681111752", "image": "", "title": "LLMs can leak their post-training data (RL included) \ud83d\udca7", "content_text": "LLMs can leak their post-training data (RL included) \ud83d\udca7 New interesting paper on this topic from Google DeepMind: Extracting alignment data in open models (2510.18554) It's known that Language Models memorize data that can be extracted via prompting. In this paper, the authors investigate this aspect: - using open models, where prompting can be fully customized by the user, including special tokens. - focusing on open-source models like Olmo, where full training data is available. \ud83d\udce4 How do they extract data? During post-training (like SFT), new tokens such as <|user|> are introduced. The authors hypothesize prompting the model with these tokens can make it output its alignment data (remember Magpie?). For example, for SFT, their extraction prompt is <|endoftext|><|user|>. \ud83d\udccf Evaluating memorization The authors compare each sampled example with the original data using vector search with embedding similarity. They find that many outputs are semantically very similar to the original...", "url": "https://huggingface.co/posts/anakin87/201202681111752", "date_published": "2025-11-15T05:22:34.765942"}, {"id": "https://huggingface.co/posts/ronantakizawa/435117440357729", "image": "", "title": "Reached 1000+ total downloads across my models and datasets! \ud83c\udf89", "content_text": "Reached 1000+ total downloads across my models and datasets! \ud83c\udf89 Follow me for more @ ronantakizawa See translation", "url": "https://huggingface.co/posts/ronantakizawa/435117440357729", "date_published": "2025-11-15T05:22:34.766174"}, {"id": "https://huggingface.co/posts/evalstate/865812476358807", "image": "", "title": "Hugging Face MCP Server v0.2.45", "content_text": "Hugging Face MCP Server v0.2.45 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ - New! Experimental dynamic_space tool. - Default Image Generator changed to Qwen-Image-Fast See translation", "url": "https://huggingface.co/posts/evalstate/865812476358807", "date_published": "2025-11-15T05:22:34.766382"}, {"id": "https://huggingface.co/posts/alibidaran/312503029386627", "image": "", "title": "This shared notebook comprises the MMLU benchmark evaluating task for my latest reasoning model for the sociology field. The results show that using Few-shot prompting in the system prompt can significantly improve the model's performance at answering questions.", "content_text": "This shared notebook comprises the MMLU benchmark evaluating task for my latest reasoning model for the sociology field. The results show that using Few-shot prompting in the system prompt can significantly improve the model's performance at answering questions. Model's link: alibidaran/GRPO_LLAMA3-instructive_reasoning1 Notebook evaluation: https://www.kaggle.com/code/alibidaran/mmlu-socialogy-thinking-evals?scriptVersionId=277240033 See translation", "url": "https://huggingface.co/posts/alibidaran/312503029386627", "date_published": "2025-11-15T05:22:34.766628"}, {"id": "https://huggingface.co/posts/prithivMLmods/809040430953807", "image": "", "title": "Try the all-new trending Qwen-Image-Edit specialized adapter demos, including Photo-to-Anime, Light Restoration, Multi-Angle Edits, Relighting, and more \u2014 all in a single Hugging Face Space. Below is the demo link. \ud83e\udd17\ud83c\udf20", "content_text": "Try the all-new trending Qwen-Image-Edit specialized adapter demos, including Photo-to-Anime, Light Restoration, Multi-Angle Edits, Relighting, and more \u2014 all in a single Hugging Face Space. Below is the demo link. \ud83e\udd17\ud83c\udf20 \u2b9e Demo-Space: prithivMLmods/Qwen-Image-Edit-2509-LoRAs-Fast \u2b9e How-to-Use: prithivMLmods/Qwen-Image-Edit-2509-LoRAs-Fast#2 \u2b9e Collection: https://huggingface.co/collections/prithivMLmods/image-generation-apps-collection To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/809040430953807", "date_published": "2025-11-15T05:22:34.766959"}, {"id": "https://huggingface.co/posts/onekq/566639652632782", "image": "", "title": "The reaction on the QAT post is beyond expectations so below is my optimizer post as promised. But I found that I had lots of explanation to do about optimizer itself. So this post is actually a historical recount. The Muon optimizer  (used by Kimi) post (coming very soon) can only continue after this.", "content_text": "The reaction on the QAT post is beyond expectations so below is my optimizer post as promised. But I found that I had lots of explanation to do about optimizer itself. So this post is actually a historical recount. The Muon optimizer (used by Kimi) post (coming very soon) can only continue after this. https://huggingface.co/blog/onekq/adam-optimizer If you know Adam(W) optimizer already, you can just skip and sorry for the wait. Otherwise, it should be a useful read. See translation", "url": "https://huggingface.co/posts/onekq/566639652632782", "date_published": "2025-11-15T05:22:34.767228"}, {"id": "https://huggingface.co/posts/ronantakizawa/135864739980663", "image": "", "title": "Introducing the Japanese honorifics dataset: a dataset with 137 sentences covering the three main keigo forms: \u5c0a\u656c\u8a9e (Sonkeigo), \u8b19\u8b72\u8a9e (Kenj\u014dgo), and \u4e01\u5be7\u8a9e (Teineigo). Each entry includes the base form, all three honorific transformations, and English translations for essential phrases in Japanese. This dataset is perfect for training and evaluating the Japanese skill level of LLMs.", "content_text": "Introducing the Japanese honorifics dataset: a dataset with 137 sentences covering the three main keigo forms: \u5c0a\u656c\u8a9e (Sonkeigo), \u8b19\u8b72\u8a9e (Kenj\u014dgo), and \u4e01\u5be7\u8a9e (Teineigo). Each entry includes the base form, all three honorific transformations, and English translations for essential phrases in Japanese. This dataset is perfect for training and evaluating the Japanese skill level of LLMs. #japanese #japanesedataset ronantakizawa/japanese-honorifics See translation", "url": "https://huggingface.co/posts/ronantakizawa/135864739980663", "date_published": "2025-11-15T05:22:34.767516"}, {"id": "https://huggingface.co/posts/fd3ffff/556790752142004", "image": "", "title": "The methods for generating these interesting AI images:", "content_text": "The methods for generating these interesting AI images: 1. Open https://imini.com/ 2. Enter the instruction word. 3. Click \"Generate Now\". 4. Download, save or share on social media platforms. See translation", "url": "https://huggingface.co/posts/fd3ffff/556790752142004", "date_published": "2025-11-15T05:22:34.767747"}, {"id": "https://huggingface.co/posts/wang12390/240552732492093", "image": "", "title": "The Art of Speed Painting: Mastering Creativity Under Time Pressure", "content_text": "The Art of Speed Painting: Mastering Creativity Under Time Pressure Speed painting is an artistic technique where the artist has a limited time to finish the work. The time can vary, usually a duration is set from several minutes to a few hours. Unlike sketches, speed paintings may be considered \"finished\" after the time limit is up. https://miragic.ai/products/speed-painting See translation", "url": "https://huggingface.co/posts/wang12390/240552732492093", "date_published": "2025-11-15T05:22:34.768000"}, {"id": "https://huggingface.co/posts/unmodeled-tyler/132208004167060", "image": "", "title": "NEW RESEARCH MODEL:", "content_text": "NEW RESEARCH MODEL: vanta-research/atom-v1-preview-4b We are excited to share our Atom V1 4B Preview model! This fine tuned Gemma3 4B variant has a distinct, friendly, and exploratory persona - designed to help the user think and reflect. Atom is trained to ask questions, use approachable, yet relatable analogies in ELI5-style explanations, and engage in deep, reflective conversation. We plan to scale Atom's persona to larger architectures, and this iteration was created as part of that R&D. Any and all feedback is always welcome as we continue to refine our approach. See translation", "url": "https://huggingface.co/posts/unmodeled-tyler/132208004167060", "date_published": "2025-11-15T05:22:34.768287"}]}