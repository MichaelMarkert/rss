{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/YerbaPage/558970453952386", "image": "", "title": "How to compress long code context? \ud83d\udcda", "content_text": "How to compress long code context? \ud83d\udcda Check out our LongCodeZip! Paper just got accepted to ASE 2025. \ud83d\udd25 Code: https://github.com/YerbaPage/LongCodeZip Paper: LongCodeZip: Compress Long Context for Code Language Models (2510.00446) See translation", "url": "https://huggingface.co/posts/YerbaPage/558970453952386", "date_published": "2025-10-04T09:20:09.244648"}, {"id": "https://huggingface.co/posts/prithivMLmods/704561076669428", "image": "", "title": "Try the Hugging Face Space demo for", "content_text": "Try the Hugging Face Space demo for Logics-MLLM/Logics-Parsing , the latest multimodal VLM from the Logics Team at Alibaba Group. It enables end-to-end document parsing with precise content extraction in markdown format, and it also generates a clean HTML representation of the document while preserving its logical structure. \ud83e\udd17\ud83d\udd25 Additionally, I\u2019ve integrated one of my recent works \u2014 prithivMLmods/Gliese-OCR-7B-Post1.0 \u2014 which also excels at document comprehension. \u2b50 Space / App : prithivMLmods/Logics-Parsing-VLM \ud83d\udcc4 Technical Report by the Logics Team, Alibaba Group : Logics-Parsing Technical Report (2509.19760) \u26a1 Collections : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Other Pages: \u2794 Multimodal VLMs - July'25 : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 \u2794 Multimodal VLMs - Aug'25 : prithivMLmods/multimodal-vlms-aug25-68a56aac39fe8084f3c168bd \u2794 VL caption \u2014 < Sep 15 \u201925 : prithivMLmods/vl-caption-sep-15-25-68c7f6d737985c63c13e2391 . . ....", "url": "https://huggingface.co/posts/prithivMLmods/704561076669428", "date_published": "2025-10-04T09:20:09.245179"}, {"id": "https://huggingface.co/posts/Sri-Vigneshwar-DJ/952078344859787", "image": "", "title": "\ud83d\ude80 Exciting News! We've released a Performance Marketing Expert Dataset from Hawky.ai [www.hawky.ai]", "content_text": "\ud83d\ude80 Exciting News! We've released a Performance Marketing Expert Dataset from Hawky.ai [www.hawky.ai] Hawky-ai This dataset empowers AI models with cutting-edge strategies for Meta, Google Ads, and TikTok campaigns. It includes: 1. Multi-platform strategies for e-commerce, DTC, B2B, and more 2. Creative optimization and audience targeting insights 3. ROI-driven recommendations based on 2025 best practices Sri-Vigneshwar-DJ/Performance-Marketing-Data See translation", "url": "https://huggingface.co/posts/Sri-Vigneshwar-DJ/952078344859787", "date_published": "2025-10-04T09:20:09.245493"}, {"id": "https://huggingface.co/posts/SelmaNajih001/721687692996128", "image": "", "title": "Introducing", "content_text": "Introducing SelmaNajih001/StockPredictionExplanation , built with GRPO and RAG: -GRPO trains the model to predict and explain stock direction. -RAG grounds explanations in historical financial news and central bank speeches. Together, they create a system that forecasts stock movements and shows the reasoning behind them. Full article: Explainable Financial Predictions \u2014 https://huggingface.co/blog/SelmaNajih001/explainable-financial-predictions Try it here: StockPredictionExplanation Space \u2014 SelmaNajih001/StockPredictionExplanation See translation", "url": "https://huggingface.co/posts/SelmaNajih001/721687692996128", "date_published": "2025-10-04T09:20:09.245825"}, {"id": "https://huggingface.co/posts/Parveshiiii/228189451590505", "image": "", "title": "\ud83d\ude80 Big news from XenArcAI!", "content_text": "\ud83d\ude80 Big news from XenArcAI! We\u2019ve just released our new dataset: **Bhagwat\u2011Gita\u2011Infinity** \ud83c\udf38\ud83d\udcd6 \u2728 What\u2019s inside: - Verse\u2011aligned Sanskrit, Hindi, and English - Clean, structured, and ready for ML/AI projects - Perfect for research, education, and open\u2011source exploration \ud83d\udd17 Hugging Face: XenArcAI/Bhagwat-Gita-Infinity Let\u2019s bring timeless wisdom into modern AI together \ud83d\ude4c See translation", "url": "https://huggingface.co/posts/Parveshiiii/228189451590505", "date_published": "2025-10-04T09:20:09.246107"}, {"id": "https://huggingface.co/posts/Nymbo/667159047100186", "image": "", "title": "I have a few Sora-2 invites - 15509N", "content_text": "I have a few Sora-2 invites - 15509N See translation", "url": "https://huggingface.co/posts/Nymbo/667159047100186", "date_published": "2025-10-04T09:20:09.246291"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/329376915558167", "image": "", "title": "Ovi - Generate Videos With Audio Like VEO 3 or SORA 2 - Run Locally - Open Source for Free", "content_text": "Ovi - Generate Videos With Audio Like VEO 3 or SORA 2 - Run Locally - Open Source for Free Download and install : https://www.patreon.com/posts/140393220 Quick demo tutorial : https://youtu.be/uE0QabiHmRw Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation Project page : https://aaxwaz.github.io/Ovi/ SECourses Ovi Pro Premium App Features Full scale ultra advanced app for Ovi - an open source project that can generate videos from both text prompts and image + text prompts with real audio. Project page is here : https://aaxwaz.github.io/Ovi/ I have developed an ultra advanced Gradio app and much better pipeline that fully supports block swapping Now we can generate full quality videos with as low as 8.2 GB VRAM Hopefully I will work on dynamic on load FP8_Scaled tomorrow to improve VRAM even further So more VRAM optimizations will come hopefully tomorrow Our implemented block swapping is the very best one out there - I took the approach from famous Kohya Musubi tuner The...", "url": "https://huggingface.co/posts/MonsterMMORPG/329376915558167", "date_published": "2025-10-04T09:20:09.246781"}, {"id": "https://huggingface.co/posts/kanaria007/107464223968156", "image": "", "title": "\u2705 New Article: *Cosmic Exploration PoC \u2014 Part 4*", "content_text": "\u2705 New Article: *Cosmic Exploration PoC \u2014 Part 4* Title: \ud83d\ude80 PoC for Space Exploration with Structured Intelligence Computers (SIC) \u2014 Long-Horizon Starship Simulation \ud83d\udd17 https://huggingface.co/blog/kanaria007/cosmic-exploration-starship --- Summary: Exploration across stars is not a question of distance \u2014 it is a question of *time*. This PoC tests whether mission identity, ethics, and governance can endure across centuries of generational drift and uncertainty. A starship becomes more than transport: it is a *self-sustaining arc of civilization*. > The challenge is not propulsion. > *It is continuity of mind and mission.* --- Why It Matters: \u2022 Demonstrates structural continuity over simulated 500-year journeys \u2022 Validates conflict mediation and governance stability without collapse \u2022 Proves that ethics and identity can persist across generations --- What\u2019s Inside: \u2022 Starship OS integrating resilience, ethics, and memory loops \u2022 Simulation of generational conflict, scarcity events, value...", "url": "https://huggingface.co/posts/kanaria007/107464223968156", "date_published": "2025-10-04T09:20:09.247349"}, {"id": "https://huggingface.co/posts/ZennyKenny/911264518414164", "image": "", "title": "\ud83e\udd4a  Big Code Arena is live!", "content_text": "\ud83e\udd4a Big Code Arena is live! bigcode/arena \ud83d\udca1 bigcode is an open scientific collaboration working on responsible training of large language models for coding applications. \ud83d\udc49 The Arena ranks LLMs based on their ability to support natural language vibe coding requests in a competitive format, based on feedback from human reviewers. \ud83e\udde0 It was a pleasure to contribute to this project led by @ terryyz and appear as an additional contributor in the Big Code Arena paper. See translation", "url": "https://huggingface.co/posts/ZennyKenny/911264518414164", "date_published": "2025-10-04T09:20:09.247644"}, {"id": "https://huggingface.co/posts/onekq/612802221121002", "image": "", "title": "WebApp1K measures an oldest and simplest kind of task predated ChatGPT. It is code completion, you can also consider it a translation task mapping test spec into code. It requires no conversation,  reasoning (which helps sometimes), or RL.", "content_text": "WebApp1K measures an oldest and simplest kind of task predated ChatGPT. It is code completion, you can also consider it a translation task mapping test spec into code. It requires no conversation, reasoning (which helps sometimes), or RL. I don't think it is on the roadmap of top labs. Otherwise, you can't explain why Claude 4 has the same 70+ score on SweBench, which is way more challenging than this benchmark. Neither do I encourage model builders to optimize towards my benchmark, which in itself won't be too hard to top the leaderboard. I just argue that we're still in a very early phase. What I witness now is still the same pattern: the dropping of generic models strategically optimized towards famous benchmarks. Meanwhile, agent builders (top labs and startups alike) painfully prompt these models to follow their expectations, and pray they won't drift overnight. See translation", "url": "https://huggingface.co/posts/onekq/612802221121002", "date_published": "2025-10-04T09:20:09.248030"}]}