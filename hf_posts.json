{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Molbap/149398104991391", "image": "", "title": "\ud83d\ude80 New blog: Maintain the unmaintainable \u2013 1M+ Python LOC, 400+ models", "content_text": "\ud83d\ude80 New blog: Maintain the unmaintainable \u2013 1M+ Python LOC, 400+ models How do you stop a million-line library built by thousands of contributors from collapsing under its own weight? At \ud83e\udd17 Transformers, we do it with explicit software-engineering tenets, principles that make the codebase hackable at scale. \ud83d\udd0d Inside the post: \u2013 One Model, One File: readability first \u2014 you can still open a modeling file and see the full logic, top to bottom. \u2013 Modular Transformers: visible inheritance that cuts maintenance cost by ~15\u00d7 while keeping models readable. \u2013 Config-Driven Performance: FlashAttention, tensor parallelism, and attention scheduling are config-level features, not rewrites. Written with @ lysandre ,@pcuenq and @ yonigozlan , this is a deep dive into how Transformers stays fast, open, and maintainable. Read it here \u2192 transformers-community/Transformers-tenets See translation", "url": "https://huggingface.co/posts/Molbap/149398104991391", "date_published": "2025-10-08T17:20:15.870131"}, {"id": "https://huggingface.co/posts/SelmaNajih001/201829706503917", "image": "", "title": "Finally, I uploaded the model I developed for my master\u2019s thesis! Given a financial event, it provides explained predictions based on a dataset of past news and central bank speeches.", "content_text": "Finally, I uploaded the model I developed for my master\u2019s thesis! Given a financial event, it provides explained predictions based on a dataset of past news and central bank speeches. Try it out here: SelmaNajih001/StockPredictionExplanation (Just restart the space and wait a minute) The dataset used for RAG can be found here: SelmaNajih001/FinancialNewsAndCentralBanksSpeeches-Summary-Rag While the dataset used for the training is: SelmaNajih001/FinancialClassification I also wrote an article to explain how I've done the training. You can find it here https://huggingface.co/blog/SelmaNajih001/explainable-financial-predictions See translation", "url": "https://huggingface.co/posts/SelmaNajih001/201829706503917", "date_published": "2025-10-08T17:20:15.870498"}, {"id": "https://huggingface.co/posts/prithivMLmods/886559259732378", "image": "", "title": "Have built the new Image Studio with the Gemini Image Gen models for the following multiple tasks:", "content_text": "Have built the new Image Studio with the Gemini Image Gen models for the following multiple tasks: imagen-4.0-fast-generate-001 model for Image Generation (Text-to-Image) and Multi-Image Editing (Image-to-Image), and Draw-to-Image powered by gemini-2.5-flash-image (aka Nano Banana). \u2b50 Gemini-Image-Studio: prithivMLmods/Gemini-Image-Studio (Latest) \ud83e\udd1e Old-App: prithivMLmods/Nano-Banana-AIO \ud83e\udd4a GitHub: https://github.com/prithivsakthiur/gemini-image-studio-hf To proceed, you need to add your Gemini API key. Your API key is stored only for the duration of your session and will be lost when you reload or exit the page. It will not be shared or exposed anywhere. See translation", "url": "https://huggingface.co/posts/prithivMLmods/886559259732378", "date_published": "2025-10-08T17:20:15.870851"}, {"id": "https://huggingface.co/posts/AdamF92/854077189361039", "image": "", "title": "Hi, I just published research paper that's introducing my Reactive Transformer (RxT) architecture. I would be grateful if you could check it and upvote on HuggingFace Daily Papers -", "content_text": "Hi, I just published research paper that's introducing my Reactive Transformer (RxT) architecture. I would be grateful if you could check it and upvote on HuggingFace Daily Papers - Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models (2510.03561) Architecture is based on stateful real-time processing with innovational asynchronous memory update. Instead of reprocessing all the conversation history for each message, it's processing only single query with all the context moved to dedicated memory layers. Memory is updated after generating the answer, so it's not influencing latency - in tests, time to first token was almost the same as generating a single token. It has also better quality/accuracy in multi-turn dialogue than the same size stateless decoder-only model. Initial experiments were small scale (12M to 160M params models trained on simple synthetic datasets), but just now I'm starting training of bigger 270M params model on...", "url": "https://huggingface.co/posts/AdamF92/854077189361039", "date_published": "2025-10-08T17:20:15.871243"}, {"id": "https://huggingface.co/posts/evijit/733521758778057", "image": "", "title": "AI for Scientific Discovery Won't Work Without Fixing How We Collaborate.", "content_text": "AI for Scientific Discovery Won't Work Without Fixing How We Collaborate. My co-author @ cgeorgiaw and I just published a paper challenging a core assumption: that the main barriers to AI in science are technical. They're not. They're social. Key findings: \ud83d\udea8 The \"AI Scientist\" myth delays progress: Waiting for AGI devalues human expertise and obscures science's real purpose: cultivating understanding, not just outputs. \ud83d\udcca Wrong incentives: Datasets have 100x longer impact than models, yet data curation is undervalued. \u26a0\ufe0f Broken collaboration: Domain scientists want understanding. ML researchers optimize performance. Without shared language, projects fail. \ud83d\udd0d Fragmentation costs years: Harmonizing just 9 cancer files took 329 hours. Why this matters: Upstream bottlenecks like efficient PDE solvers could accelerate discovery across multiple sciences. CASP mobilized a community around protein structure, enabling AlphaFold. We need this for dozens of challenges. Thus, we're launching...", "url": "https://huggingface.co/posts/evijit/733521758778057", "date_published": "2025-10-08T17:20:15.871739"}, {"id": "https://huggingface.co/posts/SelmaNajih001/229668569857091", "image": "", "title": "I found it excellent and very well done.", "content_text": "I found it excellent and very well done. One of the best explanations of embedding I've ever read. Well done, @ hesamation ! Had to share this: hesamation/primer-llm-embedding See translation", "url": "https://huggingface.co/posts/SelmaNajih001/229668569857091", "date_published": "2025-10-08T17:20:15.871954"}, {"id": "https://huggingface.co/posts/sergiopaniego/573302448191586", "image": "", "title": "Online training methods (e.g., GRPO) require real-time generation, a compute- and memory-heavy bottleneck.", "content_text": "Online training methods (e.g., GRPO) require real-time generation, a compute- and memory-heavy bottleneck. TRL has built-in vLLM support and in this new recipe, we show how to leverage it for efficient online training. Run on Colab \u26a1, scale to multi-GPU/multi-node! \ud83e\uddd1\u200d\ud83c\udf73 recipe: https://huggingface.co/learn/cookbook/grpo_vllm_online_training See translation", "url": "https://huggingface.co/posts/sergiopaniego/573302448191586", "date_published": "2025-10-08T17:20:15.872223"}, {"id": "https://huggingface.co/posts/sequelbox/166564347957621", "image": "", "title": "NEW RELEASE: Esper 3.1!", "content_text": "NEW RELEASE: Esper 3.1! - Esper is our full-stack, full-cycle coding, DevOps, and architecture specialist! - Our newest, best DeepSeek technical datasets emphasize more challenging queries and tough real-world coding tasks across a variety of programming languages and development paradigms: - Titanium 3 for coding and reasoning in DevOps and architecture: sequelbox/Titanium3-DeepSeek-V3.1-Terminus - Tachibana 3 for high-difficulty code production in a variety of topics and programming languages: - sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus - sequelbox/Tachibana3-Part2-DeepSeek-V3.2 - Mitakihara for MLOps, AI building, use, expertise, and research: sequelbox/Mitakihara-DeepSeek-R1-0528 Our first release in the Esper 3.1 series is built on Qwen3-4B-Thinking-2507. GET IT NOW, FOR EVERYONE: ValiantLabs/Qwen3-4B-Thinking-2507-Esper3.1 We'll be bringing Esper 3.1 to more, larger models as soon as we can; you can help this happen faster with a donation: sequelbox/SupportOpenSource...", "url": "https://huggingface.co/posts/sequelbox/166564347957621", "date_published": "2025-10-08T17:20:15.872632"}, {"id": "https://huggingface.co/posts/codelion/995258395015987", "image": "", "title": "\ud83d\ude80 Adaptive Classifier v0.1.0: Now with ONNX Runtime Support!", "content_text": "\ud83d\ude80 Adaptive Classifier v0.1.0: Now with ONNX Runtime Support! We're excited to announce a major update to Adaptive Classifier - a flexible, continuous learning classification system that adapts to new classes without retraining! What's New: \u26a1 ONNX Runtime Integration: Get 1.14x faster CPU inference out of the box (up to 4x on x86 processors) \ud83d\udce6 INT8 Quantization: Models are now 4x smaller with minimal accuracy loss, making deployment easier and faster \ud83c\udfaf Smart Loading: Automatically uses the best model variant for your hardware - quantized for speed by default, or unquantized for maximum accuracy \ud83d\udd04 7.5x Faster Model Loading: Get started quickly with optimized model initialization How It Works: Adaptive Classifier lets you build text classifiers that continuously learn from new examples without catastrophic forgetting. Perfect for: - Dynamic classification tasks where classes evolve over time - Few-shot learning scenarios with limited training data - Production systems that need to...", "url": "https://huggingface.co/posts/codelion/995258395015987", "date_published": "2025-10-08T17:20:15.873244"}, {"id": "https://huggingface.co/posts/sergiopaniego/862106039351008", "image": "", "title": "A few days ago, Thinking Machines Lab released \u201cLoRA Without Regret\u201d, showing that LoRA can match full fine-tuning performance when configured right.", "content_text": "A few days ago, Thinking Machines Lab released \u201cLoRA Without Regret\u201d, showing that LoRA can match full fine-tuning performance when configured right. Naturally, we decided to reproduce the results with TRL and release a guide! https://huggingface.co/docs/trl/main/en/lora_without_regret See translation", "url": "https://huggingface.co/posts/sergiopaniego/862106039351008", "date_published": "2025-10-08T17:20:15.873490"}]}