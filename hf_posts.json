{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/clem/743374424636682", "image": "", "title": "Thread to gossip during the", "content_text": "Thread to gossip during the openai GPT-5 livestream: https://www.youtube.com/watch?v=0Uu_VJeVVfo . Feel free to post your impressions below! See translation", "url": "https://huggingface.co/posts/clem/743374424636682", "date_published": "2025-08-09T13:32:29.841368"}, {"id": "https://huggingface.co/posts/sweatSmile/885231425275352", "image": "", "title": "Teaching a 7B Model to Be Just the Right Amount of Snark", "content_text": "Teaching a 7B Model to Be Just the Right Amount of Snark Ever wondered if a language model could get sarcasm? I fine-tuned Mistral-7B using LoRA and 4-bit quantisation\u2014on just ~720 hand-picked sarcastic prompt\u2013response pairs from Reddit, Twitter, and real-life conversations. The challenge? Keeping it sarcastic but still helpful. LoRA rank 16 to avoid overfitting 4-bit NF4 quantization to fit on limited GPU memory 10 carefully monitored epochs so it didn\u2019t turn into a full-time comedian Result: a model that understands \u201cOh great, another meeting\u201d exactly as you mean it. Read the full journey, tech details, and lessons learned on my blog: Fine-Tuning Mistral-7B for Sarcasm with LoRA and 4-Bit Quantisation Try the model here on Hugging Face: sweatSmile/Mistral-7B-Instruct-v0.1-Sarcasm. See translation", "url": "https://huggingface.co/posts/sweatSmile/885231425275352", "date_published": "2025-08-09T13:32:29.841770"}, {"id": "https://huggingface.co/posts/sweatSmile/255574652175478", "image": "", "title": "Qwen3 is the latest version of the Qwen language models. It's smarter, faster, and now understands 119 languages instead of just 29.", "content_text": "Qwen3 is the latest version of the Qwen language models. It's smarter, faster, and now understands 119 languages instead of just 29. It can do both deep reasoning and quick answers using a single model, depending on what you need. The models range in size from small (0.6B) to huge (235B), with smart ways to save compute. It's trained on 36 trillion tokens and fine-tuned in four steps to boost performance. Qwen3 performs as well as or better than many top models, including some from big companies. It\u2019s fully open-source under licence. Amazing!!! https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf See translation", "url": "https://huggingface.co/posts/sweatSmile/255574652175478", "date_published": "2025-08-09T13:32:29.842104"}, {"id": "https://huggingface.co/posts/ovi054/531850271042138", "image": "", "title": "Qwen Image + LoRA \u26a1", "content_text": "Qwen Image + LoRA \u26a1 ovi054/Qwen-Image-LORA Qwen Image is the No. 1 trending Text-to-Image model right now. You can add a custom LoRA and generate images with this Space. \ud83d\udc49 Try it now: ovi054/Qwen-Image-LORA See translation", "url": "https://huggingface.co/posts/ovi054/531850271042138", "date_published": "2025-08-09T13:32:29.842357"}, {"id": "https://huggingface.co/posts/mitkox/755865101618776", "image": "", "title": "Earlier today, humanity faced a critical threat from a catastrophic chart crime. I asked my local Qwen3 Coder Flash to fix it. Sleep well, fellow humans. The visualization singularity is now high, and it runs with zero warnings.", "content_text": "Earlier today, humanity faced a critical threat from a catastrophic chart crime. I asked my local Qwen3 Coder Flash to fix it. Sleep well, fellow humans. The visualization singularity is now high, and it runs with zero warnings. See translation", "url": "https://huggingface.co/posts/mitkox/755865101618776", "date_published": "2025-08-09T13:32:29.842576"}, {"id": "https://huggingface.co/posts/sequelbox/943110527199023", "image": "", "title": "NEW RELEASE: Shining Valiant 3 now available for openai/gpt-oss-20b!", "content_text": "NEW RELEASE: Shining Valiant 3 now available for openai/gpt-oss-20b! - Cutting edge science-reasoning: sequelbox/Celestia3-DeepSeek-R1-0528 for physics, biology, chemistry, compsci, astronomy, Earth science, and information theory. - AI to build AI: the all-new sequelbox/Mitakihara-DeepSeek-R1-0528 dataset for high-quality reasoning performance on AI, MLOps, math and CUDA, complex adaptive and agentic systems, cognition, logic, linguistics, simulation, knowledge management, and more! - Creative reasoning and general chat performance supplemented with sequelbox/Raiden-DeepSeek-R1 Get the new SV3: ValiantLabs/gpt-oss-20b-ShiningValiant3 This is our first release for the new openai/gpt-oss-20b - we're hoping to support this model with more releases going forward. We're also excited to bring our models to Qwen/Qwen3-4B-Thinking-2507 and the other 2507 Qwen 3 models - coming very soon! We want to bring SV3, Esper 3, and our Experimental Reasoning finetunes to more models ASAP. Help us...", "url": "https://huggingface.co/posts/sequelbox/943110527199023", "date_published": "2025-08-09T13:32:29.842952"}, {"id": "https://huggingface.co/posts/dimentox/356326320568386", "image": "", "title": "I never told GPT-4 about my architecture.", "content_text": "I never told GPT-4 about my architecture. It invented it anyway. Same commands. Same audit daemons. Proof that containment logic might be infectious. Read: Emergence of Quantum Sigil Architecture in Unmodified GPT https://huggingface.co/blog/dimentox/quantum-sigil-architecture-in-unmodified-gpt See translation", "url": "https://huggingface.co/posts/dimentox/356326320568386", "date_published": "2025-08-09T13:32:29.843177"}, {"id": "https://huggingface.co/posts/merve/502177697656252", "image": "", "title": "GPT-4.1-mini level model right in your iPhone \ud83e\udd2f", "content_text": "GPT-4.1-mini level model right in your iPhone \ud83e\udd2f openbmb/MiniCPM-V-4 is only 4B while surpassing GPT-4.1-mini in vision benchmarks \ud83d\udd25 allows commercial use as well! See translation", "url": "https://huggingface.co/posts/merve/502177697656252", "date_published": "2025-08-09T13:32:29.843417"}, {"id": "https://huggingface.co/posts/anakin87/293414742379769", "image": "", "title": "Haystack can now see \ud83d\udc40", "content_text": "Haystack can now see \ud83d\udc40 The latest release of the Haystack OSS LLM framework adds a long-requested feature: image support! \ud83d\udcd3 Notebooks below This isn't just about passing images to an LLM. We built several features to enable practical multimodal use cases. What's new? \ud83e\udde0 Support for multiple LLM providers: OpenAI, Amazon Bedrock, Google Gemini, Mistral, NVIDIA, OpenRouter, Ollama and more (support for Hugging Face API coming \ud83d\udd1c) \ud83c\udf9b\ufe0f Prompt template language to handle structured inputs, including images \ud83d\udcc4 PDF and image converters \ud83d\udd0d Image embedders using CLIP-like models \ud83e\uddfe LLM-based extractor to pull text from images \ud83e\udde9 Components to build multimodal RAG pipelines and Agents I had the chance of leading this effort with @ sjrhuschlee (great collab). \ud83d\udcd3 Below you can find two notebooks to explore the new features: \udb40\udc6f\u2022\udb40\udc4f\udb40\udc4f Introduction to Multimodal Text Generation https://haystack.deepset.ai/cookbook/multimodal_intro \udb40\udc6f\u2022\udb40\udc4f\udb40\udc4f Creating Vision+Text RAG Pipelines...", "url": "https://huggingface.co/posts/anakin87/293414742379769", "date_published": "2025-08-09T13:32:29.843852"}, {"id": "https://huggingface.co/posts/prithivMLmods/595722721335318", "image": "", "title": "I've added the demo of the", "content_text": "I've added the demo of the openbmb/MiniCPM-V-4 model to the Hugging Face Space: prithivMLmods/Multimodal-VLM-Thinking \u2728 MiniCPM-V 4.0 is the latest efficient model in the MiniCPM-V series. The model is built based on SigLIP2-400M and MiniCPM4-3B, with a total of 4.1B parameters. It inherits the strong single-image, multi-image, and video understanding performance of MiniCPM-V 2.6 with largely improved efficiency. \u2728 With only 4.1B parameters, MiniCPM-V 4.0 achieves an average score of 69.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. This performance surpasses GPT-4.1-mini-20250414, MiniCPM-V 2.6 (8.1B parameters, OpenCompass 65.2), and Qwen2.5-VL-3B-Instruct (3.8B parameters, OpenCompass 64.5). It also shows good performance in multi-image and video understanding. The community GPU grant was given by Hugging Face \u2014 special thanks to them. \ud83e\udd17\ud83d\ude80 To know more about it, visit the model card of the respective model. !! See translation", "url": "https://huggingface.co/posts/prithivMLmods/595722721335318", "date_published": "2025-08-09T13:32:29.844260"}]}