{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Reality123b/635291729211142", "image": "", "title": "Happy birthday to me!!!", "content_text": "Happy birthday to me!!! See translation", "url": "https://huggingface.co/posts/Reality123b/635291729211142", "date_published": "2026-01-10T13:32:16.314722"}, {"id": "https://huggingface.co/posts/sergiopaniego/609901844144152", "image": "", "title": "New GRPO + TRL free Colab notebook out! \ud83d\udd25", "content_text": "New GRPO + TRL free Colab notebook out! \ud83d\udd25 Fine-tune 7B+ models on T4 GPUs thanks to a ton of memory optimizations for GRPO 7B model uses only 9.2 GB VRAM (~7\u00d7 reduction) \ud83e\udd2f Try the notebook here \ud83d\udc49 https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_trl_lora_qlora.ipynb See translation", "url": "https://huggingface.co/posts/sergiopaniego/609901844144152", "date_published": "2026-01-10T13:32:16.315047"}, {"id": "https://huggingface.co/posts/hypothetical/242547767593660", "image": "", "title": "We have updated our transcription model:", "content_text": "We have updated our transcription model: TheStageAI/thewhisper-large-v3-turbo \u2013 6.00 WER on the English Open ASR Leaderboard \u2013 4.74 WER on the Multilingual Open ASR Leaderboard \u2013 Beats NVIDIA Parakeet (6.34 WER) and Whisper-large-v3-turbo (7.8 WER) \u2013 Strong improvements in Arabic, Hindi, Chinese \u2013 Maintains quality with background and environmental noise \u2013 Optimized inference engines for NVIDIA and Apple \u2013 Hugging Face Transformers interface for easy use \u2013 Best-in-class speed on NVIDIA GPUs and power efficiency on Apple devices \u2013 NVIDIA Jetson Thor support See translation", "url": "https://huggingface.co/posts/hypothetical/242547767593660", "date_published": "2026-01-10T13:32:16.315423"}, {"id": "https://huggingface.co/posts/AdinaY/677909506898469", "image": "", "title": "Qwen just released two new model series: Qwen3-VL-Embedding & Qwen3-VL-Reranker \ud83d\ude80", "content_text": "Qwen just released two new model series: Qwen3-VL-Embedding & Qwen3-VL-Reranker \ud83d\ude80 \u2728 2B / 8B - Apache2.0 \u2728 30+ languages \u2728 Supported text, images, screenshots, videos, and arbitrary multimodal combinations Qwen3-VL-Embedding: Flexible vector sizes (64\u20132048) https://huggingface.co/collections/Qwen/qwen3-vl-embedding Qwen3-VL-Reranker: Built for recall>rerank pipelines https://huggingface.co/collections/Qwen/qwen3-vl-reranker See translation", "url": "https://huggingface.co/posts/AdinaY/677909506898469", "date_published": "2026-01-10T13:32:16.315731"}, {"id": "https://huggingface.co/posts/unmodeled-tyler/111185407753079", "image": "", "title": "Atom-80B is out!:", "content_text": "Atom-80B is out!: vanta-research/atom-80b I'm excited to share the new Atom-80B from VANTA Research! A few days ago we released the largest model-to-date from our portfolio, which was Atom-27B. We've quickly scaled up to the new Qwen3 Next 80B architecture, bringing our friendly, curious, and collaborative Atom persona to cutting edge lightweight, high parameter inference. Atom is designed to work and think alongside you through curious exploration. Using Atom collaboratively in your work can help spark your own creativity or curiosity. Give it a try! See translation", "url": "https://huggingface.co/posts/unmodeled-tyler/111185407753079", "date_published": "2026-01-10T13:32:16.316054"}, {"id": "https://huggingface.co/posts/AdinaY/745173620987102", "image": "", "title": "Wechat AI is shipping!", "content_text": "Wechat AI is shipping! WeDLM \ud83d\udd25 A new language model that generates tokens in parallel, making it faster than standard LLMs , with the same Transformer setup! https://huggingface.co/collections/tencent/wedlm \u2728 7B/8B - Base & Instruct \u2728 Apache 2.0 See translation", "url": "https://huggingface.co/posts/AdinaY/745173620987102", "date_published": "2026-01-10T13:32:16.316317"}, {"id": "https://huggingface.co/posts/JLouisBiz/620644523229509", "image": "", "title": "I\u2019ve built two Firefox extensions for my personal workflow:", "content_text": "I\u2019ve built two Firefox extensions for my personal workflow: 1. **Quick Edit in Emacs** I manage over 3,500 web pages locally. With this extension, I can now click anywhere on a webpage and instantly jump into Emacs to edit the exact page (or annotate any other page I'm working on). 2. **Describe Images (and soon Videos) on the Web** Using the right-click menu, I can generate descriptions for images I come across online. These descriptions are stored and reused for my own image collections or web pages. I\u2019m planning to add the same functionality for videos soon. What makes this possible is running LLMs locally on my own machine \u2014 I\u2019ve been experimenting with models like **Mistral Vibe** and others. This lets me automate description generation and text processing entirely offline, keeping everything fast, private, and fully under my control. See translation", "url": "https://huggingface.co/posts/JLouisBiz/620644523229509", "date_published": "2026-01-10T13:32:16.316739"}, {"id": "https://huggingface.co/posts/Ujjwal-Tyagi/924115096887729", "image": "", "title": "We are sleepwalking into a crisis. I am deeply concerned about AI model safety right now because, as the community rushes to roll out increasingly powerful open-source models, we are completely neglecting the most critical aspect: safety. It seems that nobody is seriously thinking about the potential consequences of unregulated model outputs or the necessity of robust guardrails. We are essentially planting the seeds for our own destruction if we prioritize raw performance over security.", "content_text": "We are sleepwalking into a crisis. I am deeply concerned about AI model safety right now because, as the community rushes to roll out increasingly powerful open-source models, we are completely neglecting the most critical aspect: safety. It seems that nobody is seriously thinking about the potential consequences of unregulated model outputs or the necessity of robust guardrails. We are essentially planting the seeds for our own destruction if we prioritize raw performance over security. This negligence is terrifyingly evident when you look at the current landscape. Take Qwen Image 2512, for example; while it delivers undeniably strong performance, it has incredibly weak guardrails that make it dangerous to deploy. In stark contrast, Z Image might not get as much hype for its power, but it has much better safety guardrails than Qwen Image 2512. It is imperative that the open-source community and developers recognize that capability without responsibility is a liability. We must...", "url": "https://huggingface.co/posts/Ujjwal-Tyagi/924115096887729", "date_published": "2026-01-10T13:32:16.317219"}, {"id": "https://huggingface.co/posts/kanaria007/141065277072114", "image": "", "title": "\u2705 New Article: *PoC Architecture for Education & Developmental Support*", "content_text": "\u2705 New Article: *PoC Architecture for Education & Developmental Support* Title: \ud83c\udf93 Building an SI-Core Wrapped Learning Companion - PoC architecture for education and developmental support \ud83d\udd17 https://huggingface.co/blog/kanaria007/poc-architecture-for-education-development-support --- Summary: Most \u201cAI tutors\u201d are built as *LLM-first* systems. This article flips the default: * The LLM is treated as an *untrusted proposal engine* * *SI-Core owns* observation, consent, ethics, memory, and rollback * Teachers and guardians get *real oversight*, not just chat transcripts Scoped intentionally to *one subject \u00d7 a small cohort (10\u201330 learners)*, this is a PoC you can actually ship\u2014and audit. > Don\u2019t ask: \u201cCan an AI replace teachers?\u201d > Prove: \u201cCan we make an AI companion *safe, explainable, and governable* for real learners?\u201d --- Why It Matters (for AI on real stacks): \u2022 *Consent & accommodations* are first-class (especially for minors / neurodivergent learners) \u2022 *Ethics decisions are...", "url": "https://huggingface.co/posts/kanaria007/141065277072114", "date_published": "2026-01-10T13:32:16.317800"}, {"id": "https://huggingface.co/posts/Sri-Vigneshwar-DJ/997746049319991", "image": "", "title": "\ud83e\udd85 Introducing Hawky AI H1 Mini 4B: A Domain-Specific Model for Performance Marketing", "content_text": "\ud83e\udd85 Introducing Hawky AI H1 Mini 4B: A Domain-Specific Model for Performance Marketing Hey HuggingFace community! \ud83d\udc4b We're excited to share our first open-source release: **Hawky AI H1 Mini 4B Experimental** - a Gemma 3 4B model fine-tuned specifically for Meta advertising and performance marketing strategy. \ud83c\udfaf Why We Built This At [Hawky.ai]( https://hawky.ai ), we build AI-powered creative intelligence tools for performance marketers. We work with major agencies (WPP, Madison, GroupM) and brands (TVS Motors, Tanishq, Bajaj Finserv) on campaign optimization. We wanted to explore: Can a small, domain-specific model provide expert-level guidance on performance marketing? Specifically, we focused on Meta's Andromeda algorithm - the AI system that now powers ad delivery across Facebook and Instagram. Understanding Andromeda is crucial for modern media buying, but the knowledge is scattered and constantly evolving. \ud83e\udde0 What Makes This Different Chain-of-Thought Reasoning The model doesn't...", "url": "https://huggingface.co/posts/Sri-Vigneshwar-DJ/997746049319991", "date_published": "2026-01-10T13:32:16.318277"}]}