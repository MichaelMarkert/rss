{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Jaward/864148450814843", "image": "", "title": "It\u2019s absolutely mind blowing - the work Dynamics Lab is doing!!", "content_text": "It\u2019s absolutely mind blowing - the work Dynamics Lab is doing!! With just a single input image and in a few seconds, their new world engine model (Mirage 2) can generate a whole new interactive world that\u2019s physics informed and fully explorable in real-time\ud83e\udd2f Try it yourself: https://demo.dynamicslab.ai/chaos See translation", "url": "https://huggingface.co/posts/Jaward/864148450814843", "date_published": "2025-09-02T05:23:05.796027"}, {"id": "https://huggingface.co/posts/Locutusque/640139873710354", "image": "", "title": "\ud83c\udf32\ud83c\udf44 LLM Forest Orchestra: Turning Hidden States into Music", "content_text": "\ud83c\udf32\ud83c\udf44 LLM Forest Orchestra: Turning Hidden States into Music Hello everyone! I'm excited to introduce a new Space I've been developing called LLM Forest Orchestra. This project converts the hidden states and attention patterns of transformer models into layered MIDI compositions. The concept draws inspiration from mushrooms and mycelial networks in forests. Fungi create underground connections linking plants and trees, establishing what some call a \"wood-wide web\" where signals and nutrients travel. Researchers have discovered that these exchanges form patterns resembling rhythms and pulses. When translated appropriately, these patterns can become music. Transformers operate through remarkably similar principles: tokens share signals via hidden states and attention heads. This Space transforms those invisible information flows into notes, chords, and rhythms, treating the model as a digital forest orchestra. \ud83c\udf9b Features * Two compute modes: - Full model operates on a Hugging Face model...", "url": "https://huggingface.co/posts/Locutusque/640139873710354", "date_published": "2025-09-02T05:23:05.796633"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/683040638338113", "image": "", "title": "I have concluded first 8 traininings of Qwen Image LoRA - we are not at the level of FLUX yet and next 8 trainings starting hopefully - 2656x2656px image generated with 8 steps Fast Qwen LoRA + myself trained LoRA :", "content_text": "I have concluded first 8 traininings of Qwen Image LoRA - we are not at the level of FLUX yet and next 8 trainings starting hopefully - 2656x2656px image generated with 8 steps Fast Qwen LoRA + myself trained LoRA : Grid test results shared here along with App installer : https://www.patreon.com/posts/137551634 See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/683040638338113", "date_published": "2025-09-02T05:23:05.796876"}, {"id": "https://huggingface.co/posts/codelion/510406818109359", "image": "", "title": "I recently worked on a LoRA that improves tool use in LLM. Thought the approach might interest folks here.", "content_text": "I recently worked on a LoRA that improves tool use in LLM. Thought the approach might interest folks here. The issue I have had when trying to use some of the local LLMs with coding agents is this: Me: \"Find all API endpoints with authentication in this codebase\" LLM: \"You should look for @ app .route decorators and check if they have auth middleware...\" But I often want it to search the files and show me but the LLM doesn't trigger a tool use call. To fine-tune it for tool use I combined two data sources: 1. Magpie scenarios - 5000+ diverse tasks (bug hunting, refactoring, security audits) 2. Real execution - Ran these on actual repos (FastAPI, Django, React) to get authentic tool responses This ensures the model learns both breadth (many scenarios) and depth (real tool behavior). Tools We Taught: - read_file - Actually read file contents - search_files - Regex/pattern search across codebases - find_definition - Locate classes/functions - analyze_imports - Dependency tracking -...", "url": "https://huggingface.co/posts/codelion/510406818109359", "date_published": "2025-09-02T05:23:05.797521"}, {"id": "https://huggingface.co/posts/DawnC/381537695345047", "image": "", "title": "PawMatchAI \u2014 Now with SBERT-Powered Recommendations! \ud83d\udc36\u2728", "content_text": "PawMatchAI \u2014 Now with SBERT-Powered Recommendations! \ud83d\udc36\u2728 \u2b50\ufe0f NEW: Description-based recommendations are here! Just type in your lifestyle or preferences (e.g. \u201cI live in an apartment and want a quiet dog\u201d), and PawMatchAI uses SBERT semantic embeddings to understand your needs and suggest compatible breeds. What can PawMatchAI do today? \ud83d\udcf8 Upload a photo to identify your dog from 124 breeds with detailed info. \u2696\ufe0f Compare two breeds side-by-side, from grooming needs to health insights. \ud83d\udcca Visualize breed traits with radar and comparison charts. \ud83c\udfa8 Try Style Transfer to turn your dog\u2019s photo into anime, watercolor, cyberpunk, and more. What\u2019s next? \ud83c\udfaf More fine-tuned recommendations. \ud83d\udcf1 Mobile-friendly deployment. \ud83d\udc3e Expansion to additional species. My goal: To make breed discovery not only accurate but also interactive and fun \u2014 combining computer vision, semantic understanding, and creativity to help people find their perfect companion. \ud83d\udc49 Try it here: DawnC/PawMatchAI If you enjoy...", "url": "https://huggingface.co/posts/DawnC/381537695345047", "date_published": "2025-09-02T05:23:05.797993"}, {"id": "https://huggingface.co/posts/merve/771481819901416", "image": "", "title": "large AI labs have dropped so many open models last week \ud83d\udd25 don't miss out on them", "content_text": "large AI labs have dropped so many open models last week \ud83d\udd25 don't miss out on them \u2192 Apple released on-device vision LMs apple/fastvlm-68ac97b9cd5cacefdd04872e & apple/mobileclip2-68ac947dcb035c54bcd20c47 \u2192 OpenGVLab released InternVL3.5, 32 new vision LMs with one based on gpt-oss! (OS) OpenGVLab/internvl35-68ac87bd52ebe953485927fb \u2192 MSFT released a killer small TTS model (OS) microsoft/VibeVoice-1.5B find more herehttps://huggingface.co/collections/merve/august-29-releases-68b5a3754cfb8abf59e2b486 See translation", "url": "https://huggingface.co/posts/merve/771481819901416", "date_published": "2025-09-02T05:23:05.798291"}, {"id": "https://huggingface.co/posts/Bils/120204910303830", "image": "", "title": "Introducing ShortiFoley \ud83c\udfb5 \u2014 an AI tool that transforms short videos into realistic Foley audio.", "content_text": "Introducing ShortiFoley \ud83c\udfb5 \u2014 an AI tool that transforms short videos into realistic Foley audio. Built on Tencent\u2019s HunyuanVideo-Foley with SigLIP2 + CLAP, and designed for media automation pipelines like n8n \u2705 Generate Foley from video \u2705 Autosave results with metadata \u2705 MCP endpoints for workflows Bils/ShortiFoley See translation", "url": "https://huggingface.co/posts/Bils/120204910303830", "date_published": "2025-09-02T05:23:05.798546"}, {"id": "https://huggingface.co/posts/Xenova/448209562329557", "image": "", "title": "Okay this is insane... WebGPU-accelerated semantic video tracking, powered by DINOv3 and Transformers.js! \ud83e\udd2f", "content_text": "Okay this is insane... WebGPU-accelerated semantic video tracking, powered by DINOv3 and Transformers.js! \ud83e\udd2f Demo (+ source code): webml-community/DINOv3-video-tracking This will revolutionize AI-powered video editors... which can now run 100% locally in your browser, no server inference required (costs $0)! \ud83d\ude0d How does it work? \ud83e\udd14 1\ufe0f\u20e3 Generate and cache image features for each frame 2\ufe0f\u20e3 Create a list of embeddings for selected patch(es) 3\ufe0f\u20e3 Compute cosine similarity between each patch and the selected patch(es) 4\ufe0f\u20e3 Highlight those whose score is above some threshold ... et voil\u00e0! \ud83e\udd73 You can also make selections across frames to improve temporal consistency! This is super useful if the object changes its appearance slightly throughout the video. Excited to see what the community builds with it! See translation", "url": "https://huggingface.co/posts/Xenova/448209562329557", "date_published": "2025-09-02T05:23:05.798937"}, {"id": "https://huggingface.co/posts/louisbrulenaudet/591445663705551", "image": "", "title": "Supercharge Apple\u2019s Shortcuts using Cloudflare Workers and Gemini within minutes (and for free, up to 1,500 requests per day) \u2601\ufe0f\u2728", "content_text": "Supercharge Apple\u2019s Shortcuts using Cloudflare Workers and Gemini within minutes (and for free, up to 1,500 requests per day) \u2601\ufe0f\u2728 Hello everyone, last week, while experimenting for fun, I created an API that allows you to easily access AI models (in this case, Google's) from the Shortcut app in order to analyze data from my apps and make the most of it thanks to the generative capabilities of advanced models. It costs me nothing, and I think it might be good to share it so that others can build on it. In README.md, you will find everything you need to get started and put your own microservice into production, which you can call from the app\u2019s HTTP request features. You will simply be asked to have a free Cloudflare account and an API key obtained from Google's AI Studio. Feel free to take a look and get back to me if you encounter any problems during deployment. Here is the GitHub repo where you can find all the source code and run it on your own:...", "url": "https://huggingface.co/posts/louisbrulenaudet/591445663705551", "date_published": "2025-09-02T05:23:05.799361"}, {"id": "https://huggingface.co/posts/giadap/942293221747024", "image": "", "title": "I've noticed something. While we're careful about what we post on social media, we're sharing our deepest and most intimate thoughts with AI chatbots -- health concerns, financial worries, relationship issues, business ideas...", "content_text": "I've noticed something. While we're careful about what we post on social media, we're sharing our deepest and most intimate thoughts with AI chatbots -- health concerns, financial worries, relationship issues, business ideas... With OpenAI hinting at ChatGPT advertising, this matters more than ever. Unlike banner ads, AI advertising happens within the conversation itself. Sponsors could subtly influence that relationship advice or financial guidance. The good news? We have options. \ud83e\udd1d Open source AI models let us keep conversations private, avoid surveillance-based business models, and build systems that actually serve users first. Read more about it in our latest blog post, co-written with @ frimelle https://huggingface.co/blog/giadap/privacy-conversational-ai See translation", "url": "https://huggingface.co/posts/giadap/942293221747024", "date_published": "2025-09-02T05:23:05.799710"}]}