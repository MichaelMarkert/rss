{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Ujjwal-Tyagi/869541950904709", "image": "", "title": "So, Koreans are also doing great progress behind Chinese,", "content_text": "So, Koreans are also doing great progress behind Chinese, Their two open source ai models that are actually good in coding. upstage/Solar-Open-100B skt/A.X-K1 See translation", "url": "https://huggingface.co/posts/Ujjwal-Tyagi/869541950904709", "date_published": "2026-01-21T05:33:33.777253"}, {"id": "https://huggingface.co/posts/marksverdhei/460500590246249", "image": "", "title": "Inspired by the heroes of day zero quants (", "content_text": "Inspired by the heroes of day zero quants ( @ TheBloke @ danielhanchen @ shimmyshimmer @ bartowski ), I decided to join the race by releasing the first FP8 quant of glm-4.7-flash! Not as easy as i expected, but I'm happy i was still able to have it working within a few hours after the original model was released! Interested in feedback if anyone wants to try it out! marksverdhei/GLM-4.7-Flash-FP8 Note: If my PR to vLLM isn't merged yet you might have to use my fork. Cheers! \ud83e\udd17 See translation", "url": "https://huggingface.co/posts/marksverdhei/460500590246249", "date_published": "2026-01-21T05:33:33.777631"}, {"id": "https://huggingface.co/posts/DawnC/976121143006478", "image": "", "title": "VividFlow: Complete AI Image Transformation Platform \ud83c\udfac\ud83c\udfa8\u2728", "content_text": "VividFlow: Complete AI Image Transformation Platform \ud83c\udfac\ud83c\udfa8\u2728 Three powerful creative tools in one streamlined workspace. VividFlow combines professional video generation, intelligent background replacement, and artistic style transfer to transform your images with precision and creativity. \ud83c\udfad Triple Creative Powers - Cinematic Video Generation transforms static images into smooth motion sequences from 0.5 to 5 seconds. Eight curated motion categories cover portraits, products, landscapes, and artistic content with precision-tuned templates. - Intelligent Background Replacement generates photorealistic scenes from 24 professionally crafted presets spanning studios, natural environments, urban settings, and seasonal atmospheres. Advanced edge refinement handles complex subjects, while the built-in Touch Up tool eliminates artifacts through AI-powered inpainting for flawless results. - Artistic Style Transfer converts photographs into stunning interpretations across six distinct styles...", "url": "https://huggingface.co/posts/DawnC/976121143006478", "date_published": "2026-01-21T05:33:33.778285"}, {"id": "https://huggingface.co/posts/ZennyKenny/848353801795401", "image": "", "title": "\ud83d\ude0e My new personal website is live! Check out", "content_text": "\ud83d\ude0e My new personal website is live! Check out https://kennethhamilton.me to chat with an LLM about my professional skills and personal projects. \ud83d\ude48 Think of it like a really, really vain version of ChatGPT. See translation", "url": "https://huggingface.co/posts/ZennyKenny/848353801795401", "date_published": "2026-01-21T05:33:33.778550"}, {"id": "https://huggingface.co/posts/sagar007/387080486731099", "image": "", "title": "\ud83d\ude80 I built a Multimodal Vision-Language Model from using Gemma-270M + CLIP!", "content_text": "\ud83d\ude80 I built a Multimodal Vision-Language Model from using Gemma-270M + CLIP! Just finished training my multimodal model on the full LLaVA-Instruct-150K dataset (157K samples) and wanted to share the results! \ud83d\udd27 What I Built: A vision-language model that can understand images and answer questions about them, combining: - Google Gemma-3-270M (language) - OpenAI CLIP ViT-Large/14 (vision) - LoRA fine-tuning for efficiency \ud83d\udcca Training Stats: - 157,712 training samples (full LLaVA dataset) - 3 epochs on A100 40GB - ~9 hours training time - Final loss: 1.333 training / 1.430 validation - Only 18.6M trainable params (3.4% of 539M total) \ud83d\udcc8 sagar007/multigemma Benchmark Results: - VQA Accuracy: 53.8% - Works great for: animal detection, room identification, scene understanding \ud83d\udd17 **Try it yourself:** - \ud83e\udd17 Model: sagar007/multigemma - \ud83c\udfae Demo: https://huggingface.co/spaces/sagar007/Multimodal-Gemma - \ud83d\udcbb GitHub: https://github.com/sagar431/multimodal-gemma-270m Built with PyTorch Lightning + MLflow...", "url": "https://huggingface.co/posts/sagar007/387080486731099", "date_published": "2026-01-21T05:33:33.779031"}, {"id": "https://huggingface.co/posts/danielhanchen/143027024579647", "image": "", "title": "Run GLM-4.7-Flash locally on your device with 24GB RAM!\ud83d\udd25", "content_text": "Run GLM-4.7-Flash locally on your device with 24GB RAM!\ud83d\udd25 It's the best performing 30B model on SWE-Bench and GPQA. With 200K context, it excels at coding, agents, chat & reasoning. GGUF: unsloth/GLM-4.7-Flash-GGUF Guide: https://unsloth.ai/docs/models/glm-4.7-flash See translation", "url": "https://huggingface.co/posts/danielhanchen/143027024579647", "date_published": "2026-01-21T05:33:33.779291"}, {"id": "https://huggingface.co/posts/efecelik/213200184330880", "image": "", "title": "Interesting paper: PhysRVG", "content_text": "Interesting paper: PhysRVG The core idea: instead of treating physics as a soft condition the model can work around during optimization, enforce it strictly via reinforcement learning. The paper focuses on rigid body dynamics - collisions, pendulums, free fall, rolling. PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models (2601.11087) See translation", "url": "https://huggingface.co/posts/efecelik/213200184330880", "date_published": "2026-01-21T05:33:33.779538"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/425042812655046", "image": "", "title": "Compared Quality and Speed Difference (with CUDA 13 & Sage Attention) of BF16 vs GGUF Q8 vs FP8 Scaled vs NVFP4 for Z Image Turbo, FLUX Dev, FLUX SRPO, FLUX Kontext, FLUX 2 - Full 4K step by step tutorial also published", "content_text": "Compared Quality and Speed Difference (with CUDA 13 & Sage Attention) of BF16 vs GGUF Q8 vs FP8 Scaled vs NVFP4 for Z Image Turbo, FLUX Dev, FLUX SRPO, FLUX Kontext, FLUX 2 - Full 4K step by step tutorial also published Full 4K tutorial : https://youtu.be/XDzspWgnzxI Check above full 4K tutorial to learn more and see uncompressed original quality and size images It was always wondered how much quality and speed difference exists between BF16, GGUF, FP8 Scaled and NVFP4 precisions. In this tutorial I have compared all these precision and quantization variants for both speed and quality. The results are pretty surprising. Moreover, we have developed and published NVFP4 model quant generator app and FP8 Scaled quant generator apps. The links of the apps are below if you want to use them. Furthermore, upgrading ComfyUI to CUDA 13 with properly compiled libraries is now very much recommended. We have observed some noticeable performance gains with CUDA 13. So for both SwarmUI and ComfyUI...", "url": "https://huggingface.co/posts/MonsterMMORPG/425042812655046", "date_published": "2026-01-21T05:33:33.779967"}, {"id": "https://huggingface.co/posts/ZomiLanguage/879317521834791", "image": "", "title": "\ud83e\udde0\ud83c\udf0d Zomi Language AI \u2014 Community-Driven, Open-Source", "content_text": "\ud83e\udde0\ud83c\udf0d Zomi Language AI \u2014 Community-Driven, Open-Source ![Zomi Language AI \u2013 From Community to Model] The **Zomi language** carries identity, faith, and history for its people, yet it remains underrepresented in modern AI systems. This project introduces a **community-driven, open-source AI translation framework** that enables Zomi to be trained into AI systems **ethically, transparently, and sustainably**\u2014by native speakers, for future generations. ### \ud83d\udd01 How It Works \ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 Community Texts \u2192 \ud83d\udce6 Open Datasets \u2192 \ud83e\udd16 AI Training \u2192 \ud83d\udcca Evaluation \u2192 \ud83d\udd01 Community Review ### \ud83d\udd13 Why Open-Source Matters - \ud83e\udd1d Community ownership - \ud83d\udd4a\ufe0f Cultural & faith integrity - \u267b\ufe0f Long-term sustainability - \ud83d\udd0d Transparent datasets & models This initiative demonstrates how **low-resource languages can shape the future of inclusive AI** through open collaboration. > *No language should be digitally invisible.* **@Zomi Language | fb.com/ZomiLanguage** ### \ud83c\udff7\ufe0f Tags #OpenSourceAI #LowResourceLanguages #NLP #MachineTranslation...", "url": "https://huggingface.co/posts/ZomiLanguage/879317521834791", "date_published": "2026-01-21T05:33:33.780454"}, {"id": "https://huggingface.co/posts/ovi054/576187162789352", "image": "", "title": "My project, Anim-Lab-AI, won the Community Choice Award at the MCP-1st-Birthday hackathon by", "content_text": "My project, Anim-Lab-AI, won the Community Choice Award at the MCP-1st-Birthday hackathon by @ HuggingFace and @ Gradio ! \ud83c\udfc6 It turns any idea or complex concept into a clear, engaging explainer animation video. \ud83c\udfa5 I want to thank everyone in the Hugging Face community for supporting my project! MCP-1st-Birthday/anim-lab-ai See translation", "url": "https://huggingface.co/posts/ovi054/576187162789352", "date_published": "2026-01-21T05:33:33.780729"}]}