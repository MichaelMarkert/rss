{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/openfree/554631338965323", "image": "", "title": "\ud83c\udf99\ufe0f AI Podcast Generator - Professional Conversation Creation Tool", "content_text": "\ud83c\udf99\ufe0f AI Podcast Generator - Professional Conversation Creation Tool \ud83d\udcd6 Project Overview Transform any URL, PDF, or keyword into professional podcast conversations automatically! This AI-powered tool creates engaging, expert-level dialogues in minutes. \ud83d\ude80 openfree/AI-Podcast \u2728 Key Features: Multiple Input URL: Web articles, blog posts, news content PDF: Research papers, documents, reports Keywords: Topics like \"AI Ethics\", \"Quantum Computing\" \ud83e\udd16 Smart AI Conversation Generation Local LLM: Mistral-Small 24B model for privacy protection API Fallback: Together AI API support Expert Style: In-depth discussions between host and expert Length: 12-20 exchanges for comprehensive coverage \ud83c\udf0f Multilingual Support English: Alex (Host) & Jordan (Expert) Korean: Junsu (Host) & Minho (Expert) \ud83c\udfb5 High-Quality Text-to-Speech Edge-TTS: Natural cloud-based voices Spark-TTS: Local AI voice model MeloTTS: GPU-powered local synthesis \ud83d\udd0d Real-time Information Search Brave Search API for latest information...", "url": "https://huggingface.co/posts/openfree/554631338965323", "date_published": "2025-05-26T17:19:32.983062"}, {"id": "https://huggingface.co/posts/Kseniase/646284586461230", "image": "", "title": "12 Types of JEPA", "content_text": "12 Types of JEPA JEPA, or Joint Embedding Predictive Architecture, is an approach to building AI models introduced by Yann LeCun. It differs from transformers by predicting the representation of a missing or future part of the input, rather than the next token or pixel. This encourages conceptual understanding, not just low-level pattern matching. So JEPA allows teaching AI to reason abstractly. Here are 12 types of JEPA you should know about: 1. I-JEPA -> Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (2301.08243) A non-generative, self-supervised learning framework designed for processing images. It works by masking parts of the images and then trying to predict those masked parts 2. MC-JEPA -> MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features (2307.12698) Simultaneously interprets video data - dynamic elements (motion) and static details (content) - using a shared encoder 3. V-JEPA...", "url": "https://huggingface.co/posts/Kseniase/646284586461230", "date_published": "2025-05-26T17:19:32.983787"}, {"id": "https://huggingface.co/posts/nomadicsynth/536715864633357", "image": "", "title": "Anyone using AI and ML to help neurodivergent people? I'd love to hear what you're doing.", "content_text": "Anyone using AI and ML to help neurodivergent people? I'd love to hear what you're doing. See translation", "url": "https://huggingface.co/posts/nomadicsynth/536715864633357", "date_published": "2025-05-26T17:19:32.984007"}, {"id": "https://huggingface.co/posts/leonardlin/926601368121999", "image": "", "title": "BTW, in case anyone wants to kick the tires, test their \u65e5\u672c\u8a9e, I have our Shisa V2 405B model up and running temporarily:", "content_text": "BTW, in case anyone wants to kick the tires, test their \u65e5\u672c\u8a9e, I have our Shisa V2 405B model up and running temporarily: https://chat.shisa.ai/ See translation", "url": "https://huggingface.co/posts/leonardlin/926601368121999", "date_published": "2025-05-26T17:19:32.984227"}, {"id": "https://huggingface.co/posts/merve/349112163630055", "image": "", "title": "what happened in open AI past week? so many vision LM & omni releases \ud83d\udd25", "content_text": "what happened in open AI past week? so many vision LM & omni releases \ud83d\udd25 merve/releases-23-may-68343cb970bbc359f9b5fb05 multimodal \ud83d\udcac\ud83d\uddbc\ufe0f > new moondream (VLM) is out: it's 4-bit quantized (with QAT) version of moondream-2b, runs on 2.5GB VRAM at 184 tps with only 0.6% drop in accuracy (OS) \ud83c\udf1a > ByteDance released BAGEL-7B, an omni model that understands and generates both image + text. they also released Dolphin, a document parsing VLM \ud83d\udc2c (OS) > Google DeepMind dropped MedGemma in I/O, VLM that can interpret medical scans, and Gemma 3n, an omni model with competitive LLM performance > MMaDa is a new 8B diffusion language model that can generate image and text LLMs > Mistral released Devstral, a 24B coding assistant (OS) \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb > Fairy R1-32B is a new reasoning model -- distilled version of DeepSeek-R1-Distill-Qwen-32B (OS) > NVIDIA released ACEReason-Nemotron-14B, new 14B math and code reasoning model > sarvam-m is a new Indic LM with hybrid thinking mode, based on Mistral Small (OS) >...", "url": "https://huggingface.co/posts/merve/349112163630055", "date_published": "2025-05-26T17:19:32.984702"}, {"id": "https://huggingface.co/posts/jasoncorkill/660720952792703", "image": "", "title": "Benchmark Update:", "content_text": "Benchmark Update: @ google Veo3 (Text-to-Video) Two months ago, we benchmarked @ google \u2019s Veo2 model. It fell short, struggling with style consistency and temporal coherence, trailing behind Runway, Pika, @ tencent , and even @ alibaba-pai . That\u2019s changed. We just wrapped up benchmarking Veo3, and the improvements are substantial. It outperformed every other model by a wide margin across all key metrics. Not just better, dominating across style, coherence, and prompt adherence. It's rare to see such a clear lead in today\u2019s hyper-competitive T2V landscape. Dataset coming soon. Stay tuned. See translation", "url": "https://huggingface.co/posts/jasoncorkill/660720952792703", "date_published": "2025-05-26T17:19:32.985032"}, {"id": "https://huggingface.co/posts/yukiarimo/528629553860992", "image": "", "title": "Hello everyone! Good news!", "content_text": "Hello everyone! Good news! The long-awaited Yuna Ai V4 Miru model was finally released: yukiarimo/yuna-ai-v4-miru Now, she can see both images and videos! She has internal knowledge of multiple languages, including primary English, Japanese, and Russian, and she has a built-in ability for Quantum Thinking! Note that half of the features might be unstable. That\u2019s why, for the next half-year, we will not be writing a dataset; it will be created on the fly as she lives! Eventually, she will learn the needed skills in the upcoming interactions! This is a unique model\u2014the first vision model of Yuna with almost 12B parameters (closer to the atomic version, but smarter)! Weights are already on the hub, and support with good documentation will come in a week. Have fun! Please feel free to drop a little donation for our team to help us buy more Colab Compute Units, as more models are on their way! https://www.patreon.com/c/YukiArimo Thank you guys! See translation", "url": "https://huggingface.co/posts/yukiarimo/528629553860992", "date_published": "2025-05-26T17:19:32.985434"}, {"id": "https://huggingface.co/posts/fdaudens/825943855838372", "image": "", "title": "Two lines in your terminal and you have an AI agent running whatever model and tools you want \ud83e\udd2f", "content_text": "Two lines in your terminal and you have an AI agent running whatever model and tools you want \ud83e\udd2f Just tried the new Tiny Agents in Python. Asked it which team won the Italian Serie A soccer league and to export the final table to CSV. Coolest thing is you can interact with the agent, guide it, and correct its mistakes. The agent connected to web browsing tools, searched for Serie A standings, identified the champion, and generated a CSV export. The setup: pip install \"huggingface_hub[mcp]>=0.32.0\" tiny-agents run That's it. The MCP protocol handles all the tool integrations automatically - no custom APIs to write, no complex setups. Want file system access? It's already there. Need web browsing? Built in. You can swap models, change inference providers, run local models, or add new tools just by editing a simple JSON config. You can also use Gradio Spaces as MCP servers! The entire agent is ~70 lines of Python - essentially a while loop that streams responses and executes tools....", "url": "https://huggingface.co/posts/fdaudens/825943855838372", "date_published": "2025-05-26T17:19:32.985900"}, {"id": "https://huggingface.co/posts/clem/670042306060895", "image": "", "title": "Playing with Veo3 this morning. Share your prompt if you want me to create videos for you (bonus point if they funnily reference HF/open-source). These videos are \"a cat on the moon rapping \"I love Hugging Face\"\"!", "content_text": "Playing with Veo3 this morning. Share your prompt if you want me to create videos for you (bonus point if they funnily reference HF/open-source). These videos are \"a cat on the moon rapping \"I love Hugging Face\"\"! See translation", "url": "https://huggingface.co/posts/clem/670042306060895", "date_published": "2025-05-26T17:19:32.986117"}, {"id": "https://huggingface.co/posts/Tonic/623097368166351", "image": "", "title": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f Hey there folks ,", "content_text": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f Hey there folks , Yesterday the world's first \"Learn to Vibe Code\" application was released . As vibe coding is the mainstream paradigm , so now the first educational app is there to support it . You can try it out already : https://vibe.takara.ai and of course it's entirely open source, so i already made my issue and feature branch :-) \ud83d\ude80 See translation", "url": "https://huggingface.co/posts/Tonic/623097368166351", "date_published": "2025-05-26T17:19:32.986383"}]}