{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/victor/750233862472141", "image": "", "title": "Nvidia is on a roll lately. Nemotron 3 Nano is my new fav local model, but here's the real flex: they published the entire evaluation setup. Configs, prompts, logs, all of it. This is how you do open models \ud83d\udd25", "content_text": "Nvidia is on a roll lately. Nemotron 3 Nano is my new fav local model, but here's the real flex: they published the entire evaluation setup. Configs, prompts, logs, all of it. This is how you do open models \ud83d\udd25 https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe See translation", "url": "https://huggingface.co/posts/victor/750233862472141", "date_published": "2025-12-20T13:31:11.475273"}, {"id": "https://huggingface.co/posts/YatharthS/190514854652270", "image": "", "title": "\ud83e\udd2f \ud83e\udd2f Released a high quality finetuned LLM based TTS model that can generate realistic and clear 48khz audio at over 100x realtime speed! \ud83e\udd2f \ud83e\udd2f", "content_text": "\ud83e\udd2f \ud83e\udd2f Released a high quality finetuned LLM based TTS model that can generate realistic and clear 48khz audio at over 100x realtime speed! \ud83e\udd2f \ud83e\udd2f Github link: https://github.com/ysharma3501/MiraTTS Model link: https://github.com/ysharma3501/MiraTTS Blog explaining llm tts models: https://huggingface.co/blog/YatharthS/llm-tts-models See translation", "url": "https://huggingface.co/posts/YatharthS/190514854652270", "date_published": "2025-12-20T13:31:11.475553"}, {"id": "https://huggingface.co/posts/branikita/131682304514806", "image": "", "title": "Next week, we will release full documentation for the SO ARM 101 with a parallel gripper, featuring leader and follower arms and support for widely used stereo cameras.", "content_text": "Next week, we will release full documentation for the SO ARM 101 with a parallel gripper, featuring leader and follower arms and support for widely used stereo cameras. See translation", "url": "https://huggingface.co/posts/branikita/131682304514806", "date_published": "2025-12-20T13:31:11.475763"}, {"id": "https://huggingface.co/posts/Nymbo/749803216260872", "image": "", "title": "\ud83d\udea8 New tool for the", "content_text": "\ud83d\udea8 New tool for the Nymbo/Tools MCP server: The new Agent_Skills tool provides full support for Agent Skills (Claude Skills but open-source). How it works: The tool exposes the standard discover/info/resources/validate actions. Skills live in /Skills under the same File_System root, and any bundled scripts run through Shell_Command , no new infrastructure required. Agent_Skills(action= \"discover\" ) # List all available skills Agent_Skills(action= \"info\" , skill_name= \"music-downloader\" ) # Full SKILL.md Agent_Skills(action= \"resources\" , skill_name= \"music-downloader\" ) # Scripts, refs, assets I've included a music-downloader skill as a working demo, it wraps yt-dlp for YouTube/SoundCloud audio extraction. Caveat: On HF Spaces, Shell_Command works for most tasks, but some operations (like YouTube downloads) are restricted due to the container environment. For full functionality, run the server locally on your machine. Try it out ~ https://www.nymbo.net/nymbot See translation", "url": "https://huggingface.co/posts/Nymbo/749803216260872", "date_published": "2025-12-20T13:31:11.476210"}, {"id": "https://huggingface.co/posts/branikita/654655990580208", "image": "", "title": "Update: Our engineer Alan has received a batch of components for the manipulator assemblies \u2014 including clamps and metal bracket parts. Prototype assembly is planned for the beginning of next year.", "content_text": "Update: Our engineer Alan has received a batch of components for the manipulator assemblies \u2014 including clamps and metal bracket parts. Prototype assembly is planned for the beginning of next year. See translation", "url": "https://huggingface.co/posts/branikita/654655990580208", "date_published": "2025-12-20T13:31:11.476439"}, {"id": "https://huggingface.co/posts/sergiopaniego/463044021249760", "image": "", "title": "Google DeepMind releases FunctionGemma, a 240M model specialized in \ud83d\udd27 tool calling, built for fine-tuning", "content_text": "Google DeepMind releases FunctionGemma, a 240M model specialized in \ud83d\udd27 tool calling, built for fine-tuning TRL has day-0 support. To celebrate, we\u2019re sharing 2 new resources: > Colab guide to fine-tune it for \ud83c\udf10 browser control with BrowserGym OpenEnv > Standalone training script > Colab notebook: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/grpo_functiongemma_browsergym_openenv.ipynb > Training script: https://github.com/huggingface/trl/blob/main/examples/scripts/openenv/browsergym_llm.py (command to run it inside the script) > More notebooks in TRL: https://huggingface.co/docs/trl/example_overview#notebooks See translation", "url": "https://huggingface.co/posts/sergiopaniego/463044021249760", "date_published": "2025-12-20T13:31:11.476765"}, {"id": "https://huggingface.co/posts/rajkumarrawal/519682431601479", "image": "", "title": "\" An open standardized protocol enabling communication for autonomous robots to exchange data, coordinate tasks, and collaborate in real-time environments in the age of AI \". r2r-protocol (Robot2Robot Protocol) is now officially open source! \ud83d\udd13", "content_text": "\" An open standardized protocol enabling communication for autonomous robots to exchange data, coordinate tasks, and collaborate in real-time environments in the age of AI \". r2r-protocol (Robot2Robot Protocol) is now officially open source! \ud83d\udd13 \"pip install r2r-protocol\" Whether you're a developer, researcher, or tech enthusiast, we invite you to explore, use, and contribute to the project. \ud83d\udd17 Check it out here: [ https://github.com/Tech-Parivartan/r2r-protocol?tab=readme-ov-file ] Let\u2019s build the future together! \ud83d\udca1 AiParivartanResearchLab techparivartan Documentation of the r2r-protocal : [ https://techparivartanai.notion.site/Robot-to-Robot-r2r-Protocol-1f008f0fb18780439d70e8b9bbbdb869 ] The R2R Protocol enables seamless robot-to-robot interaction across industrial automation, swarm robotics, logistics, and multi-agent systems. It defines structured message formats, negotiation logic, discovery mechanisms, and extensible APIs. #r2r_protocol #robot2robot_protocol #ai...", "url": "https://huggingface.co/posts/rajkumarrawal/519682431601479", "date_published": "2025-12-20T13:31:11.477208"}, {"id": "https://huggingface.co/posts/etemiz/592416201142301", "image": "", "title": "looks like the best way to incorporate truth in AI is to use some kind of RAG.", "content_text": "looks like the best way to incorporate truth in AI is to use some kind of RAG. what are the state of the art ways to consume knowledge graphs? and what is the best way to build a knowledge graph using AI? See translation", "url": "https://huggingface.co/posts/etemiz/592416201142301", "date_published": "2025-12-20T13:31:11.477434"}, {"id": "https://huggingface.co/posts/prithivMLmods/787095126804028", "image": "", "title": "Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... \ud83e\udd17\ud83d\udd25", "content_text": "Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... \ud83e\udd17\ud83d\udd25 \u2728 TRELLIS.2-Text-to-3D [Demo]: prithivMLmods/TRELLIS.2-Text-to-3D \u2728 Multimodal Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations \u2728 Github: https://github.com/PRITHIVSAKTHIUR/TRELLIS.2-Text-to-3D To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/787095126804028", "date_published": "2025-12-20T13:31:11.477780"}, {"id": "https://huggingface.co/posts/daavoo/629869039060445", "image": "", "title": "2025: The Year of Agents.", "content_text": "2025: The Year of Agents. 2026: The Year of Local Agents? Relying on cloud-hosted LLMs is often overkill. While frontier models still lead in complex coding, local models are now more than capable of handling many agentic workflows\u2014with zero latency and total privacy. To help bridge the gap between local inference and usable agents, I\u2019m releasing agent.cpp: https://github.com/mozilla-ai/agent.cpp It provides minimal, high-performance building blocks for agents in C++, built directly around the awesome llama.cpp ecosystem. Stop sending your data to a remote API. Start building and running agents on your own hardware. See translation", "url": "https://huggingface.co/posts/daavoo/629869039060445", "date_published": "2025-12-20T13:31:11.478123"}]}