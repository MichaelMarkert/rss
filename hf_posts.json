{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/ProCreations/321100188234240", "image": "", "title": "Eyyyy 50 followers \ud83e\udd2f", "content_text": "Eyyyy 50 followers \ud83e\udd2f See translation", "url": "https://huggingface.co/posts/ProCreations/321100188234240", "date_published": "2025-05-29T17:21:57.507038"}, {"id": "https://huggingface.co/posts/hesamation/260011784391977", "image": "", "title": "I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book.", "content_text": "I really like how this seven-stage pipeline was laid out in the Ultimate Guide to Fine-Tuning book. It gives an overview, then goes into detail for each stage, even providing best practices. It\u2019s 115 pages on arxiv, definitely worth a read. Check it out: https://arxiv.org/abs/2408.13296 See translation", "url": "https://huggingface.co/posts/hesamation/260011784391977", "date_published": "2025-05-29T17:21:57.507316"}, {"id": "https://huggingface.co/posts/lukmanaj/495766537273785", "image": "", "title": "I am so happy  to share to all that I\u2019ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I\u2019m doing my best to keep up. Excited for what\u2019s ahead!", "content_text": "I am so happy to share to all that I\u2019ve just completed the first unit of the new MCP course on Hugging Face and earned my certificate! The AI acceleration track is intense and fast-paced, but I\u2019m doing my best to keep up. Excited for what\u2019s ahead! See translation", "url": "https://huggingface.co/posts/lukmanaj/495766537273785", "date_published": "2025-05-29T17:21:57.507580"}, {"id": "https://huggingface.co/posts/fdaudens/719212082746895", "image": "", "title": "Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity!", "content_text": "Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity! The final project uses the GAIA dataset - your agent has to solve tasks like analyzing Excel files, processing audio recordings, answering questions about YouTube videos, and diving into research papers. This isn't toy examples, it's the messy, multimodal stuff agents need to handle in practice. Whether you\u2019re just getting started with agents or want to go deeper with tools like LangChain, LlamaIndex, and SmolAgents, this course has tons of useful stuff. A few key insights: - Code agents are incredibly versatile once you get the architecture right - The sweet spot is finding the right balance of guidance vs autonomy for each use case - Once the logic clicks, the possibilities really are endless - it's like letting LLMs break free from the chatbox The course is free and the certification deadline is July 1st, 2025. The Hugging Face...", "url": "https://huggingface.co/posts/fdaudens/719212082746895", "date_published": "2025-05-29T17:21:57.508057"}, {"id": "https://huggingface.co/posts/DawnC/538322807718464", "image": "", "title": "VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration", "content_text": "VisionScout Major Update: Enhanced Precision Through Multi-Modal AI Integration I'm excited to share significant improvements to VisionScout that substantially enhance accuracy and analytical capabilities. \u2b50\ufe0f Key Enhancements - CLIP Zero-Shot Landmark Detection: The system now identifies famous landmarks and architectural features without requiring specific training data, expanding scene understanding beyond generic object detection. - Places365 Environmental Classification: Integration of MIT's Places365 model provides robust scene baseline classification across 365 categories, significantly improving lighting analysis accuracy and overall scene identification precision. - Enhanced Multi-Modal Fusion: Advanced algorithms now dynamically combine insights from YOLOv8, CLIP, and Places365 to optimize accuracy across diverse scenarios. - Refined LLM Narratives: Llama 3.2 integration continues to transform analytical data into fluent, contextually rich descriptions while maintaining...", "url": "https://huggingface.co/posts/DawnC/538322807718464", "date_published": "2025-05-29T17:21:57.508574"}, {"id": "https://huggingface.co/posts/dhruv3006/675063918098240", "image": "", "title": "Cua : Docker for computer-use agents", "content_text": "Cua : Docker for computer-use agents Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers. Github : https://github.com/trycua See translation", "url": "https://huggingface.co/posts/dhruv3006/675063918098240", "date_published": "2025-05-29T17:21:57.508800"}, {"id": "https://huggingface.co/posts/BestWishYsh/693532821570217", "image": "", "title": "Introducing our new work: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\u200b\u200b \ud83d\ude80", "content_text": "Introducing our new work: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\u200b\u200b \ud83d\ude80 We tackle the core challenges of \u200b\u200bSubject-to-Video Generation (S2V)\u200b\u200b by systematically building the first complete infrastructure\u2014featuring an evaluation benchmark and a million-scale dataset! \u2728 \ud83e\udde0 Introducing \u200b\u200bOpenS2V-Eval\u200b\u200b\u2014the first \u200b\u200bfine-grained S2V benchmark\u200b\u200b, with \u200b\u200b180 multi-domain prompts + real/synthetic test pairs\u200b\u200b. We propose \u200b\u200bNexusScore\u200b\u200b, \u200b\u200bNaturalScore\u200b\u200b, and \u200b\u200bGmeScore\u200b\u200b to precisely quantify model performance across \u200b\u200bsubject consistency, naturalness, and text alignment\u200b\u200b \u2714 \ud83d\udcca Using this framework, we conduct a \u200b\u200bcomprehensive evaluation of 16 leading S2V models\u200b\u200b, revealing their strengths/weaknesses in complex scenarios! \ud83d\udd25 \u200b\u200bOpenS2V-5M dataset\u200b\u200b now available! A \u200b\u200b5.4M 720P HD\u200b\u200b collection of \u200b\u200bsubject-text-video triplets\u200b\u200b, enabled by \u200b\u200bcross-video association segmentation + multi-view synthesis\u200b\u200b for \u200b\u200bdiverse subjects & high-quality...", "url": "https://huggingface.co/posts/BestWishYsh/693532821570217", "date_published": "2025-05-29T17:21:57.509312"}, {"id": "https://huggingface.co/posts/fdaudens/323840314242853", "image": "", "title": "\ud83c\udfb5 Dream come true for content creators! TIGER AI can extract voice, effects & music from ANY audio file \ud83e\udd2f", "content_text": "\ud83c\udfb5 Dream come true for content creators! TIGER AI can extract voice, effects & music from ANY audio file \ud83e\udd2f This lightweight model uses frequency band-split technology to separate speech like magic. Kudos to @ fffiloni for the amazing demo! fffiloni/TIGER-audio-extraction See translation", "url": "https://huggingface.co/posts/fdaudens/323840314242853", "date_published": "2025-05-29T17:21:57.509594"}, {"id": "https://huggingface.co/posts/codelion/194217585449623", "image": "", "title": "Introducing AutoThink: Adaptive reasoning for LLMs that improves performance by 43% on reasoning benchmarks!", "content_text": "Introducing AutoThink: Adaptive reasoning for LLMs that improves performance by 43% on reasoning benchmarks! Instead of using fixed thinking budgets, AutoThink: - Classifies query complexity (HIGH/LOW) using adaptive classification - Dynamically allocates thinking tokens based on complexity - Uses steering vectors derived from Pivotal Token Search to guide reasoning patterns Results on DeepSeek-R1-Distill-Qwen-1.5B: - GPQA-Diamond: 31.06% vs 21.72% baseline (+9.34 points) - MMLU-Pro: 26.38% vs 25.58% baseline (+0.8 points) - Uses fewer tokens than baseline approaches Works with any local reasoning model - DeepSeek, Qwen, Llama, custom models. The technique combines our research on Pivotal Token Search (PTS) implementation and adaptive classification frameworks. Paper: AutoThink: efficient inference for reasoning LLMs https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327 Code and examples: https://github.com/codelion/optillm/tree/main/optillm/autothink PTS implementation and...", "url": "https://huggingface.co/posts/codelion/194217585449623", "date_published": "2025-05-29T17:21:57.509973"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/399804017135457", "image": "", "title": "Real Steel Became a Reality \u2014 Full AI Robots Boxing Tournament \u2014 With English Subtitles \u2014 So How These Robots Working Actually in Full Details", "content_text": "Real Steel Became a Reality \u2014 Full AI Robots Boxing Tournament \u2014 With English Subtitles \u2014 So How These Robots Working Actually in Full Details Video link : https://youtu.be/fw2yezpn_bo Article (free and public) : https://www.patreon.com/posts/130031621 Summary of article \ud83c\udfae Control Philosophy AI Dominance \u2705 Complex physical movement execution \u2705 Low-level motor control and coordination \u2705 Sensor data interpretation \u2705 Safety protocol management Human Dominance \ud83c\udfaf Overall fight strategy \ud83c\udfaf Attack timing and selection \ud83c\udfaf Action intensity modulation \ud83c\udfaf Real-time tactical decisions See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/399804017135457", "date_published": "2025-05-29T17:21:57.510296"}]}