{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/sergiopaniego/384415208092213", "image": "", "title": "ICYMI, you can fine-tune open LLMs using Claude Code", "content_text": "ICYMI, you can fine-tune open LLMs using Claude Code just tell it: \u201cFine-tune Qwen3-0.6B on open-r1/codeforces-cots\u201d and Claude submits a real training job on HF GPUs using TRL. it handles everything: > dataset validation > GPU selection > training + Trackio monitoring > job submission + cost estimation when it\u2019s done, your model is on the Hub, ready to use read more about the process: https://huggingface.co/blog/hf-skills-training See translation", "url": "https://huggingface.co/posts/sergiopaniego/384415208092213", "date_published": "2025-12-12T05:26:57.313365"}, {"id": "https://huggingface.co/posts/mitkox/706030667212965", "image": "", "title": "Got to 1199.8 tokens/sec with Devstral Small -2 on my desktop GPU workstation. vLLM nightly.", "content_text": "Got to 1199.8 tokens/sec with Devstral Small -2 on my desktop GPU workstation. vLLM nightly. Works out of the box with Mistral Vibe. Next is time to test the big one. See translation", "url": "https://huggingface.co/posts/mitkox/706030667212965", "date_published": "2025-12-12T05:26:57.313615"}, {"id": "https://huggingface.co/posts/tomaarsen/853653818134091", "image": "", "title": "\ud83d\udc26\u200d\ud83d\udd25 I've just published Sentence Transformers v5.2.0! It introduces multi-processing for CrossEncoder (rerankers), multilingual NanoBEIR evaluators, similarity score outputs in mine_hard_negatives, Transformers v5 support and more. Details:", "content_text": "\ud83d\udc26\u200d\ud83d\udd25 I've just published Sentence Transformers v5.2.0! It introduces multi-processing for CrossEncoder (rerankers), multilingual NanoBEIR evaluators, similarity score outputs in mine_hard_negatives, Transformers v5 support and more. Details: - CrossEncoder multi-processing: Similar to SentenceTransformer and SparseEncoder, you can now use multi-processing with CrossEncoder rerankers. Useful for multi-GPU and CPU settings, and simple to configure: just device=[\"cuda:0\", \"cuda:1\"] or device=[\"cpu\"]*4 on the model.predict or model.rank calls. - Multilingual NanoBEIR Support: You can now use community translations of the tiny NanoBEIR retrieval benchmark instead of only the English one, by passing dataset_id , e.g. dataset_id=\"lightonai/NanoBEIR-de\" for the German benchmark. - Similarity scores in Hard Negatives Mining: When mining for hard negatives to create a strong training dataset, you can now pass output_scores=True to get similarity scores returned. This can be useful for some...", "url": "https://huggingface.co/posts/tomaarsen/853653818134091", "date_published": "2025-12-12T05:26:57.314219"}, {"id": "https://huggingface.co/posts/sergiopaniego/627155943228357", "image": "", "title": "We just released TRL v0.26.0!", "content_text": "We just released TRL v0.26.0! It comes packed with updates: > Agent training with tools in GRPO > New CISPO & SAPO losses + reasoning rewards > vLLM quantization in colocate mode > Dataset shuffling in SFT > Lots of NEW examples > Tons of fixes and documentation improvements See translation", "url": "https://huggingface.co/posts/sergiopaniego/627155943228357", "date_published": "2025-12-12T05:26:57.314495"}, {"id": "https://huggingface.co/posts/ovi054/902660269971069", "image": "", "title": "Z-Image Turbo + LoRA \u26a1", "content_text": "Z-Image Turbo + LoRA \u26a1 ovi054/Z-Image-LORA Z-Image Turbo is the No. 1 trending Text-to-Image model right now. You can add a custom LoRA and generate images with this Space. \ud83d\udc49 Try it now: ovi054/Z-Image-LORA See translation", "url": "https://huggingface.co/posts/ovi054/902660269971069", "date_published": "2025-12-12T05:26:57.314764"}, {"id": "https://huggingface.co/posts/toshas/756417426105326", "image": "", "title": "Introducing \ud83c\udde8\ud83c\uddedWindowSeat\ud83c\udde8\ud83c\udded \u2013\u2013 our new method for removing reflections from photos taken through windows, on planes, in malls, offices, and other glass-filled environments.", "content_text": "Introducing \ud83c\udde8\ud83c\uddedWindowSeat\ud83c\udde8\ud83c\udded \u2013\u2013 our new method for removing reflections from photos taken through windows, on planes, in malls, offices, and other glass-filled environments. Finetuning a foundation diffusion transformer for reflection removal quickly runs up against the limits of what existing datasets and techniques can offer. To fill that gap, we generate physically accurate examples in Blender that simulate realistic glass and reflection effects. This data enables strong performance on both established benchmarks and previously unseen images. To make this practical, the open-source Apache-2 model builds on Qwen-Image-Edit-2509, a 20B image-editing diffusion transformer that runs on a single GPU and can be fine-tuned in about a day. WindowSeat keeps its use of the underlying DiT cleanly separated from the data and training recipe, allowing future advances in base models to be incorporated with minimal friction. Try it out with your own photos in this interactive demo: \ud83e\udd17...", "url": "https://huggingface.co/posts/toshas/756417426105326", "date_published": "2025-12-12T05:26:57.315314"}, {"id": "https://huggingface.co/posts/KingNish/784885155042758", "image": "", "title": "Muon vs MuonClip vs Muon+Adamw", "content_text": "Muon vs MuonClip vs Muon+Adamw Muon has gone from an experiment to a mainstream optimizer, but does it hold up for fine\u2011tuning? We ran head\u2011to\u2011head tests on Qwen3\u20114B (10k+ high\u2011quality instruction rows) to find out. Short story: Pure Muon converged fastest at the start, but its gradient\u2011norm spikes made training unstable. MuonClip (Kimi K2\u2019s clipping) stabilizes long pretraining runs, yet in our small\u2011scale fine\u2011tune it underperformed, lower token accuracy and slower convergence. The winner was the hybrid: Muon for 2D layers + AdamW for 1D layers. It delivered the best balance of stability and final performance and even beat vanilla AdamW. Takeaway: for small-scale fine-tuning, hybrid = practical and reliable. Next Step: scale to larger models/datasets to see if Muon\u2019s spikes become catastrophic or if clipping wins out. Full Blog Link: https://huggingface.co/blog/KingNish/optimizer-part1 See translation", "url": "https://huggingface.co/posts/KingNish/784885155042758", "date_published": "2025-12-12T05:26:57.315733"}, {"id": "https://huggingface.co/posts/angt/186034800220690", "image": "", "title": "installama.sh at the TigerBeetle 1000x World Tour !", "content_text": "installama.sh at the TigerBeetle 1000x World Tour ! Last week I had the chance to give a short talk during the TigerBeetle 1000x World Tour (organized by @ jedisct1 \ud83d\udc4f ) a fantastic event celebrating high-performance engineering and the people who love pushing systems to their limits! In the talk, I focused on the CPU and Linux side of things, with a simple goal in mind: making the installation of llama.cpp instant, automatic, and optimal, no matter your OS or hardware setup. For the curious, here are the links worth checking out: Event page: https://tigerbeetle.com/event/1000x GitHub repo: https://github.com/angt/installama.sh Talk: https://youtu.be/pg5NOeJZf0o?si=9Dkcfi2TqjnT_30e More improvements are coming soon. Stay tuned! See translation", "url": "https://huggingface.co/posts/angt/186034800220690", "date_published": "2025-12-12T05:26:57.316105"}, {"id": "https://huggingface.co/posts/IliaLarchenko/908879505364910", "image": "", "title": "\ud83c\udfc6 BEHAVIOR Challenge 1st Place \u2013 Solution Summary", "content_text": "\ud83c\udfc6 BEHAVIOR Challenge 1st Place \u2013 Solution Summary My team recently won 1st place in the BEHAVIOR Challenge at NeurIPS. The competition focused on training a single policy to complete 50 long-horizon household tasks in simulation. We built an end-to-end policy based on Pi0.5 with a bunch of custom modifications. Everything is open-sourced, and it should be useful for anyone exploring VLAs or adapting them to specific tasks. Key Architecture Changes: - Replaced language model with 50 trainable task embeddings (no text at all) - Correlated noise for Flow Matching: \u03f5 \u223c N(0, 0.5I + 0.5\u03a3) using dataset action covariance - Learnable mixed-layer attention: each action expert layer attends to a trainable mix of all VLM layers - System 2 stage tracking: model predicts task stage, we smooth it with voting and feed it back as context Training: - Multi-sample Flow Matching: 15 FM samples per VLM pass to reduce gradient variance - Delta action space + per-timestamp normalization - FAST auxiliary...", "url": "https://huggingface.co/posts/IliaLarchenko/908879505364910", "date_published": "2025-12-12T05:26:57.316710"}, {"id": "https://huggingface.co/posts/NicoBBQ1/471268005218525", "image": "", "title": "What do you think of my LLM Chat app so far?", "content_text": "What do you think of my LLM Chat app so far? Here are some of the features already included (and more are coming): - Chat with AI models \u2013 Local inference via Ollama - Reasoning support \u2013 View model thinking process (DeepSeek-R1, Qwen-QwQ, etc.) - Vision models \u2013 Analyze images with llava, bakllava, moondream - Image generation \u2013 Local GGUF models with GPU acceleration (CUDA) - Fullscreen images \u2013 Click generated images to view in fullscreen - Image attachments \u2013 File picker or clipboard paste (Ctrl+V) - DeepSearch \u2013 Web search with tool use - Inference Stats \u2013 Token counts, speed, duration (like Ollama verbose) - Regenerate \u2013 Re-run any AI response - Copy \u2013 One-click copy AI responses See translation", "url": "https://huggingface.co/posts/NicoBBQ1/471268005218525", "date_published": "2025-12-12T05:26:57.317082"}]}