{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/762937246285628", "image": "", "title": "12 Types of JEPA", "content_text": "12 Types of JEPA Since Yann LeCun together with Randall Balestriero released a new paper on JEPA (Joint-Embedding Predictive Architecture), laying out its theory and introducing an efficient practical version called LeJEPA, we figured you might need even more JEPA. Here are 7 recent JEPA variants plus 5 iconic ones: 1. LeJEPA \u2192 LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics (2511.08544) Explains a full theory for JEPAs, defining the \u201cideal\u201d JEPA embedding as an isotropic Gaussian, and proposes the SIGReg objective to push JEPA toward this ideal, resulting in practical LeJEPA 2. JEPA-T \u2192 JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation (2510.00974) A text-to-image model that tokenizes images and captions with a joint predictive Transformer, enhances fusion with cross-attention and text embeddings before training loss, and generates images by iteratively denoising visual tokens conditioned on text 3. Text-JEPA \u2192...", "url": "https://huggingface.co/posts/Kseniase/762937246285628", "date_published": "2025-11-18T09:27:19.610015"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/695072229497754", "image": "", "title": "Next level Realism with Qwen Image is now possible after new realism LoRA workflow - Top images are new realism workflow - Bottom ones are older default - Full tutorial published - 4+4 Steps only", "content_text": "Next level Realism with Qwen Image is now possible after new realism LoRA workflow - Top images are new realism workflow - Bottom ones are older default - Full tutorial published - 4+4 Steps only Tutorial of realism : https://youtu.be/XWzZ2wnzNuQ Tutorial of training : https://youtu.be/DPX3eBTuO_Y This is a full comprehensive step-by-step tutorial for how to train Qwen Image models. This tutorial covers how to do LoRA training and full Fine-Tuning / DreamBooth training on Qwen Image models. It covers both the Qwen Image base model and the Qwen Image Edit Plus 2509 model. This tutorial is the product of 21 days of full R&D, costing over $800 in cloud services to find the best configurations for training. Furthermore, we have developed an amazing, ultra-easy-to-use Gradio app to use the legendary Kohya Musubi Tuner trainer with ease. You will be able to train locally on your Windows computer with GPUs with as little as 6 GB of VRAM for both LoRA and Fine-Tuning. See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/695072229497754", "date_published": "2025-11-18T09:27:19.610406"}, {"id": "https://huggingface.co/posts/theainerd/926652286906905", "image": "", "title": "Hindi Speech to Text just crossed 20 million downloads. Grateful for everyone using it.", "content_text": "Hindi Speech to Text just crossed 20 million downloads. Grateful for everyone using it. theainerd/Wav2Vec2-large-xlsr-hindi See translation", "url": "https://huggingface.co/posts/theainerd/926652286906905", "date_published": "2025-11-18T09:27:19.610627"}, {"id": "https://huggingface.co/posts/prithivMLmods/462277719397337", "image": "", "title": "Made a small write up and experimental finetuning guide for MetaCLIP2 for Image Classification on Downstream Tasks. The blog titled", "content_text": "Made a small write up and experimental finetuning guide for MetaCLIP2 for Image Classification on Downstream Tasks. The blog titled Fine Tuning MetaCLIP 2 for Image Classification on Downstream Tasks demonstrates the step by step finetuning using CIFAR10 and is also flexible for adapting to other datasets. For more details, check out the linked blog below. \ud83e\udd17\u2197\ufe0f \u2b9e Blog Article: https://huggingface.co/blog/prithivMLmods/metaclip2-downstream-finetune \u2b9e Demo Space[Zero-Shot Classification]: prithivMLmods/metaclip-2-demo Some other models \u2570\u203a MetaCLIP-2-Cifar10: prithivMLmods/MetaCLIP-2-Cifar10 \u2570\u203a MetaCLIP-2-Age-Range-Estimator: prithivMLmods/MetaCLIP-2-Age-Range-Estimator \u2570\u203a MetaCLIP-2-Gender-Identifier: prithivMLmods/MetaCLIP-2-Gender-Identifier \u2570\u203a MetaCLIP-2-Open-Scene: prithivMLmods/MetaCLIP-2-Open-Scene \u2b9e Collection: https://huggingface.co/collections/prithivMLmods/metaclip2-image-classification-experiments To know more about it, visit the app page or the respective model page! See...", "url": "https://huggingface.co/posts/prithivMLmods/462277719397337", "date_published": "2025-11-18T09:27:19.611041"}, {"id": "https://huggingface.co/posts/ronantakizawa/762624768908636", "image": "", "title": "I built a demo on how to implement Cache-Augmented Generation (CAG) in an LLM and compare its performance gains to RAG (111 stars, 20 forks).", "content_text": "I built a demo on how to implement Cache-Augmented Generation (CAG) in an LLM and compare its performance gains to RAG (111 stars, 20 forks). https://github.com/ronantakizawa/cacheaugmentedgeneration CAG preloads document content into an LLM\u2019s context as a precomputed key-value (KV) cache. This caching eliminates the need for real-time retrieval during inference, reducing token usage by up to 76% while maintaining answer quality. CAG is particularly effective for constrained knowledge bases like internal documentation, FAQs, and customer support systems, where all relevant information can fit within the model's extended context window. #rag #retrievalaugmentedgeneration See translation", "url": "https://huggingface.co/posts/ronantakizawa/762624768908636", "date_published": "2025-11-18T09:27:19.611378"}, {"id": "https://huggingface.co/posts/vikhyatk/769322438249892", "image": "", "title": "Announcing RefCOCO-M, a refreshed RefCOCO with pixel-accurate masks and the problematic prompts removed.", "content_text": "Announcing RefCOCO-M, a refreshed RefCOCO with pixel-accurate masks and the problematic prompts removed. moondream/refcoco-m See translation", "url": "https://huggingface.co/posts/vikhyatk/769322438249892", "date_published": "2025-11-18T09:27:19.611591"}, {"id": "https://huggingface.co/posts/ZennyKenny/159598235519685", "image": "", "title": "\ud83c\udf89 Wow. Congratulations", "content_text": "\ud83c\udf89 Wow. Congratulations @ bfirsh and the Replicate team on the CloudFlare acquisition! \u270c\ufe0f You've really built an incredible ecosystem and product offering and should be super proud. See translation", "url": "https://huggingface.co/posts/ZennyKenny/159598235519685", "date_published": "2025-11-18T09:27:19.611817"}, {"id": "https://huggingface.co/posts/cjerzak/918588861809536", "image": "", "title": ">>> We're writing a new book,  <Planetary Causal Inference>, on how to model counterfactuals at planetary scale by combining satellite imagery + other global data with local studies and RCTs. Forthcoming in 2026+.", "content_text": ">>> We're writing a new book, <Planetary Causal Inference>, on how to model counterfactuals at planetary scale by combining satellite imagery + other global data with local studies and RCTs. Forthcoming in 2026+. >>> Book info: https://planetarycausalinference.org/book-launch >>> All datasets used in the book will be openly available on our lab\u2019s Hugging Face hub: theaidevlab See translation", "url": "https://huggingface.co/posts/cjerzak/918588861809536", "date_published": "2025-11-18T09:27:19.612089"}, {"id": "https://huggingface.co/posts/flozi00/635605102777732", "image": "", "title": "Running large language models efficiently is more than just raw GPU power. The latest guide breaks down the essential math to determine if your LLM workload is compute-bound or memory-bound.", "content_text": "Running large language models efficiently is more than just raw GPU power. The latest guide breaks down the essential math to determine if your LLM workload is compute-bound or memory-bound. We apply these principles to a real-world example: Qwen's 32B parameter model on the new NVIDIA RTX PRO 6000 Blackwell Edition. In this guide, you will learn how to: Calculate your GPU's operational intensity (Ops:Byte Ratio) Determine your model's arithmetic intensity Identify whether your workload is memory-bound or compute-bound Read the full guide here: https://flozi.net/en/guides/ai/llm-inference-math See translation", "url": "https://huggingface.co/posts/flozi00/635605102777732", "date_published": "2025-11-18T09:27:19.612371"}, {"id": "https://huggingface.co/posts/branikita/676100544347751", "image": "", "title": "We tested the mechanical backlash of the Feetech STS3250 servo.", "content_text": "We tested the mechanical backlash of the Feetech STS3250 servo. Using an 86 mm lever arm, we measured a tip displacement of 0.64 mm, which corresponds to an angular backlash of approximately 0.43\u00b0. According to the datasheet, the maximum allowable backlash is 0.5\u00b0, so our measured value of 0.43\u00b0 falls within the specified limit. See translation", "url": "https://huggingface.co/posts/branikita/676100544347751", "date_published": "2025-11-18T09:27:19.612634"}]}