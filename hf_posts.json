{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/prithivMLmods/107899220924462", "image": "", "title": "Introducing the  Super-OCRs Demo, a comparison of state-of-the-art multimodal OCR VLMs, including HunyuanOCR, DeepSeekOCR, Dots, and Nanonets in one space for performing OCR, rendering LaTeX and Markdown, and visual grounding (layout). Find the related Spaces and models below.\ud83e\udd17\ud83d\udd25", "content_text": "Introducing the Super-OCRs Demo, a comparison of state-of-the-art multimodal OCR VLMs, including HunyuanOCR, DeepSeekOCR, Dots, and Nanonets in one space for performing OCR, rendering LaTeX and Markdown, and visual grounding (layout). Find the related Spaces and models below.\ud83e\udd17\ud83d\udd25 \u2728Super-OCRs[Demo]: prithivMLmods/Super-OCRs-Demo \u2728Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations \u2728GitHub: https://github.com/PRITHIVSAKTHIUR/Super-OCRs-Demo \u2b50 Models Used: \u2726 HunyuanOCR: tencent/HunyuanOCR \u2726 DeepSeek-OCR: (-) deepseek-ai/DeepSeek-OCR (+) prithivMLmods/DeepSeek-OCR-Latest-BF16.I64 \u2726 Dots.OCR: (-) rednote-hilab/dots.ocr (+) prithivMLmods/Dots.OCR-Latest-BF16 \u2726 Nanonets-OCR2-3B: nanonets/Nanonets-OCR2-3B \u2b50 Some Other Relevant Apps: \u2726 Qwen3-VL-HF-Demo: prithivMLmods/Qwen3-VL-HF-Demo \u2726 Qwen3-VL-Outpost: prithivMLmods/Qwen3-VL-Outpost \u2726 Multimodal-OCR: prithivMLmods/Multimodal-OCR \u2726 Multimodal-OCR2: prithivMLmods/Multimodal-OCR2 \u2726 Multimodal-OCR3:...", "url": "https://huggingface.co/posts/prithivMLmods/107899220924462", "date_published": "2025-11-28T17:21:35.164941"}, {"id": "https://huggingface.co/posts/samerzaher80/506428935925502", "image": "", "title": "AetherMind_SRL: How I beat 7B models on MMLU with 184M params and a $300 GPU", "content_text": "AetherMind_SRL: How I beat 7B models on MMLU with 184M params and a $300 GPU I\u2019m Sameer, a solo researcher from Iraq working on a single RTX 3050 8GB laptop.Today I\u2019m releasing AetherMind_SRL \u2013 a 184M-parameter NLI model that was trained only on tasks (SNLI, MNLI, ANLI, and a small clinical Alzheimer\u2019s dataset). It was never fine-tuned or even shown a single MMLU question during training.Yet here are the zero-shot MMLU (57 subjects) results:Model MMLU Zero-Shot Training Data AetherMind_SRL (me) 184M 36.05 % Only NLI (SNLI/MNLI/ANLI + ADNI) DeBERTa-v3-base 278M ~30.8 % General pre-training BERT-large 340M 27\u201330 % General pre-training LLaMA-1 7B 7B 34\u201335 % Massive text corpus LLaMA-2 7B 7B ~45 % Bigger + better data Yes \u2013 my 184M model beats every classic 300\u2013400M model and the original 7-billion-parameter LLaMA-1, all while running at 300+ samples/sec on a $300 laptop GPU.How did this happen?I built a standardized self-improvement loop called AetherMind Self-Reflective Learning (SRL)...", "url": "https://huggingface.co/posts/samerzaher80/506428935925502", "date_published": "2025-11-28T17:21:35.165583"}, {"id": "https://huggingface.co/posts/flozi00/715911271022864", "image": "", "title": "When models get too large for a single GPU, simply stacking layers vertically (Pipeline Parallelism) isn't always the answer. Sometimes, you need to slice the matrices themselves.", "content_text": "When models get too large for a single GPU, simply stacking layers vertically (Pipeline Parallelism) isn't always the answer. Sometimes, you need to slice the matrices themselves. My latest guide breaks down the hardware mechanics of Tensor Parallelism (TP). We look at how to shard individual operations across devices to make a cluster function as one massive accelerator. This isn't high-level theory\u2014it is a look at the bare metal implementation. Here is what is covered in the deep dive: The Strategies: Column vs. Row Parallelism We analyze how to split weight matrices (W) and inputs (X). Column-Linear: Splits weights by columns. Requires an All-Gather to reconstruct the output. Row-Linear: Splits weights by rows. Requires an All-Reduce to sum partial results. The \"Megatron-LM\" Optimization Efficiency comes from minimizing communication. By sandwiching the non-linearity (GeLU) between a Column-Parallel layer and a Row-Parallel layer, we can skip synchronization entirely during the...", "url": "https://huggingface.co/posts/flozi00/715911271022864", "date_published": "2025-11-28T17:21:35.166093"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/938257136422472", "image": "", "title": "FLUX 2 vs FLUX SRPO, New FLUX Training Kohya SS GUI Premium App With Presets & Features :", "content_text": "FLUX 2 vs FLUX SRPO, New FLUX Training Kohya SS GUI Premium App With Presets & Features : https://youtu.be/RQHmyJVOHXo FLUX 2 has been published and I have compared it to the very best FLUX base model known as FLUX SRPO. Moreover, we have updated our FLUX Training APP and presets to the next level. Massive speed up gaings with 0 quality loss and lots of new features. I will show all of the new features we have with new SECourses Kohya SS GUI Premium app and compare FLUX SRPO trained model results with FLUX 2. https://youtu.be/RQHmyJVOHXo Get the SECourses Premium Kohya Trainer DreamBooth / Fine Tuning : [ https://www.patreon.com/posts/Kohya-FLUX-DreamBooth-Trainer-App-112099700 ] Get the SECourses Premium Kohya Trainer LoRA : [ https://www.patreon.com/posts/Kohya-FLUX-LoRA-Trainer-App-110879657 ] DreamBooth Training Tutorial: [ https://www.youtube.com/watch?v=FvpWy1x5etM ] LoRA Training Tutorial: [ https://www.youtube.com/watch?v=nySGu12Y05k ] Qwen Image Realism Tutorial: [...", "url": "https://huggingface.co/posts/MonsterMMORPG/938257136422472", "date_published": "2025-11-28T17:21:35.166649"}, {"id": "https://huggingface.co/posts/Nymbo/982136402285178", "image": "", "title": "\ud83d\ude80 I've just shipped a major update to the", "content_text": "\ud83d\ude80 I've just shipped a major update to the Nymbo/Tools MCP server: the Agent_Terminal , a single \"master tool\" that cuts token usage by over 90%! Anthropic found 98.7% context savings using code execution with MCP, Cloudflare published similar findings. This is my open-source implementation of the same idea. # The Problem Traditional MCP exposes every tool definition directly to the model. With 12 tools, that's thousands of tokens consumed *before the conversation even starts*. Each tool call also passes intermediate results through the context window \u2014 a 10,000-row spreadsheet? That's all going into context just to sum a column. # The Solution: One Tool to Rule Them All Agent_Terminal wraps all 12 tools ( Web_Search , Web_Fetch , File_System , Generate_Image , Generate_Speech , Generate_Video , Deep_Research , Memory_Manager , Obsidian_Vault , Shell_Command , Code_Interpreter ) into a single Python code execution gateway. Instead of the model making individual tool calls, it writes...", "url": "https://huggingface.co/posts/Nymbo/982136402285178", "date_published": "2025-11-28T17:21:35.167154"}, {"id": "https://huggingface.co/posts/Babsie/682016122905240", "image": "", "title": "Accidentally made a totally blank, non-functioning HF Space while poking \u201cwhat does this do\u201d buttons without reading anything.", "content_text": "Accidentally made a totally blank, non-functioning HF Space while poking \u201cwhat does this do\u201d buttons without reading anything. I\u2019d already named it its-totally-supposed-to-do-that . It is, in fact, totally supposed to do that. Trickster QA passed. See translation", "url": "https://huggingface.co/posts/Babsie/682016122905240", "date_published": "2025-11-28T17:21:35.167425"}, {"id": "https://huggingface.co/posts/sergiopaniego/367599205240435", "image": "", "title": "nanochat is now in transformers!", "content_text": "nanochat is now in transformers! The LLM by @ karpathy is officially in the library, and we wrote a blog covering: how did we port the model, differences from the original, and how to run or train it. go read it \ud83e\udd13 nanochat-students/transformers See translation", "url": "https://huggingface.co/posts/sergiopaniego/367599205240435", "date_published": "2025-11-28T17:21:35.167650"}, {"id": "https://huggingface.co/posts/anakin87/824755368598190", "image": "", "title": "I made a visualization based on the Prime Intellect INTELLECT-3 technical report.", "content_text": "I made a visualization based on the Prime Intellect INTELLECT-3 technical report. Wild to see how far they pushed GLM-4.5-Air-Base with SFT + RL. SOTA for its size and competitive with models 3x larger. All open. Congrats on the release! Model: PrimeIntellect/INTELLECT-3 Technical report: https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf Chat: https://chat.primeintellect.ai/ See translation", "url": "https://huggingface.co/posts/anakin87/824755368598190", "date_published": "2025-11-28T17:21:35.167878"}, {"id": "https://huggingface.co/posts/wang12390/296088170632926", "image": "", "title": "How To Use Miragic V1.1 Image Generator for Marketing.", "content_text": "How To Use Miragic V1.1 Image Generator for Marketing. Hello, I was trying to generate face recognition video for social marketing as our company is biometrics company, so I was in need of image materials, used some services for image generation but this Miragic V1.1 Image Generator was best. If you are also interested, please visit https://miragic.ai/products/image-generator See translation", "url": "https://huggingface.co/posts/wang12390/296088170632926", "date_published": "2025-11-28T17:21:35.168096"}, {"id": "https://huggingface.co/posts/mrmanna/120434278383706", "image": "", "title": "\ud835\uddd7\ud835\uddd7\ud835\udde6\ud835\uddd8 \ud835\uddd9\ud835\uddfc\ud835\ude02\ud835\uddfb\ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddfd\ud835\ude02\ud835\uddef\ud835\uddf9\ud835\uddf6\ud835\ude00\ud835\uddf5\ud835\uddf2\ud835\ude00 \ud835\uddd6\ud835\uddd8\ud835\uddd9 \u2014 \ud835\uddee\ud835\uddfb \ud835\udde2\ud835\udde5\ud835\udde0 \ud835\uddf3\ud835\uddfc\ud835\uddff \ud835\uddd6\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\ude05\ud835\ude01 \ud835\uddd8\ud835\uddfb\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf6\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde6\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00", "content_text": "\ud835\uddd7\ud835\uddd7\ud835\udde6\ud835\uddd8 \ud835\uddd9\ud835\uddfc\ud835\ude02\ud835\uddfb\ud835\uddf1\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb \ud835\uddfd\ud835\ude02\ud835\uddef\ud835\uddf9\ud835\uddf6\ud835\ude00\ud835\uddf5\ud835\uddf2\ud835\ude00 \ud835\uddd6\ud835\uddd8\ud835\uddd9 \u2014 \ud835\uddee\ud835\uddfb \ud835\udde2\ud835\udde5\ud835\udde0 \ud835\uddf3\ud835\uddfc\ud835\uddff \ud835\uddd6\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\ude05\ud835\ude01 \ud835\uddd8\ud835\uddfb\ud835\uddf4\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\uddf2\ud835\uddff\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\uddf6\ud835\uddfb \ud835\udddf\ud835\udddf\ud835\udde0 \ud835\udde6\ud835\ude06\ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfa\ud835\ude00 Just as Hibernate abstracts databases for transactions, CEF abstracts knowledge stores for Context Engineering. Build, test, and benchmark intelligent context models in minutes, without the complexity of enterprise graph infrastructure. https://github.com/ddse-foundation/cef https://ddse-foundation.github.io/cef/ See translation", "url": "https://huggingface.co/posts/mrmanna/120434278383706", "date_published": "2025-11-28T17:21:35.168357"}]}