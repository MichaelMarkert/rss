{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/sagar007/387080486731099", "image": "", "title": "\ud83d\ude80 I built a Multimodal Vision-Language Model from using Gemma-270M + CLIP!", "content_text": "\ud83d\ude80 I built a Multimodal Vision-Language Model from using Gemma-270M + CLIP! Just finished training my multimodal model on the full LLaVA-Instruct-150K dataset (157K samples) and wanted to share the results! \ud83d\udd27 What I Built: A vision-language model that can understand images and answer questions about them, combining: - Google Gemma-3-270M (language) - OpenAI CLIP ViT-Large/14 (vision) - LoRA fine-tuning for efficiency \ud83d\udcca Training Stats: - 157,712 training samples (full LLaVA dataset) - 3 epochs on A100 40GB - ~9 hours training time - Final loss: 1.333 training / 1.430 validation - Only 18.6M trainable params (3.4% of 539M total) \ud83d\udcc8 sagar007/multigemma Benchmark Results: - VQA Accuracy: 53.8% - Works great for: animal detection, room identification, scene understanding \ud83d\udd17 **Try it yourself:** - \ud83e\udd17 Model: sagar007/multigemma - \ud83c\udfae Demo: https://huggingface.co/spaces/sagar007/Multimodal-Gemma - \ud83d\udcbb GitHub: https://github.com/sagar431/multimodal-gemma-270m Built with PyTorch Lightning + MLflow...", "url": "https://huggingface.co/posts/sagar007/387080486731099", "date_published": "2026-01-19T09:39:55.602003"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/425042812655046", "image": "", "title": "Compared Quality and Speed Difference (with CUDA 13 & Sage Attention) of BF16 vs GGUF Q8 vs FP8 Scaled vs NVFP4 for Z Image Turbo, FLUX Dev, FLUX SRPO, FLUX Kontext, FLUX 2 - Full 4K step by step tutorial also published", "content_text": "Compared Quality and Speed Difference (with CUDA 13 & Sage Attention) of BF16 vs GGUF Q8 vs FP8 Scaled vs NVFP4 for Z Image Turbo, FLUX Dev, FLUX SRPO, FLUX Kontext, FLUX 2 - Full 4K step by step tutorial also published Full 4K tutorial : https://youtu.be/XDzspWgnzxI Check above full 4K tutorial to learn more and see uncompressed original quality and size images It was always wondered how much quality and speed difference exists between BF16, GGUF, FP8 Scaled and NVFP4 precisions. In this tutorial I have compared all these precision and quantization variants for both speed and quality. The results are pretty surprising. Moreover, we have developed and published NVFP4 model quant generator app and FP8 Scaled quant generator apps. The links of the apps are below if you want to use them. Furthermore, upgrading ComfyUI to CUDA 13 with properly compiled libraries is now very much recommended. We have observed some noticeable performance gains with CUDA 13. So for both SwarmUI and ComfyUI...", "url": "https://huggingface.co/posts/MonsterMMORPG/425042812655046", "date_published": "2026-01-19T09:39:55.602450"}, {"id": "https://huggingface.co/posts/Sunny111/812628018751645", "image": "", "title": "Are you familiar with reverse residual connections or looping in language models?", "content_text": "Are you familiar with reverse residual connections or looping in language models? Excited to share my Looped-GPT blog post and codebase \ud83d\ude80 https://github.com/sanyalsunny111/Looped-GPT TL;DR: looping during pre-training improves generalization. Plot shows GPT2 LMs pre-trained with 15.73B OWT tokens P.S. This is my first post here \u2014 I have ~4 followers and zero expectations for reach \ud83d\ude04 See translation", "url": "https://huggingface.co/posts/Sunny111/812628018751645", "date_published": "2026-01-19T09:39:55.602755"}, {"id": "https://huggingface.co/posts/davidmezzetti/675473701665593", "image": "", "title": "Tired of paying a bunch of money for large LLMs? Then read this notebook and/or watch this video on how to distill knowledge into tiny LLMs.", "content_text": "Tired of paying a bunch of money for large LLMs? Then read this notebook and/or watch this video on how to distill knowledge into tiny LLMs. Notebook: https://github.com/neuml/txtai/blob/master/examples/80_Distilling_Knowledge_into_Tiny_LLMs.ipynb Video: https://www.youtube.com/watch?v=Ol560ktgkf0 See translation", "url": "https://huggingface.co/posts/davidmezzetti/675473701665593", "date_published": "2026-01-19T09:39:55.602993"}, {"id": "https://huggingface.co/posts/unmodeled-tyler/270028211989897", "image": "", "title": "NEW MODEL:", "content_text": "NEW MODEL: vanta-research/mox-tiny-1 Excited to share the first model from the new Mox-1 model family from VANTA Research! Mox-Tiny-1 is finetuned from Llama's 3.1 8B architecture specifically for thinking partnership and high-fidelity AI interaction. Training domains were carefully selected with this goal in mind, and all datasets used were synthetically generated for the Mox-1 models specifically. Domains include but not limited to: - Direct opinions (clear stances on various topics) - Constructive disagreement (Pushing back on flawed premises) - Epistemic confidence (calibrated certainty and uncertainty) - Warmth & presence (Empathetic, human responses) - Wonder & puzzlement (Intellectual curiosity) Interaction Examples: Direct opinions: User: What 's the best programming language? Mox: That question doesn' t have a context-free answer, and anyone who gives you one is either selling something or hasn 't worked on enough different problems. But I' ll give you my actual take across...", "url": "https://huggingface.co/posts/unmodeled-tyler/270028211989897", "date_published": "2026-01-19T09:39:55.603495"}, {"id": "https://huggingface.co/posts/efecelik/312692243758686", "image": "", "title": "My First MCP Server: DataView", "content_text": "My First MCP Server: DataView Browse HuggingFace datasets directly from your AI assistant. -Search & filter datasets -View rows & stats -SQL queries & Parquet export efecelik/dataview-mcp See translation", "url": "https://huggingface.co/posts/efecelik/312692243758686", "date_published": "2026-01-19T09:39:55.603717"}, {"id": "https://huggingface.co/posts/kanaria007/501473502590930", "image": "", "title": "\u2705 New Article: *Observations, Under-Observation, and Repair Loops* (v0.1)", "content_text": "\u2705 New Article: *Observations, Under-Observation, and Repair Loops* (v0.1) Title: \ud83d\udc41\ufe0f Observations, Under-Observation, and Repair Loops: The OBS Cookbook for SI-Core \ud83d\udd17 https://huggingface.co/blog/kanaria007/observations-under-observation --- Summary: SI-Core\u2019s rule is simple: *No effectful Jump without PARSED observations.* This article turns that slogan into an operational design: define *observation units* (sem_type/scope/status/confidence/backing_refs), detect *under-observation* (missing / degraded / biased), and run *repair loops* instead of \u201cjumping in the dark.\u201d Key clarification: under-observed conditions may still run *read / eval_pre / jump-sandbox*, but must not commit or publish (sandbox: publish_result=false , memory_writes=disabled ). --- Why It Matters: \u2022 Prevents \u201cwe had logs, so we had context\u201d failures: *logs \u2260 observations* unless typed + contract-checked \u2022 Makes safety real: even PARSED observations should be gated by *coverage/confidence minima* (declared...", "url": "https://huggingface.co/posts/kanaria007/501473502590930", "date_published": "2026-01-19T09:39:55.604367"}, {"id": "https://huggingface.co/posts/rajkumarrawal/757354629447713", "image": "", "title": "\"Recursive Language Models\" Paper become \"3rd Paper of the day\" on Hugging Face", "content_text": "\"Recursive Language Models\" Paper become \"3rd Paper of the day\" on Hugging Face Recursive Language Models (2512.24601) See translation", "url": "https://huggingface.co/posts/rajkumarrawal/757354629447713", "date_published": "2026-01-19T09:39:55.604581"}, {"id": "https://huggingface.co/posts/AdinaY/484665846337512", "image": "", "title": "After a VLM, StepFun dropped a new audio model: Step-Audio-R1.1, enabling thinking while speaking \ud83d\udd25", "content_text": "After a VLM, StepFun dropped a new audio model: Step-Audio-R1.1, enabling thinking while speaking \ud83d\udd25 stepfun-ai/Step-Audio-R1.1 \u2728 Apache 2.0 \u2728 Combines dual-brain architecture and acoustic-grounded reasoning to enable real-time dialogue with SOTA-level reasoning See translation", "url": "https://huggingface.co/posts/AdinaY/484665846337512", "date_published": "2026-01-19T09:39:55.604828"}, {"id": "https://huggingface.co/posts/danielhanchen/641905091288769", "image": "", "title": "You can now do reinforcement learning training with 7\u00d7 longer context and no accuracy loss, via our new batching algorithms.", "content_text": "You can now do reinforcement learning training with 7\u00d7 longer context and no accuracy loss, via our new batching algorithms. Long reasoning chains in RL are costly, but now we enable you to train gpt-oss with GRPO & reach 380K context on a 192GB GPU. Blog: https://unsloth.ai/docs/new/grpo-long-context See translation", "url": "https://huggingface.co/posts/danielhanchen/641905091288769", "date_published": "2026-01-19T09:39:55.605120"}]}