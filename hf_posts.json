{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/SeaWolf-AI/974248994007181", "image": "", "title": "FINAL Bench Released: The Real Bottleneck to AGI Is Self-Correction", "content_text": "FINAL Bench Released: The Real Bottleneck to AGI Is Self-Correction We release FINAL Bench, the first benchmark for measuring functional metacognition in LLMs \u2014 the ability to detect and correct one's own reasoning errors. Every existing benchmark measures final-answer accuracy. None measures whether AI knows it is wrong. Dataset: [FINAL-Bench/Metacognitive]( FINAL-Bench/Metacognitive ) | 100 Tasks | 15 Domains | 8 TICOS Types | Apache 2.0 Leaderboard: FINAL-Bench/Leaderboard Article: https://huggingface.co/blog/FINAL-Bench/metacognitive Core Innovation Our 5-axis rubric separates what no prior benchmark could: MA (Metacognitive Accuracy) \u2014 the ability to say \"I might be wrong\", and ER (Error Recovery) \u2014 the ability to actually fix it. This maps directly to the monitoring-control model of Nelson & Narens (1990) in cognitive psychology. Three Findings Across 9 SOTA Models We evaluated GPT-5.2, Claude Opus 4.6, Gemini 3 Pro, DeepSeek-V3.2, Kimi K2.5, and others across 100 expert-level...", "url": "https://huggingface.co/posts/SeaWolf-AI/974248994007181", "date_published": "2026-02-21T17:29:18.780064"}, {"id": "https://huggingface.co/posts/kostakoff/295634314130096", "image": "", "title": "I found it very funny that the Hugging Face profile has a specific section where we can share our hardware.", "content_text": "I found it very funny that the Hugging Face profile has a specific section where we can share our hardware. It really brings back memories of the good old days when we used to flex our custom PC specs on enthusiast forums 20 years ago! That inspired me to fill out my own profile and share it here. And this is my first set of GPUs that I am using to learn MLOps: - RTX 3090 \u2013 the best one; unfortunately it doesn't support the latest FP8 and FP4, but it\u2019s still very powerful. - Tesla V100 \u2013 performance is almost like the RTX 3090, just much older. - Tesla P100 \u2013 old, and doesn't have tensor cores, but still can handle small models. - Radeon MI50 \u2013 old, similar to the P100, but uses ROCm instead of CUDA, which is actually a pretty good experience to setup. - GTX 1080 Ti \u2013 mostly useless, no FP16 support. - GTX 1660 \u2013 first generation of the Turing architecture, but mostly useless. llmlaba See translation", "url": "https://huggingface.co/posts/kostakoff/295634314130096", "date_published": "2026-02-21T17:29:18.780568"}, {"id": "https://huggingface.co/posts/danielhanchen/880887158811597", "image": "", "title": "We collabed with HF on showing how you can use HF Jobs and Unsloth!", "content_text": "We collabed with HF on showing how you can use HF Jobs and Unsloth! https://huggingface.co/blog/unsloth-jobs See translation", "url": "https://huggingface.co/posts/danielhanchen/880887158811597", "date_published": "2026-02-21T17:29:18.780783"}, {"id": "https://huggingface.co/posts/qgallouedec/326351655871382", "image": "", "title": "@CohereLabs", "content_text": "@ CohereLabs just released \ud83c\udf3f Tiny Aya: a fully open-source 3B parameter model that speaks 70+ languages \ud83c\udf0d! But there\u2019s a catch: Tiny Aya is just a language model. It doesn\u2019t support tool calling, the key capability that turns frontier models into powerful *agents*. So the real question is: How hard is it to turn Tiny Aya into an agent? Turns out\u2026 it\u2019s simple, thanks to Hugging Face TRL. We\u2019re sharing a hands-on example showing how to train Tiny Aya to turn it into a tool-calling agent using TRL, unlocking what could become the first *massively multilingual open agent*. Small model. Global reach. Agent capabilities. \ud83d\udc49 https://github.com/huggingface/trl/blob/main/examples/notebooks/sft_tool_calling.ipynb See translation", "url": "https://huggingface.co/posts/qgallouedec/326351655871382", "date_published": "2026-02-21T17:29:18.781195"}, {"id": "https://huggingface.co/posts/DavidAU/794418191907672", "image": "", "title": "The \"ERNIE\" 21B MOE Distill High Reasoning Fine Tune Invasion:", "content_text": "The \"ERNIE\" 21B MOE Distill High Reasoning Fine Tune Invasion: 3 Ernie 21B-A3B MOE Models (64 experts) fine tuned with Unsloth using Gemini Pro 3, Claude 4.5 Opus, and GLM 4.7 Flash high reasoning datasets. All benched, all exceeding org model specs too. https://huggingface.co/DavidAU/models?search=ernie Enjoy the freedom and added power. See translation", "url": "https://huggingface.co/posts/DavidAU/794418191907672", "date_published": "2026-02-21T17:29:18.781462"}, {"id": "https://huggingface.co/posts/prithivMLmods/100967030465591", "image": "", "title": "Try the demo for Qwen3-VL-abliterated-MAX-Fast:", "content_text": "Try the demo for Qwen3-VL-abliterated-MAX-Fast: prithivMLmods/Qwen3-VL-abliterated-MAX-Fast (Unredacted: Ask Anything with Near-Zero Refusal Rates). The full model series collection is available here: https://huggingface.co/collections/prithivMLmods/unredacted-max-vl See translation", "url": "https://huggingface.co/posts/prithivMLmods/100967030465591", "date_published": "2026-02-21T17:29:18.781712"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/571719932509243", "image": "", "title": "SECourses Ultimate Video and Image Upscaler Pro is now V2.1 and massive improvements has arrived", "content_text": "SECourses Ultimate Video and Image Upscaler Pro is now V2.1 and massive improvements has arrived Check all below screenshots to see all amazing features 20 Feburary 2026 Update V2.1 This is a pretty big update We have 100% changed the FlashVSR+ backend to a new repo and I have significantly upgraded this repo The new FlashVSR+ works amazing and I think it is better than SeedVR2 for high res videos upscale like upscaling 720p into higher resolution Top menu navigation bar updated into a better version and view FlashVSR+ tab remade and all the features are now working For lower VRAM a button is added which you can use if you get OOM Read the updated UI to understand how to use FlashVSR+ now can upscale images very well as well Image Based GAN upscalers tab also improved and some bugs fixed Output & Comparison tab Video Output was not working properly and this issue fix fixed In Output & Comparison tab, new multi video and multi image comparison sliders added which is super useful to...", "url": "https://huggingface.co/posts/MonsterMMORPG/571719932509243", "date_published": "2026-02-21T17:29:18.782188"}, {"id": "https://huggingface.co/posts/sergiopaniego/682471247625526", "image": "", "title": "Tiny Aya \ud83c\udf3f just dropped from", "content_text": "Tiny Aya \ud83c\udf3f just dropped from @ CohereLabs , a really powerful multilingual small model! To celebrate, we cooked up fresh resources to train it for tool calling \ud83d\udd27 > Free Google Colab guide: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_tool_calling.ipynb > Standalone training script: https://github.com/huggingface/trl/blob/main/examples/scripts/sft_tiny_aya_tool_calling.py See translation", "url": "https://huggingface.co/posts/sergiopaniego/682471247625526", "date_published": "2026-02-21T17:29:18.782478"}, {"id": "https://huggingface.co/posts/umarbutler/969492302070598", "image": "", "title": "@abdurrahmanbutler", "content_text": "@ abdurrahmanbutler and I just dropped Legal RAG Bench, the first benchmark for legal RAG systems to simultaneously evaluate hallucinations, retrieval failures, and reasoning errors. Our key takeaways are: 1. Embedding models, not generative models, are the primary driver of RAG accuracy. Switching from a general-purpose embedder like OpenAI's Text Embedding 3 Large to a legal domain embedder like Isaacus' Kanon 2 Embedder can raise accuracy by ~19 points. 2. Hallucinations are often triggered by retrieval failures. Fix your retrieval stack, and, in most cases, you end up fixing hallucinations. 3. Once you have a solid legal retrieval engine like Kanon 2 Embedder, it doesn\u2019t matter as much what generative model you use; GPT-5.2 and Gemini 3.1 Pro perform relatively similarly, with Gemini 3.1 Pro achieving slightly better accuracy at the cost of more hallucinations. 4. Google's latest LLM, Gemini 3.1 Pro, is actually a bit worse than its predecessor at legal RAG, achieving 79.3%...", "url": "https://huggingface.co/posts/umarbutler/969492302070598", "date_published": "2026-02-21T17:29:18.783124"}, {"id": "https://huggingface.co/posts/fabiosuizu/561306920677381", "image": "", "title": "Hi everyone!", "content_text": "Hi everyone! I've been working on a pronunciation assessment engine optimized for edge deployment and real-time feedback. Wanted to share it with the community and get feedback. **What it does**: Scores English pronunciation at 4 levels of granularity \u2014 phoneme, word, sentence, and overall (0-100 each). Returns IPA and ARPAbet notation for every phoneme. **Key specs**: - 17MB total model size (NeMo Citrinet-256, INT4 quantized) - 257ms median inference on CPU - Exceeds human inter-annotator agreement at phone-level (+4.5%) and sentence-level (+5.2%) - Benchmarked on speechocean762 (2,500 test utterances) - Tested across 7 L1 backgrounds (Chinese, Japanese, Korean, Arabic, Spanish, Vietnamese, Russian) **Architecture**: CTC forced alignment + Viterbi decoding + GOP (Goodness of Pronunciation) scoring + MLP/XGBoost ensemble heads. No wav2vec2 dependency \u2014 the entire pipeline runs in 17MB. **Try it**: fabiosuizu/pronunciation-assessment The demo lets you record audio or upload a file,...", "url": "https://huggingface.co/posts/fabiosuizu/561306920677381", "date_published": "2026-02-21T17:29:18.783670"}]}