{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/384482543815919", "image": "", "title": "13 New types of LoRA", "content_text": "13 New types of LoRA LoRA (Low-Rank Adaptation) is a popular lightweight method for fine-tuning AI models. It doesn't update the full model, it adds small trainable components, low-rank matrices, while keeping the original weights frozen. Only these adapters are trained. Recently, many interesting new LoRA variations came out, so it\u2019s a great time to take a look at these 13 clever approaches: 1. T-LoRA \u2192 T-LoRA: Single Image Diffusion Model Customization Without Overfitting (2507.05964) A timestep-dependent LoRA method for adapting diffusion models with a single image. It dynamically adjusts updates and uses orthogonal initialization to reduce overlap, achieving better fidelity\u2013alignment balance than standard LoRA 2. SingLoRA \u2192 SingLoRA: Low Rank Adaptation Using a Single Matrix (2507.05566) Simplifies LoRA by using only one small matrix instead of usual two, and multiplying it by its own transpose (like A \u00d7 A\u1d40). It uses half the parameters of LoRA and avoids scale mismatch between...", "url": "https://huggingface.co/posts/Kseniase/384482543815919", "date_published": "2025-07-15T13:39:45.638771"}, {"id": "https://huggingface.co/posts/prithivMLmods/700925755780035", "image": "", "title": "Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. \ud83e\udd17\ud83d\udcc4", "content_text": "Excited to bring the new models that are performing exceptionally well in document OCR, image captioning, and visual understanding tasks. Megalodon-OCR and Perseus-Doc-VL have both demonstrated significant improvements across key areas. You can explore live demos on Hugging Face Spaces to compare their performance with other top-tier models available on the hub. \ud83e\udd17\ud83d\udcc4 Spaces & Models : > Doc-VLMs-OCR : prithivMLmods/Doc-VLMs-OCR > core-OCR : prithivMLmods/core-OCR > Megalodon-OCR (3B) : prithivMLmods/Megalodon-OCR-Sync-0713 > Perseus-Doc-vl (7B): prithivMLmods/Perseus-Doc-vl-0712 Datasets Caption Mix : > Corvus-OCR-Caption-Mix : prithivMLmods/Corvus-OCR-Caption-Mix > Corvus-OCR-Caption-Mini-Mix : prithivMLmods/Corvus-OCR-Caption-Mini-Mix Collections : > Corvus OCR Caption Mix: prithivMLmods/corvus-ocr-caption-mix-687349bfaceffbd10976f0cc > Captioning / OCR / DocTable : prithivMLmods/captioning-ocr-doctable-687382e1da822008bb5c06f2 GitHub : > OCR-ReportLab :...", "url": "https://huggingface.co/posts/prithivMLmods/700925755780035", "date_published": "2025-07-15T13:39:45.639377"}, {"id": "https://huggingface.co/posts/Tonic/414083244384754", "image": "", "title": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f Normalize adding compute & runtime traces to your model cards", "content_text": "\ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f Normalize adding compute & runtime traces to your model cards See translation", "url": "https://huggingface.co/posts/Tonic/414083244384754", "date_published": "2025-07-15T13:39:45.639595"}, {"id": "https://huggingface.co/posts/danielhanchen/383539577783045", "image": "", "title": "Made some 245GB (80% size reduction) 1.8bit quants for Kimi K2!", "content_text": "Made some 245GB (80% size reduction) 1.8bit quants for Kimi K2! unsloth/Kimi-K2-Instruct-GGUF See translation", "url": "https://huggingface.co/posts/danielhanchen/383539577783045", "date_published": "2025-07-15T13:39:45.639801"}, {"id": "https://huggingface.co/posts/jasoncorkill/847126227827487", "image": "", "title": "\"Why did the bee get married?\"", "content_text": "\"Why did the bee get married?\" \"Because he found his honey!\" This was the \"funniest\" joke out of 10'000 jokes we generated with LLMs. With 68% of respondents rating it as \"funny\". Original jokes are particularly hard for LLMs, as jokes are very nuanced and a lot of context is needed to understand if something is \"funny\". Something that can only reliably be measured using humans. LLMs are not equally good at generating jokes in every language. Generated English jokes turned out to be way funnier than the Japanese ones. 46% of English-speaking voters on average found the generated joke funny. The same statistic for other languages: Vietnamese: 44% Portuguese: 40% Arabic: 37% Japanese: 28% There is not much variance in generation quality among models for any fixed language. But still Claude Sonnet 4 slightly outperforms others in Vietnamese, Arabic and Japanese and Gemini 2.5 Flash in Portuguese and English We have release the 1 Million (!) native speaker ratings and the 10'000 jokes...", "url": "https://huggingface.co/posts/jasoncorkill/847126227827487", "date_published": "2025-07-15T13:39:45.640196"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/139363304280237", "image": "", "title": "MultiTalk Levelled Up - Way Better Animation Compared to Before with New Workflows - Image to Video >", "content_text": "MultiTalk Levelled Up - Way Better Animation Compared to Before with New Workflows - Image to Video > https://youtu.be/wgCtUeog41g MultiTalk is greatly upgraded. After doing more than 1 day more research with MultiTalk by using 8x A6000 48 GB GPUs, I have significantly improved the MultiTalk workflows and now I am sharing 4 different category workflows with you. VRAM usages and speeds are same but just better quality and animation. Moreover I am introducing a new app which is image and video comparison sliders. Ultra fast and lightweight. Runs as a html app and no GPU is required. https://youtu.be/wgCtUeog41g MultiTalk Full Tutorial With 1-Click Installer - Make Talking and Singing Videos From Static Images > https://youtu.be/8cMIwS9qo4M By using MeiGen MultiTalk you can generate amazing fully animated real-like videos from given audio input. Not only talking but also animating the body movements is possible. In this video I will show you how to install ComfyUI on Windows and...", "url": "https://huggingface.co/posts/MonsterMMORPG/139363304280237", "date_published": "2025-07-15T13:39:45.640623"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/377698831818015", "image": "", "title": "\ud83d\udce2 Generate your own data in simulation using two new free and customizable data-generating Scenarios on Duality's FalconCloud service.", "content_text": "\ud83d\udce2 Generate your own data in simulation using two new free and customizable data-generating Scenarios on Duality's FalconCloud service. \ud83d\ude4c These multi-class Scenarios are designed to target model weaknesses for our recent Kaggle competition, but they are free to anyone for non-commercial use! \ud83d\udcf8 Control object and camera posing \ud83d\udc49 Select random variable ranges \ud83d\uddbc\ufe0f Set post-processing effects \u2795 and more to create a robust dataset for strong model training. Access the 2 Scenarios here: \ud83d\udca0 https://falcon.duality.ai/secure/scenarios/edit/9e90e036-8af9-41e4-8af0-1343b8e8f467?utm_source=Kaggle&utm_medium=post&utm_campaign=competition_4 \ud83d\udca0 https://falcon.duality.ai/secure/scenarios/edit/e3294c19-49d4-4f64-9ca8-8373876c2c94?utm_source=Kaggle&utm_medium=post&utm_campaign=competition_4 See translation", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/377698831818015", "date_published": "2025-07-15T13:39:45.640994"}, {"id": "https://huggingface.co/posts/AdinaY/423045666935241", "image": "", "title": "Kimi-K2 is now available on the hub\ud83d\udd25\ud83d\ude80", "content_text": "Kimi-K2 is now available on the hub\ud83d\udd25\ud83d\ude80 This is a trillion-parameter MoE model focused on long context, code, reasoning, and agentic behavior. moonshotai/kimi-k2-6871243b990f2af5ba60617d \u2728 Base & Instruct \u2728 1T total / 32B active - Modified MIT License \u2728 128K context length \u2728 Muon optimizer for stable trillion-scale training See translation", "url": "https://huggingface.co/posts/AdinaY/423045666935241", "date_published": "2025-07-15T13:39:45.641271"}, {"id": "https://huggingface.co/posts/merve/294233425076045", "image": "", "title": "past week had huuuge releases \ud83d\udc97", "content_text": "past week had huuuge releases \ud83d\udc97 here's our picks \ud83d\udd25 find more models, datasets, demos here merve/releases-july-11-68750452c358c98b0fa663f7 > moonshotai/Kimi-K2-Instruct is the new sota LLM with 1T total 32B active parameters \ud83e\udd2f > HuggingFaceTB/SmolLM3-3B is the new best LM for it's size, offers thinking mode \ud83d\udcad as well as the dataset HuggingFaceTB/smoltalk2 > Alibaba-NLP/WebSailor-3B is the new agentic LLM for complex browsing > Google DeepMind released medical vision LMs with an agentic doctor-patient app google/medgemma-release-680aade845f90bec6a3f60c4 > fal released a LoRA to improve details on face images fal/Realism-Detailer-Kontext-Dev-LoRA See translation", "url": "https://huggingface.co/posts/merve/294233425076045", "date_published": "2025-07-15T13:39:45.641621"}, {"id": "https://huggingface.co/posts/DawnC/760622875415705", "image": "", "title": "\ud83c\udfaf Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science!", "content_text": "\ud83c\udfaf Excited to share my comprehensive deep dive into VisionScout's multimodal AI architecture, now published as a three-part series on Towards Data Science! This isn't just another computer vision project. VisionScout represents a fundamental shift from simple object detection to genuine scene understanding, where four specialized AI models work together to interpret what's actually happening in an image. \ud83c\udfd7\ufe0f Part 1: Architecture Foundation How careful system design transforms independent models into collaborative intelligence through proper layering and coordination strategies. \u2699\ufe0f Part 2: Deep Technical Implementation The five core algorithms powering the system: dynamic weight adjustment, attention mechanisms, statistical methods, lighting analysis, and CLIP's zero-shot learning. \ud83c\udf0d Part 3: Real-World Validation Concrete case studies from indoor spaces to cultural landmarks, demonstrating how integrated systems deliver insights no single model could achieve. What makes this valuable:...", "url": "https://huggingface.co/posts/DawnC/760622875415705", "date_published": "2025-07-15T13:39:45.642179"}]}