{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/openfree/652290136793730", "image": "", "title": "\ud83d\ude80 Llama-4 Model-Based Agentic AI System Released!", "content_text": "\ud83d\ude80 Llama-4 Model-Based Agentic AI System Released! \ud83d\udd25 Introducing the Latest Llama-4 Models Hello AI enthusiasts! Today we're excited to introduce our free API service powered by the cutting-edge Llama-4-Maverick-17B and Llama-4-Scout-17B models! These state-of-the-art models will upgrade your AI experience with remarkable stability and speed. Link1: openfree/Llama-4-Maverick-17B-Research Link2: openfree/Llama-4-Scout-17B-Research \ud83e\udde0 The Innovation of Agentic AI: Deep Research Feature The standout feature of our service is the revolutionary \"Deep Research\" functionality! This innovative Agentic AI system includes: \ud83d\udd0d Optimized Keyword Extraction: LLM automatically generates the most effective keywords for searches \ud83c\udf10 Real-time Web Search: Collects the latest information through the SerpHouse API \ud83d\udcca Intelligent Information Analysis: Precise analysis utilizing the LLM's reasoning capabilities based on collected information \ud83d\udcdd Contextualized Response Generation: Provides accurate answers...", "url": "https://huggingface.co/posts/openfree/652290136793730", "date_published": "2025-04-08T05:22:23.461757"}, {"id": "https://huggingface.co/posts/seawolf2357/883323339740165", "image": "", "title": "\ud83c\udfa8 Ghibli-Style Image Generation with Multilingual Text Integration: FLUX.1 Hugging Face Edition \ud83c\udf0f\u2728", "content_text": "\ud83c\udfa8 Ghibli-Style Image Generation with Multilingual Text Integration: FLUX.1 Hugging Face Edition \ud83c\udf0f\u2728 Hello creators! Today I'm introducing a special image generator that combines the beautiful aesthetics of Studio Ghibli with multilingual text integration! \ud83d\ude0d seawolf2357/Ghibli-Multilingual-Text-rendering \u2728 Key Features Ghibli-Style Image Generation - High-quality animation-style images based on FLUX.1 Multilingual Text Rendering - Support for Korean, Japanese, English, and all languages! \ud83c\uddf0\ud83c\uddf7\ud83c\uddef\ud83c\uddf5\ud83c\uddec\ud83c\udde7 Automatic Image Editing with Simple Prompts - Just input your desired text and you're done! Two Stylistic Variations Provided - Get two different results from a single prompt Full Hugging Face Spaces Support - Deploy and share instantly! \ud83d\ude80 How Does It Work? Enter a prompt describing your desired image (e.g., \"a cat sitting by the window\") Input the text you want to add (any language works!) Select the text position, size, and color Two different versions are automatically generated! \ud83d\udcaf...", "url": "https://huggingface.co/posts/seawolf2357/883323339740165", "date_published": "2025-04-08T05:22:23.462439"}, {"id": "https://huggingface.co/posts/ginipick/807578740801859", "image": "", "title": "\ud83c\udfef Open Ghibli Studio: Transform Your Photos into Ghibli-Style Artwork! \u2728", "content_text": "\ud83c\udfef Open Ghibli Studio: Transform Your Photos into Ghibli-Style Artwork! \u2728 Hello AI enthusiasts! \ud83d\ude4b\u200d\u2640\ufe0f Today I'm introducing a truly magical project: Open Ghibli Studio \ud83c\udfa8 ginigen/FLUX-Open-Ghibli-Studio \ud83c\udf1f What Can It Do? Upload any regular photo and watch it transform into a beautiful, fantastical image reminiscent of Hayao Miyazaki's Studio Ghibli animations! \ud83c\udfde\ufe0f\u2728 \ud83d\udd27 How Does It Work? \ud83d\udcf8 Upload your photo \ud83e\udd16 Florence-2 AI analyzes the image and generates a description \u270f\ufe0f \"Ghibli style\" is added to the description \ud83c\udfad Magic transformation happens using the FLUX.1 model and Ghibli LoRA! \u2699\ufe0f Customization Options Want more control? Adjust these in the advanced settings: \ud83c\udfb2 Set a seed (for reproducible results) \ud83d\udccf Adjust image dimensions \ud83d\udd0d Guidance scale (prompt adherence) \ud83d\udd04 Number of generation steps \ud83d\udcab Ghibli style intensity \ud83d\ude80 Try It Now! Click the \"Transform to Ghibli Style\" button below to create your own Ghibli world! Ready to meet Totoro, Howl, Sophie, or Chihiro? \ud83c\udf08 \ud83c\udf3f Note: For best results,...", "url": "https://huggingface.co/posts/ginipick/807578740801859", "date_published": "2025-04-08T05:22:23.462949"}, {"id": "https://huggingface.co/posts/openfree/925352420925810", "image": "", "title": "\ud83d\udd25 'Open Meme Studio': Your Creative Meme Factory \ud83c\udfad\u2728", "content_text": "\ud83d\udd25 'Open Meme Studio': Your Creative Meme Factory \ud83c\udfad\u2728 Hello everyone! Today I'm introducing 'Open Meme Studio', an amazing space where you can easily create and transform fun and original meme images. \ud83d\ude80 VIDraft/Open-Meme-Studio \ud83c\udfaf Taking Meme Creation to the Next Level! This application leverages the powerful Kolors model and IP-Adapter-Plus to upgrade your meme-making abilities. Go beyond simple image editing and experience a completely new meme world powered by AI! \ud83d\udee0\ufe0f Features You'll Love \ud83d\udcf8 Transform and reinterpret existing meme templates \ud83c\udfad Freely change expressions and poses \ud83d\udc53 Add props (sunglasses, hats, etc.) \ud83c\udfde\ufe0f Change backgrounds and composite characters \ud83c\udfa8 Apply various artistic styles \ud83d\udcaa Why 'Open Meme Studio' is So Effective Fast Meme Generation: High-quality memes completed in seconds Unlimited Creativity: Completely different results just by changing prompts User-Friendly Interface: Simple prompt input and image upload is all you need Fine-tuned Control: Adjust how much of...", "url": "https://huggingface.co/posts/openfree/925352420925810", "date_published": "2025-04-08T05:22:23.463612"}, {"id": "https://huggingface.co/posts/aiqtech/202174985893140", "image": "", "title": "\u2728 High-Resolution Ghibli Style Image Generator \u2728", "content_text": "\u2728 High-Resolution Ghibli Style Image Generator \u2728 \ud83c\udf1f Introducing FLUX Ghibli LoRA Hello everyone! Today I'm excited to present a special LoRA model for FLUX Dev.1. This model leverages a LoRA trained on high-resolution Ghibli images for FLUX Dev.1 to easily create beautiful Ghibli-style images with stunning detail! \ud83c\udfa8 space: aiqtech/FLUX-Ghibli-Studio-LoRA model: openfree/flux-chatgpt-ghibli-lora \ud83d\udd2e Key Features Trained on High-Resolution Ghibli Images - Unlike other LoRAs, this one is trained on high-resolution images, delivering sharper and more beautiful results Powered by FLUX Dev.1 - Utilizing the latest FLUX model for faster generation and superior quality User-Friendly Interface - An intuitive UI that allows anyone to create Ghibli-style images with ease Diverse Creative Possibilities - Express various themes in Ghibli style, from futuristic worlds to fantasy elements \ud83d\uddbc\ufe0f Sample Images Include \"Ghibli style\" in your prompts Try combining nature, fantasy elements, futuristic...", "url": "https://huggingface.co/posts/aiqtech/202174985893140", "date_published": "2025-04-08T05:22:23.464186"}, {"id": "https://huggingface.co/posts/jsulz/745335361364732", "image": "", "title": "Huge week for", "content_text": "Huge week for xet-team as Llama 4 is the first major model on Hugging Face uploaded with Xet providing the backing! Every byte downloaded comes through our infrastructure. Using Xet on Hugging Face is the fastest way to download and iterate on open source models and we've proved it with Llama 4 giving a boost of ~25% across all models. We expect builders on the Hub to see even more improvements, helping power innovation across the community. With the models on our infrastructure, we can peer in and see how well our dedupe performs across the Llama 4 family. On average, we're seeing ~25% dedupe, providing huge savings to the community who iterate on these state-of-the-art models. The attached image shows a few selected models and how they perform on Xet. Thanks to the meta-llama team for launching on Xet! See translation", "url": "https://huggingface.co/posts/jsulz/745335361364732", "date_published": "2025-04-08T05:22:23.464650"}, {"id": "https://huggingface.co/posts/wassemgtk/755158543554585", "image": "", "title": "I\u2019ve been diving into the iRoPE architecture from Llama 4\u2014a game-changer for long-context models! It interleaves local attention (with RoPE) for short contexts and global attention (with inference-time temp scaling) for long-range reasoning, aiming for infinite context. I\u2019m going to try writing iRoPE\u2014who wants to help?", "content_text": "I\u2019ve been diving into the iRoPE architecture from Llama 4\u2014a game-changer for long-context models! It interleaves local attention (with RoPE) for short contexts and global attention (with inference-time temp scaling) for long-range reasoning, aiming for infinite context. I\u2019m going to try writing iRoPE\u2014who wants to help? Code: https://github.com/wassemgtk/iRoPE-try/blob/main/iRoPE.ipynb See translation", "url": "https://huggingface.co/posts/wassemgtk/755158543554585", "date_published": "2025-04-08T05:22:23.464956"}, {"id": "https://huggingface.co/posts/hesamation/410985277524863", "image": "", "title": "The best researchers from Yale, Stanford, Google DeepMind, and Microsoft laid out all we know about Agents in a 264-page paper [book],", "content_text": "The best researchers from Yale, Stanford, Google DeepMind, and Microsoft laid out all we know about Agents in a 264-page paper [book], Here are some of their key findings: They build a mapping of different agent components, such as perception, memory, and world modelling, to different regions of the human brain and compare them: - brain is much more energy-efficient - no genuine experience in agents - brain learns continuously, agent is static An agent is broken down to: - Perception: the agent's input mechanism. can be improved with multi-modality, feedback mechanisms (e.g., human corrections), etc. - Cognition: learning, reasoning, planning, memory. LLMs are key in this part. - Action: agent's output and tool use. Agentic memory is represented as: - Sensory memory or short-term holding of inputs which is not emphasized much in agents. - Short-term memory which is the LLM context window - Long-term memory which is the external storage such as RAG or knowledge graphs. The memory in...", "url": "https://huggingface.co/posts/hesamation/410985277524863", "date_published": "2025-04-08T05:22:23.465497"}, {"id": "https://huggingface.co/posts/AtAndDev/141454132915403", "image": "", "title": "Llama 4 is out...", "content_text": "Llama 4 is out...", "url": "https://huggingface.co/posts/AtAndDev/141454132915403", "date_published": "2025-04-08T05:22:23.465718"}, {"id": "https://huggingface.co/posts/sr-rai/596076021766754", "image": "", "title": "ExLlamaV3 is out. And it introduces EXL3 - a new SOTA quantization format!", "content_text": "ExLlamaV3 is out. And it introduces EXL3 - a new SOTA quantization format! \"The conversion process is designed to be simple and efficient and requires only an input model (in HF format) and a target bitrate. By computing Hessians on the fly and thanks to a fused Viterbi kernel, the quantizer can convert a model in a single step, taking a couple of minutes for smaller models, up to a few hours for larger ones (70B+) (on a single RTX 4090 or equivalent GPU.)\" Repo: https://github.com/turboderp-org/exllamav3 See translation", "url": "https://huggingface.co/posts/sr-rai/596076021766754", "date_published": "2025-04-08T05:22:23.466011"}]}