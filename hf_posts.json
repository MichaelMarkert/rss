{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/fdaudens/719212082746895", "image": "", "title": "Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity!", "content_text": "Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity! The final project uses the GAIA dataset - your agent has to solve tasks like analyzing Excel files, processing audio recordings, answering questions about YouTube videos, and diving into research papers. This isn't toy examples, it's the messy, multimodal stuff agents need to handle in practice. Whether you\u2019re just getting started with agents or want to go deeper with tools like LangChain, LlamaIndex, and SmolAgents, this course has tons of useful stuff. A few key insights: - Code agents are incredibly versatile once you get the architecture right - The sweet spot is finding the right balance of guidance vs autonomy for each use case - Once the logic clicks, the possibilities really are endless - it's like letting LLMs break free from the chatbox The course is free and the certification deadline is July 1st, 2025. The Hugging Face...", "url": "https://huggingface.co/posts/fdaudens/719212082746895", "date_published": "2025-05-28T13:35:37.268856"}, {"id": "https://huggingface.co/posts/clem/541152505631903", "image": "", "title": "It's just become easier to share your apps on the biggest AI app store (aka HF spaces) for unlimited storage, more visibility and community interactions.", "content_text": "It's just become easier to share your apps on the biggest AI app store (aka HF spaces) for unlimited storage, more visibility and community interactions. Just pick a React, Svelte, or Vue template when you create your space or add app_build_command: npm run build in your README's YAML and app_file: build/index.html in your README's YAML block. Or follow this link: https://huggingface.co/new-space?sdk=static Let's build! See translation", "url": "https://huggingface.co/posts/clem/541152505631903", "date_published": "2025-05-28T13:35:37.269139"}, {"id": "https://huggingface.co/posts/ProCreations/321100188234240", "image": "", "title": "Eyyyy 50 followers \ud83e\udd2f", "content_text": "Eyyyy 50 followers \ud83e\udd2f See translation", "url": "https://huggingface.co/posts/ProCreations/321100188234240", "date_published": "2025-05-28T13:35:37.269332"}, {"id": "https://huggingface.co/posts/merve/349112163630055", "image": "", "title": "what happened in open AI past week? so many vision LM & omni releases \ud83d\udd25", "content_text": "what happened in open AI past week? so many vision LM & omni releases \ud83d\udd25 merve/releases-23-may-68343cb970bbc359f9b5fb05 multimodal \ud83d\udcac\ud83d\uddbc\ufe0f > new moondream (VLM) is out: it's 4-bit quantized (with QAT) version of moondream-2b, runs on 2.5GB VRAM at 184 tps with only 0.6% drop in accuracy (OS) \ud83c\udf1a > ByteDance released BAGEL-7B, an omni model that understands and generates both image + text. they also released Dolphin, a document parsing VLM \ud83d\udc2c (OS) > Google DeepMind dropped MedGemma in I/O, VLM that can interpret medical scans, and Gemma 3n, an omni model with competitive LLM performance > MMaDa is a new 8B diffusion language model that can generate image and text LLMs > Mistral released Devstral, a 24B coding assistant (OS) \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb > Fairy R1-32B is a new reasoning model -- distilled version of DeepSeek-R1-Distill-Qwen-32B (OS) > NVIDIA released ACEReason-Nemotron-14B, new 14B math and code reasoning model > sarvam-m is a new Indic LM with hybrid thinking mode, based on Mistral Small (OS) >...", "url": "https://huggingface.co/posts/merve/349112163630055", "date_published": "2025-05-28T13:35:37.269819"}, {"id": "https://huggingface.co/posts/jasoncorkill/660720952792703", "image": "", "title": "Benchmark Update:", "content_text": "Benchmark Update: @ google Veo3 (Text-to-Video) Two months ago, we benchmarked @ google \u2019s Veo2 model. It fell short, struggling with style consistency and temporal coherence, trailing behind Runway, Pika, @ tencent , and even @ alibaba-pai . That\u2019s changed. We just wrapped up benchmarking Veo3, and the improvements are substantial. It outperformed every other model by a wide margin across all key metrics. Not just better, dominating across style, coherence, and prompt adherence. It's rare to see such a clear lead in today\u2019s hyper-competitive T2V landscape. Dataset coming soon. Stay tuned. See translation", "url": "https://huggingface.co/posts/jasoncorkill/660720952792703", "date_published": "2025-05-28T13:35:37.270158"}, {"id": "https://huggingface.co/posts/AdinaY/689443142454195", "image": "", "title": "Orsta \ud83d\udd25 vision language models trained with V-Triune, a unified reinforcement learning system by MiniMax AI", "content_text": "Orsta \ud83d\udd25 vision language models trained with V-Triune, a unified reinforcement learning system by MiniMax AI One-RL-to-See-Them-All/one-rl-to-see-them-all-6833d27abce23898b2f9815a \u2728 7B & 32B with MIT license \u2728 Masters 8 visual tasks: math, science QA, charts, puzzles, object detection, grounding, OCR, and counting \u2728 Uses Dynamic IoU rewards for better visual understanding \u2728Strong performance in visual reasoning and perception See translation", "url": "https://huggingface.co/posts/AdinaY/689443142454195", "date_published": "2025-05-28T13:35:37.270460"}, {"id": "https://huggingface.co/posts/m-ric/682683815641001", "image": "", "title": "A new research paper from KAIST builds on smolagents to push boundaries of distillation \ud83e\udd73", "content_text": "A new research paper from KAIST builds on smolagents to push boundaries of distillation \ud83e\udd73 \u27a1\ufe0f \"Distilling LLM Agent into Small Models with Retrieval and Code Tools\" teaches that, when trying to distil reasoning capability from a strong LLM (\"teacher\") into a smaller one (\"student\"), it's much better to use Agent traces than CoT traces. Advantages are: 1. Improved generalization Intuitively, this is because your agent can encounter more \"surprising\" results by interacting with its environment : for example, a web research called by the LLM teacher in agent mode can bring results that the LLM teacher would not have generated in CoT. 2. Reduce hallucinations The trace won't hallucinate tool call outputs! Thank you @ akseljoonas for mentioning this paper! See translation", "url": "https://huggingface.co/posts/m-ric/682683815641001", "date_published": "2025-05-28T13:35:37.270835"}, {"id": "https://huggingface.co/posts/dhruv3006/675063918098240", "image": "", "title": "Cua : Docker for computer-use agents", "content_text": "Cua : Docker for computer-use agents Cua is the Docker for Computer-Use Agent, an open-source framework that enables AI agents to control full operating systems within high-performance, lightweight virtual containers. Github : https://github.com/trycua See translation", "url": "https://huggingface.co/posts/dhruv3006/675063918098240", "date_published": "2025-05-28T13:35:37.271064"}, {"id": "https://huggingface.co/posts/BestWishYsh/693532821570217", "image": "", "title": "Introducing our new work: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\u200b\u200b \ud83d\ude80", "content_text": "Introducing our new work: OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation\u200b\u200b \ud83d\ude80 We tackle the core challenges of \u200b\u200bSubject-to-Video Generation (S2V)\u200b\u200b by systematically building the first complete infrastructure\u2014featuring an evaluation benchmark and a million-scale dataset! \u2728 \ud83e\udde0 Introducing \u200b\u200bOpenS2V-Eval\u200b\u200b\u2014the first \u200b\u200bfine-grained S2V benchmark\u200b\u200b, with \u200b\u200b180 multi-domain prompts + real/synthetic test pairs\u200b\u200b. We propose \u200b\u200bNexusScore\u200b\u200b, \u200b\u200bNaturalScore\u200b\u200b, and \u200b\u200bGmeScore\u200b\u200b to precisely quantify model performance across \u200b\u200bsubject consistency, naturalness, and text alignment\u200b\u200b \u2714 \ud83d\udcca Using this framework, we conduct a \u200b\u200bcomprehensive evaluation of 16 leading S2V models\u200b\u200b, revealing their strengths/weaknesses in complex scenarios! \ud83d\udd25 \u200b\u200bOpenS2V-5M dataset\u200b\u200b now available! A \u200b\u200b5.4M 720P HD\u200b\u200b collection of \u200b\u200bsubject-text-video triplets\u200b\u200b, enabled by \u200b\u200bcross-video association segmentation + multi-view synthesis\u200b\u200b for \u200b\u200bdiverse subjects & high-quality...", "url": "https://huggingface.co/posts/BestWishYsh/693532821570217", "date_published": "2025-05-28T13:35:37.271585"}, {"id": "https://huggingface.co/posts/merve/601276092251484", "image": "", "title": "emerging trend: models that can understand image + text and generate image + text", "content_text": "emerging trend: models that can understand image + text and generate image + text don't miss out \u2935\ufe0f > MMaDA: single 8B diffusion model aligned with CoT (reasoning!) + UniGRPO Gen-Verse/MMaDA > BAGEL: 7B MoT model based on Qwen2.5, SigLIP-so-400M, Flux VAE ByteDance-Seed/BAGEL both by ByteDance! \ud83d\ude31 I keep track of all any input \u2192 any output models here merve/any-to-any-models-6822042ee8eb7fb5e38f9b62 See translation", "url": "https://huggingface.co/posts/merve/601276092251484", "date_published": "2025-05-28T13:35:37.271877"}]}