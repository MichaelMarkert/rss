{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/hesamation/830297477341251", "image": "", "title": "longer context doesn't generate better responses. it can even hurt your llm/agent.  1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it.", "content_text": "longer context doesn't generate better responses. it can even hurt your llm/agent. 1M context window doesn't automatically make models smarter as it's not about the size; it's how you use it. here are 4 types of context failure and why each one happens: 1. context poisoning: if hallucination finds its way into your context, the agent will rely on that false information to make its future moves. for example if the agent hallucinates about the \"task description\", all of its planning to solve the task would also be corrupt. 2. context distraction: when the context becomes too bloated, the model focuses too much on it rather than come up with novel ideas or to follow what it has learned during training. as Gemini 2.5 Pro technical report points out, as context grows significantly from 100K tokens, \"the agent showed a tendency toward favoring repeating actions from its vast history rather than synthesizing novel plans\". 3. context confusion: everyone lost it when MCPs became popular, it...", "url": "https://huggingface.co/posts/hesamation/830297477341251", "date_published": "2025-07-25T09:29:35.177393"}, {"id": "https://huggingface.co/posts/mitkox/940300076081193", "image": "", "title": "I run Qwen3-Coder 480B locally on my Z8, with a 1-million token context window. It\u2019s the equivalent of parallel-parking a Nimitz-class carrier in a kiddie pool. Thanks to whatever dark pact the llama.cpp, CUDA, and kernel folks signed, hybrid inferencing + VRAM\u2194RAM offload let me stream the model\u2019s synapses across Xeon, RAM, and four lonely A6000s without summoning either the OOM killer or a small house fire.", "content_text": "I run Qwen3-Coder 480B locally on my Z8, with a 1-million token context window. It\u2019s the equivalent of parallel-parking a Nimitz-class carrier in a kiddie pool. Thanks to whatever dark pact the llama.cpp, CUDA, and kernel folks signed, hybrid inferencing + VRAM\u2194RAM offload let me stream the model\u2019s synapses across Xeon, RAM, and four lonely A6000s without summoning either the OOM killer or a small house fire. See translation", "url": "https://huggingface.co/posts/mitkox/940300076081193", "date_published": "2025-07-25T09:29:35.177720"}, {"id": "https://huggingface.co/posts/andito/542123544707457", "image": "", "title": "Many VLMs claim to process hours of video. But can they follow the story?\ud83e\udd14", "content_text": "Many VLMs claim to process hours of video. But can they follow the story?\ud83e\udd14 Today, we introduce TimeScope: The benchmark that separates true temporal understanding from marketing hype. Let's see how much VLMs really understand!\u23f3 We test three skills that matter for real-world use: \ud83d\udd0e Localized Retrieval: Find a specific action. \ud83e\udde9 Information Synthesis: Piece together scattered clues. \ud83c\udfc3 Fine-Grained Perception: Analyze detailed motion (e.g., count how many times a person swings an axe). The results are in, and they're revealing. Only Gemini 2.5 pro handles 1-hour-long videos. Performance drops sharply with duration, proving that long video understanding is still challenging. We've found the breaking points\u2014now the community can start fixing them.\ud83d\udcc8 Want to learn more? TimeScope is 100% open-source. Benchmark your model and help us build the next generation of video AI. \ud83d\udcd6 Blog: https://huggingface.co/blog/timescope-video-lmm-benchmark \ud83d\udc69\u200d\ud83d\udcbb Leaderboard & Demo: Apollo-LMMs/TimeScope \ud83d\udcca...", "url": "https://huggingface.co/posts/andito/542123544707457", "date_published": "2025-07-25T09:29:35.178193"}, {"id": "https://huggingface.co/posts/AdinaY/439685097705510", "image": "", "title": "KAT-V1 \ud83d\udd25 a LLM that tackles overthinking by switching between reasoning and direct answers, by Kuaishou.", "content_text": "KAT-V1 \ud83d\udd25 a LLM that tackles overthinking by switching between reasoning and direct answers, by Kuaishou. Kwaipilot/KAT-V1-40B \u2728 40B \u2728 Step-SRPO: smarter reasoning control via RL \u2728 MTP + Distillation: efficient training, lower cost See translation", "url": "https://huggingface.co/posts/AdinaY/439685097705510", "date_published": "2025-07-25T09:29:35.178451"}, {"id": "https://huggingface.co/posts/prithivMLmods/906521786731164", "image": "", "title": "olmOCR [Allen AI] just got an upgrade! \ud83d\udcc8\ud83e\uddd1\u200d\ud83c\udf73", "content_text": "olmOCR [Allen AI] just got an upgrade! \ud83d\udcc8\ud83e\uddd1\u200d\ud83c\udf73 The allenai/olmOCR-7B-0725 \u2014 fine-tuned with allenai/olmOCR-mix-0225 on top of Qwen/Qwen2.5-VL-7B-Instruct , pushing the boundaries of OCR technology. It takes a single document image as input, with the longest side resized to 1288 pixels. High-quality, openly available approach to parsing pdfs and other complex documents optical character recognition. Try the demo here: prithivMLmods/Multimodal-OCR \u2728 Model: allenai/olmOCR-7B-0725 \u2728 Model [fp8]: allenai/olmOCR-7B-0725-FP8 \u2728 Multimodal Implementations Space Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See translation", "url": "https://huggingface.co/posts/prithivMLmods/906521786731164", "date_published": "2025-07-25T09:29:35.178824"}, {"id": "https://huggingface.co/posts/danielhanchen/754522453041743", "image": "", "title": "It's Qwen3 week! \ud83d\udc9c We uploaded Dynamic 2-bit GGUFs for:", "content_text": "It's Qwen3 week! \ud83d\udc9c We uploaded Dynamic 2-bit GGUFs for: Qwen3-Coder: unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF Qwen3-2507: unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF So you can run them both locally! Guides are in model cards. See translation", "url": "https://huggingface.co/posts/danielhanchen/754522453041743", "date_published": "2025-07-25T09:29:35.179079"}, {"id": "https://huggingface.co/posts/wenhuach/293322564041371", "image": "", "title": "\ud83d\ude80 AutoRound(", "content_text": "\ud83d\ude80 AutoRound( https://github.com/intel/auto-round ) Now Supports GGUF Export & Custom Bit Settings! We're excited to announce that AutoRound now supports: \u2705 GGUF format export \u2013 for seamless compatibility with popular inference engines. \u2705 Custom bit settings \u2013 tailor quantization to your needs for optimal performance. Check out these newly released models: \ud83d\udd39Intel/Qwen3-235B-A22B-Instruct-2507-gguf-q4km-AutoRound \ud83d\udd39Intel/Qwen3-235B-A22B-Instruct-2507-gguf-q2ks-mixed-AutoRound \ud83d\udd39Intel/Kimi-K2-Instruct-gguf-q2ks-mixed-AutoRound Stay tuned! An even more advanced algorithm for some configurations is coming soon. See translation", "url": "https://huggingface.co/posts/wenhuach/293322564041371", "date_published": "2025-07-25T09:29:35.179406"}, {"id": "https://huggingface.co/posts/AdinaY/235205941898959", "image": "", "title": "Qwen3-Coder \ud83d\udcbb agentic code model by Alibaba Qwen team\ud83d\ude80", "content_text": "Qwen3-Coder \ud83d\udcbb agentic code model by Alibaba Qwen team\ud83d\ude80 Qwen/Qwen3-Coder-480B-A35B-Instruct \u2728 480B total, 35B activated MoE \u2728 Agentic Coding + Browser Use \u2192 Top code model performance \u2728 256K context (up to 1M via Yarn) for repo-scale understanding See translation", "url": "https://huggingface.co/posts/AdinaY/235205941898959", "date_published": "2025-07-25T09:29:35.179658"}, {"id": "https://huggingface.co/posts/prithivMLmods/432897219160306", "image": "", "title": "Excited to introduce the new experimental model \"Qwen2.5-VL-7B-Abliterated-Caption-it\", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.\ud83e\uddea\ud83e\udd17", "content_text": "Excited to introduce the new experimental model \"Qwen2.5-VL-7B-Abliterated-Caption-it\", which is performing exceptionally well on image captioning tasks. This variant is specifically tailored for Abliterated Captioning and Uncensored Image Captioning. It is designed to generate highly detailed and descriptive captions across a broad range of visual categories including images with complex, sensitive, or nuanced content while handling varying aspect ratios and resolutions.\ud83e\uddea\ud83e\udd17 \u2728 Try the demo here : prithivMLmods/Qwen2.5-VL \u2728 Qwen2.5-VL-7B-Abliterated-Caption-it : prithivMLmods/Qwen2.5-VL-7B-Abliterated-Caption-it \u2728 Multimodal VLMs : prithivMLmods/multimodal-vlms-until-july25-688312e6b840e1e156f13027 \u2728 Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 . . . To know more about it, visit the model card of the respective model. !! See translation", "url": "https://huggingface.co/posts/prithivMLmods/432897219160306", "date_published": "2025-07-25T09:29:35.180039"}, {"id": "https://huggingface.co/posts/AdinaY/225572236442446", "image": "", "title": "Qwen is on fire this week \ud83d\udd25", "content_text": "Qwen is on fire this week \ud83d\udd25 They just released Qwen3-MT \ud83c\udf0d a translation model supports 92 languages. Demo is available on the hub. Qwen/Qwen3-MT-Demo \u2728 Highly Customizable: Supports custom terms, domain prompts, and translation memory for accurate, context-aware results. \u2728 Fast and affordable: $0.5 per million tokens. See translation", "url": "https://huggingface.co/posts/AdinaY/225572236442446", "date_published": "2025-07-25T09:29:35.180316"}]}