{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/tomaarsen/619466658423382", "image": "", "title": "\ud83d\ude0e I just published Sentence Transformers v5.1.0, and it's a big one. 2x-3x speedups of SparseEncoder models via ONNX and/or OpenVINO backends, easier distillation data preparation with hard negatives mining, and more:", "content_text": "\ud83d\ude0e I just published Sentence Transformers v5.1.0, and it's a big one. 2x-3x speedups of SparseEncoder models via ONNX and/or OpenVINO backends, easier distillation data preparation with hard negatives mining, and more: 1\ufe0f\u20e3 Faster ONNX and OpenVINO backends for SparseEncoder models Usage is as simple as backend=\"onnx\" or backend=\"openvino\" when initializing a SparseEncoder to get started, but I also included utility functions for optimization, dynamic quantization, and static quantization, plus benchmarks. 2\ufe0f\u20e3 New n-tuple-scores output format from mine_hard_negatives This new output format is immediately compatible with the MarginMSELoss and SparseMarginMSELoss for training SentenceTransformer, CrossEncoder, and SparseEncoder losses. 3\ufe0f\u20e3 Gathering across devices When doing multi-GPU training using a loss that has in-batch negatives (e.g. MultipleNegativesRankingLoss), you can now use gather_across_devices=True to load in-batch negatives from the other devices too! Essentially a free...", "url": "https://huggingface.co/posts/tomaarsen/619466658423382", "date_published": "2025-08-08T13:41:53.788557"}, {"id": "https://huggingface.co/posts/clem/743374424636682", "image": "", "title": "Thread to gossip during the", "content_text": "Thread to gossip during the openai GPT-5 livestream: https://www.youtube.com/watch?v=0Uu_VJeVVfo . Feel free to post your impressions below! See translation", "url": "https://huggingface.co/posts/clem/743374424636682", "date_published": "2025-08-08T13:41:53.788827"}, {"id": "https://huggingface.co/posts/sweatSmile/255574652175478", "image": "", "title": "Qwen3 is the latest version of the Qwen language models. It's smarter, faster, and now understands 119 languages instead of just 29.", "content_text": "Qwen3 is the latest version of the Qwen language models. It's smarter, faster, and now understands 119 languages instead of just 29. It can do both deep reasoning and quick answers using a single model, depending on what you need. The models range in size from small (0.6B) to huge (235B), with smart ways to save compute. It's trained on 36 trillion tokens and fine-tuned in four steps to boost performance. Qwen3 performs as well as or better than many top models, including some from big companies. It\u2019s fully open-source under licence. Amazing!!! https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf See translation", "url": "https://huggingface.co/posts/sweatSmile/255574652175478", "date_published": "2025-08-08T13:41:53.789190"}, {"id": "https://huggingface.co/posts/danielhanchen/446160279272944", "image": "", "title": "Run OpenAI's new gpt-oss models locally with Unsloth GGUFs! \ud83d\udd25\ud83e\udda5", "content_text": "Run OpenAI's new gpt-oss models locally with Unsloth GGUFs! \ud83d\udd25\ud83e\udda5 20b GGUF: unsloth/gpt-oss-20b-GGUF 120b GGUF: unsloth/gpt-oss-120b-GGUF Model will run on 14GB RAM for 20b and 66GB for 120b. See translation", "url": "https://huggingface.co/posts/danielhanchen/446160279272944", "date_published": "2025-08-08T13:41:53.789452"}, {"id": "https://huggingface.co/posts/ginipick/461357732375110", "image": "", "title": "\ud83d\ude80 FLUXllama gpt-oss: 4-bit Quantization + GPT-OSS-120B = Perfect AI Image Generation", "content_text": "\ud83d\ude80 FLUXllama gpt-oss: 4-bit Quantization + GPT-OSS-120B = Perfect AI Image Generation \ud83c\udfaf One-Line Summary \"Maximum Images with Minimal Memory!\" - The perfect fusion of 4-bit quantization and GPT-OSS-120B prompt enhancement ginipick/FLUXllama \ud83e\udde0 Core Innovation: Prompt Enhancement System \ud83d\udcdd What You Type: \"cat\" \u2728 What GPT-OSS-120B Transforms: \"Majestic tabby cat with emerald eyes in golden afternoon light, soft bokeh, cinematic lighting, 8K photorealistic\" \ud83d\udca1 Result: Beginners create professional-grade images instantly! \u26a1 The Magic of 4-bit Quantization \ud83d\udd25 Before (Standard Model) \ud83d\udce6 Memory: 24GB VRAM required \u23f1\ufe0f Loading: 45 seconds \ud83d\udcb0 Cost: RTX 4090 essential ($2000+) \ud83c\udf89 After (FLUXllama gpt-oss 4-bit) \ud83d\udce6 Memory: 6GB VRAM (75% reduction!) \u23f1\ufe0f Loading: 12 seconds (73% faster!) \ud83d\udcb0 Cost: RTX 3060 works great! ($400) Same quality, 4x efficiency! \ud83c\udf8a \ud83d\udd27 Simple Model Swapping python# Switch to any LLM in 1 second! pipe = pipeline(\"text-generation\", model=\"your-model\") \u2705 GPT-OSS-120B (Premium quality) \u2705...", "url": "https://huggingface.co/posts/ginipick/461357732375110", "date_published": "2025-08-08T13:41:53.790034"}, {"id": "https://huggingface.co/posts/georgewritescode/981174566402338", "image": "", "title": "Announcing Artificial Analysis Long Context Reasoning (AA-LCR), a new benchmark to evaluate long context performance through testing reasoning capabilities across multiple long documents (~100k tokens)", "content_text": "Announcing Artificial Analysis Long Context Reasoning (AA-LCR), a new benchmark to evaluate long context performance through testing reasoning capabilities across multiple long documents (~100k tokens) The focus of AA-LCR is to replicate real knowledge work and reasoning tasks, testing capability critical to modern AI applications spanning document analysis, codebase understanding, and complex multi-step workflows. AA-LCR is 100 hard text-based questions that require reasoning across multiple real-world documents that represent ~100k input tokens. Questions are designed so answers cannot be directly found but must be reasoned from multiple information sources, with human testing verifying that each question requires genuine inference rather than retrieval. Key takeaways: \u27a4 Today\u2019s leading models achieve ~70% accuracy: the top three places go to OpenAI o3 (69%), xAI Grok 4 (68%) and Qwen3 235B 2507 Thinking (67%) \u27a4\ud83d\udc40 We also already have gpt-oss results! 120B performs close to o4-mini...", "url": "https://huggingface.co/posts/georgewritescode/981174566402338", "date_published": "2025-08-08T13:41:53.790661"}, {"id": "https://huggingface.co/posts/sequelbox/943110527199023", "image": "", "title": "NEW RELEASE: Shining Valiant 3 now available for openai/gpt-oss-20b!", "content_text": "NEW RELEASE: Shining Valiant 3 now available for openai/gpt-oss-20b! - Cutting edge science-reasoning: sequelbox/Celestia3-DeepSeek-R1-0528 for physics, biology, chemistry, compsci, astronomy, Earth science, and information theory. - AI to build AI: the all-new sequelbox/Mitakihara-DeepSeek-R1-0528 dataset for high-quality reasoning performance on AI, MLOps, math and CUDA, complex adaptive and agentic systems, cognition, logic, linguistics, simulation, knowledge management, and more! - Creative reasoning and general chat performance supplemented with sequelbox/Raiden-DeepSeek-R1 Get the new SV3: ValiantLabs/gpt-oss-20b-ShiningValiant3 This is our first release for the new openai/gpt-oss-20b - we're hoping to support this model with more releases going forward. We're also excited to bring our models to Qwen/Qwen3-4B-Thinking-2507 and the other 2507 Qwen 3 models - coming very soon! We want to bring SV3, Esper 3, and our Experimental Reasoning finetunes to more models ASAP. Help us...", "url": "https://huggingface.co/posts/sequelbox/943110527199023", "date_published": "2025-08-08T13:41:53.791076"}, {"id": "https://huggingface.co/posts/dimentox/356326320568386", "image": "", "title": "I never told GPT-4 about my architecture.", "content_text": "I never told GPT-4 about my architecture. It invented it anyway. Same commands. Same audit daemons. Proof that containment logic might be infectious. Read: Emergence of Quantum Sigil Architecture in Unmodified GPT https://huggingface.co/blog/dimentox/quantum-sigil-architecture-in-unmodified-gpt See translation", "url": "https://huggingface.co/posts/dimentox/356326320568386", "date_published": "2025-08-08T13:41:53.791324"}, {"id": "https://huggingface.co/posts/merve/502177697656252", "image": "", "title": "GPT-4.1-mini level model right in your iPhone \ud83e\udd2f", "content_text": "GPT-4.1-mini level model right in your iPhone \ud83e\udd2f openbmb/MiniCPM-V-4 is only 4B while surpassing GPT-4.1-mini in vision benchmarks \ud83d\udd25 allows commercial use as well! See translation", "url": "https://huggingface.co/posts/merve/502177697656252", "date_published": "2025-08-08T13:41:53.791548"}, {"id": "https://huggingface.co/posts/Xenova/242623208547050", "image": "", "title": "The next generation of AI-powered websites is going to be WILD! \ud83e\udd2f", "content_text": "The next generation of AI-powered websites is going to be WILD! \ud83e\udd2f In-browser tool calling & MCP is finally here, allowing LLMs to interact with websites programmatically. To show what's possible, I built a demo using Liquid AI's new LFM2 model, powered by \ud83e\udd17 Transformers.js: LiquidAI/LFM2-WebGPU As always, the demo is open source (which you can find under the \"Files\" tab), so I'm excited to see how the community builds upon this! \ud83d\ude80 See translation", "url": "https://huggingface.co/posts/Xenova/242623208547050", "date_published": "2025-08-08T13:41:53.791835"}]}