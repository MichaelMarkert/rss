{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/hesamation/842061188959684", "image": "", "title": "this repo is gold! a collection of LLM apps with multi-agents, MCP, RAG and so much more.", "content_text": "this repo is gold! a collection of LLM apps with multi-agents, MCP, RAG and so much more. the best way to learn is by building, and this repo provides the blueprint. Repo: https://github.com/Shubhamsaboo/awesome-llm-apps See translation", "url": "https://huggingface.co/posts/hesamation/842061188959684", "date_published": "2025-06-14T09:25:11.361805"}, {"id": "https://huggingface.co/posts/openfree/428786122279500", "image": "", "title": "\ud83e\udd17 I'm leading 'Openfree AI', Korea's most prominent AI open-source community. First and foremost, I'd like to express my deepest gratitude for Hugging Face's continuous support and efforts. \ud83d\udc99", "content_text": "\ud83e\udd17 I'm leading 'Openfree AI', Korea's most prominent AI open-source community. First and foremost, I'd like to express my deepest gratitude for Hugging Face's continuous support and efforts. \ud83d\udc99 Our Openfree AI collaborates with various AI communities across Korea, contributing to knowledge sharing and ecosystem development. \ud83e\udd1d I've been actively promoting the critical importance of Hugging Face as Korea's AI infrastructure backbone, engaging with senior government officials, National Assembly members, university leaders, and media executives to emphasize how Hugging Face represents Korea's AI future at a national policy level. I consider myself a 'voluntary Korean ambassador for Hugging Face'. \ud83c\uddf0\ud83c\uddf7\u2728 Let me share our community's achievements on the Hugging Face platform over the past year: \ud83c\udfaf \ud83d\ude80 Published hundreds of models and spaces \ud83d\udc65 Surpassed 10 million cumulative visitors \ud83d\udcc8 Achieved 1.7 million Monthly Active Users (MAU) \ud83c\udfa8 Generated over 1 million images/videos per month These...", "url": "https://huggingface.co/posts/openfree/428786122279500", "date_published": "2025-06-14T09:25:11.362411"}, {"id": "https://huggingface.co/posts/a-r-r-o-w/231008365980283", "image": "", "title": "Recently, I've been focusing my learning on the following topics:", "content_text": "Recently, I've been focusing my learning on the following topics: - Pytorch internals, specifically the inductor system (roughly ~1 month of experience) - Triton internals (~8 moe) - CUDA (~3 moe) - Understanding fusion patterns in compilers and how to improve them (~1 moe) - Parallelism strategies for large scale inference optimization (~6-7 moe) I thought it would be nice to document it somewhere for no particular reason. Maybe someone will find it useful? It's also because I want to get into the habit of writing, but had no motivation to do so. Maybe writing short informal posts will help build the habit. Since I don't have a personal site, and don't plan to create one in the near future, I think HF posts are best suited for short and informal documentation to share my little discoveries and learnings. If you're interested, strap in! First post in this series will be on basic study of Pytorch's float32 matmuls and their Triton implementation (nothing much, just the tutorial...", "url": "https://huggingface.co/posts/a-r-r-o-w/231008365980283", "date_published": "2025-06-14T09:25:11.362814"}, {"id": "https://huggingface.co/posts/csabakecskemeti/762115035937109", "image": "", "title": "Has anyone ever backed up a model to a sequential tape drive, or I'm the world first? :D", "content_text": "Has anyone ever backed up a model to a sequential tape drive, or I'm the world first? :D Just played around with my retro PC that has got a tape drive\u2014did it just because I can. See translation", "url": "https://huggingface.co/posts/csabakecskemeti/762115035937109", "date_published": "2025-06-14T09:25:11.363049"}, {"id": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/399941870774449", "image": "", "title": "Can AI models trained solely on 100% synthetic data achieve top-tier accuracy in real-world object detection?", "content_text": "Can AI models trained solely on 100% synthetic data achieve top-tier accuracy in real-world object detection? \ud83d\udc49 Sergio Sanz, PhD just proved it while winning Duality AI\u2019s Synthetic-to-Real Object Detection Challenge using Falcon-generated imagery. His model achieved perfect real-world detection accuracy without a single real image in the training loop. In this blog, Dr. Sanz walks us through his method, which includes the design and training of an advanced pipeline to achieve 100% detection accuracy. His full technical breakdown covers: \ud83d\udccd Synthetic-only training \ud83d\udccd Data augmentation with an ensemble learning approach for better generalization \ud83d\udccd Custom occlusion generation \ud83d\udccd A Faster R-CNN model fine-tuned with Falcon generated data \ud83d\udccd And much more! The results speak for themselves! \ud83d\udcd6 Read the blog here: https://www.duality.ai/blog/leveraging-synthetic-data-for-real-world-object-detection Congratulations Sergio! We can't wait to see what you do next. \ud83d\udd14 Ready to take on the next...", "url": "https://huggingface.co/posts/DualityAI-RebekahBogdanoff/399941870774449", "date_published": "2025-06-14T09:25:11.363527"}, {"id": "https://huggingface.co/posts/codelion/378799954783125", "image": "", "title": "New Research: Theoretical Foundations for In-Context Learning in Transformers", "content_text": "New Research: Theoretical Foundations for In-Context Learning in Transformers I'm excited to share our latest theoretical work that formally proves an interesting property of large language models: base transformer models can approximate fine-tuned capabilities using only inference-time techniques like in-context learning. The core question we investigated: Can specialized behaviors typically acquired through expensive supervised fine-tuning be elicited from base models without any parameter updates? Our theoretical contribution: We provide a formal proof, grounded in the Turing completeness of transformers, showing that this is indeed possible under certain assumptions. The work establishes mathematical bounds on the minimal dataset sizes needed for approximation. Key theoretical results: - For text generation tasks: O(mV/\u03b5\u00b2) examples suffice (where m = number of contexts, V = vocabulary size, \u03b5 = error tolerance) - For linear classification: O(d/\u03b5) examples (where d = input...", "url": "https://huggingface.co/posts/codelion/378799954783125", "date_published": "2025-06-14T09:25:11.364058"}, {"id": "https://huggingface.co/posts/davidberenstein1957/888709551300235", "image": "", "title": "I created a collection of FLUX.1 models but 4x faster", "content_text": "I created a collection of FLUX.1 models but 4x faster PrunaAI/flux1-but-4x-faster-66c0b7340836dd7a55e9c0ea See translation", "url": "https://huggingface.co/posts/davidberenstein1957/888709551300235", "date_published": "2025-06-14T09:25:11.364253"}, {"id": "https://huggingface.co/posts/burtenshaw/642764546410723", "image": "", "title": "Super excited to release Autotrain MCP. This is an MCP server for training AI models, so you can use your AI tools to train your AI models \ud83e\udd2f.", "content_text": "Super excited to release Autotrain MCP. This is an MCP server for training AI models, so you can use your AI tools to train your AI models \ud83e\udd2f. \ud83d\udd17 burtenshaw/autotrain-mcp Use this MCP server with tools like Claude Desktop, Cursor, VSCode, or Continue to do this: - Define an ML problem like Image Classification, LLM fine-tuning, Text Classification, etc. - The AI can retrieve models and datasets from the hub using the hub MCP. - Training happens on a Hugging Face space, so no worries about hardware restraints. - Models are pushed to the hub to be used inference tools like Llama.cpp, vLLM, MLX, etc. - Built on top of the AutoTrain library, so it has full integration with transformers and other libraries. Everything is still under active development, but I\u2019m super excited to hear what people build, and I\u2019m open to contributions! See translation", "url": "https://huggingface.co/posts/burtenshaw/642764546410723", "date_published": "2025-06-14T09:25:11.364659"}, {"id": "https://huggingface.co/posts/Narsil/207015264611430", "image": "", "title": "Me: This function is too slow. Find a faster algorithm.", "content_text": "Me: This function is too slow. Find a faster algorithm. Cursor: Hold my beer. Me: *Slacking off with colleagues* Cursor: Ping. Me: \ud83e\udd2f See translation", "url": "https://huggingface.co/posts/Narsil/207015264611430", "date_published": "2025-06-14T09:25:11.364886"}, {"id": "https://huggingface.co/posts/a-r-r-o-w/709852031491261", "image": "", "title": "New diffusion model for text-to-image and video-to-world generation: Cosmos Predict-2 \ud83d\udc7d", "content_text": "New diffusion model for text-to-image and video-to-world generation: Cosmos Predict-2 \ud83d\udc7d Model collection: nvidia/cosmos-predict2-68028efc052239369a0f2959 Diffusers support: https://github.com/huggingface/diffusers/pull/11695 Documentation: https://huggingface.co/docs/diffusers/main/en/api/pipelines/cosmos These are results with the 2B param model. Imagine what you could do with the 14B version! Go check it out now! See translation", "url": "https://huggingface.co/posts/a-r-r-o-w/709852031491261", "date_published": "2025-06-14T09:25:11.365152"}]}