{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/MonsterMMORPG/682969091190201", "image": "", "title": "WAN 2.1 FusionX + Self Forcing LoRA are the New Best of Local Video Generation with Only 8 Steps + FLUX Upscaling Guide :", "content_text": "WAN 2.1 FusionX + Self Forcing LoRA are the New Best of Local Video Generation with Only 8 Steps + FLUX Upscaling Guide : https://www.youtube.com/watch?v=Xbn93GRQKsQ Tutorial : https://www.youtube.com/watch?v=Xbn93GRQKsQ Video Chapters 0:00 Introduction to the New FusionX Video Model & FLUX Upscaling 0:30 One-Click Presets & The SwarmUI Model Downloader Explained 1:07 Achieving Hyper-Realism with the FLUX 2x Latent Upscale Preset 1:58 How to Download & Install the SwarmUI Model Downloader 2:49 Downloading Full Models vs. Downloading Just The LoRAs 3:48 Final Setup: Updating SwarmUI & Importing The New Presets 4:32 Generating a Video: Applying the FusionX Image-to-Video Preset 5:03 Critical Step: Correcting The Model's Native Resolution Metadata 5:55 Finalizing Image-to-Video Settings (Frame Count & RIFE Interpolation) 6:49 Troubleshooting Performance: Identifying Low GPU Usage & Shared VRAM Bug 8:35 The Solution: Disabling Sage Attention for Image-to-Video Models 10:02 Final Result:...", "url": "https://huggingface.co/posts/MonsterMMORPG/682969091190201", "date_published": "2025-06-21T05:22:34.503858"}, {"id": "https://huggingface.co/posts/clem/645090569186989", "image": "", "title": "We got a visitor to the office today!", "content_text": "We got a visitor to the office today! pollen-robotics , lerobot , unitreerobotics meetings! See translation", "url": "https://huggingface.co/posts/clem/645090569186989", "date_published": "2025-06-21T05:22:34.504101"}, {"id": "https://huggingface.co/posts/multimodalart/420236527922092", "image": "", "title": "Self-Forcing - a real-time video distilled model from Wan 2.1 by", "content_text": "Self-Forcing - a real-time video distilled model from Wan 2.1 by @ adobe is out, and they open sourced it \ud83d\udc10 I've built a live real time demo on Spaces \ud83d\udcf9\ud83d\udca8 multimodalart/self-forcing See translation", "url": "https://huggingface.co/posts/multimodalart/420236527922092", "date_published": "2025-06-21T05:22:34.504344"}, {"id": "https://huggingface.co/posts/merve/300457326273979", "image": "", "title": "y'all have been asking my opinion on how OCR models compare to each other \ud83d\udc40", "content_text": "y'all have been asking my opinion on how OCR models compare to each other \ud83d\udc40 I will leave three apps to compare newest models by @ prithivMLmods instead \u2935\ufe0f > compare Nanonets-OCR-s, Qwen2-VL-OCR-2B-Instruct, RolmOCR, Aya-Vision prithivMLmods/Multimodal-OCR > SmolDocling, Nanonets-OCR-s, MonkeyOCR, Typhoon-OCR-7B prithivMLmods/Multimodal-OCR2 > docscopeOCR, MonkeyOCR, coreOCR prithivMLmods/core-OCR See translation", "url": "https://huggingface.co/posts/merve/300457326273979", "date_published": "2025-06-21T05:22:34.504632"}, {"id": "https://huggingface.co/posts/giadap/887797954746272", "image": "", "title": "\ud83d\udde3\ufe0f Whose voice do we hear when AI speaks?", "content_text": "\ud83d\udde3\ufe0f Whose voice do we hear when AI speaks? Every language carries its own cultural values and worldviews. So, when we build AI systems, we're not just deciding how they speak but also whose perspectives they represent. Even choosing which dialect to train on in Norway becomes a question of inclusion and power. In Kenya, will AI speak Swahili from Nairobi or coastal regions? What about indigenous languages with rich oral traditions but limited written text, like Quechua in Peru or Cherokee in North America? The path forward? Building WITH communities, not just FOR them. Working with local partners (libraries, universities, civil society), testing for cultural alignment, and asking hard questions about representation. Just published some thoughts on this after my keynote in Norway a few weeks ago: https://huggingface.co/blog/giadap/when-ai-speaks See translation", "url": "https://huggingface.co/posts/giadap/887797954746272", "date_published": "2025-06-21T05:22:34.505020"}, {"id": "https://huggingface.co/posts/merve/819283171637748", "image": "", "title": "stop using VLMs blindly \u270b\ud83c\udffb", "content_text": "stop using VLMs blindly \u270b\ud83c\udffb compare different VLM outputs on a huge variety of inputs (from reasoning to OCR!) \ud83d\udd25 visionLMsftw/comparevlms > has support for multiple VLMs: google/gemma-3-27b-it , Qwen/Qwen2.5-VL-7B-Instruct , Qwen/Qwen2.5-VL-32B-Instruct , meta-llama/Llama-4-Maverick-17B-128E-Instruct , HuggingFaceTB/SmolVLM2-2.2B-Instruct > recommend us new models or inputs, we'll add \ud83e\udee1 so far I figured out > for fact-checks, you need a relatively bigger size (7B is ok!) > Gemma 3 gets downgrade without pan and scan (especially for \ud83d\udcd1) > Qwen2.5VL-32B is very talkative, great for reasoning but not good for simple tasks \ud83d\udde3\ufe0f See translation", "url": "https://huggingface.co/posts/merve/819283171637748", "date_published": "2025-06-21T05:22:34.505390"}, {"id": "https://huggingface.co/posts/dhruv3006/605499380772806", "image": "", "title": "Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs.", "content_text": "Introducing Windows Sandbox support - run computer-use agents on Windows business apps without VMs or cloud costs. Your enterprise software runs on Windows, but testing agents required expensive cloud instances. Windows Sandbox changes this - it's Microsoft's built-in lightweight virtualization sitting on every Windows 10/11 machine, ready for instant agent development. Enterprise customers kept asking for AutoCAD automation, SAP integration, and legacy Windows software support. Traditional VM testing was slow and resource-heavy. Windows Sandbox solves this with disposable, seconds-to-boot Windows environments for safe agent testing. What you can build: AutoCAD drawing automation, SAP workflow processing, Bloomberg terminal trading bots, manufacturing execution system integration, or any Windows-only enterprise software automation - all tested safely in disposable sandbox environments. Free with Windows 10/11, boots in seconds, completely disposable. Perfect for development and...", "url": "https://huggingface.co/posts/dhruv3006/605499380772806", "date_published": "2025-06-21T05:22:34.505743"}, {"id": "https://huggingface.co/posts/codelion/412568329805459", "image": "", "title": "DeepThink Plugin: Bringing Gemini 2.5's Parallel Reasoning to Open Models", "content_text": "DeepThink Plugin: Bringing Gemini 2.5's Parallel Reasoning to Open Models Just released an open-source plugin that implements Google's \"Deep Think\" reasoning approach for models like DeepSeek R1, Qwen3, and other open models. Google's recent Gemini 2.5 report introduced Deep Think - a technique where models generate multiple hypotheses in parallel and critique them before arriving at final answers. It achieves SOTA results on math olympiads and competitive coding benchmarks. Our implementation works by modifying the inference pipeline to explore multiple solution paths simultaneously, then synthesizing the best approach. Instead of single-pass generation, models run an internal debate before responding. Key features: - Works with any model that supports structured reasoning patterns - Implements parallel thinking during response generation - Particularly effective for complex reasoning tasks, math, and coding problems - Increases inference time but significantly improves answer...", "url": "https://huggingface.co/posts/codelion/412568329805459", "date_published": "2025-06-21T05:22:34.506207"}, {"id": "https://huggingface.co/posts/Jaward/804892295096878", "image": "", "title": "not sure of what to make of this but solving autonomous/selective reflection seems like a big deal in current agent frameworks. We did hit on this with iterative self-refinement in our AutoAgents framework (", "content_text": "not sure of what to make of this but solving autonomous/selective reflection seems like a big deal in current agent frameworks. We did hit on this with iterative self-refinement in our AutoAgents framework ( https://ijcai.org/proceedings/2024/0003.pdf ). Nice read, looking forward to the code. Paper: Scaling Test-time Compute for LLM Agents (2506.12928) See translation", "url": "https://huggingface.co/posts/Jaward/804892295096878", "date_published": "2025-06-21T05:22:34.506451"}, {"id": "https://huggingface.co/posts/prithivMLmods/142374005596149", "image": "", "title": "The demo for smoldocling / nanonets ocr / typhoon ocr /  monkey ocr  explores the document OCR capabilities of various newly released multimodal VLMs in a single space. And if you're experiencing or demoing long document image OCR, kindly use the Smoldocling 256M preview [ Smoldocling is back in demo here. ] \ud83e\udd17.", "content_text": "The demo for smoldocling / nanonets ocr / typhoon ocr / monkey ocr explores the document OCR capabilities of various newly released multimodal VLMs in a single space. And if you're experiencing or demoing long document image OCR, kindly use the Smoldocling 256M preview [ Smoldocling is back in demo here. ] \ud83e\udd17. \u2726 Try the demo here : prithivMLmods/Multimodal-OCR2 \u2937 MonkeyOCR Recognition : echo840/MonkeyOCR \u2937 Nanonets-OCR-s : nanonets/Nanonets-OCR-s \u2937 SmolDocling-256M-preview : ds4sd/SmolDocling-256M-preview \u2937 typhoon-ocr-7b : scb10x/typhoon-ocr-7b \u2937 Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 \u2937 Github : https://github.com/PRITHIVSAKTHIUR/Multimodal-OCR2 The community GPU grant was given by Hugging Face \u2014 special thanks to them. \ud83e\udd17\ud83d\ude80 To know more about it, visit the model card of the respective model. !! See translation", "url": "https://huggingface.co/posts/prithivMLmods/142374005596149", "date_published": "2025-06-21T05:22:34.506843"}]}