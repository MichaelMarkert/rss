{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/mitkox/604222926195865", "image": "", "title": "XBai o4 claims to beat Claude Opus 4 and o3-mini, and they provide verifiable proof. My skepticism circuits overloaded, but my local AI FOMO module screamed louder.", "content_text": "XBai o4 claims to beat Claude Opus 4 and o3-mini, and they provide verifiable proof. My skepticism circuits overloaded, but my local AI FOMO module screamed louder. I've thrown this 33B monoblock LLM onto a single GPU and used Roo Code for some\u2026 let\u2019s call it \u201cvibe testing\u201d. It\u2019s terrifyingly competent. As an architect, it\u2019s the best open-weight model I\u2019ve touched this side of 2025. See translation", "url": "https://huggingface.co/posts/mitkox/604222926195865", "date_published": "2025-08-05T13:45:29.858156"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/425722660691765", "image": "", "title": "Wan 2.2 & FLUX Krea Full Tutorial - Automated Install - Ready Perfect Presets - SwarmUI with ComfyUI - Install Wan 2.2 and FLUX Krea with literally 1-click and use our pre-made most amazing quality presets :", "content_text": "Wan 2.2 & FLUX Krea Full Tutorial - Automated Install - Ready Perfect Presets - SwarmUI with ComfyUI - Install Wan 2.2 and FLUX Krea with literally 1-click and use our pre-made most amazing quality presets : https://youtu.be/8MvvuX4YPeo https://youtu.be/8MvvuX4YPeo Video Chapters 0:00 Introduction: The Ultimate Wan 2.2 Tutorial with Optimized Presets 1:03 Free Prompt Generation Tool & Introducing the New FLUX Krea Dev Model 2:01 How SwarmUI & ComfyUI Enable Video Generation on Low-End Hardware 2:46 Quick Start Guide: Downloading the Latest SwarmUI & ComfyUI Installers 3:10 Step-by-Step: How to Update or Perform a Fresh Installation of ComfyUI 3:51 Step-by-Step: How to Update or Perform a Fresh Installation of SwarmUI 4:18 Essential Setup: Configuring the SwarmUI Backend for ComfyUI 4:53 One-Click Setup: Downloading All Required Wan 2.2 Models Automatically 5:46 Importing the Ultimate SwarmUI Presets Pack for Best Results 6:22 Wan 2.2 Image-to-Video Generation: A Complete Step-by-...", "url": "https://huggingface.co/posts/MonsterMMORPG/425722660691765", "date_published": "2025-08-05T13:45:29.858695"}, {"id": "https://huggingface.co/posts/prithivMLmods/372876915549424", "image": "", "title": "Qwen Image \u2013 The Latest Image Generation Model\ud83d\udd25", "content_text": "Qwen Image \u2013 The Latest Image Generation Model\ud83d\udd25 Below are some samples generated using the Qwen Image Diffusion Model. Qwen-Image, a 20B MMDiT model for next-generation text-to-image generation, preserves typographic details, layout coherence, and contextual harmony with stunning accuracy. It is especially strong at creating stunning graphic posters with native text. The model is now open-source. [ \ud835\ude80\ud835\udea0\ud835\ude8e\ud835\ude97-\ud835\ude78\ud835\ude96\ud835\ude8a\ud835\ude90\ud835\ude8e : Qwen/Qwen-Image ] \u2937 Try the Qwen Image demo here: prithivMLmods/Qwen-Image-Diffusion , Qwen/Qwen-Image & more ... \u2937 Qwen-Image Technical Report : Qwen-Image Technical Report (2508.02324) \u2937 Qwen Image [GitHub] : https://github.com/QwenLM/Qwen-Image Even more impressively, it demonstrates a strong ability to understand images. The model supports a wide range of vision-related tasks such as object detection, semantic segmentation, depth and edge (Canny) estimation, novel view synthesis, and image super-resolution. While each task is technically distinct, they can all be viewed...", "url": "https://huggingface.co/posts/prithivMLmods/372876915549424", "date_published": "2025-08-05T13:45:29.859258"}, {"id": "https://huggingface.co/posts/JingzeShi/527939367728783", "image": "", "title": "Trainable selective sampling and sparse attention kernels are indispensable in the era of context engineering. We hope our work will be helpful to everyone! \ud83e\udd17", "content_text": "Trainable selective sampling and sparse attention kernels are indispensable in the era of context engineering. We hope our work will be helpful to everyone! \ud83e\udd17 Trainable Dynamic Mask Sparse Attention (2508.02124) See translation", "url": "https://huggingface.co/posts/JingzeShi/527939367728783", "date_published": "2025-08-05T13:45:29.859520"}, {"id": "https://huggingface.co/posts/sergiopaniego/720514750677796", "image": "", "title": "Just included example scripts for aligning models using GSPO (including VLM example) \ud83d\ude46\u200d\u2642\ufe0f\ud83d\ude46\u200d\u2642\ufe0f", "content_text": "Just included example scripts for aligning models using GSPO (including VLM example) \ud83d\ude46\u200d\u2642\ufe0f\ud83d\ude46\u200d\u2642\ufe0f GSPO is the latest RL alignment algo by @Alibaba_Qwen and it's already supported in the latest TRL v0.20 release. Super-easy-to-get-started example scripts below, GO run them!\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb \ud83e\uddd1\u200d\ud83c\udfa8 Script: https://github.com/huggingface/trl/blob/main/examples/scripts/gspo.py \ud83e\udd84 VLM script: https://github.com/huggingface/trl/blob/main/examples/scripts/gspo_vlm.py \ud83e\udde9 More TRL examples: https://huggingface.co/docs/trl/main/en/example_overview \ud83e\uddd9\u200d\u2642\ufe0f GSPO paper: Group Sequence Policy Optimization (2507.18071) See translation", "url": "https://huggingface.co/posts/sergiopaniego/720514750677796", "date_published": "2025-08-05T13:45:29.859832"}, {"id": "https://huggingface.co/posts/codelion/145733180850232", "image": "", "title": "Extended the ICM paper to show cross-model capability transfer - used Qwen3's mathematical reasoning to improve Gemma3 without any human supervision.", "content_text": "Extended the ICM paper to show cross-model capability transfer - used Qwen3's mathematical reasoning to improve Gemma3 without any human supervision. Key results: Qwen3-0.6B: 63.2 \u2192 66.0 on MATH-500 (+4%) Gemma3-1B: 41.0 \u2192 45.6 on MATH-500 (+11%) The method extracts coherent reasoning patterns from one model via Internal Coherence Maximization, converts them to DPO training data, and uses that to improve a completely different model architecture. This goes beyond the original ICM paper which only improved models using their own labels. We're showing you can transfer capabilities between any models - imagine extracting capabilities from strong models to improve your local ones. Models available: codelion/Qwen3-0.6B-ICM-DPO codelion/gemma-3-1b-it-ICM-DPO Complete collection with code and datasets: codelion/internal-coherence-maximization-687a1bd1c1f5f1d6f76e9b3b Full methodology and results: https://huggingface.co/blog/codelion/internal-coherence-maximization Planning to extend this...", "url": "https://huggingface.co/posts/codelion/145733180850232", "date_published": "2025-08-05T13:45:29.860269"}, {"id": "https://huggingface.co/posts/Kseniase/651523050744942", "image": "", "title": "12 Powerful World Models", "content_text": "12 Powerful World Models World models are one of the most challenging areas in AI, pushing the boundaries of reasoning, perception, and planning. They're gen AI systems that help models and agents learn internal representations of real-world environments. Today, we invite you to take a look at 12 standout examples: 1. WorldVLA \u2192 WorldVLA: Towards Autoregressive Action World Model (2506.21539) This autoregressive world model integrates action prediction and visual world modeling in a single framework, allowing each to enhance the other. It introduces an attention masking strategy to reduce action prediction errors 2. SimuRA \u2192 https://arxiv.org/abs/2507.23773 A generalized world model that uses a language-based world model to simulate and plan actions before execution, enabling more general and flexible reasoning 3. PAN (Physical, Agentic, and Nested) world models \u2192 Critiques of World Models (2507.05169) Has a hybrid architecture that combines discrete concept-based reasoning (via...", "url": "https://huggingface.co/posts/Kseniase/651523050744942", "date_published": "2025-08-05T13:45:29.860916"}, {"id": "https://huggingface.co/posts/mrs83/268112975581936", "image": "", "title": "Introducing Completionist, an open-source command-line tool that automates synthetic dataset generation.", "content_text": "Introducing Completionist, an open-source command-line tool that automates synthetic dataset generation. It works by iterating over an existing HF dataset and by using a LLM to create completions. - Problem: You need a fast way to create custom datasets for fine-tuning or RAG, but you want the flexibility to use different LLM backends or your own infrastructure. - Solution: Completionist connects with any OpenAI-compatible endpoint, including Ollama and LM Studio, or a Hugging Face inference endpoint. A simple CLI like Completionist gives you the possibility to take full control of your synthetic data generation workflow. \ud83d\udc49 Check out Completionist on GitHub: https://github.com/ethicalabs-ai/completionist Synthetic Dataset Example: ethicalabs/kurtis-mental-health-v2-sft-reasoning See translation", "url": "https://huggingface.co/posts/mrs83/268112975581936", "date_published": "2025-08-05T13:45:29.861285"}, {"id": "https://huggingface.co/posts/Abhaykoul/625756342268823", "image": "", "title": "\ud83d\ude80 Dhanishtha-2.0-preview-0825 Is Here", "content_text": "\ud83d\ude80 Dhanishtha-2.0-preview-0825 Is Here The Intermediate Thinking Model just leveled up again. With sharper reasoning, better tool use, and expanded capabilities, Dhanishtha-2.0-preview-0825 is now live and ready to impress. \ud83e\udde0 What Makes Dhanishtha Special? Unlike typical CoT models that only thinks one time, Dhanishtha thinks iteratively: > Think \u2192 Answer \u2192 Rethink \u2192 Improve \u2192 Rethink again if needed. \ud83d\udd17 Try it now: HelpingAI/Dhanishtha-2.0-preview-0825 \ud83d\udd1e Dhanishtha NSFW Preview For those exploring more expressive and immersive roleplay scenarios, we\u2019re also releasing: HelpingAI/Dhanishtha-nsfw A specialized version tuned for adult-themed interactions and character-driven roleplay. \ud83d\udd17 Explore it here: HelpingAI/Dhanishtha-nsfw \ud83d\udcac You can also try all of these live at chat.helpingai.co See translation", "url": "https://huggingface.co/posts/Abhaykoul/625756342268823", "date_published": "2025-08-05T13:45:29.861684"}, {"id": "https://huggingface.co/posts/codelion/631631354493607", "image": "", "title": "\ud83e\uddec We just published our comprehensive analysis of OpenEvolve - an open-source evolutionary coding agent that automatically optimizes algorithms using LLMs!", "content_text": "\ud83e\uddec We just published our comprehensive analysis of OpenEvolve - an open-source evolutionary coding agent that automatically optimizes algorithms using LLMs! Our key findings from 29 experiments across 10 models: - Gemini Flash 2.5 achieved 2.04x speedup across 30 benchmark tasks - Open models like Gemma 3 27B (1.63x) and Qwen3-Coder 480B (1.41x) rivaled proprietary models - The system discovered entirely new algorithms - not just code optimizations! - One task evolved from DFS to BFS to Union-Find approaches - Specialized coding models outperformed much larger general models 200 iterations beat 100 iterations by 24% - Ensembles surprisingly failed due to conflicting optimization strategies Most fascinating: watching models evolve code step-by-step, like transforming matrix operations from basic eigendecomposition to vectorized one-liners with 32x speedup. Our systematic experimental approach reveals that open-source evolutionary coding is becoming seriously competitive with...", "url": "https://huggingface.co/posts/codelion/631631354493607", "date_published": "2025-08-05T13:45:29.862192"}]}