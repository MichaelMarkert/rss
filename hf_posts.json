{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/prithivMLmods/245628043869780", "image": "", "title": "I'm a Hugging Face Fellow now, guys!\ud83e\udd17\u2764\ufe0f", "content_text": "I'm a Hugging Face Fellow now, guys!\ud83e\udd17\u2764\ufe0f With the same passion, trust, and momentum to contribute to the community, I\u2019m excited to do some amazing things to wrap up Q3 and Q4 of 2025. And importantly, I\u2019ve been lucky enough to receive some knowledge and guidance from @ merve to build open-source demos and stuff. Thank you for the belief. Thank you \u2014 much love. Long live open source! \u2014 Prithiv See translation", "url": "https://huggingface.co/posts/prithivMLmods/245628043869780", "date_published": "2025-09-18T09:24:14.635287"}, {"id": "https://huggingface.co/posts/salma-remyx/494327094243098", "image": "", "title": "Reproducing research code shouldn't take longer than reading the paper.", "content_text": "Reproducing research code shouldn't take longer than reading the paper. For papers that include code, setting up the right environment often means hours of dependency hell and configuration debugging. At Remyx AI, we built an agent that automatically creates and tests Docker images for research papers, then shares them publicly so anyone can reproduce results with a single command. We just submitted PR #908 to integrate this directly into arXiv Labs. If you believe in making reproducible research accessible to everyone, give it a bump!: https://github.com/arXiv/arxiv-browse/pull/908 See translation", "url": "https://huggingface.co/posts/salma-remyx/494327094243098", "date_published": "2025-09-18T09:24:14.635588"}, {"id": "https://huggingface.co/posts/lysandre/194539610907979", "image": "", "title": "We're kick-starting the process of Transformers v5, with", "content_text": "We're kick-starting the process of Transformers v5, with @ ArthurZ and @ cyrilvallez ! v5 should be significant: we're using it as a milestone for performance optimizations, saner defaults, and a much cleaner code base worthy of 2025. Fun fact: v4.0.0-rc-1 came out on Nov 19, 2020, nearly five years ago! See translation", "url": "https://huggingface.co/posts/lysandre/194539610907979", "date_published": "2025-09-18T09:24:14.635850"}, {"id": "https://huggingface.co/posts/merve/445414862119862", "image": "", "title": "IBM just released small swiss army knife for the document models: granite-docling-258M on Hugging Face \ud83d\udd25", "content_text": "IBM just released small swiss army knife for the document models: granite-docling-258M on Hugging Face \ud83d\udd25 > not only a document converter but also can do document question answering, understand multiple languages \ud83e\udd2f > best part: released with Apache 2.0 license \ud83d\udc4f use it with your commercial projects! > it supports transformers, vLLM and MLX from the get-go! \ud83e\udd17 > built on SigLIP2 & granite-165M model: ibm-granite/granite-docling-258M demo: ibm-granite/granite-docling-258m-demo \ud83d\udc97 See translation", "url": "https://huggingface.co/posts/merve/445414862119862", "date_published": "2025-09-18T09:24:14.636159"}, {"id": "https://huggingface.co/posts/ZennyKenny/754070932382171", "image": "", "title": "The open source Synthetic Data SDK from MOSTLY AI:", "content_text": "The open source Synthetic Data SDK from MOSTLY AI: mostlyai offers the ability to generate realistic, privacy-safe synthetic data with just a few lines of Python. Try it out yourself in a No Code UI in the SDK Demo Space: mostlyai/synthetic-sdk-demo See translation", "url": "https://huggingface.co/posts/ZennyKenny/754070932382171", "date_published": "2025-09-18T09:24:14.636386"}, {"id": "https://huggingface.co/posts/salma-remyx/227310661152992", "image": "", "title": "Search is such a fundamental part of content discovery, yet ends up overlooked or poorly implemented in so many apps we use every day.", "content_text": "Search is such a fundamental part of content discovery, yet ends up overlooked or poorly implemented in so many apps we use every day. We built hundreds of Docker images for arXiv papers with a codebase - it's tough to find what you're looking for unless you happen to have the arXiv id handy using DockerHub's search. So we added full text search over these resources so that you're that much closer to testing a new promising idea. More resources to be indexed soon! Full Demo: https://www.youtube.com/watch?v=GjYReWbQZw8 Try it here!: https://engine.remyx.ai/resources Join us at Experiment 2025!: https://experiment.remyx.ai See translation", "url": "https://huggingface.co/posts/salma-remyx/227310661152992", "date_published": "2025-09-18T09:24:14.636675"}, {"id": "https://huggingface.co/posts/meg/340948346361550", "image": "", "title": "\ud83e\udd16 As AI-generated content is shared in movies/TV/across the web, there's one simple low-hanging fruit \ud83c\udf47 to help know what's real: Visible watermarks. With the Gradio team, I've made sure it's trivially easy to add this disclosure to images, video, chatbot text. See how:", "content_text": "\ud83e\udd16 As AI-generated content is shared in movies/TV/across the web, there's one simple low-hanging fruit \ud83c\udf47 to help know what's real: Visible watermarks. With the Gradio team, I've made sure it's trivially easy to add this disclosure to images, video, chatbot text. See how: https://huggingface.co/blog/watermarking-with-gradio Thanks to the code collab in particular from @ abidlabs and Yuvraj Sharma. See translation", "url": "https://huggingface.co/posts/meg/340948346361550", "date_published": "2025-09-18T09:24:14.636961"}, {"id": "https://huggingface.co/posts/mitkox/209940111097655", "image": "", "title": "I\u2019ve built my blocker for AI-generated content. It\u2019s a local AI running on my laptop with a browser extension that classifies and scrubs synthetic content from my eyeballs. I\u2019m too old for this synthetic noise.", "content_text": "I\u2019ve built my blocker for AI-generated content. It\u2019s a local AI running on my laptop with a browser extension that classifies and scrubs synthetic content from my eyeballs. I\u2019m too old for this synthetic noise. TL;DR I\u2019m going full John Connor on the AI content apocalypse Think of it as an on device AI ad-blocker, but for: Em-dash overdose. Seriously, why is everything suddenly revolutionary\u2014disruptive\u2014life-changing? AI influencers\u2019 auto-generated posts and images, auto-posted, all hands-free. Fake news, fake images, fake people... puff. Surprisingly, it works. I suppose it will block some human-generated content. However, I would rather read a 2007 Myspace blog than another \u201c10 Growth Hacks Powered By ChatGPT\u201d post. See translation", "url": "https://huggingface.co/posts/mitkox/209940111097655", "date_published": "2025-09-18T09:24:14.637327"}, {"id": "https://huggingface.co/posts/Tonic/265427502664617", "image": "", "title": "COMPUTER CONTROL IS ON-DEVICE !", "content_text": "COMPUTER CONTROL IS ON-DEVICE ! \ud83c\udfe1\ud83e\udd16 78 % of EU smart-home owners DON\u2019T trust cloud voice assistants. So we killed the cloud. Meet Ext\u00e9: a palm-sized Android device that sees, hears & speaks your language - 100 % offline, 0 % data sent anywhere. \ud83d\udd13 We submitted our technologies for consideration to the Liquid AI hackathon. \ud83d\udcca Dataset: 79 k UI-action pairs on Hugging Face (largest Android-control corpus ever) Tonic/android-operator-episodes \u26a1 Model: 98 % task accuracy, 678MB compressed , fits on existing android devices ! Tonic/l-android-control \ud83d\udee4\ufe0f Experiment Tracker : check out the training on our TrackioApp Tonic/l-android-control \ud83c\udfae Live Model Demo: Upload an Android Screenshot and instructions to see the model in action ! Tonic/l-operator-demo Built in a garage, funded by pre-orders, no VC. Now we\u2019re scaling to 1 k installer units. We\u2019re giving 50 limited-edition prototypes to investors , installers & researchers who want to co-design the sovereign smart home. \ud83d\udc47 Drop \u201cEUSKERA\u201d in the...", "url": "https://huggingface.co/posts/Tonic/265427502664617", "date_published": "2025-09-18T09:24:14.637809"}, {"id": "https://huggingface.co/posts/aposadasn/143665230984018", "image": "", "title": "My team at", "content_text": "My team at arclabmit created a robotic teleoperation and learning software for controlling robots, recording datasets, and training physical AI models, which is compatible with lerobot . This work was part of a paper we published to ICCR Kyoto 2025. Check out or code here: https://github.com/ARCLab-MIT/beavr-bot/tree/main Our work aims to solve two key problems in the world of robotic manipulation: 1. The lack of a well-developed, open-source, accessible teleoperation system that can work out of the box. 2. No performant end-to-end control, recording, and learning platform for robots that is completely hardware agnostic. If you are curious to learn more or have any questions please feel free to reach out! Paper: BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots (2508.09606) See translation", "url": "https://huggingface.co/posts/aposadasn/143665230984018", "date_published": "2025-09-18T09:24:14.638151"}]}