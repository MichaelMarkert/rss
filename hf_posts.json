{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/AdinaY/473496396970919", "image": "", "title": "MiniCPM-V 4.5 \ud83d\ude80 New MLLM for image, multi-image & video understanding, running even on your phone, released by OpenBMB", "content_text": "MiniCPM-V 4.5 \ud83d\ude80 New MLLM for image, multi-image & video understanding, running even on your phone, released by OpenBMB openbmb/MiniCPM-V-4_5 \u2728 SOTA vision language capability \u2728 96\u00d7 video token compression > high-FPS & long video reasoning \u2728 Switchable fast vs deep thinking modes \u2728 Strong OCR, document parsing, supports 30+ languages See translation", "url": "https://huggingface.co/posts/AdinaY/473496396970919", "date_published": "2025-08-28T13:31:53.100667"}, {"id": "https://huggingface.co/posts/merve/292180054306518", "image": "", "title": "first vision language model built off", "content_text": "first vision language model built off openai/gpt-oss-20b just dropped! \ud83d\udd25 InternVL3.5 comes with 32 models \ud83e\udd2f pre-trained, fine-tuned, aligned in various sizes OpenGVLab/internvl35-68ac87bd52ebe953485927fb comes with gpt-oss or Qwen3 for LLM part \u2935\ufe0f See translation", "url": "https://huggingface.co/posts/merve/292180054306518", "date_published": "2025-08-28T13:31:53.100964"}, {"id": "https://huggingface.co/posts/codelion/968650774475150", "image": "", "title": "I recently added a recipe in ellora to improve reasoning capabilities to Gemma-3-1B using self-supervised learning. Model now shows step-by-step thinking in <think> tags before answering.", "content_text": "I recently added a recipe in ellora to improve reasoning capabilities to Gemma-3-1B using self-supervised learning. Model now shows step-by-step thinking in <think> tags before answering. Logic puzzle accuracy: 61% \u2192 84%. 3 hours training on single GPU. \ud83e\udde0 Used GRPO where model generates multiple responses and learns to prefer better reasoning. Works surprisingly well for making smaller models more transparent. \ud83d\udd17 Colab: https://colab.research.google.com/github/codelion/ellora/blob/main/Ellora_Recipe_2_Reasoning_LoRA_with_Self-Rewarding_GRPO.ipynb \ud83e\udd17 Model: codelion/gemma-3-1b-it-reasoning-grpo-lora \ud83d\udcbb Code: https://github.com/codelion/ellora See translation", "url": "https://huggingface.co/posts/codelion/968650774475150", "date_published": "2025-08-28T13:31:53.101329"}, {"id": "https://huggingface.co/posts/ginipick/809439997973106", "image": "", "title": "\ud83c\udf89 Fashion Fit 360: The New Standard in AI Virtual Try-On!", "content_text": "\ud83c\udf89 Fashion Fit 360: The New Standard in AI Virtual Try-On! \ud83d\ude80 Now Live and Free to Use!Say goodbye to online shopping uncertainty - \"Will this look good on me?\" - with our revolutionary solution!Fashion Fit 360 is a cutting-edge AI-powered virtual fitting service that transforms your fashion shopping experience. LINK: ginigen/Fashion-Fit360 \u2728 Core Features \ud83d\udd04 360-Degree Multi-Pose Generation Transform a single front-facing photo into 6 different viewing angles! Front, side, and back views for complete visualization Experience a real fitting room mirror effect Check fit and style from every perspective \ud83d\udc57 15 Fashion Item Categories Apparel: Tops, bottoms, dresses Jewelry: Necklaces, earrings, rings, bracelets Accessories: Sunglasses, eyewear, hats, ties, bow ties, belts Essentials: Bags, shoes \ud83c\udfaf Perfect For: \ud83d\udecd\ufe0f Online Shopping Enthusiasts: Preview before purchase - zero return hassles! \ud83d\udc8d Jewelry Lovers: Virtually try expensive pieces before investing \ud83c\udf81 Thoughtful Gift-Givers: Test items...", "url": "https://huggingface.co/posts/ginipick/809439997973106", "date_published": "2025-08-28T13:31:53.101957"}, {"id": "https://huggingface.co/posts/prithivMLmods/835910169748717", "image": "", "title": "OpenGVLab's InternVL3_5-2B-MPO [Mixed Preference Optimization (MPO)] is a compact vision-language model in the InternVL3.5 series. You can now experience it in the Tiny VLMs Lab, an app featuring 15+ multimodal VLMs ranging from 250M to 4B parameters. These models support tasks such as OCR, reasoning, single-shot answering with small models, and captioning (including ablated variants), across a broad range of visual categories. They are also capable of handling images with complex, sensitive, or nuanced content, while adapting to varying aspect ratios and resolutions.", "content_text": "OpenGVLab's InternVL3_5-2B-MPO [Mixed Preference Optimization (MPO)] is a compact vision-language model in the InternVL3.5 series. You can now experience it in the Tiny VLMs Lab, an app featuring 15+ multimodal VLMs ranging from 250M to 4B parameters. These models support tasks such as OCR, reasoning, single-shot answering with small models, and captioning (including ablated variants), across a broad range of visual categories. They are also capable of handling images with complex, sensitive, or nuanced content, while adapting to varying aspect ratios and resolutions. \u2728 Space/App : prithivMLmods/Tiny-VLMs-Lab \ud83e\uded9 Model : OpenGVLab/InternVL3_5-2B-MPO \u2197\ufe0f Collection: OpenGVLab/internvl35-68ac87bd52ebe953485927fb \ud83d\uddde\ufe0f Paper : https://arxiv.org/pdf/2508.18265 \u2197\ufe0f Multimodal Space Collection : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 To learn more, visit the relevant spaces, collections, and model cards. See translation", "url": "https://huggingface.co/posts/prithivMLmods/835910169748717", "date_published": "2025-08-28T13:31:53.102392"}, {"id": "https://huggingface.co/posts/codelion/741062673202173", "image": "", "title": "I wanted to share a technique that's been working really well for recovering performance after INT4 quantization.", "content_text": "I wanted to share a technique that's been working really well for recovering performance after INT4 quantization. Typically, quantizing the LLM to INT4 (unlike say INT8) for inference can incur some accuracy loss. Instead of accepting the quality loss, we used the FP16 model as a teacher to train a tiny LoRA adapter (rank=16) for the quantized model. The cool part: the model generates its own training data using the Magpie technique so no external datasets needed. This is critical because we want to remain as much as possible in the distribution of the model's natural responses. Last year Apple's foundational models paper ( https://arxiv.org/pdf/2407.21075 ) had proposed a similar technique and found \"By using accuracy-recovery LoRA adapters with only rank 16, Alpaca win rate can be improved by 7-18%, GMS8K accuracy is boosted by 5-10%.\" (page 47). We saw similar results on Qwen3-0.6B: Perplexity: 2.40 \u2192 2.09 (only 5.7% degradation from FP16 baseline) Memory: Only 0.28GB vs 1.0GB...", "url": "https://huggingface.co/posts/codelion/741062673202173", "date_published": "2025-08-28T13:31:53.104042"}, {"id": "https://huggingface.co/posts/openfree/464899901167090", "image": "", "title": "\ud83d\udd12 Ansim Blur: Privacy-First Face Blurring for the AI Era", "content_text": "\ud83d\udd12 Ansim Blur: Privacy-First Face Blurring for the AI Era \ud83d\udea8 The Privacy Crisis is Now Smart CCTVs \ud83d\udcf9, delivery robots \ud83e\udd16, and autonomous vehicles \ud83d\ude97 are everywhere. Your face is being captured, transmitted, and stored without your knowledge or consent. openfree/Face-blurring The privacy threat is real: 24/7 surveillance cameras recording your every move Companies harvesting facial biometric data at scale Your face becoming a commodity without your permission \ud83d\udca1 The Solution: Ansim Blur Real-time face anonymization powered by YOLOv8 \ud83c\udfaf \u2705 Process images, videos, and live streams \u2705 Automatic GPU/CPU detection for universal deployment \u2705 Choose between Gaussian blur or mosaic pixelation \u2705 Fine-tune detection sensitivity for your needs \u2705 Preserve audio tracks in video processing \ud83d\udee1\ufe0f Real-World Applications Enterprise Use Cases Privacy compliance for robotics and drone footage CCTV feed anonymization for regulatory requirements Customer data protection in retail analytics Personal Protection...", "url": "https://huggingface.co/posts/openfree/464899901167090", "date_published": "2025-08-28T13:31:53.104685"}, {"id": "https://huggingface.co/posts/AdinaY/738786462635771", "image": "", "title": "\ud83c\udde8\ud83c\uddf3 China\u2019s State Council just released its \u201cAI+\u201d Action Plan (2025)", "content_text": "\ud83c\udde8\ud83c\uddf3 China\u2019s State Council just released its \u201cAI+\u201d Action Plan (2025) <The State Council\u2019s Guidance on Deepened Implementation of the \u2018AI+\u2019 Strategy> zh-ai-community/china-ai-policy-research \u2728Goal: By 2035, AI will deeply empower all sectors, reshape productivity & society \u2728Focus on 6 pillars: >Science & Tech >Industry >Consumption >Public welfare >Governance >Global cooperation \u2728Highlights: >Models: advance theory, efficient training/inference, evaluation system >Data: high-quality datasets, IP/copyright reform, new incentives >Compute: boost chips & clusters, improve national network, promote cloud standardization, and ensure inclusive, efficient, green, secure supply. >Applications: AI-as-a-service, test bases, new standards >Open-source: support communities, encourage contributions (incl. university credits & recognition), foster new application approaches, and build globally impactful ecosystems \ud83d\udc40 >Talent, policy & safety frameworks to secure sustainable growth See translation", "url": "https://huggingface.co/posts/AdinaY/738786462635771", "date_published": "2025-08-28T13:31:53.105106"}, {"id": "https://huggingface.co/posts/jeffboudier/150612686071818", "image": "", "title": "Quick 30s demo of the new Hub > Azure AI integration to deploy HF models in your own Azure account. Now with Py and CLI!", "content_text": "Quick 30s demo of the new Hub > Azure AI integration to deploy HF models in your own Azure account. Now with Py and CLI! GG @ alvarobartt @ kramp @ pagezyhf See translation", "url": "https://huggingface.co/posts/jeffboudier/150612686071818", "date_published": "2025-08-28T13:31:53.105337"}, {"id": "https://huggingface.co/posts/tsungyi/147340620272288", "image": "", "title": "Cosmos Reason just topped Physical Reasoning Leaderboard on Hugging Face. \ud83d\udc4f\ud83d\udd25", "content_text": "Cosmos Reason just topped Physical Reasoning Leaderboard on Hugging Face. \ud83d\udc4f\ud83d\udd25 Cosmos Reason is an open, customizable, commercial-ready 7B-parameter, reasoning vision language model (VLM) for physical AI and robotics. The VLM empowers robots and vision AI agents to reason like humans, leveraging prior knowledge, physics understanding, and common sense to understand and operate intelligently in the real world. This model unlocks advanced capabilities for robotics, autonomous vehicles, and real-world operations\u2014from cities to high-tech factories. Key use cases include: Data curation & annotation: Automate high-quality dataset curation and annotation at scale. Robot planning & reasoning: Serve as the \"brain\" for deliberate, methodical decision-making with vision language action (VLA) models. Video analytics AI agents: Extract actionable insights and perform root-cause analysis on massive video datasets. Ready to build the next generation of physical AI? Get started \ud83d\udc49 nvidia/Cosmos-...", "url": "https://huggingface.co/posts/tsungyi/147340620272288", "date_published": "2025-08-28T13:31:53.105757"}]}