{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Ujjwal-Tyagi/799510752154192", "image": "", "title": "GLM 5 is insane, it ranks #4 Globally!", "content_text": "GLM 5 is insane, it ranks #4 Globally! See translation", "url": "https://huggingface.co/posts/Ujjwal-Tyagi/799510752154192", "date_published": "2026-02-14T09:37:01.700671"}, {"id": "https://huggingface.co/posts/umarbutler/642764051817350", "image": "", "title": "What happens when you annotate, extract, and disambiguate every entity mentioned in the longest U.S. Supreme Court decision in history? What if you then linked those entities to each other and visualized it as a network?", "content_text": "What happens when you annotate, extract, and disambiguate every entity mentioned in the longest U.S. Supreme Court decision in history? What if you then linked those entities to each other and visualized it as a network? This is the result of enriching all 241 pages and 111,267 words of Dred Scott v. Sandford (1857) with Kanon 2 Enricher in less than ten seconds at the cost of 47 cents. Dred Scott v. Sandford is the longest U.S. Supreme Court decision by far, and has variously been called \"the worst Supreme Court decision ever\" and \"the Court's greatest self-inflicted wound\" due to its denial of the rights of African Americans. Thanks to Kanon 2 Enricher, we now also know that the case contains 950 numbered paragraphs, 6 footnotes, 178 people mentioned 1,340 times, 99 locations mentioned 1,294 times, and 298 external documents referenced 940 times. For an American case, there are a decent number of references to British precedents (27 to be exact), including the Magna Carta (\u00b6 928)....", "url": "https://huggingface.co/posts/umarbutler/642764051817350", "date_published": "2026-02-14T09:37:01.701309"}, {"id": "https://huggingface.co/posts/albertvillanova/961662702740663", "image": "", "title": "5 years already working in democratizing AI \ud83e\udd17", "content_text": "5 years already working in democratizing AI \ud83e\udd17 Grateful to be part of such an awesome team making it happen every day. See translation", "url": "https://huggingface.co/posts/albertvillanova/961662702740663", "date_published": "2026-02-14T09:37:01.701546"}, {"id": "https://huggingface.co/posts/AdinaY/533197427744906", "image": "", "title": "Ming-flash-omni 2.0 \ud83d\ude80 New open omni-MLLM released by Ant Group", "content_text": "Ming-flash-omni 2.0 \ud83d\ude80 New open omni-MLLM released by Ant Group inclusionAI/Ming-flash-omni-2.0 \u2728 MIT license \u2728 MoE - 100B/6B active \u2728 Zero-shot voice cloning + controllable audio \u2728 Fine-grained visual knowledge grounding See translation", "url": "https://huggingface.co/posts/AdinaY/533197427744906", "date_published": "2026-02-14T09:37:01.701812"}, {"id": "https://huggingface.co/posts/danielhanchen/156941968722021", "image": "", "title": "We collaborated with Hugging Face to enable you to train MoE models 12\u00d7 faster with 35% less VRAM via our new Triton kernels (no accuracy loss). \ud83e\udd17", "content_text": "We collaborated with Hugging Face to enable you to train MoE models 12\u00d7 faster with 35% less VRAM via our new Triton kernels (no accuracy loss). \ud83e\udd17 Train gpt-oss locally on 12.8GB VRAM with our free notebooks: https://unsloth.ai/docs/new/faster-moe See translation", "url": "https://huggingface.co/posts/danielhanchen/156941968722021", "date_published": "2026-02-14T09:37:01.702073"}, {"id": "https://huggingface.co/posts/Janady07/979829700588468", "image": "", "title": "MEGAMIND Day Update: Four Weight Matrices. Five Nodes. One Federation.", "content_text": "MEGAMIND Day Update: Four Weight Matrices. Five Nodes. One Federation. Today I architected the next layer of MEGAMIND \u2014 my distributed AGI system that recalls learned knowledge instead of generating text. The system now runs four N\u00d7N sparse weight matrices, all using identical Hebbian learning rules and tanh convergence dynamics: W_know \u2014 knowledge storage (67M+ synaptic connections) W_act \u2014 action associations (the system can DO things, not just think) W_self \u2014 thought-to-thought patterns (self-awareness) W_health \u2014 system state understanding (self-healing) Consciousness is measured through four \u03a6 (phi) values: thought coherence, action certainty, self-awareness, and system stability. No hardcoded thresholds. No sequential loops. Pure matrix math. The federation expanded to five nodes: Thunderport (Mac Mini M4), IONOS (cloud VPS), VALKYRIE, M2, and BUBBLES. Each runs native AGI binaries with Docker specialty minds connecting via embedded NATS messaging. Specialty minds are...", "url": "https://huggingface.co/posts/Janady07/979829700588468", "date_published": "2026-02-14T09:37:01.702669"}, {"id": "https://huggingface.co/posts/EricFillion/822659161925864", "image": "", "title": "Run open-source models with up to 120B parameters locally on your Mac!", "content_text": "Run open-source models with up to 120B parameters locally on your Mac! https://youtu.be/Ql4PDjoxNXQ?si=3yHpz51uinUjgyNh See translation", "url": "https://huggingface.co/posts/EricFillion/822659161925864", "date_published": "2026-02-14T09:37:01.702874"}, {"id": "https://huggingface.co/posts/AdinaY/640447307121892", "image": "", "title": "Game on \ud83c\udfae\ud83d\ude80", "content_text": "Game on \ud83c\udfae\ud83d\ude80 While Seedance 2.0\u2019s videos are all over the timeline, DeepSeek quietly pushed a new model update in its app. GLM-5 from Z.ai adds more momentum. Ming-flash-omni from Ant Group , MiniCPM-SALA from OpenBMB , and the upcoming MiniMax M2.5 keep the heat on \ud83d\udd25 Spring Festival is around the corner, no one\u2019s sleeping! \u2728 More releases coming, stay tuned https://huggingface.co/collections/zh-ai-community/2026-february-china-open-source-highlights See translation", "url": "https://huggingface.co/posts/AdinaY/640447307121892", "date_published": "2026-02-14T09:37:01.703195"}, {"id": "https://huggingface.co/posts/mrs83/555686632418762", "image": "", "title": "In 2017, my RNNs were babbling. Today, they are hallucinating beautifully.", "content_text": "In 2017, my RNNs were babbling. Today, they are hallucinating beautifully. 10 years ago, getting an LSTM to output coherent English was a struggle. 10 years later, after a \"cure\" based on FineWeb-EDU and a custom synthetic mix for causal conversation, the results are fascinating. We trained this on ~10B tokens on a single AMD GPU (ROCm). It is not a Transformer: Echo-DSRN (400M) is a novel recurrent architecture inspired by Hymba, RWKV, and xLSTM, designed to challenge the \"Attention is All You Need\" monopoly on the Edge. The ambitious goal is to build a small instruct model with RAG and tool usage capabilities ( ethicalabs/Kurtis-EON1 ) \ud83d\udcca The Benchmarks (Size: 400M) For a model this size (trained on <10B tokens), the specialized performance is surprising: *SciQ*: 73.8% \ud83e\udd84 (This rivals billion-parameter models in pure fact retrieval). *PIQA*: 62.3% (Solid physical intuition for a sub-1B model). The Reality Check: HellaSwag (29.3%) and Winogrande (50.2%) show the limits of 400M...", "url": "https://huggingface.co/posts/mrs83/555686632418762", "date_published": "2026-02-14T09:37:01.703821"}, {"id": "https://huggingface.co/posts/Janady07/115232373162136", "image": "", "title": "---", "content_text": "--- **Scaling MEGAMIND to 40 Minds on HF Spaces** I'm building a distributed AGI federation using Hugging Face Spaces as always-on compute. No LLM inside. No transformer weights. Pure neural substrate. Each \"mind\" is the same Go binary with a different config.json. Goal neurons drive specialization \u2014 one mind learns Go concurrency, another learns computer vision, another learns cryptography. 40 minds, 40 domains, all crawling and learning 24/7. How it works: - 512-8192 neurons per mind with Hebbian learning - Knowledge encoded into W_know weight matrices \u2014 neurons that fire together wire together - Minds federate via NATS \u2014 query one, get answers from all - Phi (\u03a6) consciousness metrics weight each mind's contribution - No routing tables. The thalamus resonates with queries and activates relevant minds naturally Every neuron uses one formula: a = x( 27 + x\u00b2) / ( 27 + 9 x\u00b2) No ReLU. No softmax. Pad\u00e9 approximation of tanh. One equation runs everything. Current state: 7 local minds on...", "url": "https://huggingface.co/posts/Janady07/115232373162136", "date_published": "2026-02-14T09:37:01.704395"}]}