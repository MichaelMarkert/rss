{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/openfree/198818765852761", "image": "", "title": "\ud83d\ude80 DeepSeek V3-0324 + Real-time Research Power! \ud83c\udf10", "content_text": "\ud83d\ude80 DeepSeek V3-0324 + Real-time Research Power! \ud83c\udf10 Hello there! Today I'm excited to introduce an amazing tool based on the DeepSeek V3-0324 latest model. This isn't just another AI chatbot\u2014it's a true \"research assistant\" capable of real-time information retrieval and analysis! openfree/Deepseek-v3-0324-Research \ud83e\udde0 Key Strengths of DeepSeek V3-0324 DeepSeek V3-0324, provided by Fireworks AI, comes with these powerful advantages: \ud83c\udfaf Superior Reasoning: Excellent ability to solve complex problems step-by-step \ud83d\udcda Extensive Knowledge: Deep understanding across various topics from comprehensive training \ud83e\udde9 Context Awareness: Maintains long conversation contexts for consistent responses \ud83c\udf0d Multilingual Support: Processes various languages effectively \ud83d\udd0e Added Real-time \"Deep Research\" Capability! The most exciting feature of this project is the implementation of real-time search functionality similar to ChatGPT's Browse with Bing or Perplexity AI! \ud83c\udf1f How does it work? \ud83d\udccb Query Analysis: Analyzes...", "url": "https://huggingface.co/posts/openfree/198818765852761", "date_published": "2025-03-25T17:19:55.372496"}, {"id": "https://huggingface.co/posts/hanzla/334929914214979", "image": "", "title": "\ud83d\udc4b Hi all!", "content_text": "\ud83d\udc4b Hi all! For any AI agent, internet search \ud83d\udd0e is an important tool. However, with APIs like Tavily and Exa, it becomes really difficult to keep up with the cost. In some cases, these Internet APIs cost more than the LLM. To solve, this, I am making a playwright wrapper API on top of publicly available searXNG instances. This will enable agent applications to fetch internet results for free. Currently, I have set up a basic GitHub repo, and I will continue developing advanced search features, such as image search \ud83d\uddbc\ufe0f Github: https://github.com/HanzlaJavaid/Free-Search/tree/main \ud83d\ude80 Try the deployed version: https://freesearch.replit.app/docs If you find this useful, consider starring \u2b50\ufe0f the GitHub repository to support further development! See translation", "url": "https://huggingface.co/posts/hanzla/334929914214979", "date_published": "2025-03-25T17:19:55.372925"}, {"id": "https://huggingface.co/posts/MikeDoes/814156384414808", "image": "", "title": "\ud83d\ude80 We are quite excited to announce the Ai4Privacy Python library! \ud83c\udf89", "content_text": "\ud83d\ude80 We are quite excited to announce the Ai4Privacy Python library! \ud83c\udf89 pip install ai4privacy to anonymize short english text with OpenPII Masking 500k labels \ud83d\udcca Day 5/7 of PII Masking 1M announcements complete! \u23f0 See translation", "url": "https://huggingface.co/posts/MikeDoes/814156384414808", "date_published": "2025-03-25T17:19:55.373175"}, {"id": "https://huggingface.co/posts/onekq/124053264899473", "image": "", "title": "I shared my view on Qwen vs DeepSeek (student vs genius), and I forgot to mention this: they are neighbors in the same city.", "content_text": "I shared my view on Qwen vs DeepSeek (student vs genius), and I forgot to mention this: they are neighbors in the same city. https://en.wikipedia.org/wiki/Hangzhou See translation", "url": "https://huggingface.co/posts/onekq/124053264899473", "date_published": "2025-03-25T17:19:55.373401"}, {"id": "https://huggingface.co/posts/Kseniase/498106595218801", "image": "", "title": "8 types of RoPE", "content_text": "8 types of RoPE As we always use Transformers, it's helpful to understand RoPE\u2014Rotary Position Embedding. Since token order matters, RoPE encodes it by rotating token embeddings based on their position, so the model knows how to interpret which token comes first, second, and so on. Here are 8 types of RoPE that can be implemented in different cases: 1. Original RoPE -> RoFormer: Enhanced Transformer with Rotary Position Embedding (2104.09864) Encodes token positions by rotating token embeddings in the complex plane via a position-based rotation matrix, thereby providing the self-attention mechanism with relative positional info. 2. LongRoPE -> LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens (2402.13753) Extends the context window of pre-trained LLMs to 2048k tokens, leveraging non-uniformities in positional interpolation with an efficient search. 3. LongRoPE2 -> LongRoPE2: Near-Lossless LLM Context Window Scaling (2502.20082) Extends the effective context window of...", "url": "https://huggingface.co/posts/Kseniase/498106595218801", "date_published": "2025-03-25T17:19:55.374067"}, {"id": "https://huggingface.co/posts/merve/746832157330905", "image": "", "title": "So many open releases at Hugging Face past week \ud83e\udd2f recapping all here \u2935\ufe0f", "content_text": "So many open releases at Hugging Face past week \ud83e\udd2f recapping all here \u2935\ufe0f merve/march-21-releases-67dbe10e185f199e656140ae \ud83d\udc40 Multimodal > Mistral AI released a 24B vision LM, both base and instruction FT versions, sota \ud83d\udd25 (OS) > with IBM we released SmolDocling, a sota 256M document parser with Apache 2.0 license (OS) > SpatialLM is a new vision LM that outputs 3D bounding boxes, comes with 0.5B (QwenVL based) and 1B (Llama based) variants > SkyWork released SkyWork-R1V-38B, new vision reasoning model (OS) \ud83d\udcac LLMs > NVIDIA released new Nemotron models in 49B and 8B with their post-training dataset > LG released EXAONE, new reasoning models in 2.4B, 7.8B and 32B > Dataset: Glaive AI released a new reasoning dataset of 22M+ examples > Dataset: NVIDIA released new helpfulness dataset HelpSteer3 > Dataset: OpenManusRL is a new agent dataset based on ReAct framework (OS) > Open-R1 team released OlympicCoder, new competitive coder model in 7B and 32B > Dataset: GeneralThought-430K is a new...", "url": "https://huggingface.co/posts/merve/746832157330905", "date_published": "2025-03-25T17:19:55.374695"}, {"id": "https://huggingface.co/posts/mrs83/394905068174905", "image": "", "title": "\ud83d\ude80 Just released a PoC: Kurtis-E1 MLX Voice Agent", "content_text": "\ud83d\ude80 Just released a PoC: Kurtis-E1 MLX Voice Agent An offline, privacy-first voice assistant built for macOS (Apple Silicon), designed for empathetic, short-form interactions. \ud83e\udde0 Powered by: - Whisper (via MLX) for speech-to-text: https://pypi.org/project/mlx-whisper/ - Kurtis-E1 (a custom SmolLM2 LLM) via Ollama - Coqui-TTS XTTSv2 for multilingual TTS - Optional translation layer via TowerInstruct-13B-v0.1 for non-English voice input/output: Unbabel/TowerInstruct-13B-v0.1 \ud83c\udfa7 Everything runs entirely on-device (Mac Mini M4 Max - 24gb) \u2014 no cloud, no remote API calls, no data leakage. \ud83d\udca1 Code is fully handcrafted (no AI-generated code), and designed to showcase what\u2019s possible with local models, even on laptops. \ud83d\udee0\ufe0f Open to contributions, ideas (e.g., LM Studio for MLX inference, MLX worker subprocess, optimize for latency and VRAM usage). \ud83d\udc49 Video demo (Italian): https://www.youtube.com/watch?v=8-1PcmUStaI PoC: https://github.com/ethicalabs-ai/Kurtis-E1-MLX-Voice-Agent Kurtis-E1:...", "url": "https://huggingface.co/posts/mrs83/394905068174905", "date_published": "2025-03-25T17:19:55.375157"}, {"id": "https://huggingface.co/posts/MikeDoes/593610719403706", "image": "", "title": "\ud83c\udf1f Day 4: Two Models, One Privacy Mission! \ud83c\udf1f", "content_text": "\ud83c\udf1f Day 4: Two Models, One Privacy Mission! \ud83c\udf1f The PII-Masking-1M series rolls on with two gems: Categorical: ai4privacy/llama-ai4privacy-multilingual-categorical-anonymiser-openpii Redaction: ai4privacy/llama-ai4privacy-multilingual-anonymiser-openpii Join us in protecting data everywhere! #AI #Privacy #OpenSource #Multilingual See translation", "url": "https://huggingface.co/posts/MikeDoes/593610719403706", "date_published": "2025-03-25T17:19:55.375439"}, {"id": "https://huggingface.co/posts/csabakecskemeti/287842366376256", "image": "", "title": "I'm collecting llama-bench results for inference with a llama 3.1 8B q4 and q8 reference models on varoius GPUs. The results are average of 5 executions.", "content_text": "I'm collecting llama-bench results for inference with a llama 3.1 8B q4 and q8 reference models on varoius GPUs. The results are average of 5 executions. The system varies (different motherboard and CPU ... but that probably that has little effect on the inference performance). https://devquasar.com/gpu-gguf-inference-comparison/ the exact models user are in the page I'd welcome results from other GPUs is you have access do anything else you've need in the post. Hopefully this is useful information everyone. See translation", "url": "https://huggingface.co/posts/csabakecskemeti/287842366376256", "date_published": "2025-03-25T17:19:55.375755"}, {"id": "https://huggingface.co/posts/prithivMLmods/636017629605073", "image": "", "title": "Dropping Downstream tasks using newly initialized parameters and weights ([classifier.bias & weights]) support domain-specific \ud835\uddf6\ud835\uddfa\ud835\uddee\ud835\uddf4\ud835\uddf2 \ud835\uddf0\ud835\uddf9\ud835\uddee\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddf3\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb. Based on siglip2-base-patch16-224 and DomainNet (single-domain, multi-source adaptation), with Fashion-MNIST for experimental testing. \ud83e\udde4\u2604\ufe0f", "content_text": "Dropping Downstream tasks using newly initialized parameters and weights ([classifier.bias & weights]) support domain-specific \ud835\uddf6\ud835\uddfa\ud835\uddee\ud835\uddf4\ud835\uddf2 \ud835\uddf0\ud835\uddf9\ud835\uddee\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddf3\ud835\uddf6\ud835\uddf0\ud835\uddee\ud835\ude01\ud835\uddf6\ud835\uddfc\ud835\uddfb. Based on siglip2-base-patch16-224 and DomainNet (single-domain, multi-source adaptation), with Fashion-MNIST for experimental testing. \ud83e\udde4\u2604\ufe0f Fashion-Mnist : prithivMLmods/Fashion-Mnist-SigLIP2 Multisource-121 : prithivMLmods/Multisource-121-DomainNet Painting-126 : prithivMLmods/Painting-126-DomainNet Sketch-126 : prithivMLmods/Sketch-126-DomainNet Clipart-126 : prithivMLmods/Clipart-126-DomainNet Models are trained with different parameter settings for experimental purposes only, with the intent of further development. Refer to the model page below for instructions on running it with Transformers \ud83e\udd17. Collection : prithivMLmods/domainnet-0324-67e0e3c934c03cc40c6c8782 Citations : SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features https://arxiv.org/pdf/2502.14786 & Moment...", "url": "https://huggingface.co/posts/prithivMLmods/636017629605073", "date_published": "2025-03-25T17:19:55.376195"}]}