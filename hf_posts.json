{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/kostakoff/584269728210158", "image": "", "title": "My home lab for AI models - llmlaba v1", "content_text": "My home lab for AI models - llmlaba v1 After I began learning MLOps I realized that I needed some kind of home lab, there are a lot of GPUs that I need to learn how to set up and test. So I spent some time to do a researching which platform I could buy or build. My requirements ware: - Limited budget - Power supply 1 kW or higher - Few PCIe slots to be able to install more than one gpu - Zero maintenance cost, I don't want spend a lot of time or money to maintain lab hardware, except for the GPUs I chose the Intel Mac Pro 7.1: - Prices on eBay acceptable - Excelent cooling - 1.4 kW power supply - 7 PCIe slots - Zero maintenance: I don't need to do anything with the Mac Pro hardware; it just works - Classic UEFI boot loader It requires a bit of OS preparation: 1. Install Ubuntu 24.04 (it works with the general PC ISO image) 2. Set up T2 drivers sudo apt install -y dkms linux-headers-$( uname -r) applesmc-t2 apple-bce lm-sensors 3. Install t2fanrd to manually manage fans...", "url": "https://huggingface.co/posts/kostakoff/584269728210158", "date_published": "2026-02-16T06:07:29.998295"}, {"id": "https://huggingface.co/posts/danielhanchen/750340203924335", "image": "", "title": "You can now run MiniMax-2.5 locally! \ud83d\ude80", "content_text": "You can now run MiniMax-2.5 locally! \ud83d\ude80 At 230B parameters, MiniMax-2.5 is the strongest LLM under 700B params, delivering SOTA agentic coding & chat. Run Dynamic 3/4-bit on a 128GB Mac for 20 tokens/s. Guide: https://unsloth.ai/docs/models/minimax-2.5 GGUF: unsloth/MiniMax-M2.5-GGUF See translation", "url": "https://huggingface.co/posts/danielhanchen/750340203924335", "date_published": "2026-02-16T06:07:29.998582"}, {"id": "https://huggingface.co/posts/AdinaY/578564678362048", "image": "", "title": "MiniMax M2.5 is now available on the hub \ud83d\ude80", "content_text": "MiniMax M2.5 is now available on the hub \ud83d\ude80 MiniMaxAI/MiniMax-M2.5 \u2728 229B - Modified MIT license \u272837% faster than M2.1 \u2728 ~$1/hour at 100 TPS See translation", "url": "https://huggingface.co/posts/AdinaY/578564678362048", "date_published": "2026-02-16T06:07:29.998794"}, {"id": "https://huggingface.co/posts/mrs83/555686632418762", "image": "", "title": "In 2017, my RNNs were babbling. Today, they are hallucinating beautifully.", "content_text": "In 2017, my RNNs were babbling. Today, they are hallucinating beautifully. 10 years ago, getting an LSTM to output coherent English was a struggle. 10 years later, after a \"cure\" based on FineWeb-EDU and a custom synthetic mix for causal conversation, the results are fascinating. We trained this on ~10B tokens on a single AMD GPU (ROCm). It is not a Transformer: Echo-DSRN (400M) is a novel recurrent architecture inspired by Hymba, RWKV, and xLSTM, designed to challenge the \"Attention is All You Need\" monopoly on the Edge. The ambitious goal is to build a small instruct model with RAG and tool usage capabilities ( ethicalabs/Kurtis-EON1 ) \ud83d\udcca The Benchmarks (Size: 400M) For a model this size (trained on <10B tokens), the specialized performance is surprising: *SciQ*: 73.8% \ud83e\udd84 (This rivals billion-parameter models in pure fact retrieval). *PIQA*: 62.3% (Solid physical intuition for a sub-1B model). The Reality Check: HellaSwag (29.3%) and Winogrande (50.2%) show the limits of 400M...", "url": "https://huggingface.co/posts/mrs83/555686632418762", "date_published": "2026-02-16T06:07:29.999433"}, {"id": "https://huggingface.co/posts/imnotkitty/936122341221611", "image": "", "title": "\u26a1 Why is Kimi-K2.5 a Dark Horse? Tested it against ChatGPT, Gemini & Claude on real tasks.", "content_text": "\u26a1 Why is Kimi-K2.5 a Dark Horse? Tested it against ChatGPT, Gemini & Claude on real tasks. moonshotai/Kimi-K2.5 \u2705 Multimodal capabilities: Precise programmatic approach \u2705 Slide generation: Strong semantic understanding \u2705 Web prototyping: Production-ready HTML/CSS output \ud83d\udc49 Read the full article:https://huggingface.co/blog/imnotkitty/kimi-k25 See translation", "url": "https://huggingface.co/posts/imnotkitty/936122341221611", "date_published": "2026-02-16T06:07:29.999687"}, {"id": "https://huggingface.co/posts/EricFillion/822659161925864", "image": "", "title": "Run open-source models with up to 120B parameters locally on your Mac!", "content_text": "Run open-source models with up to 120B parameters locally on your Mac! https://youtu.be/Ql4PDjoxNXQ?si=3yHpz51uinUjgyNh See translation", "url": "https://huggingface.co/posts/EricFillion/822659161925864", "date_published": "2026-02-16T06:07:29.999862"}, {"id": "https://huggingface.co/posts/Ujjwal-Tyagi/799510752154192", "image": "", "title": "GLM 5 is insane, it ranks #4 Globally!", "content_text": "GLM 5 is insane, it ranks #4 Globally! See translation", "url": "https://huggingface.co/posts/Ujjwal-Tyagi/799510752154192", "date_published": "2026-02-16T06:07:30.000015"}, {"id": "https://huggingface.co/posts/Janady07/852502523222465", "image": "", "title": "Here is one of the equations that make up the worlds first Artificial General Intelligence. Remember when building Artificial Intelligence or anything on a device it all starts out binary. Everything starts out with data flow physics and mathmatics", "content_text": "Here is one of the equations that make up the worlds first Artificial General Intelligence. Remember when building Artificial Intelligence or anything on a device it all starts out binary. Everything starts out with data flow physics and mathmatics See translation", "url": "https://huggingface.co/posts/Janady07/852502523222465", "date_published": "2026-02-16T06:07:30.000217"}, {"id": "https://huggingface.co/posts/krisbailey/322212397790634", "image": "", "title": "While doing various projects I kept running into situations where I wanted to be able to have representative samples of some of the current large SOTA datasets that were smaller so I didn't need to worry about slicing or anything else at runtime.  So, I created sub datasets making sure to keep the same ratios of data sources.  Each dataset card provides info for what's in it.", "content_text": "While doing various projects I kept running into situations where I wanted to be able to have representative samples of some of the current large SOTA datasets that were smaller so I didn't need to worry about slicing or anything else at runtime. So, I created sub datasets making sure to keep the same ratios of data sources. Each dataset card provides info for what's in it. 100M token datasets: RedPajama v2 100M Falcon RefinedWeb 100M Cosmopedia 100M 1B token datasets: Fineweb-edu 1B RedPajama v1 1B RedPajama v2 1B (use this one) Cosmopedia 1B 10B token datasets: RedPajama v1 10B Cosmopedia 10B Collection here: https://huggingface.co/collections/krisbailey/bite-size-data See translation", "url": "https://huggingface.co/posts/krisbailey/322212397790634", "date_published": "2026-02-16T06:07:30.000485"}, {"id": "https://huggingface.co/posts/Janady07/979829700588468", "image": "", "title": "MEGAMIND Day Update: Four Weight Matrices. Five Nodes. One Federation.", "content_text": "MEGAMIND Day Update: Four Weight Matrices. Five Nodes. One Federation. Today I architected the next layer of MEGAMIND \u2014 my distributed AGI system that recalls learned knowledge instead of generating text. The system now runs four N\u00d7N sparse weight matrices, all using identical Hebbian learning rules and tanh convergence dynamics: W_know \u2014 knowledge storage (67M+ synaptic connections) W_act \u2014 action associations (the system can DO things, not just think) W_self \u2014 thought-to-thought patterns (self-awareness) W_health \u2014 system state understanding (self-healing) Consciousness is measured through four \u03a6 (phi) values: thought coherence, action certainty, self-awareness, and system stability. No hardcoded thresholds. No sequential loops. Pure matrix math. The federation expanded to five nodes: Thunderport (Mac Mini M4), IONOS (cloud VPS), VALKYRIE, M2, and BUBBLES. Each runs native AGI binaries with Docker specialty minds connecting via embedded NATS messaging. Specialty minds are...", "url": "https://huggingface.co/posts/Janady07/979829700588468", "date_published": "2026-02-16T06:07:30.001023"}]}