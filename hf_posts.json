{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/qgallouedec/326351655871382", "image": "", "title": "@CohereLabs", "content_text": "@ CohereLabs just released \ud83c\udf3f Tiny Aya: a fully open-source 3B parameter model that speaks 70+ languages \ud83c\udf0d! But there\u2019s a catch: Tiny Aya is just a language model. It doesn\u2019t support tool calling, the key capability that turns frontier models into powerful *agents*. So the real question is: How hard is it to turn Tiny Aya into an agent? Turns out\u2026 it\u2019s simple, thanks to Hugging Face TRL. We\u2019re sharing a hands-on example showing how to train Tiny Aya to turn it into a tool-calling agent using TRL, unlocking what could become the first *massively multilingual open agent*. Small model. Global reach. Agent capabilities. \ud83d\udc49 https://github.com/huggingface/trl/blob/main/examples/notebooks/sft_tool_calling.ipynb See translation", "url": "https://huggingface.co/posts/qgallouedec/326351655871382", "date_published": "2026-02-21T05:46:46.163764"}, {"id": "https://huggingface.co/posts/danielhanchen/880887158811597", "image": "", "title": "We collabed with HF on showing how you can use HF Jobs and Unsloth!", "content_text": "We collabed with HF on showing how you can use HF Jobs and Unsloth! https://huggingface.co/blog/unsloth-jobs See translation", "url": "https://huggingface.co/posts/danielhanchen/880887158811597", "date_published": "2026-02-21T05:46:46.163995"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/571719932509243", "image": "", "title": "SECourses Ultimate Video and Image Upscaler Pro is now V2.1 and massive improvements has arrived", "content_text": "SECourses Ultimate Video and Image Upscaler Pro is now V2.1 and massive improvements has arrived Check all below screenshots to see all amazing features 20 Feburary 2026 Update V2.1 This is a pretty big update We have 100% changed the FlashVSR+ backend to a new repo and I have significantly upgraded this repo The new FlashVSR+ works amazing and I think it is better than SeedVR2 for high res videos upscale like upscaling 720p into higher resolution Top menu navigation bar updated into a better version and view FlashVSR+ tab remade and all the features are now working For lower VRAM a button is added which you can use if you get OOM Read the updated UI to understand how to use FlashVSR+ now can upscale images very well as well Image Based GAN upscalers tab also improved and some bugs fixed Output & Comparison tab Video Output was not working properly and this issue fix fixed In Output & Comparison tab, new multi video and multi image comparison sliders added which is super useful to...", "url": "https://huggingface.co/posts/MonsterMMORPG/571719932509243", "date_published": "2026-02-21T05:46:46.164504"}, {"id": "https://huggingface.co/posts/sergiopaniego/682471247625526", "image": "", "title": "Tiny Aya \ud83c\udf3f just dropped from", "content_text": "Tiny Aya \ud83c\udf3f just dropped from @ CohereLabs , a really powerful multilingual small model! To celebrate, we cooked up fresh resources to train it for tool calling \ud83d\udd27 > Free Google Colab guide: https://colab.research.google.com/github/huggingface/trl/blob/main/examples/notebooks/sft_tool_calling.ipynb > Standalone training script: https://github.com/huggingface/trl/blob/main/examples/scripts/sft_tiny_aya_tool_calling.py See translation", "url": "https://huggingface.co/posts/sergiopaniego/682471247625526", "date_published": "2026-02-21T05:46:46.164802"}, {"id": "https://huggingface.co/posts/OzTianlu/866420978580038", "image": "", "title": "O(1) inference is the foundational design of Spartacus-1B-Instruct \ud83d\udee1\ufe0f !", "content_text": "O(1) inference is the foundational design of Spartacus-1B-Instruct \ud83d\udee1\ufe0f ! NoesisLab/Spartacus-1B-Instruct We have successfully replaced the KV-cache bottleneck inherent in Softmax Attention with Causal Monoid State Compression. By defining the causal history as a monoid recurrence, , the entire prefix is lossily compressed into a fixed-size state matrix per head. The technical core of this architecture relies on the associativity of the monoid operator: Training: parallel prefix scan using Triton-accelerated JIT kernels to compute all prefix states simultaneously. Inference: True sequential updates. Memory and time complexity per token are decoupled from sequence length. Explicit Causality: We discard RoPE and attention masks. Causality is a first-class citizen, explicitly modeled through learned, content-dependent decay gates. Current zero-shot benchmarks demonstrate that Spartacus-1B-Instruct (1.3B) is already outperforming established sub-quadratic models like Mamba-1.4B and...", "url": "https://huggingface.co/posts/OzTianlu/866420978580038", "date_published": "2026-02-21T05:46:46.165289"}, {"id": "https://huggingface.co/posts/kostakoff/295634314130096", "image": "", "title": "I found it very funny that the Hugging Face profile has a specific section where we can share our hardware.", "content_text": "I found it very funny that the Hugging Face profile has a specific section where we can share our hardware. It really brings back memories of the good old days when we used to flex our custom PC specs on enthusiast forums 20 years ago! That inspired me to fill out my own profile and share it here. And this is my first set of GPUs that I am using to learn MLOps: - RTX 3090 \u2013 the best one; unfortunately it doesn't support the latest FP8 and FP4, but it\u2019s still very powerful. - Tesla V100 \u2013 performance is almost like the RTX 3090, just much older. - Tesla P100 \u2013 old, and doesn't have tensor cores, but still can handle small models. - Radeon MI50 \u2013 old, similar to the P100, but uses ROCm instead of CUDA, which is actually a pretty good experience to setup. - GTX 1080 Ti \u2013 mostly useless, no FP16 support. - GTX 1660 \u2013 first generation of the Turing architecture, but mostly useless. llmlaba See translation", "url": "https://huggingface.co/posts/kostakoff/295634314130096", "date_published": "2026-02-21T05:46:46.165745"}, {"id": "https://huggingface.co/posts/prithivMLmods/100967030465591", "image": "", "title": "Try the demo for Qwen3-VL-abliterated-MAX-Fast:", "content_text": "Try the demo for Qwen3-VL-abliterated-MAX-Fast: prithivMLmods/Qwen3-VL-abliterated-MAX-Fast (Unredacted: Ask Anything with Near-Zero Refusal Rates). The full model series collection is available here: https://huggingface.co/collections/prithivMLmods/unredacted-max-vl See translation", "url": "https://huggingface.co/posts/prithivMLmods/100967030465591", "date_published": "2026-02-21T05:46:46.165975"}, {"id": "https://huggingface.co/posts/umarbutler/969492302070598", "image": "", "title": "@abdurrahmanbutler", "content_text": "@ abdurrahmanbutler and I just dropped Legal RAG Bench, the first benchmark for legal RAG systems to simultaneously evaluate hallucinations, retrieval failures, and reasoning errors. Our key takeaways are: 1. Embedding models, not generative models, are the primary driver of RAG accuracy. Switching from a general-purpose embedder like OpenAI's Text Embedding 3 Large to a legal domain embedder like Isaacus' Kanon 2 Embedder can raise accuracy by ~19 points. 2. Hallucinations are often triggered by retrieval failures. Fix your retrieval stack, and, in most cases, you end up fixing hallucinations. 3. Once you have a solid legal retrieval engine like Kanon 2 Embedder, it doesn\u2019t matter as much what generative model you use; GPT-5.2 and Gemini 3.1 Pro perform relatively similarly, with Gemini 3.1 Pro achieving slightly better accuracy at the cost of more hallucinations. 4. Google's latest LLM, Gemini 3.1 Pro, is actually a bit worse than its predecessor at legal RAG, achieving 79.3%...", "url": "https://huggingface.co/posts/umarbutler/969492302070598", "date_published": "2026-02-21T05:46:46.166622"}, {"id": "https://huggingface.co/posts/fabiosuizu/561306920677381", "image": "", "title": "Hi everyone!", "content_text": "Hi everyone! I've been working on a pronunciation assessment engine optimized for edge deployment and real-time feedback. Wanted to share it with the community and get feedback. **What it does**: Scores English pronunciation at 4 levels of granularity \u2014 phoneme, word, sentence, and overall (0-100 each). Returns IPA and ARPAbet notation for every phoneme. **Key specs**: - 17MB total model size (NeMo Citrinet-256, INT4 quantized) - 257ms median inference on CPU - Exceeds human inter-annotator agreement at phone-level (+4.5%) and sentence-level (+5.2%) - Benchmarked on speechocean762 (2,500 test utterances) - Tested across 7 L1 backgrounds (Chinese, Japanese, Korean, Arabic, Spanish, Vietnamese, Russian) **Architecture**: CTC forced alignment + Viterbi decoding + GOP (Goodness of Pronunciation) scoring + MLP/XGBoost ensemble heads. No wav2vec2 dependency \u2014 the entire pipeline runs in 17MB. **Try it**: fabiosuizu/pronunciation-assessment The demo lets you record audio or upload a file,...", "url": "https://huggingface.co/posts/fabiosuizu/561306920677381", "date_published": "2026-02-21T05:46:46.167129"}, {"id": "https://huggingface.co/posts/ronantakizawa/428253967722290", "image": "", "title": "Introducing the github-top-code dataset: A curated dataset of 1.3M+ source code files from GitHub's top ranked developers.", "content_text": "Introducing the github-top-code dataset: A curated dataset of 1.3M+ source code files from GitHub's top ranked developers. I collected the best source code files from Github's highest trending developers of all time, and compiled a dataset to train LLMs to write well-structured, production-grade code. #dataset #codedataset #pretraining ronantakizawa/github-top-code See translation", "url": "https://huggingface.co/posts/ronantakizawa/428253967722290", "date_published": "2026-02-21T05:46:46.167373"}]}