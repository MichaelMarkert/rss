{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/openfree/674788441765421", "image": "", "title": "\ud83e\udde0 SOMA: The Core Architecture for AGI Level 1 \ud83d\ude80", "content_text": "\ud83e\udde0 SOMA: The Core Architecture for AGI Level 1 \ud83d\ude80 VIDraft/SOMA-AGI \ud83c\udfaf The First Step Toward AGI SOMA (Self-Orchestrating Modular Architect) is a revolutionary architecture that fulfills the essential requirements for AGI (Artificial General Intelligence) Level 1. It perfectly implements the common AGI prerequisites emphasized by Yann LeCun (Meta), OpenAI, and Google DeepMind within a single LLM. \ud83d\udccb AGI Level 1 Core Requirements = SOMA's Perfect Implementation \u2705 \ud83c\udfaf Planning Capability \u2192 Supervisor AI autonomously designs and executes comprehensive analysis roadmaps \ud83e\udde9 Role Differentiation & Modularity \u2192 A single LLM instantly differentiates into 5 expert AIs for collaboration \ud83d\udd04 Self-reflection & Feedback Loops \u2192 Evaluator AI continuously validates and directs improvements \ud83d\udee0\ufe0f Tool-use & Autonomy \u2192 Full automation from web search to report generation \ud83c\udfae Long-term Agency Structure \u2192 Completes complex 11-stage collaborative processes end-to-end \ud83d\udd37 SOMA's Three Core Structures \ud83e\udded Self-...", "url": "https://huggingface.co/posts/openfree/674788441765421", "date_published": "2025-06-30T05:26:53.770959"}, {"id": "https://huggingface.co/posts/eaddario/678649623001881", "image": "", "title": "Layer-wise and Pruned versions of Qwen/Qwen3-30B-A3B", "content_text": "Layer-wise and Pruned versions of Qwen/Qwen3-30B-A3B * Tesor-wise: eaddario/Qwen3-30B-A3B-GGUF * Pruned: eaddario/Qwen3-30B-A3B-pruned-GGUF Even though the Perplexity scores of the pruned version are 3 times higher, the ARC, HellaSwag, MMLU, Truthful QA and WinoGrande scores are holding remarkably well, considering two layers were removed (5 and 39). This seems to support Xin Men et al conclusions in ShortGPT: Layers in Large Language Models are More Redundant Than You Expect (2403.03853) Results summary in the model's card and test results in the ./scores directory. Questions/feedback is always welcomed. See translation", "url": "https://huggingface.co/posts/eaddario/678649623001881", "date_published": "2025-06-30T05:26:53.771291"}, {"id": "https://huggingface.co/posts/Kseniase/947704683052150", "image": "", "title": "10 Open-source Deep Research assistants", "content_text": "10 Open-source Deep Research assistants Deep Research agents are quickly becoming our daily co-workers \u2014 built for complex investigations, not just chat. With modular architecture, advanced tool use and real web access, they go far beyond typical AI. While big-name agents get the spotlight, we want to highlight some powerful recent open-source alternatives: 1. DeerFlow -> https://github.com/bytedance/deer-flow A modular multi-agent system combining LMs and tools for automated research and code analysis. It links a coordinator, planner, team of specialized agent, and reporter, and converts reports to speech via Text-to-Speech (TTS) 2. Alita -> https://github.com/CharlesQ9/Alita Uses a single problem-solving module for scalable reasoning through simplicity. It self-evolves by generating and reusing Model Context Protocols (MCPs) from open-source tools to build external capabilities for diverse tasks 3. WebThinker -> https://github.com/RUC-NLPIR/WebThinker Lets reasoning models...", "url": "https://huggingface.co/posts/Kseniase/947704683052150", "date_published": "2025-06-30T05:26:53.771906"}, {"id": "https://huggingface.co/posts/wewittc/347088512345422", "image": "", "title": "I find it really annoying that you can use a Zero GPU model that will throw an error but it\u2019ll still take the processing time from your quota. So youre just left with less time and nothing to show for it.", "content_text": "I find it really annoying that you can use a Zero GPU model that will throw an error but it\u2019ll still take the processing time from your quota. So youre just left with less time and nothing to show for it. See translation", "url": "https://huggingface.co/posts/wewittc/347088512345422", "date_published": "2025-06-30T05:26:53.772138"}, {"id": "https://huggingface.co/posts/AdinaY/115721386328243", "image": "", "title": "Hunyuan-A13B \ud83d\udd25 New MoE LLM by TencentHunyuan", "content_text": "Hunyuan-A13B \ud83d\udd25 New MoE LLM by TencentHunyuan tencent/Hunyuan-A13B-Instruct \u272880B total / 13B active params \u2728256K context window \u2728Dual-mode reasoning: fast & slow thinking \u2728Efficient inference (GQA + quantization) See translation", "url": "https://huggingface.co/posts/AdinaY/115721386328243", "date_published": "2025-06-30T05:26:53.772384"}, {"id": "https://huggingface.co/posts/fdaudens/569299354714492", "image": "", "title": "Three big AI copyright updates this week alone. Tracking it all is getting almost impossible!", "content_text": "Three big AI copyright updates this week alone. Tracking it all is getting almost impossible! That\u2019s why @ BrigitteTousi and I built this interactive tracker to keep you up to date fdaudens/ai-copyright-lawsuits (Prototyped in minutes with DeepSite!) See translation", "url": "https://huggingface.co/posts/fdaudens/569299354714492", "date_published": "2025-06-30T05:26:53.772623"}, {"id": "https://huggingface.co/posts/YerbaPage/653320741659718", "image": "", "title": "Curated list of **Next Gen Code Generation** papers & benchmarks! \ud83d\udd25 with 100+ \u2b50\ufe0f now!", "content_text": "Curated list of **Next Gen Code Generation** papers & benchmarks! \ud83d\udd25 with 100+ \u2b50\ufe0f now! Stay ahead with the latest in: \u2705 Repo-level Issue Resolution (SWE-bench, Agents) \u2705 Repo-level Code Completion (Repo understanding) \u2705 Repo-level Code QA/Translation \u2705 Datasets & Benchmarks \ud83d\udc49 Check it out: https://github.com/YerbaPage/Awesome-Repo-Level-Code-Generation \ud83d\udd25 \ud83d\udca1PRs are welcomed! See translation", "url": "https://huggingface.co/posts/YerbaPage/653320741659718", "date_published": "2025-06-30T05:26:53.772890"}, {"id": "https://huggingface.co/posts/ginipick/250323904227045", "image": "", "title": "\ud83c\udfa8 Flux-Kontext FaceLORA - AI Portrait Style Transfer", "content_text": "\ud83c\udfa8 Flux-Kontext FaceLORA - AI Portrait Style Transfer \ud83c\udf1f Introduction Transform your photos into masterpieces! Flux-Kontext FaceLORA is an innovative AI-powered tool that converts portrait photos into various artistic styles using cutting-edge technology. ginigen/Flux-Kontext-FaceLORA \u2728 Key Features \ud83d\udcf8 Easy to Use: Upload photo \u2192 Select style \u2192 Click Generate! \ud83c\udfa8 7 Art Styles: Famous painter styles including Van Gogh, Monet, Renoir \ud83e\udd16 Face Preservation: AI maintains your facial features while transforming the style \u26a1 Fast Generation: Get results in seconds with ZeroGPU support \ud83c\udfaf Custom LoRA: Use any LoRA model from HuggingFace \ud83d\uddbc\ufe0f Available Styles \ud83c\udfef Studio Ghibli - Whimsical anime art style \ud83c\udf0a Winslow Homer - American realist watercolor \ud83c\udf3b Van Gogh - Post-impressionist with swirling brushstrokes \ud83c\udf4e Paul C\u00e9zanne - Geometric post-impressionist structure \ud83c\udf38 Renoir - Impressionist with soft luminous light \ud83e\udeb7 Claude Monet - Impressionist light and color \u2694\ufe0f Fantasy Art - Epic magical character...", "url": "https://huggingface.co/posts/ginipick/250323904227045", "date_published": "2025-06-30T05:26:53.773472"}, {"id": "https://huggingface.co/posts/blaise-tk/599826348587266", "image": "", "title": "A few months ago, I shared that I was building with", "content_text": "A few months ago, I shared that I was building with @ deeivihh something like \"the Steam for open source apps\"... \ud83d\ude80 Today, I\u2019m excited to announce that Dione is now open source and live in public beta! Our mission is simple: make it easier to discover, use, and contribute to open source applications. \ud83d\udd17 GitHub: https://github.com/dioneapp/dioneapp \ud83d\udcac Join the community: https://discord.gg/JDFJp33vrM Want to give it a try? I\u2019d love your feedback! \ud83d\udc40 See translation", "url": "https://huggingface.co/posts/blaise-tk/599826348587266", "date_published": "2025-06-30T05:26:53.773767"}, {"id": "https://huggingface.co/posts/codelion/512779846806813", "image": "", "title": "\ud83d\ude80 Just published: \"OpenEvolve: Open-Source Evolutionary Code Optimization with Real-World GPU Kernel Discovery\"", "content_text": "\ud83d\ude80 Just published: \"OpenEvolve: Open-Source Evolutionary Code Optimization with Real-World GPU Kernel Discovery\" We built the first open-source implementation of Google's AlphaEvolve system and used it to automatically discover GPU kernel optimizations that outperform human engineers! Key results: - 21.8% average decode speed improvement on Apple Silicon - 36.7% improvement on long-context transformer attention - Discovered novel vectorization patterns and 2-pass softmax algorithm The system evolved a Metal kernel for Qwen3's Grouped Query Attention from a basic 3-pass implementation into something with sophisticated Apple Silicon optimizations that would take experts months to discover manually. The evolved kernel automatically found the optimal vec<T,8> operations for 128-dim attention heads and fused softmax computation with value accumulation. Really excited about the potential here - imagine evolutionary algorithms automatically discovering optimizations across all our AI...", "url": "https://huggingface.co/posts/codelion/512779846806813", "date_published": "2025-06-30T05:26:53.774217"}]}