{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/merve/227325205054335", "image": "", "title": "IN: video fine-tuning support for", "content_text": "IN: video fine-tuning support for facebook V-JEPA 2 in HF transformers \ud83d\udd25 it comes with > four models fine-tuned on Diving48 and SSv2 dataset facebook/v-jepa-2-6841bad8413014e185b497a6 > FastRTC demo on V-JEPA2 SSv2 qubvel-hf/vjepa2-streaming-video-classification > fine-tuning script on UCF-101 https://gist.github.com/ariG23498/28bccc737c11d1692f6d0ad2a0d7cddb > fine-tuning notebook on UCF-101 https://colab.research.google.com/drive/16NWUReXTJBRhsN3umqznX4yoZt2I7VGc?usp=sharing we're looking forward to see what you will build! \ud83e\udd17 See translation", "url": "https://huggingface.co/posts/merve/227325205054335", "date_published": "2025-06-18T05:24:03.496962"}, {"id": "https://huggingface.co/posts/merve/593764353844896", "image": "", "title": "stop writing CUDA kernels yourself", "content_text": "stop writing CUDA kernels yourself we have launched Kernel Hub: easy optimized kernels for all models on Hugging Face \ud83d\udd25 use them right away! it's where the community populates optimized kernels \ud83e\udd1d this release comes in three parts > Kernel Hub: contains (as of now) 14 kernels > kernels: Python library to load kernels from Kernel Hub > kernel-builder: Nix package to build kernels for PyTorch (made using PyTorch C++ frontend) when building models, your regular workflow should be pulling kernels from Hub and building your model with them \ud83e\udd17 here's a practical example with RMSNorm: 1. pull the kernel from Hub with get_kernel 2. decorate with use_kernel_forward_from_hub 3. inject it to your model we'd love to hear your feedback! \ud83d\ude4f\ud83c\udffb we also welcome kernel contributions by community \ud83e\udd79\ud83d\udc97 - request kernels here: kernels-community/README#1 - check out this org: kernels-community - read the blog: https://huggingface.co/blog/hello-hf-kernels See translation", "url": "https://huggingface.co/posts/merve/593764353844896", "date_published": "2025-06-18T05:24:03.497408"}, {"id": "https://huggingface.co/posts/AdinaY/465653536234983", "image": "", "title": "Kimi-Dev \ud83d\udcbb New coding model by Moonshot AI", "content_text": "Kimi-Dev \ud83d\udcbb New coding model by Moonshot AI moonshotai/Kimi-Dev-72B \u2728 72B - MIT license \u2728 60.4% on SWE-bench Verified \u2728 RL-trained to patch real repos in Docker \u2728 Only rewarded if full test suite passes See translation", "url": "https://huggingface.co/posts/AdinaY/465653536234983", "date_published": "2025-06-18T05:24:03.497647"}, {"id": "https://huggingface.co/posts/frascuchon/832207442100886", "image": "", "title": "Extending datasets just got a whole lot easier! \ud83d\ude80 With Sheets, I was able to create a Spanish version of the popular fka/awesome-chatgpt-prompts dataset in just a few minutes \u23f1\ufe0f.", "content_text": "Extending datasets just got a whole lot easier! \ud83d\ude80 With Sheets, I was able to create a Spanish version of the popular fka/awesome-chatgpt-prompts dataset in just a few minutes \u23f1\ufe0f. Check out the resulting dataset: frascuchon/fka_awesome_chatgpt_es \ud83d\udcca Want to try it out for yourself? Head over to the Sheets space and see how easy it is to extend and modify existing datasets \ud83e\udd2f. The possibilities are endless! \ud83c\udf10 See translation", "url": "https://huggingface.co/posts/frascuchon/832207442100886", "date_published": "2025-06-18T05:24:03.497941"}, {"id": "https://huggingface.co/posts/Kseniase/659118746872319", "image": "", "title": "11 Types of JEPA", "content_text": "11 Types of JEPA Since Meta released the newest V-JEPA 2 this week, we thought it's a good time to revisit a few other interesting JEPA variants. JEPA, or Joint Embedding Predictive Architecture, a self-supervised learning framework that predicts the latent representation of a missing part of the input. Here are 11 JEPA types that you should know about: 1. V-JEPA 2 -> V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning (2506.09985) Trained on 1M+ hours of internet videos and a little bit of robot interaction data, V-JEPA 2 can watch, understand, answer questions, and help robots plan and act in physical world 2. Time-Series-JEPA (TS-JEPA) -> Time-Series JEPA for Predictive Remote Control under Capacity-Limited Networks (2406.04853) It's a time-series predictive model that learns compact, meaningful representations. A self-supervised semantic actor then uses them to generate control commands without raw data 3. Denoising JEPA (D-JEPA) -> Denoising...", "url": "https://huggingface.co/posts/Kseniase/659118746872319", "date_published": "2025-06-18T05:24:03.498636"}, {"id": "https://huggingface.co/posts/pagezyhf/120505097797502", "image": "", "title": "Webinar Alert", "content_text": "Webinar Alert Build your first chatbot with a Hugging Face Spaces frontend and Gaudi-powered backend with @ bconsolvo ! He will teach you how to build an LLM-powered chatbot using Streamlit and Hugging Face Spaces\u2014integrating a model endpoint hosted on an Intel\u00ae Gaudi\u00ae accelerator. Beginners are welcome https://web.cvent.com/event/70e11f23-7c52-4994-a918-96fa9d5e935f/summary See translation", "url": "https://huggingface.co/posts/pagezyhf/120505097797502", "date_published": "2025-06-18T05:24:03.498916"}, {"id": "https://huggingface.co/posts/merve/840226106342254", "image": "", "title": "#CVPR2025 Paper Picks #1", "content_text": "#CVPR2025 Paper Picks #1 VisionZip is a compression technique that reduces number of visual tokens to improve performance AND prefill time for vision language models demo: Senqiao/VisionZip paper: VisionZip: Longer is Better but Not Necessary in Vision Language Models (2412.04467) most of the image tokens are redundant for the LLM, so the authors ask \"are all visual tokens necessary?\" the method is simple: find which tokens have the highest attention score, merge rest of the tokens based on similarity, then merge both their method is both training-free and for fine-tuning the authors report 5 point improvement on average of vision language tasks + 8x improvement in prefilling time for Llava-Next 7B and 13B \ud83e\udd2f removing redundant tokens improve image token quality too \ud83e\udd79 See translation", "url": "https://huggingface.co/posts/merve/840226106342254", "date_published": "2025-06-18T05:24:03.499289"}, {"id": "https://huggingface.co/posts/ginipick/718905723783644", "image": "", "title": "\ud83c\udfac VEO3 Directors - All-in-One AI Video Creation Suite", "content_text": "\ud83c\udfac VEO3 Directors - All-in-One AI Video Creation Suite \ud83d\ude80 What is VEO3 Directors? VEO3 Directors is a revolutionary end-to-end AI video creation platform that transforms your ideas into cinematic reality. From story conception to final video with synchronized audio - all in one seamless workflow! \ud83d\udd17 Try It Now ginigen/VEO3-Directors ginigen/VEO3-Free ginigen/VEO3-Free-mirror \u2728 Key Features \ud83d\udcdd Story Seed Generator \ud83c\udfb2 Instantly generate creative story ideas across multiple genres \ud83c\udf0f Bilingual support (English/Korean) \ud83c\udfad Rich categories: Genre, Setting, Characters, and more \ud83c\udfa5 AI Script & Prompt Crafting \ud83d\udcac Powered by Friendli API for Hollywood-quality prompts \ud83e\udd16 AI Director writes detailed cinematography instructions \ud83c\udfac Professional elements: camera movements, lighting, VFX \ud83c\udfac Video + Audio Generation \ud83c\udfa8 Wan2.1-T2V-14B for stunning visual quality \u26a1 NAG 4-step inference - 10x faster generation \ud83c\udfb5 MMAudio auto-generates matching soundscapes \ud83c\udf9b\ufe0f Full control over resolution, duration, and style...", "url": "https://huggingface.co/posts/ginipick/718905723783644", "date_published": "2025-06-18T05:24:03.499848"}, {"id": "https://huggingface.co/posts/prithivMLmods/854747069091698", "image": "", "title": "The demo for the MonkeyOCR Recognition model, which adopts a Structure-Recognition-Relation (SRR) triplet paradigm, along with Nanonets-OCR-s a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction and other experimental document OCR models, is combined into a single space.", "content_text": "The demo for the MonkeyOCR Recognition model, which adopts a Structure-Recognition-Relation (SRR) triplet paradigm, along with Nanonets-OCR-s a powerful, state-of-the-art image-to-markdown OCR model that goes far beyond traditional text extraction and other experimental document OCR models, is combined into a single space. \u2726 Try the demo here : prithivMLmods/core-OCR \u2726 Try Nanonets-OCR-s demo here : prithivMLmods/Multimodal-OCR \u2937 MonkeyOCR Recognition : echo840/MonkeyOCR \u2937 docscopeOCR-7B-050425-exp : prithivMLmods/docscopeOCR-7B-050425-exp \u2937 coreOCR-7B-050325-preview : prithivMLmods/coreOCR-7B-050325-preview \u2937 Nanonets-OCR-s : nanonets/Nanonets-OCR-s \u2937 Multimodal Implementations : prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 Also, include a sample OCR test using the VisionOCR-3B-061125 model and the Qwen2-VL-OCR-2B-Instruct model. \u2937 Blog : https://huggingface.co/blog/prithivMLmods/visionocr-3b-061125-vs-qwen2-vl-ocr-2b-instruct To know more about it, visit the...", "url": "https://huggingface.co/posts/prithivMLmods/854747069091698", "date_published": "2025-06-18T05:24:03.500263"}, {"id": "https://huggingface.co/posts/sequelbox/131730191560559", "image": "", "title": "a list of what's coming up soon from us:", "content_text": "a list of what's coming up soon from us: - Shining Valiant 3 for Valiant Labs, powered by the full size Celestia 3 and other soon to be released high-difficulty reasoning datasets - a new type of reasoning model and dataset we're very excited about - would love to bring out an alpha release here as soon as possible - more model releases for Esper 3 (weigh in if there are any models you'd like us to prioritize!) - other New Things not sure of the exact release order yet, but we'll look to get everything out as quick as we can :) with excitement, allegra See translation", "url": "https://huggingface.co/posts/sequelbox/131730191560559", "date_published": "2025-06-18T05:24:03.500541"}]}