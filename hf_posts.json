{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/openfree/554631338965323", "image": "", "title": "\ud83c\udf99\ufe0f AI Podcast Generator - Professional Conversation Creation Tool", "content_text": "\ud83c\udf99\ufe0f AI Podcast Generator - Professional Conversation Creation Tool \ud83d\udcd6 Project Overview Transform any URL, PDF, or keyword into professional podcast conversations automatically! This AI-powered tool creates engaging, expert-level dialogues in minutes. \ud83d\ude80 openfree/AI-Podcast \u2728 Key Features: Multiple Input URL: Web articles, blog posts, news content PDF: Research papers, documents, reports Keywords: Topics like \"AI Ethics\", \"Quantum Computing\" \ud83e\udd16 Smart AI Conversation Generation Local LLM: Mistral-Small 24B model for privacy protection API Fallback: Together AI API support Expert Style: In-depth discussions between host and expert Length: 12-20 exchanges for comprehensive coverage \ud83c\udf0f Multilingual Support English: Alex (Host) & Jordan (Expert) Korean: Junsu (Host) & Minho (Expert) \ud83c\udfb5 High-Quality Text-to-Speech Edge-TTS: Natural cloud-based voices Spark-TTS: Local AI voice model MeloTTS: GPU-powered local synthesis \ud83d\udd0d Real-time Information Search Brave Search API for latest information...", "url": "https://huggingface.co/posts/openfree/554631338965323", "date_published": "2025-05-27T13:35:32.547653"}, {"id": "https://huggingface.co/posts/fdaudens/719212082746895", "image": "", "title": "Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity!", "content_text": "Just completed the AI Agents course and wow, that capstone project really makes you understand how to build agents that can handle real-world complexity! The final project uses the GAIA dataset - your agent has to solve tasks like analyzing Excel files, processing audio recordings, answering questions about YouTube videos, and diving into research papers. This isn't toy examples, it's the messy, multimodal stuff agents need to handle in practice. Whether you\u2019re just getting started with agents or want to go deeper with tools like LangChain, LlamaIndex, and SmolAgents, this course has tons of useful stuff. A few key insights: - Code agents are incredibly versatile once you get the architecture right - The sweet spot is finding the right balance of guidance vs autonomy for each use case - Once the logic clicks, the possibilities really are endless - it's like letting LLMs break free from the chatbox The course is free and the certification deadline is July 1st, 2025. The Hugging Face...", "url": "https://huggingface.co/posts/fdaudens/719212082746895", "date_published": "2025-05-27T13:35:32.548160"}, {"id": "https://huggingface.co/posts/clem/541152505631903", "image": "", "title": "It's just become easier to share your apps on the biggest AI app store (aka HF spaces) for unlimited storage, more visibility and community interactions.", "content_text": "It's just become easier to share your apps on the biggest AI app store (aka HF spaces) for unlimited storage, more visibility and community interactions. Just pick a React, Svelte, or Vue template when you create your space or add app_build_command: npm run build in your README's YAML and app_file: build/index.html in your README's YAML block. Or follow this link: https://huggingface.co/new-space?sdk=static Let's build! See translation", "url": "https://huggingface.co/posts/clem/541152505631903", "date_published": "2025-05-27T13:35:32.548421"}, {"id": "https://huggingface.co/posts/jasoncorkill/660720952792703", "image": "", "title": "Benchmark Update:", "content_text": "Benchmark Update: @ google Veo3 (Text-to-Video) Two months ago, we benchmarked @ google \u2019s Veo2 model. It fell short, struggling with style consistency and temporal coherence, trailing behind Runway, Pika, @ tencent , and even @ alibaba-pai . That\u2019s changed. We just wrapped up benchmarking Veo3, and the improvements are substantial. It outperformed every other model by a wide margin across all key metrics. Not just better, dominating across style, coherence, and prompt adherence. It's rare to see such a clear lead in today\u2019s hyper-competitive T2V landscape. Dataset coming soon. Stay tuned. See translation", "url": "https://huggingface.co/posts/jasoncorkill/660720952792703", "date_published": "2025-05-27T13:35:32.548767"}, {"id": "https://huggingface.co/posts/Kseniase/646284586461230", "image": "", "title": "12 Types of JEPA", "content_text": "12 Types of JEPA JEPA, or Joint Embedding Predictive Architecture, is an approach to building AI models introduced by Yann LeCun. It differs from transformers by predicting the representation of a missing or future part of the input, rather than the next token or pixel. This encourages conceptual understanding, not just low-level pattern matching. So JEPA allows teaching AI to reason abstractly. Here are 12 types of JEPA you should know about: 1. I-JEPA -> Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture (2301.08243) A non-generative, self-supervised learning framework designed for processing images. It works by masking parts of the images and then trying to predict those masked parts 2. MC-JEPA -> MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features (2307.12698) Simultaneously interprets video data - dynamic elements (motion) and static details (content) - using a shared encoder 3. V-JEPA...", "url": "https://huggingface.co/posts/Kseniase/646284586461230", "date_published": "2025-05-27T13:35:32.549501"}, {"id": "https://huggingface.co/posts/AdinaY/689443142454195", "image": "", "title": "Orsta \ud83d\udd25 vision language models trained with V-Triune, a unified reinforcement learning system by MiniMax AI", "content_text": "Orsta \ud83d\udd25 vision language models trained with V-Triune, a unified reinforcement learning system by MiniMax AI One-RL-to-See-Them-All/one-rl-to-see-them-all-6833d27abce23898b2f9815a \u2728 7B & 32B with MIT license \u2728 Masters 8 visual tasks: math, science QA, charts, puzzles, object detection, grounding, OCR, and counting \u2728 Uses Dynamic IoU rewards for better visual understanding \u2728Strong performance in visual reasoning and perception See translation", "url": "https://huggingface.co/posts/AdinaY/689443142454195", "date_published": "2025-05-27T13:35:32.549805"}, {"id": "https://huggingface.co/posts/merve/349112163630055", "image": "", "title": "what happened in open AI past week? so many vision LM & omni releases \ud83d\udd25", "content_text": "what happened in open AI past week? so many vision LM & omni releases \ud83d\udd25 merve/releases-23-may-68343cb970bbc359f9b5fb05 multimodal \ud83d\udcac\ud83d\uddbc\ufe0f > new moondream (VLM) is out: it's 4-bit quantized (with QAT) version of moondream-2b, runs on 2.5GB VRAM at 184 tps with only 0.6% drop in accuracy (OS) \ud83c\udf1a > ByteDance released BAGEL-7B, an omni model that understands and generates both image + text. they also released Dolphin, a document parsing VLM \ud83d\udc2c (OS) > Google DeepMind dropped MedGemma in I/O, VLM that can interpret medical scans, and Gemma 3n, an omni model with competitive LLM performance > MMaDa is a new 8B diffusion language model that can generate image and text LLMs > Mistral released Devstral, a 24B coding assistant (OS) \ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb > Fairy R1-32B is a new reasoning model -- distilled version of DeepSeek-R1-Distill-Qwen-32B (OS) > NVIDIA released ACEReason-Nemotron-14B, new 14B math and code reasoning model > sarvam-m is a new Indic LM with hybrid thinking mode, based on Mistral Small (OS) >...", "url": "https://huggingface.co/posts/merve/349112163630055", "date_published": "2025-05-27T13:35:32.550285"}, {"id": "https://huggingface.co/posts/nomadicsynth/536715864633357", "image": "", "title": "Anyone using AI and ML to help neurodivergent people? I'd love to hear what you're doing.", "content_text": "Anyone using AI and ML to help neurodivergent people? I'd love to hear what you're doing. See translation", "url": "https://huggingface.co/posts/nomadicsynth/536715864633357", "date_published": "2025-05-27T13:35:32.550479"}, {"id": "https://huggingface.co/posts/shukdevdatta123/533287086907274", "image": "", "title": "Excited to share my latest project: an AI-powered Educational Content Creator Assistant! \ud83d\udcda\u2728 Built with Gradio and OpenAI, it transforms PDF/DOCX documents into engaging study materials like interactive flashcards, quizzes, summaries, and lesson plans. Features GPU acceleration and downloadable HTML outputs. Perfect for educators and students! \ud83d\ude80 #EdTech #AI #Python #ML #LLM", "content_text": "Excited to share my latest project: an AI-powered Educational Content Creator Assistant! \ud83d\udcda\u2728 Built with Gradio and OpenAI, it transforms PDF/DOCX documents into engaging study materials like interactive flashcards, quizzes, summaries, and lesson plans. Features GPU acceleration and downloadable HTML outputs. Perfect for educators and students! \ud83d\ude80 #EdTech #AI #Python #ML #LLM Features Include: - \u2705 AI-powered content generation - \u2705 Document processing (PDF/DOCX) - \u2705 Interactive flashcards with slider functionality - \u2705 Comprehensive summaries - \u2705 Structured study notes - \u2705 Quiz generation with multiple choice, short answer, and essay questions - \u2705 Mind map structure creation - \u2705 Detailed lesson plans - \u2705 In-depth concept explanations - \u2705 Practice problems (beginner to challenge levels) - \u2705 Downloadable HTML outputs with interactivity - \u2705 Gradio-based user interface - \u2705 OpenAI API integration via OpenRouter - \u2705 GPU acceleration with ZeroGPU - \u2705 Real-time status updates for API and...", "url": "https://huggingface.co/posts/shukdevdatta123/533287086907274", "date_published": "2025-05-27T13:35:32.551001"}, {"id": "https://huggingface.co/posts/yukiarimo/528629553860992", "image": "", "title": "Hello everyone! Good news!", "content_text": "Hello everyone! Good news! The long-awaited Yuna Ai V4 Miru model was finally released: yukiarimo/yuna-ai-v4-miru Now, she can see both images and videos! She has internal knowledge of multiple languages, including primary English, Japanese, and Russian, and she has a built-in ability for Quantum Thinking! Note that half of the features might be unstable. That\u2019s why, for the next half-year, we will not be writing a dataset; it will be created on the fly as she lives! Eventually, she will learn the needed skills in the upcoming interactions! This is a unique model\u2014the first vision model of Yuna with almost 12B parameters (closer to the atomic version, but smarter)! Weights are already on the hub, and support with good documentation will come in a week. Have fun! Please feel free to drop a little donation for our team to help us buy more Colab Compute Units, as more models are on their way! https://www.patreon.com/c/YukiArimo Thank you guys! See translation", "url": "https://huggingface.co/posts/yukiarimo/528629553860992", "date_published": "2025-05-27T13:35:32.551411"}]}