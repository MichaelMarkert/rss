{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/raincandy-u/660562661454335", "image": "", "title": "\ud83e\udd17 Just released Rain-100M, an experimental ~97M-parameter Qwen3-style language model trained from random initialization.", "content_text": "\ud83e\udd17 Just released Rain-100M, an experimental ~97M-parameter Qwen3-style language model trained from random initialization. Repo: raincandy-u/Rain-100M Data: HuggingFaceFW/fineweb-edu , ~3B tokens, English only Tokenizer: custom 16k BPE, context length 4096 Architecture: 12 Transformer layers, hidden size 768, 12 heads, MLP 2048, SiLU, bf16 Rain-100M is a raw base model (not instruction-tuned or safety-aligned), aimed at small-scale research, debugging training pipelines, and CPU/edge experiments. If you run evaluations, finetunes, or visualizations with it, I would be very interested in your results! See translation", "url": "https://huggingface.co/posts/raincandy-u/660562661454335", "date_published": "2026-01-26T05:35:43.779774"}, {"id": "https://huggingface.co/posts/consome2/284818675921006", "image": "", "title": "We\u2019ve released two conversational speech datasets from oto on Hugging Face \ud83e\udd17", "content_text": "We\u2019ve released two conversational speech datasets from oto on Hugging Face \ud83e\udd17 Both are based on real, casual, full-duplex conversations, but with slightly different focuses. Dataset 1: Processed / curated subset otoearth/otoSpeech-full-duplex-processed-141h * Full-duplex, spontaneous multi-speaker conversations * Participants filtered for high audio quality * PII removal and audio enhancement applied * Designed for training and benchmarking S2S or dialogue models Dataset 2: Larger raw(er) release otoearth/otoSpeech-full-duplex-280h * Same collection pipeline, with broader coverage * More diversity in speakers, accents, and conversation styles * Useful for analysis, filtering, or custom preprocessing experiments We intentionally split the release to support different research workflows: clean and ready-to-use vs. more exploratory and research-oriented use. The datasets are currently private, but we\u2019re happy to approve access requests \u2014 feel free to request access if you\u2019re interested....", "url": "https://huggingface.co/posts/consome2/284818675921006", "date_published": "2026-01-26T05:35:43.780256"}, {"id": "https://huggingface.co/posts/codelion/260857666290873", "image": "", "title": "Reverse Engineering a $500M Mystery: From HashHop to Memory-Augmented Language Models", "content_text": "Reverse Engineering a $500M Mystery: From HashHop to Memory-Augmented Language Models I wrote a deep dive into how Magic AI's 100M token context window might work, starting from their HashHop benchmark and building up to MALM - a Memory-Augmented Language Model. Key insight: treating each key as a single token enables perfect retrieval at unlimited context lengths. The article covers: - How HashHop works and why its perfect accuracy is suspicious - Building a tokenized solver that achieves 100% accuracy - Scaling to MALM for real code search tasks - Why this approach could handle 100M+ tokens Read the full article: https://huggingface.co/blog/codelion/reverse-engineering-magic-hashhop Try the model: codelion/malm-165m Code: https://github.com/codelion/hash-hop See translation", "url": "https://huggingface.co/posts/codelion/260857666290873", "date_published": "2026-01-26T05:35:43.780566"}, {"id": "https://huggingface.co/posts/IlyasMoutawwakil/703555138750194", "image": "", "title": "After 2 months of refinement, I'm happy to announce that a lot of Transformers' modeling code is now significantly more torch-compile & export-friendly \ud83d\udd25", "content_text": "After 2 months of refinement, I'm happy to announce that a lot of Transformers' modeling code is now significantly more torch-compile & export-friendly \ud83d\udd25 Why it had to be done \ud83d\udc47 PyTorch's Dynamo compiler is increasingly becoming the default interoperability layer for ML systems. Anything that relies on torch.export or torch.compile, from model optimization to cross-framework integrations, benefits directly when models can be captured as a single dynamo-traced graph ! Transformers models are now easier to: \u2699\ufe0f Compile end-to-end with torch.compile backends \ud83d\udce6 Export reliably via torch.export and torch.onnx.export \ud83d\ude80 Deploy to ONNX / ONNX Runtime, Intel Corporation's OpenVINO, NVIDIA AutoDeploy (TRT-LLM), AMD's Quark, Meta's Executorch and more hardware-specific runtimes. This work aims at unblocking entire TorchDynamo-based toolchains that rely on exporting Transformers across runtimes and accelerators. We are doubling down on Transformers commitment to be a first-class citizen of the...", "url": "https://huggingface.co/posts/IlyasMoutawwakil/703555138750194", "date_published": "2026-01-26T05:35:43.781000"}, {"id": "https://huggingface.co/posts/DavidAU/879154559678091", "image": "", "title": "Uncensored, Heretic GGUF quants of GLM 4.7 (30B-A3B) with correct Llamacpp and all updates ; NEO-CODE Imatrix W 16 bit OTs.", "content_text": "Uncensored, Heretic GGUF quants of GLM 4.7 (30B-A3B) with correct Llamacpp and all updates ; NEO-CODE Imatrix W 16 bit OTs. Also specialized quants (balanced for this model), and all quants are NEO-CODE Imatrix W 16 bit output tensor. DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF \"Reg quants, non-heretic\" : Also 16 bit ot, NEO-CODE Imatrix and specialized: DavidAU/GLM-4.7-Flash-NEO-CODE-Imatrix-MAX-GGUF See translation", "url": "https://huggingface.co/posts/DavidAU/879154559678091", "date_published": "2026-01-26T05:35:43.781239"}, {"id": "https://huggingface.co/posts/Benedictat/483321622062325", "image": "", "title": "Tencent HunyuanImage 3.0-Instruct is seriously impressive", "content_text": "Tencent HunyuanImage 3.0-Instruct is seriously impressive skyrocketed to 2nd place globally on the LMArena leaderboard, only trailing Google Nano-banana Pro. What excites me most is its newly launched image editing and multi-image fusion capabilities its semantic understanding is rock-solid this Instruct-following capability basically enables one-sentence end-to-end workflows, delivering a dimensionality-reducing boost in efficiency. Frankly, it nails the pain points of frontline creators: old photo restoration, text modification, even extracting people from multiple images to create group shots. Previously, tweaking the fusion quality took tons of effort, but now the out-of-the-box realism and emotional expression are top-tier zero cheap AI artifacts \ud83d\udc49 Repo: https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&modelId=Hunyuan-Image-3.0-Instruct technical report\uff1ahttps://arxiv.org/abs/2509.23951 See translation", "url": "https://huggingface.co/posts/Benedictat/483321622062325", "date_published": "2026-01-26T05:35:43.781580"}, {"id": "https://huggingface.co/posts/kelsend/790764867019535", "image": "", "title": "I'm absolutely stunned by the aesthetics of HunyuanImage-3.0", "content_text": "I'm absolutely stunned by the aesthetics of HunyuanImage-3.0 The visual effects of this model are simply beyond imagination it\u2019s every bit as good as NanoBanana, no compromise at all. I fine-tuned my micro-scene prompts by adding text overlays and background effects, and its adaptability is truly breathtaking. With just one prompt, you can generate scene posters for any movie or novel. Every detail, from scene design to text style and atmospheric effects, perfectly aligns with the tone of the original material. No forced elements, just seamless, film-grade visual effects that exactly match what I envisioned. \ud83d\udc49 Repo: https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&modelId=Hunyuan-Image-3.0-Instruct See translation", "url": "https://huggingface.co/posts/kelsend/790764867019535", "date_published": "2026-01-26T05:35:43.781884"}, {"id": "https://huggingface.co/posts/wangbuer999/572194851679348", "image": "", "title": "HunyuanImage 3.0-Instruct just dropped", "content_text": "HunyuanImage 3.0-Instruct just dropped fresh -sourceImage 3.0model! Spent 20 mins testing it on a Messi + retro scrambler fusion case Ran on diffusers v0.26.3 + CUDA 12.1 | 8B MoE params (1.3B activated) | zero VRAM issues strength=0.9 Messi #10 kit/tattoo sharp, moto\u2019s rusted metal texture blurred (classic open-source pain) strength=0.7 Moto/cobblestone background crisp, Messi\u2019s jersey details faded completely strength=0.75 + prompt \"Blend seamlessly, keep all original details\": both subject & background sharp No ControlNet, no manual masking the model\u2019s chain-of-thought reasoning parses image+prompt first Already outperforms Qwen-Image-Edit 2511 (GSB eval +25.7% on single-image edits) | 100% open-source \ud83d\udc49 Repo: https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&modelId=Hunyuan-Image-3.0-Instruct technical report\uff1ahttps://arxiv.org/abs/2509.23951 Anyone else struggled with strength tweaks for fusion? This fixed it for my Messi+moto case did it work as well for yours?...", "url": "https://huggingface.co/posts/wangbuer999/572194851679348", "date_published": "2026-01-26T05:35:43.782260"}, {"id": "https://huggingface.co/posts/prithivMLmods/575874119029258", "image": "", "title": "Introducing QIE-2511-Zoom-Master for highlight-guided area zoom-in, enabling lossless zooming within a drawn square area, and QIE-2511-Object-Remover-v2 for precise object or highlight-guided area cleanup. These experimental adapters are trained based on QIE-2511. Find the adapters below.", "content_text": "Introducing QIE-2511-Zoom-Master for highlight-guided area zoom-in, enabling lossless zooming within a drawn square area, and QIE-2511-Object-Remover-v2 for precise object or highlight-guided area cleanup. These experimental adapters are trained based on QIE-2511. Find the adapters below. \ud83d\udd79\ufe0fQIE-2511-Zoom-Master : prithivMLmods/QIE-2511-Zoom-Master \ud83d\udd79\ufe0fQIE-2511-Object-Remover-v2: prithivMLmods/QIE-2511-Object-Remover-v2 \ud83e\udd17Demo: prithivMLmods/Qwen-Image-Edit-Object-Manipulator \ud83d\udcc2Collection: https://huggingface.co/collections/prithivMLmods/qwen-image-edit-exps To learn more, visit the app page or the respective model pages. See translation", "url": "https://huggingface.co/posts/prithivMLmods/575874119029258", "date_published": "2026-01-26T05:35:43.782551"}, {"id": "https://huggingface.co/posts/branikita/329296258572984", "image": "", "title": "Robonine (Educational Robotics) completed a structural optimization of our 6-DOF robotic manipulator after a structural optimization study. By increasing structural rigidity through topology optimization and design refinement, we reduced end-effector deflection by over 60% (from ~1.05 mm to ~0.41 mm) and improved motion stability. The final configuration delivers higher precision and reliability for industrial applications.", "content_text": "Robonine (Educational Robotics) completed a structural optimization of our 6-DOF robotic manipulator after a structural optimization study. By increasing structural rigidity through topology optimization and design refinement, we reduced end-effector deflection by over 60% (from ~1.05 mm to ~0.41 mm) and improved motion stability. The final configuration delivers higher precision and reliability for industrial applications. Article: https://robonine.com/increasing-the-structural-rigidity-of-the-manipulator/ See translation", "url": "https://huggingface.co/posts/branikita/329296258572984", "date_published": "2026-01-26T05:35:43.782775"}]}