{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/Kseniase/300455492795256", "image": "", "title": "6 Comprehensive Resources on AI Coding", "content_text": "6 Comprehensive Resources on AI Coding AI coding is moving fast, and it\u2019s getting harder to tell what actually works. Agents, workflows, context management and many other aspects are reshaping how software gets built. We\u2019ve collected a set of resources to help you understand how AI coding is evolving today and what building strategies work best: 1. AI Agentic Programming: A Survey of Techniques, Challenges, and Opportunities (2508.11126) Provides a clear taxonomy, compares agent architectures, and exposes practical gaps in tools, benchmarks, and reliability that AI coding agents now struggle with 2. Does AI-Assisted Coding Deliver? A Difference-in-Differences Study of Cursor's Impact on Software Projects (2511.04427) This survey from Carnegie Mellon University shows causal evidence that LLM agent assistants deliver short-term productivity gains but have lasting quality costs that can slow development over time 3. A Survey of Vibe Coding with Large Language Models (2510.12399) Turns...", "url": "https://huggingface.co/posts/Kseniase/300455492795256", "date_published": "2025-12-16T09:31:27.560665"}, {"id": "https://huggingface.co/posts/danielhanchen/963278821580490", "image": "", "title": "NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model! \ud83d\udd25", "content_text": "NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model! \ud83d\udd25 Has 1M context window & best in class performance for SWE-Bench, reasoning & chat. Run the MoE model locally with 24GB RAM. GGUF: unsloth/Nemotron-3-Nano-30B-A3B-GGUF \ud83d\udc9a Step-by-step Guide: https://docs.unsloth.ai/models/nemotron-3 See translation", "url": "https://huggingface.co/posts/danielhanchen/963278821580490", "date_published": "2025-12-16T09:31:27.560971"}, {"id": "https://huggingface.co/posts/daqc/540565360726745", "image": "", "title": "Check out your 2025 Hugging Face Wrapped, a small experimental recap", "content_text": "Check out your 2025 Hugging Face Wrapped, a small experimental recap hf-wrapped/2025 See translation", "url": "https://huggingface.co/posts/daqc/540565360726745", "date_published": "2025-12-16T09:31:27.561189"}, {"id": "https://huggingface.co/posts/DawnC/393405474084583", "image": "", "title": "Intelligent Inpainting for Precise Creative Control \ud83c\udfa8\u2728", "content_text": "Intelligent Inpainting for Precise Creative Control \ud83c\udfa8\u2728 Transform your images with AI-powered precision! SceneWeaver delivers professional-quality image composition with intelligent background replacement and advanced object manipulation. What's New in This Update? \ud83d\udd8c\ufe0f Object Replacement \u2014 Select and transform any element in your scene with natural language prompts while maintaining perfect visual consistency with surrounding content \ud83d\uddd1\ufe0f Object Removal \u2014 Intelligently remove unwanted objects with context-aware generation that preserves natural lighting, shadows, and scene coherence \ud83c\udfaf Context-Aware Processing \u2014 Advanced inpainting technology ensures seamless integration across all regenerated regions Core Capabilities \u26a1 One-click transformation with smart subject detection, 24 curated professional backgrounds, custom scene generation through text prompts, and studio-quality results powered by BiRefNet, Stable Diffusion XL, and ControlNet Inpainting. Current Infrastructure & Future...", "url": "https://huggingface.co/posts/DawnC/393405474084583", "date_published": "2025-12-16T09:31:27.561811"}, {"id": "https://huggingface.co/posts/sanaka87/963485970840656", "image": "", "title": "\ud83d\ude80 Introducing VideoCoF: Unified Video Editing with a Temporal Reasoner (Chain-of-Frames)!", "content_text": "\ud83d\ude80 Introducing VideoCoF: Unified Video Editing with a Temporal Reasoner (Chain-of-Frames)! We\u2019re excited to introduce VideoCoF, a unified framework for instruction-based video editing that enables temporal reasoning and ~4\u00d7 video length extrapolation, trained with only 50k video pairs. \ud83d\udd25 \ud83d\udd0d What makes VideoCoF different? \ud83e\udde0 Chain-of-Frames reasoning , mimic human thinking process like Seeing \u2192 Reasoning \u2192 Editing to apply edits accurately over time without external masks, ensuring physically plausible results. \ud83d\udcc8 Strong length generalization \u2014 trained on 33-frame clips, yet supports multi-shot editing and long-video extrapolation (~4\u00d7). \ud83c\udfaf Unified fine-grained editing \u2014 Object Removal, Addition, Swap, and Local Style Transfer, with instance-level & part-level, spatial-aware control. \u26a1 Fast inference update \ud83d\ude80 H100: ~20s / video with 4-step inference, making high-quality video editing far more practical for real-world use. \ud83d\udd17 Links \ud83d\udcc4 Paper: https://arxiv.org/abs/2512.07469 \ud83d\udcbb Code:...", "url": "https://huggingface.co/posts/sanaka87/963485970840656", "date_published": "2025-12-16T09:31:27.562324"}, {"id": "https://huggingface.co/posts/nicolay-r/350400879019559", "image": "", "title": "\ud83d\udce2 For those who interested in applying LLM for inferring iterators of data with CoT / prompts, this update might be relevant. Deligted to share the new release of the bulk-chain. This is a framework that contributes to efficient AI querying in synthetic data generation scenarios.", "content_text": "\ud83d\udce2 For those who interested in applying LLM for inferring iterators of data with CoT / prompts, this update might be relevant. Deligted to share the new release of the bulk-chain. This is a framework that contributes to efficient AI querying in synthetic data generation scenarios. \ud83c\udf1f bulk-chain: https://github.com/nicolay-r/bulk-chain \ud83d\udd11 This features the no-string framework for quierrying LLMs in various modes: sync, async and with optional support for output streaming. \ud83d\udce6\ufe0f In the latest 1.2.0 release, the updates on outlining API parameters for inference mode. \ud83c\udf1f Integration into web: https://github.com/nicolay-r/bulk-chain-web-integration See translation", "url": "https://huggingface.co/posts/nicolay-r/350400879019559", "date_published": "2025-12-16T09:31:27.562707"}, {"id": "https://huggingface.co/posts/MikeDoes/696146647062201", "image": "", "title": "Making LLMs fast with KV-cache sharing is great. A new paper reports it's also a huge privacy risk.", "content_text": "Making LLMs fast with KV-cache sharing is great. A new paper reports it's also a huge privacy risk. That's why we're excited to see the \"SafeKV\" paper from researchers at the University of Connecticut, Peking University, and others. Their solution-oriented framework selectively shares non-sensitive data while isolating PII. To validate the \"Safe\" part of their system, they needed a robust, multilingual privacy benchmark. We're proud that the Ai4Privacy pii-masking dataset was used for this critical evaluation related to privacy. This is a perfect win-win. Our open-source data enables researchers to build and validate more effective security solutions for core AI infrastructure. Their work, in turn, helps make the entire LLM ecosystem safer, showing that performance and privacy don't have to be mutually exclusive. Kudos to Kexin Chu, Zecheng Lin, Dawei Xiang, \u6c88\u5b50\u65ed, Jianchang Su, cheng chu, Yiwei Yang, Wenhui Zhang, Wenfei Wu, and Wei Zhang on this beautiful work. \ud83d\udd17 Check out their...", "url": "https://huggingface.co/posts/MikeDoes/696146647062201", "date_published": "2025-12-16T09:31:27.563229"}, {"id": "https://huggingface.co/posts/unmodeled-tyler/439099944779481", "image": "", "title": "New Preview Model:", "content_text": "New Preview Model: unmodeled-tyler/vanta-research-loux-preview VANTA Research is excited to announce a small lab preview of our new 675B fine tune, Loux-Large. Loux is an AI model with a sophisticated, rebellious edge designed to assist and collaborate with engineers, builders, and people working on technical projects. If you enjoy working with Loux and would like full access, let us know by liking the space or opening a discussion in the community! See translation", "url": "https://huggingface.co/posts/unmodeled-tyler/439099944779481", "date_published": "2025-12-16T09:31:27.563517"}, {"id": "https://huggingface.co/posts/martinsu/283521322948177", "image": "", "title": "How POTUS Completely Broke My Flash 2.5-Based Guardrail", "content_text": "https://huggingface.co/blog/martinsu/potus-broke-my-pipeline How POTUS Completely Broke My Flash 2.5-Based Guardrail Did quite a bit of deep research on this one, since it IMHO matters. At first I used this story to amuse fellow MLOps guys, but then I went deeper and was surprised. To those who don't want to read too much, in plain English: when you give the model a high-stakes statement that clashes with what it \"knows\" about the world, it gets more brittle. Sometimes to a point of being unusable. Or an even shorter version: do not clash with the model's given worldview\u2014it will degrade to some extent. And in practice, it means that in lower-resource languages like Latvian and Finnish (and probably others), Flash 2.5 is an unreliable guardrail model when something clashes with the model's general \"worldview\". However, I'm sure this degradation applies to other languages and models as well to varying extents. In one totally normal week of MLOps, my news summarization pipeline started...", "url": "https://huggingface.co/posts/martinsu/283521322948177", "date_published": "2025-12-16T09:31:27.564084"}, {"id": "https://huggingface.co/posts/tomaarsen/853653818134091", "image": "", "title": "\ud83d\udc26\u200d\ud83d\udd25 I've just published Sentence Transformers v5.2.0! It introduces multi-processing for CrossEncoder (rerankers), multilingual NanoBEIR evaluators, similarity score outputs in mine_hard_negatives, Transformers v5 support and more. Details:", "content_text": "\ud83d\udc26\u200d\ud83d\udd25 I've just published Sentence Transformers v5.2.0! It introduces multi-processing for CrossEncoder (rerankers), multilingual NanoBEIR evaluators, similarity score outputs in mine_hard_negatives, Transformers v5 support and more. Details: - CrossEncoder multi-processing: Similar to SentenceTransformer and SparseEncoder, you can now use multi-processing with CrossEncoder rerankers. Useful for multi-GPU and CPU settings, and simple to configure: just device=[\"cuda:0\", \"cuda:1\"] or device=[\"cpu\"]*4 on the model.predict or model.rank calls. - Multilingual NanoBEIR Support: You can now use community translations of the tiny NanoBEIR retrieval benchmark instead of only the English one, by passing dataset_id , e.g. dataset_id=\"lightonai/NanoBEIR-de\" for the German benchmark. - Similarity scores in Hard Negatives Mining: When mining for hard negatives to create a strong training dataset, you can now pass output_scores=True to get similarity scores returned. This can be useful for some...", "url": "https://huggingface.co/posts/tomaarsen/853653818134091", "date_published": "2025-12-16T09:31:27.564656"}]}