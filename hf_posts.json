{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/danielhanchen/563432838268940", "image": "", "title": "Mistral's new SOTA coding models Devstral 2 can now be Run locally! (25GB RAM) \ud83d\udc31", "content_text": "Mistral's new SOTA coding models Devstral 2 can now be Run locally! (25GB RAM) \ud83d\udc31 We fixed the chat template, so performance should be much better now! 24B: unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF 123B: unsloth/Devstral-2-123B-Instruct-2512-GGUF \ud83e\udde1Step-by-step Guide: https://docs.unsloth.ai/models/devstral-2 See translation", "url": "https://huggingface.co/posts/danielhanchen/563432838268940", "date_published": "2025-12-14T09:24:53.048548"}, {"id": "https://huggingface.co/posts/martinsu/305383997158992", "image": "", "title": "I wasted days on a GPU node on a bug that shouldn't exist", "content_text": "I wasted days on a GPU node on a bug that shouldn't exist So I was fine-tuning TildeOPEN-30B and the outputs were... weird. Token ID 179 (<0x00>) kept appearing between almost every token pair. Took me a bit to figure out what was going on. Turns out I used the fast tokenizer for training, but the model was trained on the slow one. Silent failure. Well... long story short\u2014TGI uses (forces) the fast tokenizer, no questions asked. And you'll have agile's kryptonite: silent failure. If the model was trained on slow, it's a silent disaster. I got curious and wrote a quick script to check how common this is. Ran it on 6,014 LLM HF models overnight. Roughly 10% of HF model downloads have mismatched tokenizers. Not all mismatches are catastrophic, but some are brutal \u2014 like chat template markers inflating from 1 token to 3, silently wrecking context windows and causing model act weird. This wasn't rigorous research, but the drift is real. And the worst part? 968 models(out of 500+...", "url": "https://huggingface.co/posts/martinsu/305383997158992", "date_published": "2025-12-14T09:24:53.049246"}, {"id": "https://huggingface.co/posts/YatharthS/731858391215619", "image": "", "title": "I just released LayaCodec, a highly efficient neural audio tokenizer/codec for TTS models, far better than most previous audio tokenizers.", "content_text": "I just released LayaCodec, a highly efficient neural audio tokenizer/codec for TTS models, far better than most previous audio tokenizers. \ud83e\udd2f Next-gen TTS models that use this could achieve several 100s of times real-time speed while producing clearer audio!! \ud83e\udd2f GitHub repo: https://github.com/ysharma3501/LayaCodec Model: YatharthS/LayaCodec See translation", "url": "https://huggingface.co/posts/YatharthS/731858391215619", "date_published": "2025-12-14T09:24:53.049540"}, {"id": "https://huggingface.co/posts/XiangpengYang/572526773544518", "image": "", "title": "\ud83d\ude80 Introducing VideoCoF: Unified Video Editing with a Temporal Reasoner (Chain-of-Frames)!", "content_text": "\ud83d\ude80 Introducing VideoCoF: Unified Video Editing with a Temporal Reasoner (Chain-of-Frames)! We\u2019re excited to introduce VideoCoF, a unified framework for instruction-based video editing that enables temporal reasoning and ~4\u00d7 video length extrapolation, trained with only 50k video pairs. \ud83d\udd25 \ud83d\udd0d What makes VideoCoF different? \ud83e\udde0 Chain-of-Frames reasoning , mimic human thinking process like Seeing \u2192 Reasoning \u2192 Editing to apply edits accurately over time without external masks, ensuring physically plausible results. \ud83d\udcc8 Strong length generalization \u2014 trained on 33-frame clips, yet supports multi-shot editing and long-video extrapolation (~4\u00d7). \ud83c\udfaf Unified fine-grained editing \u2014 Object Removal, Addition, Swap, and Local Style Transfer, with instance-level & part-level, spatial-aware control. \u26a1 Fast inference update \ud83d\ude80 H100: ~20s / video with 4-step inference, making high-quality video editing far more practical for real-world use. \ud83d\udd17 Links \ud83d\udcc4 Paper: https://arxiv.org/abs/2512.07469 \ud83d\udcbb Code:...", "url": "https://huggingface.co/posts/XiangpengYang/572526773544518", "date_published": "2025-12-14T09:24:53.050054"}, {"id": "https://huggingface.co/posts/sergiopaniego/621181656886485", "image": "", "title": "\ud83c\udf84 last talk of the year about open AI and HF today at Universidad Rey Juan Carlos for undergrad students", "content_text": "\ud83c\udf84 last talk of the year about open AI and HF today at Universidad Rey Juan Carlos for undergrad students always a pleasure to be back at my alma mater \ud83c\udf85 slides: https://github.com/sergiopaniego/talks See translation", "url": "https://huggingface.co/posts/sergiopaniego/621181656886485", "date_published": "2025-12-14T09:24:53.050304"}, {"id": "https://huggingface.co/posts/prabhatkr/593169299364014", "image": "", "title": "Language Dexterity Benchmark", "content_text": "Language Dexterity Benchmark I am working on a new benchmark to establish human language dexterity. My hypothesis is that certain language allow for more accurate dexterous behaviour - Pointed, unambigous, and confusion-free references of parts of speech in small and large contexts. There are certain languages with high degree of accurate grammar like Sanskrit, Esperanto, and Turkish. I am native Sanskrit speaker. I have plans to establish this benchmark and test this hypothesis across 100 langauges. I have created 25 task prompts for text, image, video and robotics manipulation. We can test langauges across multiple popular models. Here is the github link: https://github.com/ParamTatva-org/Linguistic-Dexterity-Benchmark See translation", "url": "https://huggingface.co/posts/prabhatkr/593169299364014", "date_published": "2025-12-14T09:24:53.050630"}, {"id": "https://huggingface.co/posts/KingNish/784885155042758", "image": "", "title": "Muon vs MuonClip vs Muon+Adamw", "content_text": "Muon vs MuonClip vs Muon+Adamw Muon has gone from an experiment to a mainstream optimizer, but does it hold up for fine\u2011tuning? We ran head\u2011to\u2011head tests on Qwen3\u20114B (10k+ high\u2011quality instruction rows) to find out. Short story: Pure Muon converged fastest at the start, but its gradient\u2011norm spikes made training unstable. MuonClip (Kimi K2\u2019s clipping) stabilizes long pretraining runs, yet in our small\u2011scale fine\u2011tune it underperformed, lower token accuracy and slower convergence. The winner was the hybrid: Muon for 2D layers + AdamW for 1D layers. It delivered the best balance of stability and final performance and even beat vanilla AdamW. Takeaway: for small-scale fine-tuning, hybrid = practical and reliable. Next Step: scale to larger models/datasets to see if Muon\u2019s spikes become catastrophic or if clipping wins out. Full Blog Link: https://huggingface.co/blog/KingNish/optimizer-part1 See translation", "url": "https://huggingface.co/posts/KingNish/784885155042758", "date_published": "2025-12-14T09:24:53.051049"}, {"id": "https://huggingface.co/posts/daqc/540565360726745", "image": "", "title": "Check out your 2025 Hugging Face Wrapped, a small experimental recap", "content_text": "Check out your 2025 Hugging Face Wrapped, a small experimental recap hf-wrapped/2025 See translation", "url": "https://huggingface.co/posts/daqc/540565360726745", "date_published": "2025-12-14T09:24:53.051252"}, {"id": "https://huggingface.co/posts/etemiz/601408246654891", "image": "", "title": "Today's winner is Ling 1T with a score of 38!", "content_text": "Today's winner is Ling 1T with a score of 38! Btw AHA2 is in the works, with more domains, better comparison LLMs and questions, overall better signal. See translation", "url": "https://huggingface.co/posts/etemiz/601408246654891", "date_published": "2025-12-14T09:24:53.051456"}, {"id": "https://huggingface.co/posts/sanaka87/963485970840656", "image": "", "title": "\ud83d\ude80 Introducing VideoCoF: Unified Video Editing with a Temporal Reasoner (Chain-of-Frames)!", "content_text": "\ud83d\ude80 Introducing VideoCoF: Unified Video Editing with a Temporal Reasoner (Chain-of-Frames)! We\u2019re excited to introduce VideoCoF, a unified framework for instruction-based video editing that enables temporal reasoning and ~4\u00d7 video length extrapolation, trained with only 50k video pairs. \ud83d\udd25 \ud83d\udd0d What makes VideoCoF different? \ud83e\udde0 Chain-of-Frames reasoning , mimic human thinking process like Seeing \u2192 Reasoning \u2192 Editing to apply edits accurately over time without external masks, ensuring physically plausible results. \ud83d\udcc8 Strong length generalization \u2014 trained on 33-frame clips, yet supports multi-shot editing and long-video extrapolation (~4\u00d7). \ud83c\udfaf Unified fine-grained editing \u2014 Object Removal, Addition, Swap, and Local Style Transfer, with instance-level & part-level, spatial-aware control. \u26a1 Fast inference update \ud83d\ude80 H100: ~20s / video with 4-step inference, making high-quality video editing far more practical for real-world use. \ud83d\udd17 Links \ud83d\udcc4 Paper: https://arxiv.org/abs/2512.07469 \ud83d\udcbb Code:...", "url": "https://huggingface.co/posts/sanaka87/963485970840656", "date_published": "2025-12-14T09:24:53.051939"}]}