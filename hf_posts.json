{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/AdinaY/115721386328243", "image": "", "title": "Hunyuan-A13B \ud83d\udd25 New MoE LLM by TencentHunyuan", "content_text": "Hunyuan-A13B \ud83d\udd25 New MoE LLM by TencentHunyuan tencent/Hunyuan-A13B-Instruct \u272880B total / 13B active params \u2728256K context window \u2728Dual-mode reasoning: fast & slow thinking \u2728Efficient inference (GQA + quantization) See translation", "url": "https://huggingface.co/posts/AdinaY/115721386328243", "date_published": "2025-06-29T05:26:09.890444"}, {"id": "https://huggingface.co/posts/jsulz/866221600890917", "image": "", "title": "It's been a bit since I took a step back and looked at", "content_text": "It's been a bit since I took a step back and looked at xet-team progress to migrate Hugging Face from Git LFS to Xet, but every time I do it boggles the mind. A month ago there were 5,500 users/orgs on Xet with 150K repos and 4PB. Today? \ud83e\udd17 700,000 users/orgs \ud83d\udcc8 350,000 repos \ud83d\ude80 15PB Meanwhile, our migrations have pushed throughput to numbers that are bonkers. In June, we hit upload speeds of 577Gb/s (crossing 500Gb/s for the first time). These are hard numbers to put into context, but let's try: The latest run of the Common Crawl from commoncrawl was 471 TB. We now have ~32 crawls stored in Xet. At peak upload speed we could move the latest crawl into Xet in about two hours. We're moving to a new phase in the process, so stay tuned. This shift in gears means it's also time to roll up our sleeves and look at all the bytes we have and the value we're adding to the community. I already have some homework from @ RichardErkhov to look at the dedupe across their uploads, and I'll be doing...", "url": "https://huggingface.co/posts/jsulz/866221600890917", "date_published": "2025-06-29T05:26:09.890951"}, {"id": "https://huggingface.co/posts/fdaudens/569299354714492", "image": "", "title": "Three big AI copyright updates this week alone. Tracking it all is getting almost impossible!", "content_text": "Three big AI copyright updates this week alone. Tracking it all is getting almost impossible! That\u2019s why @ BrigitteTousi and I built this interactive tracker to keep you up to date fdaudens/ai-copyright-lawsuits (Prototyped in minutes with DeepSite!) See translation", "url": "https://huggingface.co/posts/fdaudens/569299354714492", "date_published": "2025-06-29T05:26:09.891215"}, {"id": "https://huggingface.co/posts/FlameF0X/436788445658511", "image": "", "title": "SnowflakeCore-G1 development update: We're building a 24-layer transformer with 32K context and 1024 embedding dimensions - pretty ambitious! Even running at batch_size=1 with heavy gradient accumulation, we're hitting memory walls at 300GB RAM. Scaling up to ~1TB will take some time, but the architecture is looking promising. Thanks for following along with the journey! \ud83d\ude05", "content_text": "SnowflakeCore-G1 development update: We're building a 24-layer transformer with 32K context and 1024 embedding dimensions - pretty ambitious! Even running at batch_size=1 with heavy gradient accumulation, we're hitting memory walls at 300GB RAM. Scaling up to ~1TB will take some time, but the architecture is looking promising. Thanks for following along with the journey! \ud83d\ude05 See translation", "url": "https://huggingface.co/posts/FlameF0X/436788445658511", "date_published": "2025-06-29T05:26:09.891485"}, {"id": "https://huggingface.co/posts/a-r-r-o-w/674823997424742", "image": "", "title": "As you might have already heard, FLUX.1-Kontext-dev is now released and taken the generative community by storm!", "content_text": "As you might have already heard, FLUX.1-Kontext-dev is now released and taken the generative community by storm! In case you haven't come across it, you can get started with Kontext using \ud83e\udd17 diffusers. See the official [model]( black-forest-labs/FLUX.1-Kontext-dev ) and [docs]( https://huggingface.co/docs/diffusers/main/en/api/pipelines/flux#flux ). Want to know how inference companies like Fal & Replicate are able to run the model so fast and in under 2 seconds per image? Check out this [gist]( https://gist.github.com/a-r-r-o-w/d08c37e8bd3e9c26b4ce80360be148c6 ) for some details! See translation", "url": "https://huggingface.co/posts/a-r-r-o-w/674823997424742", "date_published": "2025-06-29T05:26:09.891813"}, {"id": "https://huggingface.co/posts/Jaward/802029609122095", "image": "", "title": "Mind2Web 2 is out - this time featuring eval and benchmark for deep research\ud83d\udd25", "content_text": "Mind2Web 2 is out - this time featuring eval and benchmark for deep research\ud83d\udd25 Paper: Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge (2506.21506) Project: https://osu-nlp-group.github.io/Mind2Web-2/ See translation", "url": "https://huggingface.co/posts/Jaward/802029609122095", "date_published": "2025-06-29T05:26:09.892052"}, {"id": "https://huggingface.co/posts/eaddario/678649623001881", "image": "", "title": "Layer-wise and Pruned versions of Qwen/Qwen3-30B-A3B", "content_text": "Layer-wise and Pruned versions of Qwen/Qwen3-30B-A3B * Tesor-wise: eaddario/Qwen3-30B-A3B-GGUF * Pruned: eaddario/Qwen3-30B-A3B-pruned-GGUF Even though the Perplexity scores of the pruned version are 3 times higher, the ARC, HellaSwag, MMLU, Truthful QA and WinoGrande scores are holding remarkably well, considering two layers were removed (5 and 39). This seems to support Xin Men et al conclusions in ShortGPT: Layers in Large Language Models are More Redundant Than You Expect (2403.03853) Results summary in the model's card and test results in the ./scores directory. Questions/feedback is always welcomed. See translation", "url": "https://huggingface.co/posts/eaddario/678649623001881", "date_published": "2025-06-29T05:26:09.892345"}, {"id": "https://huggingface.co/posts/fdaudens/514291892599532", "image": "", "title": "This is what efficient AI looks like: Gemma 3n just dropped - a natively multimodal model that runs entirely on your device. No cloud. No API calls.", "content_text": "This is what efficient AI looks like: Gemma 3n just dropped - a natively multimodal model that runs entirely on your device. No cloud. No API calls. \ud83e\udde0 Text, image, audio, and video - handled locally. \u26a1\ufe0fOnly needs 2B in GPU memory to run \ud83e\udd2f First sub-10B model to hit 1300+ Elo \u2705 Plug-and-play with Hugging Face, MLX, llama.cpp, and more. Plus: Multilingual out of the box (140+ languages), fine-tune in a free Colab notebook. google/gemma-3n-685065323f5984ef315c93f4 See translation", "url": "https://huggingface.co/posts/fdaudens/514291892599532", "date_published": "2025-06-29T05:26:09.892648"}, {"id": "https://huggingface.co/posts/Ameeeee/808240826417311", "image": "", "title": "\ud83e\udd17 Here\u2019s a fun educational video I made to show how Sheets and AI can upgrade your structured content.", "content_text": "\ud83e\udd17 Here\u2019s a fun educational video I made to show how Sheets and AI can upgrade your structured content. Better tables and clearer messages with just a little help from open-source AI! aisheets/sheets See translation", "url": "https://huggingface.co/posts/Ameeeee/808240826417311", "date_published": "2025-06-29T05:26:09.892868"}, {"id": "https://huggingface.co/posts/yeonseok-zeticai/894951796717498", "image": "", "title": "\ud83d\ude80 Real-Time On-Device AI Agent with Polaris-4B \u2014 Run It Yourself, No Cloud, No Cost", "content_text": "\ud83d\ude80 Real-Time On-Device AI Agent with Polaris-4B \u2014 Run It Yourself, No Cloud, No Cost We just deployed a real-time on-device AI agent using the Polaris-4B-Preview model \u2014 one of the top-performing <6B open LLMs on Hugging Face. \ud83d\udcf1 What\u2019s remarkable? This model runs entirely on a mobile device, without cloud, and without any manual optimization. It was built using ZETIC.MLange, and the best part? \u27a1\ufe0f It\u2019s totally automated, free to use, and anyone can do it. You don\u2019t need to write deployment code, tweak backends, or touch device-specific SDKs. Just upload your model \u2014 and ZETIC.MLange handles the rest. \ud83e\udde0 About the Model - Model: Polaris-4B-Preview - Size: ~4B parameters - Ranking: Top 3 on Hugging Face LLM Leaderboard (<6B) - Tokenizer: Token-incremental inference supported - Modifications: None \u2014 stock weights, just optimized for mobile \u2699\ufe0f What ZETIC.MLange Does ZETIC.MLange is a fully automated deployment framework for On-Device AI, built for AI engineers who want to focus on models \u2014...", "url": "https://huggingface.co/posts/yeonseok-zeticai/894951796717498", "date_published": "2025-06-29T05:26:09.893509"}]}