{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/openfree/251902401084370", "image": "", "title": "Datasets Convertor \ud83d\ude80", "content_text": "Datasets Convertor \ud83d\ude80 openfree/Datasets-Convertor Welcome to Datasets Convertor, the cutting-edge solution engineered for seamless and efficient data format conversion. Designed with both data professionals and enthusiasts in mind, our tool simplifies the transformation process between CSV, Parquet, and JSONL, XLS file formats, ensuring that your data is always in the right shape for your next analytical or development challenge. \ud83d\udcbb\u2728 Why Choose Datasets Convertor? In today\u2019s data-driven world, managing and converting large datasets can be a daunting task. Our converter is built on top of robust technologies like Pandas and Gradio, delivering reliable performance with a modern, intuitive interface. Whether you\u2019re a data scientist, analyst, or developer, Datasets Convertor empowers you to effortlessly switch between formats while maintaining data integrity and optimizing storage. Key Features and Capabilities: CSV \u21c6 Parquet Conversion: Easily transform your CSV files into the highly...", "url": "https://huggingface.co/posts/openfree/251902401084370", "date_published": "2025-02-24T05:20:37.564607"}, {"id": "https://huggingface.co/posts/prithivMLmods/319015417669347", "image": "", "title": "It's really interesting about the deployment of a new state of matter in Majorana 1: the world\u2019s first quantum processor powered by topological qubits. If you missed this news this week, here are some links for you:", "content_text": "It's really interesting about the deployment of a new state of matter in Majorana 1: the world\u2019s first quantum processor powered by topological qubits. If you missed this news this week, here are some links for you: \ud83c\udd71\ufe0fTopological qubit arrays: https://arxiv.org/pdf/2502.12252 \u269b\ufe0f Quantum Blog: https://azure.microsoft.com/en-us/blog/quantum/2025/02/19/microsoft-unveils-majorana-1-the-worlds-first-quantum-processor-powered-by-topological-qubits/ \ud83d\udcd6 Read the story: https://news.microsoft.com/source/features/innovation/microsofts-majorana-1-chip-carves-new-path-for-quantum-computing/ \ud83d\udcdd Majorana 1 Intro: https://youtu.be/Q4xCR20Dh1E?si=Z51DbEYnZFp_88Xp \ud83c\udf00The Path to a Million Qubits: https://youtu.be/wSHmygPQukQ?si=TS80EhI62oWiMSHK See translation", "url": "https://huggingface.co/posts/prithivMLmods/319015417669347", "date_published": "2025-02-24T05:20:37.565031"}, {"id": "https://huggingface.co/posts/Kseniase/557379551700019", "image": "", "title": "8 Free Sources about AI Agents:", "content_text": "8 Free Sources about AI Agents: Agents seem to be everywhere and this collection is for a deep dive into the theory and practice: 1. \"Agents\" Google's whitepaper by Julia Wiesinger, Patrick Marlow and Vladimir Vuskovic -> https://www.kaggle.com/whitepaper-agents Covers agents, their functions, tool use and how they differ from models 2. \"Agents in the Long Game of AI. Computational Cognitive Modeling for Trustworthy, Hybrid AI\" book by Marjorie McShane, Sergei Nirenburg, and Jesse English -> https://direct.mit.edu/books/oa-monograph/5833/Agents-in-the-Long-Game-of-AIComputational Explores building AI agents, using Hybrid AI, that combines ML with knowledge-based reasoning 3. \"AI Engineer Summit 2025: Agent Engineering\" 8-hour video -> https://www.youtube.com/watch?v=D7BzTxVVMuw Experts' talks that share insights on the freshest Agent Engineering advancements, such as Google Deep Research, scaling tips and more 4. AI Agents Course from Hugging Face ->...", "url": "https://huggingface.co/posts/Kseniase/557379551700019", "date_published": "2025-02-24T05:20:37.565564"}, {"id": "https://huggingface.co/posts/lysandre/966361810633890", "image": "", "title": "SmolVLM-2 and SigLIP-2 are now part of", "content_text": "SmolVLM-2 and SigLIP-2 are now part of transformers in dedicated releases! They're added on top of the v4.49.0 release, and can be installed from the following tags: v4.49.0-SmolVLM-2 and v4.49.0-SigLIP-2 . This marks a new beginning for the release process of transformers. For the past five years, we've been doing monthly releases featuring many models (v4.49.0, the latest release, features 9 new architectures). Starting with SmolVLM-2 & SigLIP2, we'll now additionally release tags supporting new models on a stable branch. These models are therefore directly available for use by installing from the tag itself. These tags will continue to be updated with fixes applied to these models. Going forward, continue expecting software releases following semantic versioning: v4.50.0 will have ~10 new architectures compared to v4.49.0, as well as a myriad of new features, improvements and bug fixes. Accompanying these software releases, we'll release tags offering brand new models as fast as...", "url": "https://huggingface.co/posts/lysandre/966361810633890", "date_published": "2025-02-24T05:20:37.565989"}, {"id": "https://huggingface.co/posts/jjokah/811590134220421", "image": "", "title": "The past few years have been a blast for artificial intelligence, with large language models (LLMs) stunning everyone with their capabilities and powering everything from chatbots to code assistants. However, not all applications demand the massive size and complexity of LLMs, the computational power required makes them impractical for many use cases. This is why Small Language Models (SLMs) entered the scene to make powerful AI models more accessible by shrinking in size.", "content_text": "The past few years have been a blast for artificial intelligence, with large language models (LLMs) stunning everyone with their capabilities and powering everything from chatbots to code assistants. However, not all applications demand the massive size and complexity of LLMs, the computational power required makes them impractical for many use cases. This is why Small Language Models (SLMs) entered the scene to make powerful AI models more accessible by shrinking in size. In this article we went through what SLMs are, how they are made small, their benefits and limitations, real-world use cases, and how they can be used on mobile and desktop devices. https://huggingface.co/blog/jjokah/small-language-model See translation", "url": "https://huggingface.co/posts/jjokah/811590134220421", "date_published": "2025-02-24T05:20:37.566332"}, {"id": "https://huggingface.co/posts/nicolay-r/986619870856670", "image": "", "title": "\ud83d\udce2 If you're looking for translating massive dataset of JSON-lines / CSV data with various set of source fields, then the following update would be relevant. So far and experimenting with adapting language specific Sentiment Analysis model, got a change to reforge and relaese bulk-translate 0.25.2.", "content_text": "\ud83d\udce2 If you're looking for translating massive dataset of JSON-lines / CSV data with various set of source fields, then the following update would be relevant. So far and experimenting with adapting language specific Sentiment Analysis model, got a change to reforge and relaese bulk-translate 0.25.2. \u2b50\ufe0f https://github.com/nicolay-r/bulk-translate/releases/tag/0.25.2 The update has the following major features - Supporting schemas: all the columns to be translated are now could be declared within the same prompt-style format. using json this automatically allows to map them onto output fields - The related updates for shell execution mode: schema parameter is now available alongside with just a prompt usage before. Benefit is that your output is invariant. You can extend and stack various translators with separated shell laucnhes. Screenshot below is the application of the google-translate engine in manual batching mode. \ud83d\ude80 Performance: 2.5 it / sec (in the case of a single field...", "url": "https://huggingface.co/posts/nicolay-r/986619870856670", "date_published": "2025-02-24T05:20:37.566832"}, {"id": "https://huggingface.co/posts/fdaudens/422173269922572", "image": "", "title": "Trying something new to keep you ahead of the curve: The 5 AI stories of the week - a weekly curation of the most important AI news you need to know. Do you like it?", "content_text": "Trying something new to keep you ahead of the curve: The 5 AI stories of the week - a weekly curation of the most important AI news you need to know. Do you like it? For more AI stories and deeper analysis, check out my newsletter: https://open.substack.com/pub/fdaudens/p/ai-competition-heats-up-grok-3-iphone See translation", "url": "https://huggingface.co/posts/fdaudens/422173269922572", "date_published": "2025-02-24T05:20:37.567121"}, {"id": "https://huggingface.co/posts/mmhamdy/257109472269745", "image": "", "title": "\ud83c\udf89 We're excited to introduce MemoryCode, a novel synthetic dataset designed to rigorously evaluate LLMs' ability to track and execute coding instructions across multiple sessions. MemoryCode simulates realistic workplace scenarios where a mentee (the LLM) receives coding instructions from a mentor amidst a stream of both relevant and irrelevant information.", "content_text": "\ud83c\udf89 We're excited to introduce MemoryCode, a novel synthetic dataset designed to rigorously evaluate LLMs' ability to track and execute coding instructions across multiple sessions. MemoryCode simulates realistic workplace scenarios where a mentee (the LLM) receives coding instructions from a mentor amidst a stream of both relevant and irrelevant information. \ud83d\udca1 But what makes MemoryCode unique?! The combination of the following: \u2705 Multi-Session Dialogue Histories: MemoryCode consists of chronological sequences of dialogues between a mentor and a mentee, mirroring real-world interactions between coworkers. \u2705 Interspersed Irrelevant Information: Critical instructions are deliberately interspersed with unrelated content, replicating the information overload common in office environments. \u2705 Instruction Updates: Coding rules and conventions can be updated multiple times throughout the dialogue history, requiring LLMs to track and apply the most recent information. \u2705 Prospective Memory:...", "url": "https://huggingface.co/posts/mmhamdy/257109472269745", "date_published": "2025-02-24T05:20:37.567788"}, {"id": "https://huggingface.co/posts/stas/738116252437953", "image": "", "title": "Do you want ArcticTraining at", "content_text": "Do you want ArcticTraining at @ SnowflakeDB to add an ability to post-train DeepSeek V3/R1 models with DPO using just a few GPU nodes? Please vote here and tell others about it: https://github.com/snowflakedb/ArcticTraining/discussions/58 ArcticTraining is an open-source, easy to use post-training framework for NVIDIA GPUs built on top of DeepSpeed. See translation", "url": "https://huggingface.co/posts/stas/738116252437953", "date_published": "2025-02-24T05:20:37.568095"}, {"id": "https://huggingface.co/posts/JingzeShi/354750943862398", "image": "", "title": "\ud83e\udd17Welcome to the Doge Edge Device Small language Model.", "content_text": "\ud83e\udd17Welcome to the Doge Edge Device Small language Model. SmallDoge/Doge-160M-Instruct See translation", "url": "https://huggingface.co/posts/JingzeShi/354750943862398", "date_published": "2025-02-24T05:20:37.568328"}]}