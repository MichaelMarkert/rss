{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/MonsterMMORPG/210519935872315", "image": "", "title": "Wan 2.2 Complete Training Tutorial - Text to Image, Text to Video, Image to Video, Windows & Cloud :", "content_text": "Wan 2.2 Complete Training Tutorial - Text to Image, Text to Video, Image to Video, Windows & Cloud : https://youtu.be/ocEkhAsPOs4 Wan 2.2 training is now so easy. I have done over 64 different unique Wan 2.2 trainings to prepare the very best working training configurations for you. The configurations are fully working locally with as low as 6 GB GPUs. So you will be able to train your awesome Wan 2.2 image or video generation LoRAs on your Windows computer with easiness. Moreover, I have shown how to train on cloud platforms RunPod and Massed Compute so even if you have no GPU or you want faster training, you can train on cloud for very cheap prices fully privately. Full step by step tutorial : https://youtu.be/ocEkhAsPOs4 \u23f1\ufe0f Video Chapters: 0:00 Introduction to Wan 2.2 Training & Capabilities 0:56 Installing & Updating Musubi Tuner Locally 2:20 Explanation of Optimized Presets & Research Logic 4:00 Differences Between T2I, T2V, and I2V Configs 5:36 Extracting Files & Running...", "url": "https://huggingface.co/posts/MonsterMMORPG/210519935872315", "date_published": "2025-12-23T05:29:09.846851"}, {"id": "https://huggingface.co/posts/DawnC/213826857024975", "image": "", "title": "PawMatchAI \u2014 Smarter, Safer, and More Thoughtful Recommendations \ud83d\udc15\u2728", "content_text": "PawMatchAI \u2014 Smarter, Safer, and More Thoughtful Recommendations \ud83d\udc15\u2728 \ud83d\udc3e Recommendation system update \u2014 deeper reasoning, safer decisions Over the past weeks, user feedback led me to rethink how PawMatchAI handles description-based breed recommendations. Instead of only matching surface-level preferences, the system now implements a multi-dimensional semantic reasoning architecture that emphasizes real-life compatibility and risk awareness. Key technical improvements: - SBERT-powered semantic understanding with dynamic weight allocation across six constraint dimensions (space, activity, noise, grooming, experience, family) - Hierarchical constraint management distinguishing critical safety constraints from flexible preferences, with progressive relaxation when needed -Multi-head scoring system combining semantic matching (15%), lifestyle compatibility (70%), constraint adherence (10%), and confidence calibration (5%) -Intelligent risk filtering that applies graduated penalties (-10% to...", "url": "https://huggingface.co/posts/DawnC/213826857024975", "date_published": "2025-12-23T05:29:09.847494"}, {"id": "https://huggingface.co/posts/John1604/712509372180068", "image": "", "title": "\u6211\u5373\u5c06\u8fbe\u5230\u516c\u5171\u5b58\u50a8\u7a7a\u95f4\u4e0a\u9650\u3002\u6211\u53d1\u73b0\u6211\u7684\u4ed3\u5e93 John1604/Kimi-K2-Thinking-q6K-gguf \u6ca1\u6709\u83b7\u5f97\u8db3\u591f\u7684\u4e0b\u8f7d\u91cf\uff0c\u51e0\u4e4e\u5360\u7528\u4e86 1T \u5b58\u50a8\u7a7a\u95f4\u3002\u5c3d\u7ba1\u6211\u559c\u7231 Kimi K2 \u7684\u601d\u8003\u65b9\u5f0f\uff0c\u4f46\u53ef\u80fd\u4e0d\u5f97\u4e0d\u5220\u9664\u8fd9\u4e2a\u6a21\u578b\u3002\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u771f\u6b63\u7684\u5f00\u6e90 1T LLM\uff0c\u4e0e\u4efb\u4f55\u524d\u6cbf\u7684 LLM \u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u5728 AI \u7ade\u4e89\u4e2d\uff0c\u7f8e\u56fd\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1axAI,  OpenAI, \u8c37\u6b4c\u548cAnthropologie\u3002\u4e2d\u56fd\u4e5f\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1a\u963f\u91cc\u5df4\u5df4, Kimi, DeepSeek\u548cGLM\u3002\u76ee\u524d\u53cc\u65b9\u52bf\u5747\u529b\u654c\u3002", "content_text": "\u6211\u5373\u5c06\u8fbe\u5230\u516c\u5171\u5b58\u50a8\u7a7a\u95f4\u4e0a\u9650\u3002\u6211\u53d1\u73b0\u6211\u7684\u4ed3\u5e93 John1604/Kimi-K2-Thinking-q6K-gguf \u6ca1\u6709\u83b7\u5f97\u8db3\u591f\u7684\u4e0b\u8f7d\u91cf\uff0c\u51e0\u4e4e\u5360\u7528\u4e86 1T \u5b58\u50a8\u7a7a\u95f4\u3002\u5c3d\u7ba1\u6211\u559c\u7231 Kimi K2 \u7684\u601d\u8003\u65b9\u5f0f\uff0c\u4f46\u53ef\u80fd\u4e0d\u5f97\u4e0d\u5220\u9664\u8fd9\u4e2a\u6a21\u578b\u3002\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u771f\u6b63\u7684\u5f00\u6e90 1T LLM\uff0c\u4e0e\u4efb\u4f55\u524d\u6cbf\u7684 LLM \u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u5728 AI \u7ade\u4e89\u4e2d\uff0c\u7f8e\u56fd\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1axAI, OpenAI, \u8c37\u6b4c\u548cAnthropologie\u3002\u4e2d\u56fd\u4e5f\u6709\u56db\u5bb6\u516c\u53f8\u62e5\u67091T+\u6a21\u578b\uff1a\u963f\u91cc\u5df4\u5df4, Kimi, DeepSeek\u548cGLM\u3002\u76ee\u524d\u53cc\u65b9\u52bf\u5747\u529b\u654c\u3002 I'm about to reach my public storage limit. I've discovered that my repository John1604/Kimi-K2-Thinking-q6K-gguf isn't getting enough downloads and is nearly consuming 1TB of storage. While I love Kimi K2's way of thinking, I have to delete this model because it's a true open-source 1TB LLM, comparable to any cutting-edge LLM model. In the AI \u200b\u200brace, four US companies have 1TB+ models: xAI, OpenAI, Google, and Anthropic. China also has four companies with 1TB+ models: Alibaba, Kimi, DeepSeek, and GLM. Currently, the two sides are evenly matched. Only American team and Chinese team have LLM with 1T+ parameters. Let's cheer for them to reach AGI in next 5 to 10 years. Maybe a 64T chinese model will do it -- Human and cat brain...", "url": "https://huggingface.co/posts/John1604/712509372180068", "date_published": "2025-12-23T05:29:09.847938"}, {"id": "https://huggingface.co/posts/Kseniase/746586334382009", "image": "", "title": "From Prompt Engineering to Context Engineering: Main Design Patterns", "content_text": "From Prompt Engineering to Context Engineering: Main Design Patterns Earlier on, we relied on clever prompt wording, but now structured, complete context matters more than just magic phrasing. The next year is going to be a year of context engineering which expands beyond prompt engineering. The two complement each other: prompt engineering shapes how we ask, while context engineering shapes what the model knows, sees, and can do. To keep things clear, here are the main techniques and design patterns in both areas, with some useful resources for further exploration: \u25aa\ufe0f 9 Prompt Engineering Techniques (configuring input text) 1. Zero-shot prompting \u2013 giving a single instruction without examples. Relies entirely on pretrained knowledge. 2. Few-shot prompting \u2013 adding input\u2013output examples to encourage model to show the desired behavior. \u27f6 https://arxiv.org/abs/2005.14165 3. Role prompting \u2013 assigning a persona or role (e.g. \"You are a senior researcher,\" \"Say it as a specialist in...", "url": "https://huggingface.co/posts/Kseniase/746586334382009", "date_published": "2025-12-23T05:29:09.848576"}, {"id": "https://huggingface.co/posts/Parveshiiii/283996096084237", "image": "", "title": "Hey everyone!", "content_text": "Hey everyone! We\u2019re excited to introduce our new Telegram group: https://t.me/XenArcAI This space is built for **model builders, tech enthusiasts, and developers** who want to learn, share, and grow together. Whether you\u2019re just starting out or already deep into AI/ML, you\u2019ll find a supportive community ready to help with knowledge, ideas, and collaboration. \ud83d\udca1 Join us to: - Connect with fellow developers and AI enthusiasts - Share your projects, insights, and questions - Learn from others and contribute to a growing knowledge base \ud83d\udc49 If you\u2019re interested, hop in and be part of the conversation: https://t.me/XenArcAI See translation", "url": "https://huggingface.co/posts/Parveshiiii/283996096084237", "date_published": "2025-12-23T05:29:09.848925"}, {"id": "https://huggingface.co/posts/dhruv3006/288574780413766", "image": "", "title": "Editor-Neutral, Tool-Neutral API Artifacts", "content_text": "Editor-Neutral, Tool-Neutral API Artifacts One thing we hear from developers : API docs and files often get stuck inside specific editors or tools. That friction slows teams down especially when people use different setups. At Voiden, we believe your API artifacts should work anywhere your team does. Our files open seamlessly in VS Code, GitHub, custom Electron clients, and more, without being locked into a specific workspace or tool. Key benefits: -Portability across editors, repos, platforms, and team setups -No proprietary workspace: the repo is the workspace -Easy integration with CI pipelines, linters, and future tools -Future-proof your API workflows with open and flexible artifacts Voiden empowers devs and teams collaborate and stay agile as tools and platforms evolve. Check out whats different about Voiden here: https://voiden.md/features See translation", "url": "https://huggingface.co/posts/dhruv3006/288574780413766", "date_published": "2025-12-23T05:29:09.849260"}, {"id": "https://huggingface.co/posts/telcom/422373414213997", "image": "", "title": "arXiv CS endorsement", "content_text": "arXiv CS endorsement It's Javad, my Google Scholar Profile: https://scholar.google.com/citations?user=bja6GwoAAAAJ&hl=en I would like to share my articles with you on Hugging Face, I'm asking for endorsement* in Computer Science arxiv.org. If you would like to endorse me, please visit the following URL: https://arxiv.org/auth/endorse?x=NVUAPL If that URL does not work for you, please visit http://arxiv.org/auth/endorse.php and enter the following six-digit alphanumeric string: Endorsement Code: NVUAPL Thanks you in advance. Javad Taghia * Who is qualified to endorse? To endorse another user to submit to the cs.AI (Artificial Intelligence) subject class, an arXiv submitter must have submitted 3 papers to any of cs.AI, cs.AR, cs.CC, cs.CE, cs.CG, cs.CL, cs.CR, cs.CV, cs.CY, cs.DB, cs.DC, cs.DL, cs.DM, cs.DS, cs.ET, cs.FL, cs.GL, cs.GR, cs.GT, cs.HC, cs.IR, cs.IT, cs.LG, cs.LO, cs.MA, cs.MM, cs.MS, cs.NA, cs.NE, cs.NI, cs.OH, cs.OS, cs.PF, cs.PL, cs.RO, cs.SC, cs.SD, cs.SE, cs.SI or...", "url": "https://huggingface.co/posts/telcom/422373414213997", "date_published": "2025-12-23T05:29:09.849626"}, {"id": "https://huggingface.co/posts/kanaria007/536602781074542", "image": "", "title": "\u2705 New Article: *Operating an SI-Core System in Production*", "content_text": "\u2705 New Article: *Operating an SI-Core System in Production* Title: \ud83d\udee0\ufe0f Operating an SI-Core System in Production \ud83d\udd17 https://huggingface.co/blog/kanaria007/operating-si-core-system --- Summary: Specs and whitepapers tell you *what* SI-Core is. This article answers a different question: > \u201cIf I\u2019m on call for an SI-Core / SI-NOS stack wrapped around LLMs and tools, > *what do I actually look at \u2014 and what do I do when it goes weird?*\u201d It\u2019s an operator\u2019s guide to running Structured Intelligence in production: how CAS, EAI, RBL, RIR, SCover, ACR, etc. show up on dashboards, how to set thresholds, and how to turn incidents into structural learning instead of panic. --- Why It Matters: * Bridges *theory \u2192 SRE/MLOps practice* for SI-Core & guardrailed LLM systems * Shows how to treat metrics as *symptoms of structural health*, not vanity numbers * Gives concrete patterns for *alerts, safe-mode, rollback tiers, and ethics outages* * Helps teams run SI-wrapped AI systems *safely, explainably,...", "url": "https://huggingface.co/posts/kanaria007/536602781074542", "date_published": "2025-12-23T05:29:09.850236"}, {"id": "https://huggingface.co/posts/prithivMLmods/761836377624422", "image": "", "title": "Introducing demos for new SOTA models from AI2: SAGE-MM (Smart Any-Horizon Agents for Long-Video Reasoning) and Molmo-2, an open vision-language model that supports multi-image (QA and pointing) and video (QA, pointing, and tracking). The respective demo-related collections are listed below. \ud83c\udf83\ud83d\udd25", "content_text": "Introducing demos for new SOTA models from AI2: SAGE-MM (Smart Any-Horizon Agents for Long-Video Reasoning) and Molmo-2, an open vision-language model that supports multi-image (QA and pointing) and video (QA, pointing, and tracking). The respective demo-related collections are listed below. \ud83c\udf83\ud83d\udd25 \u2728 SAGE-MM [Video-Reasoning]: prithivMLmods/SAGE-MM-Video-Reasoning \u2728 Molmo2 [Demo]: prithivMLmods/Molmo2-HF-Demo \ud83c\udf83 GitHub[SAGE-MM]: https://github.com/PRITHIVSAKTHIUR/SAGE-MM-Video-Reasoning \ud83c\udf83 GitHub[Molmo2]: https://github.com/PRITHIVSAKTHIUR/Molmo2-HF-Demo \ud83c\udf83 Multimodal Implementations: https://huggingface.co/collections/prithivMLmods/multimodal-implementations To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/761836377624422", "date_published": "2025-12-23T05:29:09.850610"}, {"id": "https://huggingface.co/posts/prithivMLmods/787095126804028", "image": "", "title": "Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... \ud83e\udd17\ud83d\udd25", "content_text": "Introducing TRELLIS.2 Text-to-3D. The demo for the TRELLIS.2-4B (Image-to-3D) model is streamlined with the Z-Image Turbo image generation model to enable Text-to-3D functionality. There is no need for input assets, making a small leap forward for ideation. Optionally, it also includes default support for Image-to-3D inference using direct image assets. Find the demo and related collections below... \ud83e\udd17\ud83d\udd25 \u2728 TRELLIS.2-Text-to-3D [Demo]: prithivMLmods/TRELLIS.2-Text-to-3D \u2728 Multimodal Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations \u2728 Github: https://github.com/PRITHIVSAKTHIUR/TRELLIS.2-Text-to-3D To know more about it, visit the app page or the respective model page! See translation", "url": "https://huggingface.co/posts/prithivMLmods/787095126804028", "date_published": "2025-12-23T05:29:09.850966"}]}