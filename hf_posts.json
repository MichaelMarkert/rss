{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/codelion/510406818109359", "image": "", "title": "I recently worked on a LoRA that improves tool use in LLM. Thought the approach might interest folks here.", "content_text": "I recently worked on a LoRA that improves tool use in LLM. Thought the approach might interest folks here. The issue I have had when trying to use some of the local LLMs with coding agents is this: Me: \"Find all API endpoints with authentication in this codebase\" LLM: \"You should look for @ app .route decorators and check if they have auth middleware...\" But I often want it to search the files and show me but the LLM doesn't trigger a tool use call. To fine-tune it for tool use I combined two data sources: 1. Magpie scenarios - 5000+ diverse tasks (bug hunting, refactoring, security audits) 2. Real execution - Ran these on actual repos (FastAPI, Django, React) to get authentic tool responses This ensures the model learns both breadth (many scenarios) and depth (real tool behavior). Tools We Taught: - read_file - Actually read file contents - search_files - Regex/pattern search across codebases - find_definition - Locate classes/functions - analyze_imports - Dependency tracking -...", "url": "https://huggingface.co/posts/codelion/510406818109359", "date_published": "2025-08-31T05:20:40.360930"}, {"id": "https://huggingface.co/posts/codelion/741062673202173", "image": "", "title": "I wanted to share a technique that's been working really well for recovering performance after INT4 quantization.", "content_text": "I wanted to share a technique that's been working really well for recovering performance after INT4 quantization. Typically, quantizing the LLM to INT4 (unlike say INT8) for inference can incur some accuracy loss. Instead of accepting the quality loss, we used the FP16 model as a teacher to train a tiny LoRA adapter (rank=16) for the quantized model. The cool part: the model generates its own training data using the Magpie technique so no external datasets needed. This is critical because we want to remain as much as possible in the distribution of the model's natural responses. Last year Apple's foundational models paper ( https://arxiv.org/pdf/2407.21075 ) had proposed a similar technique and found \"By using accuracy-recovery LoRA adapters with only rank 16, Alpaca win rate can be improved by 7-18%, GMS8K accuracy is boosted by 5-10%.\" (page 47). We saw similar results on Qwen3-0.6B: Perplexity: 2.40 \u2192 2.09 (only 5.7% degradation from FP16 baseline) Memory: Only 0.28GB vs 1.0GB...", "url": "https://huggingface.co/posts/codelion/741062673202173", "date_published": "2025-08-31T05:20:40.361505"}, {"id": "https://huggingface.co/posts/prithivMLmods/604588784783928", "image": "", "title": "Introducing", "content_text": "Introducing prithivMLmods/DeepCaption-VLA-7B , a multimodal VLM designed for reasoning with long-shot captions (Captioning and Vision-Language Attribution). It focuses on defining visual properties, object attributes, and scene details across a wide spectrum of images and aspect ratios, generating attribute-rich image captions. The model supports creative, artistic, and technical applications that require detailed descriptions. \ud83e\udd17\ud83d\udd25 \u2726\ufe0e Models: prithivMLmods/DeepCaption-VLA-7B , also includes prithivMLmods/DeepAttriCap-VLA-3B , an experimental model for vision-language attribution. \u2726\ufe0e Try the demo here: prithivMLmods/VisionScope-R2 \u2726\ufe0e Try it now on Google Colab, with support for T4 GPUs in 4-bit quant_type: https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-Notebooks/blob/main/DeepCaption-VLA-7B%5B4bit%20-%20notebook%20demo%5D/DeepCaption-VLA-7B.ipynb \u2726\ufe0e Collection: prithivMLmods/deepcaption-attr-68b041172ebcb867e45c556a . . . To know more about it, visit the model card of the...", "url": "https://huggingface.co/posts/prithivMLmods/604588784783928", "date_published": "2025-08-31T05:20:40.361939"}, {"id": "https://huggingface.co/posts/takarajordan/290509757896054", "image": "", "title": "I'm currently looking into what makes a scientific paper more popular than others on a platform like Hugging Face. I conducted a huge array of tests, content length, time based information even semantic feature extraction to get to some sort of answer around...", "content_text": "I'm currently looking into what makes a scientific paper more popular than others on a platform like Hugging Face. I conducted a huge array of tests, content length, time based information even semantic feature extraction to get to some sort of answer around... What actually drives popularity of these papers, why do some papers get zero upvotes and why do some get thousands? The answer is absolutely nothing. Yes that's right. Nothing about the actual paper itself drives popularity, the paper's popularity is driven by external factors like it's authors, external marketing and others. So next time you see a research paper with a lot of upvotes, just remember it's not because of the efforts of the authors. Remain objective. See translation", "url": "https://huggingface.co/posts/takarajordan/290509757896054", "date_published": "2025-08-31T05:20:40.362253"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/626834107736478", "image": "", "title": "Nano Banana (Gemini 2.5 Flash Image) Full Tutorial \u2014 27 Unique Cases vs Qwen Image Edit \u2014 Free 2 Use :", "content_text": "Nano Banana (Gemini 2.5 Flash Image) Full Tutorial \u2014 27 Unique Cases vs Qwen Image Edit \u2014 Free 2 Use : https://youtu.be/qPUreQxB8zQ Tutorial link : https://youtu.be/qPUreQxB8zQ Nano Banana AI image editing model was published by Google today. It is officially named the Google Gemini 2.5 Flash Image model. It is the most advanced zero-shot image editing model ever made. I have conducted a thorough, in-depth review of this model with 27 unique cases. All prompts, images used, and results are demonstrated in real-time\u2014live in this tutorial. Moreover, I have compared each result with the state-of-the-art (SOTA) best open-source, locally available, and free-to-use Qwen Image Edit model, so we can see which model performs better at which tasks. Video Chapters 0:00 Introduction to Google's \"Nano Banana\" (Gemini 2.5 Flash) 0:28 Comparing Gemini vs. Qwen Image Edit Model (27 Test Cases) 1:33 Solving Gemini's Low Resolution with SUPIR Upscaling 2:28 Teaser: Upcoming Qwen Image LoRA Training...", "url": "https://huggingface.co/posts/MonsterMMORPG/626834107736478", "date_published": "2025-08-31T05:20:40.362886"}, {"id": "https://huggingface.co/posts/dhruv3006/852628837357270", "image": "", "title": "Pair a vision grounding model with a reasoning LLM with Cua", "content_text": "Pair a vision grounding model with a reasoning LLM with Cua Cua just shipped v0.4 of the Cua Agent framework with Composite Agents - you can now pair a vision/grounding model with a reasoning LLM using a simple modelA+modelB syntax. Best clicks + best plans. The problem: every GUI model speaks a different dialect. \u2022 some want pixel coordinates \u2022 others want percentages \u2022 a few spit out cursed tokens like <|loc095|> We built a universal interface that works the same across Anthropic, OpenAI, Hugging Face, etc.: agent = ComputerAgent( model=\"anthropic/claude-3-5-sonnet-20241022\", tools=[computer] ) But here\u2019s the fun part: you can combine models by specialization. Grounding model (sees + clicks) + Planning model (reasons + decides) \u2192 agent = ComputerAgent( model=\"huggingface-local/HelloKKMe/GTA1-7B+openai/gpt-4o\", tools=[computer] ) This gives GUI skills to models that were never built for computer use. One handles the eyes/hands, the other the brain. Think driver + navigator working...", "url": "https://huggingface.co/posts/dhruv3006/852628837357270", "date_published": "2025-08-31T05:20:40.363365"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/262579091589206", "image": "", "title": "Huge updates made for SECourses Musubi Tuner - 1-Click to Install App for LoRA Training and Full Fine Tuning Qwen Image, Qwen Image Edit, Wan 2.1 and Wan 2.2 Models with Musubi Tuner with Ready Presets", "content_text": "Huge updates made for SECourses Musubi Tuner - 1-Click to Install App for LoRA Training and Full Fine Tuning Qwen Image, Qwen Image Edit, Wan 2.1 and Wan 2.2 Models with Musubi Tuner with Ready Presets 1-Click to install app link : https://www.patreon.com/posts/137551634 Check all the screenshots 30 August 2025 Update V7 Dataset TOML file generate error fixed Qwen2.5-VL image captioning turns out working perfect on Windows It turns out my model file was corrupted even though it was same size Therefore I have updated the model downloader and now it will check and verify SHA 256 of files therefore it will be 100% accurate Prompt file selection folder icon issue fixed Downloader file will use generated venv of installation Make sure to run it after installation completed Fixed skip existing captions functionality in Image Captioning with Qwen2.5-VL Previously skipping was happening after caption generation which was destroying the skip logic Now properly checks for existing captions...", "url": "https://huggingface.co/posts/MonsterMMORPG/262579091589206", "date_published": "2025-08-31T05:20:40.363883"}, {"id": "https://huggingface.co/posts/dhruv3006/688931199495783", "image": "", "title": "Human in the Loop for computer use agents (instant handoff from AI to you)", "content_text": "Human in the Loop for computer use agents (instant handoff from AI to you) Sometimes the best \u201cagent\u201d is you. We\u2019re introducing Human in the Loop: instantly hand off from automation to human control when a task needs judgment. Yesterday we shared our HUD evals for measuring agents at scale. Today you can become the agent when it matters take over the same session see what the agent sees and keep the workflow moving. Lets you create clean training demos, establish ground truth for tricky cases, intervene on edge cases ( CAPTCHAs, ambiguous UIs) or step through debug without context switching. You have full human control when you want.We even a fallback version where in it starts automated but escalate to a human only when needed. Works across common stacks (OpenAI, Anthropic, Hugging Face) and with our Composite Agents. Same tools, same environment take control when needed. Feedback welcome,curious how you\u2019d use this in your workflows. Blog : https://www.trycua.com/blog/human-in-the-...", "url": "https://huggingface.co/posts/dhruv3006/688931199495783", "date_published": "2025-08-31T05:20:40.364315"}, {"id": "https://huggingface.co/posts/tsungyi/147340620272288", "image": "", "title": "Cosmos Reason just topped Physical Reasoning Leaderboard on Hugging Face. \ud83d\udc4f\ud83d\udd25", "content_text": "Cosmos Reason just topped Physical Reasoning Leaderboard on Hugging Face. \ud83d\udc4f\ud83d\udd25 Cosmos Reason is an open, customizable, commercial-ready 7B-parameter, reasoning vision language model (VLM) for physical AI and robotics. The VLM empowers robots and vision AI agents to reason like humans, leveraging prior knowledge, physics understanding, and common sense to understand and operate intelligently in the real world. This model unlocks advanced capabilities for robotics, autonomous vehicles, and real-world operations\u2014from cities to high-tech factories. Key use cases include: Data curation & annotation: Automate high-quality dataset curation and annotation at scale. Robot planning & reasoning: Serve as the \"brain\" for deliberate, methodical decision-making with vision language action (VLA) models. Video analytics AI agents: Extract actionable insights and perform root-cause analysis on massive video datasets. Ready to build the next generation of physical AI? Get started \ud83d\udc49 nvidia/Cosmos-...", "url": "https://huggingface.co/posts/tsungyi/147340620272288", "date_published": "2025-08-31T05:20:40.364763"}, {"id": "https://huggingface.co/posts/Jaward/864148450814843", "image": "", "title": "It\u2019s absolutely mind blowing - the work Dynamics Lab is doing!!", "content_text": "It\u2019s absolutely mind blowing - the work Dynamics Lab is doing!! With just a single input image and in a few seconds, their new world engine model (Mirage 2) can generate a whole new interactive world that\u2019s physics informed and fully explorable in real-time\ud83e\udd2f Try it yourself: https://demo.dynamicslab.ai/chaos See translation", "url": "https://huggingface.co/posts/Jaward/864148450814843", "date_published": "2025-08-31T05:20:40.365020"}]}