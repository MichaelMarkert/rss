{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/seawolf2357/553204712342429", "image": "", "title": "\ud83d\udcda Papers Leaderboard - See the Latest AI Research Trends at a Glance! \u2728", "content_text": "\ud83d\udcda Papers Leaderboard - See the Latest AI Research Trends at a Glance! \u2728 Hello, AI research community! Today I'm introducing a new tool for exploring research papers. Papers Leaderboard is an open-source dashboard that makes it easy to find and filter the latest AI research papers. Heartsync/Papers-Leaderboard \ud83c\udf1f Key Features Date Filtering: View only papers published within a specific timeframe (from May 5, 2023 to present) Title Search: Quickly find papers containing your keywords of interest Abstract Search: Explore paper content more deeply by searching for keywords within abstracts Automatic Updates: The database is updated with the latest papers every hour \ud83d\udca1 How to Use It? Select a start date and end date Enter keywords you want to find in titles or abstracts Adjust the maximum number of search results for abstract searches Results are displayed neatly in table format See translation", "url": "https://huggingface.co/posts/seawolf2357/553204712342429", "date_published": "2025-04-21T05:23:10.582787"}, {"id": "https://huggingface.co/posts/aiqtech/339494015180136", "image": "", "title": "\ud83c\udf10 AI Token Visualization Tool with Perfect Multilingual Support", "content_text": "\ud83c\udf10 AI Token Visualization Tool with Perfect Multilingual Support Hello! Today I'm introducing my Token Visualization Tool with comprehensive multilingual support. This web-based application allows you to see how various Large Language Models (LLMs) tokenize text. aiqtech/LLM-Token-Visual \u2728 Key Features \ud83e\udd16 Multiple LLM Tokenizers: Support for Llama 4, Mistral, Gemma, Deepseek, QWQ, BERT, and more \ud83d\udd04 Custom Model Support: Use any tokenizer available on HuggingFace \ud83d\udcca Detailed Token Statistics: Analyze total tokens, unique tokens, compression ratio, and more \ud83c\udf08 Visual Token Representation: Each token assigned a unique color for visual distinction \ud83d\udcc2 File Analysis Support: Upload and analyze large files \ud83c\udf0f Powerful Multilingual Support The most significant advantage of this tool is its perfect support for all languages: \ud83d\udcdd Asian languages including Korean, Chinese, and Japanese fully supported \ud83d\udd24 RTL (right-to-left) languages like Arabic and Hebrew supported \ud83c\ude3a Special characters and emoji...", "url": "https://huggingface.co/posts/aiqtech/339494015180136", "date_published": "2025-04-21T05:23:10.583454"}, {"id": "https://huggingface.co/posts/openfree/953705052271600", "image": "", "title": "\ud83e\udde0 ThinkFlow: The Revolutionary Platform That Gives LLMs the Power to Think \ud83d\ude80", "content_text": "\ud83e\udde0 ThinkFlow: The Revolutionary Platform That Gives LLMs the Power to Think \ud83d\ude80 Hello AI community! We're excited to introduce you to ThinkFlow, an innovative service that transforms how language models solve problems. \ud83c\udf89 VIDraft/ThinkFlow-llama \u2728 What is ThinkFlow? ThinkFlow is a groundbreaking platform that automatically applies step-by-step reasoning capabilities to existing LLM models without any modifications. It makes complex problem-solving transparent, allowing you to witness the model's thought process in real-time. \ud83d\udd0d Key Features Reasoning Without Model Modifications: Add step-by-step reasoning while utilizing existing LLMs as they are \u2699\ufe0f Visualized Thinking Process: See exactly how the model analyzes and solves problems \ud83d\udc41\ufe0f Before & After Comparison: Compare standard responses with reasoning-enhanced outputs in real-time \ud83d\udcca Improved Accuracy: Deliver more accurate solutions for complex math and logic problems \ud83d\udcc8 Educational Value: Teach students systematic approaches to problem-...", "url": "https://huggingface.co/posts/openfree/953705052271600", "date_published": "2025-04-21T05:23:10.584097"}, {"id": "https://huggingface.co/posts/ginipick/639223795082206", "image": "", "title": "\ud83e\udd16 AI Academic Paper Generator: Your Research Partner \ud83c\udf93", "content_text": "\ud83e\udd16 AI Academic Paper Generator: Your Research Partner \ud83c\udf93 Hello, researchers! Today I'm introducing my AI Academic Paper Generation System. This application is built with Streamlit and provides AI agents to assist with every stage of the academic research process. ginipick/AgentX-Papers \u2728 Key Features \ud83d\udcda Literature Research: AI reviews and summarizes relevant research \ud83d\udcdd Paper Outline: Generates a well-structured paper outline \u270d\ufe0f Draft Writing: Creates a paper draft based on your research topic \ud83d\udd17 Citation Generation: Automatically generates academic citations \ud83d\udd8b\ufe0f Editing & Polishing: Checks grammar, context, and logical flow \ud83c\udf10 Multilingual Support: Interface available in English and Korean \ud83d\ude80 How to Use Enter basic information like research topic, paper title, and deadline AI agents generate everything from literature review to final paper Download your completed paper or consult with the chatbot for further assistance \ud83d\udca1 What Makes It Special This tool integrates all stages of academic...", "url": "https://huggingface.co/posts/ginipick/639223795082206", "date_published": "2025-04-21T05:23:10.584599"}, {"id": "https://huggingface.co/posts/openfree/406218973183679", "image": "", "title": "\ud83d\udcca Papers Impact: Instant AI Grading for Your Research Papers! \ud83d\ude80", "content_text": "\ud83d\udcca Papers Impact: Instant AI Grading for Your Research Papers! \ud83d\ude80 \ud83c\udf1f Introduction Hello, AI research community! \ud83c\udf89 Introducing Papers Impact - the revolutionary AI tool that automatically grades and predicts the potential impact of research papers! \ud83e\udde0\ud83d\udca1 VIDraft/PapersImpact \u2728 Key Feature: Instant Paper Grading The core functionality is brilliantly simple: Just enter an arXiv paper ID or URL, and our AI instantly analyzes and grades the paper's potential academic impact! No need to read through the entire paper yourself - our system automatically evaluates the title and abstract to generate a normalized impact score between 0 and 1. \ud83c\udfaf How It Works Enter Paper ID or URL: Simply paste an arXiv ID (e.g., \"2504.11651\") or full URL Automatic Fetching: The system retrieves the paper's title and abstract AI Analysis: Our advanced LLaMA-based transformer model analyzes the content Instant Grading: Receive an impact score and corresponding letter grade in seconds! \ud83d\udca1 Who Can Benefit? \ud83d\udd2c Researchers:...", "url": "https://huggingface.co/posts/openfree/406218973183679", "date_published": "2025-04-21T05:23:10.585168"}, {"id": "https://huggingface.co/posts/Kseniase/836565977783893", "image": "", "title": "11 new types of RAG", "content_text": "11 new types of RAG RAG is evolving fast, keeping pace with cutting-edge AI trends. Today it becomes more agentic and smarter at navigating complex structures like hypergraphs. Here are 11 latest RAG types: 1. InstructRAG -> InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning (2504.13032) Combines RAG with a multi-agent framework, using a graph-based structure, an RL agent to expand task coverage, and a meta-learning agent for better generalization 2. CoRAG (Collaborative RAG) -> CoRAG: Collaborative Retrieval-Augmented Generation (2504.01883) A collaborative framework that extends RAG to settings where clients train a shared model using a joint passage store 3. ReaRAG -> ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation (2503.21729) It uses a Thought-Action-Observation loop to decide at each step whether to retrieve information or finalize an answer,...", "url": "https://huggingface.co/posts/Kseniase/836565977783893", "date_published": "2025-04-21T05:23:10.585897"}, {"id": "https://huggingface.co/posts/merterbak/685608427471874", "image": "", "title": "Here\u2019s a cool paper I found: \u201cMassive Image Embedding Benchmark (MIEB).\u201d It is a new tool to test how good image embedding models are. It has 130 different tasks grouped into 8 categories, like image search, classification, clustering similar images, answering questions based on images, and understanding documents. It even covers 38 different languages.", "content_text": "Here\u2019s a cool paper I found: \u201cMassive Image Embedding Benchmark (MIEB).\u201d It is a new tool to test how good image embedding models are. It has 130 different tasks grouped into 8 categories, like image search, classification, clustering similar images, answering questions based on images, and understanding documents. It even covers 38 different languages. The authors tested 50 models and found that no single model was best at everything. Some models were great at recognizing text inside images but struggled to handle complicated tasks like matching images and text that appear together. Paper: https://arxiv.org/pdf/2504.10471v1 Code: https://github.com/embeddings-benchmark/mteb See translation", "url": "https://huggingface.co/posts/merterbak/685608427471874", "date_published": "2025-04-21T05:23:10.586311"}, {"id": "https://huggingface.co/posts/zhiminy/357364609122297", "image": "", "title": "# \ud83d\ude80 SE Arena: Evaluating Foundation Models for Software Engineering", "content_text": "# \ud83d\ude80 SE Arena: Evaluating Foundation Models for Software Engineering **SE Arena** is the first open-source platform for evaluating foundation models in real-world software engineering workflows. ## What makes it unique? - **RepoChat**: Automatically injects repository context (issues, commits, PRs) into conversations for more realistic evaluations - **Multi-round interactions**: Tests models through iterative workflows, not just single prompts - **Novel metrics**: Includes a \"consistency score\" that measures model determinism through self-play matches Try it now: SE-Arena/Software-Engineering-Arena ## Why it matters Traditional evaluation frameworks don't capture how developers actually use models in their daily work. SE Arena creates a testing environment that mirrors real engineering workflows, helping you choose the right model for your specific software development needs. From debugging to requirement refinement, see which models truly excel at software engineering tasks! See...", "url": "https://huggingface.co/posts/zhiminy/357364609122297", "date_published": "2025-04-21T05:23:10.586748"}, {"id": "https://huggingface.co/posts/eaddario/332413870990082", "image": "", "title": "Tensor-wise (TWQ) and Layer-wise quantization (LWQ) now available in llama.cpp!", "content_text": "Tensor-wise (TWQ) and Layer-wise quantization (LWQ) now available in llama.cpp! As of version b5125 users can now do TWQ, whereby you quantize a whole tensor at a specific level, or perform LWQ by choosing specific layers per tensor/s The new --tensor-type option enables llama-quantize to apply user-defined quant levels to any combination of allowed tensors (i.e. tensors with 2 or more dimensions) and layer number, with support for regex patterns. For example, to TWQ the Attention Value tensor you would use --tensor-type attn_v=q6_k and to perform LWQ you'll use something like --tensor-type \"\\.([0-9]|1[01257]|31)\\.attn_v=q4_k\" In the next few days/weeks I'll update the models in my HF repo (and will add some others) but eaddario/DeepSeek-R1-Distill-Llama-8B-GGUF and eaddario/DeepSeek-R1-Distill-Qwen-7B-GGUF have been already LWQed. For reference, compared to the naive Q4_K_M model, the LWQ Qwen-7B is almost 11% smaller (4.68GB vs 4.18GB) with only a 0.35% penalty on PPL! I'll update...", "url": "https://huggingface.co/posts/eaddario/332413870990082", "date_published": "2025-04-21T05:23:10.587200"}, {"id": "https://huggingface.co/posts/nyuuzyou/125489545915443", "image": "", "title": "\ud83e\udd85 SmolLM2-Eagle Collection -", "content_text": "\ud83e\udd85 SmolLM2-Eagle Collection - nyuuzyou/smollm2-eagle-680263bf97f0c7e6bbe4936b Collection of fine-tuned bilingual language models featuring: - Models in three parameter sizes: 135M, 360M, and 1.7B based on HuggingFaceTB's SmolLM2 models - Both standard and GGUF formats for flexible deployment in llama.cpp and Ollama - Fine-tuned on nyuuzyou/EagleSFT dataset (536,231 Russian-English QA pairs derived from 739k+ real user queries) - Experimental Russian language capabilities while maintaining English performance - Limited Russian capabilities due to SFT-only approach without Russian pre-training - Environmental impact: ~19.75 kg CO2eq This collection provides compact models for research on bilingual language capabilities, resource-constrained environments, and educational applications. Not recommended for production use due to experimental nature and inherent limitations. Available under Apache 2.0 license. See translation", "url": "https://huggingface.co/posts/nyuuzyou/125489545915443", "date_published": "2025-04-21T05:23:10.587610"}]}