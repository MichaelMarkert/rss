{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/DmitryRyumin/213442382070723", "image": "", "title": "\ud83d\ude80\ud83d\udca1\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\ude84\ud83d\ude80", "content_text": "\ud83d\ude80\ud83d\udca1\ud83c\udf1f New Research Alert - ICCV 2025 (Oral)! \ud83c\udf1f\ud83e\ude84\ud83d\ude80 \ud83d\udcc4 Title: LoftUp: Learning a Coordinate-based Feature Upsampler for Vision Foundation Models \ud83d\udd1d \ud83d\udcdd Description: LoftUp is a coordinate-based transformer that upscales the low-resolution features of VFMs (e.g. DINOv2 and CLIP) using cross-attention and self-distilled pseudo-ground truth (pseudo-GT) from SAM. \ud83d\udc65 Authors: Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang \ud83d\udcc5 Conference: ICCV, 19 \u2013 23 Oct, 2025 | Honolulu, Hawai'i, USA \ud83c\uddfa\ud83c\uddf8 \ud83d\udcc4 Paper: LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models (2504.14032) \ud83c\udf10 Github Page: https://andrehuang.github.io/loftup-site \ud83d\udcc1 Repository: https://github.com/andrehuang/loftup \ud83d\ude80 ICCV-2023-25-Papers: https://github.com/DmitryRyumin/ICCV-2023-25-Papers \ud83d\ude80 Added to the Foundation Models and Representation Learning Section: https://github.com/DmitryRyumin/ICCV-2023-25-Papers/blob/main/sections/2025/main/foundation-models-and-representation-learning.md \ud83d\udcda...", "url": "https://huggingface.co/posts/DmitryRyumin/213442382070723", "date_published": "2025-10-31T09:26:01.890878"}, {"id": "https://huggingface.co/posts/sourceoftruthdata/665062314942834", "image": "", "title": "What a fantastic community!", "content_text": "What a fantastic community! See translation", "url": "https://huggingface.co/posts/sourceoftruthdata/665062314942834", "date_published": "2025-10-31T09:26:01.891096"}, {"id": "https://huggingface.co/posts/piercus/167394123498038", "image": "", "title": "Starts erasing! \ud83c\udf89 \ud83c\udf89 \ud83c\udf89", "content_text": "Starts erasing! \ud83c\udf89 \ud83c\udf89 \ud83c\udf89 This is made with a one-step SD1.5 LBM [1] eraser ! Data is open. Data pipeline is open. Training code is open. On our LBM fork : https://github.com/finegrain-ai/LBM [1] LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) See translation", "url": "https://huggingface.co/posts/piercus/167394123498038", "date_published": "2025-10-31T09:26:01.891357"}, {"id": "https://huggingface.co/posts/wang12390/946400201713761", "image": "", "title": "AI Speedpainting of a Tranquil Mountain Temple!", "content_text": "AI Speedpainting of a Tranquil Mountain Temple! Just upload one image then it will generate hand-drawn video. Please watch till the end, if you like the result, please upvote. See translation", "url": "https://huggingface.co/posts/wang12390/946400201713761", "date_published": "2025-10-31T09:26:01.891575"}, {"id": "https://huggingface.co/posts/meg/795374277994612", "image": "", "title": "\ud83e\udd16 Did you know your voice might be cloned without your consent from just *one sentence* of audio?", "content_text": "\ud83e\udd16 Did you know your voice might be cloned without your consent from just *one sentence* of audio? That's not great. So with @ frimelle , we brainstormed a new idea for developers who want to curb malicious use: \u2728The Voice Consent Gate.\u2728 Details, code, here: https://huggingface.co/blog/voice-consent-gate See translation", "url": "https://huggingface.co/posts/meg/795374277994612", "date_published": "2025-10-31T09:26:01.891856"}, {"id": "https://huggingface.co/posts/nouamanetazi/972464132222376", "image": "", "title": "After training \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25\ud835\udc0b\ud835\udc0c\ud835\udfd1 on \ud835\udfd1\ud835\udfd6\ud835\udfd2 \ud835\udc07\ud835\udfcf\ud835\udfce\ud835\udfce\ud835\udc2c for nearly a month, I've come to realize something most people overlook: \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc24\ud835\udc1e-\ud835\udc28\ud835\udc2b-\ud835\udc1b\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc24 \ud835\udc1f\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc22\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20. \ud83d\udd25", "content_text": "After training \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25\ud835\udc0b\ud835\udc0c\ud835\udfd1 on \ud835\udfd1\ud835\udfd6\ud835\udfd2 \ud835\udc07\ud835\udfcf\ud835\udfce\ud835\udfce\ud835\udc2c for nearly a month, I've come to realize something most people overlook: \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc22\ud835\udc2c \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc26\ud835\udc1a\ud835\udc24\ud835\udc1e-\ud835\udc28\ud835\udc2b-\ud835\udc1b\ud835\udc2b\ud835\udc1e\ud835\udc1a\ud835\udc24 \ud835\udc1f\ud835\udc1a\ud835\udc1c\ud835\udc2d\ud835\udc28\ud835\udc2b \ud835\udc22\ud835\udc27 \ud835\udc0b\ud835\udc0b\ud835\udc0c \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20. \ud83d\udd25 Everyone talks about model architecture and data quality. And yes, those matter immensely. But here's what nobody tells you: when your training run fails at 2 AM because of mysterious \ud835\udc0d\ud835\udc02\ud835\udc02\ud835\udc0b \ud835\udc1e\ud835\udc2b\ud835\udc2b\ud835\udc28\ud835\udc2b\ud835\udc2c, or when your expensive GPU cluster is running at \ud835\udfd4\ud835\udfce% \ud835\udc1e\ud835\udc1f\ud835\udc1f\ud835\udc22\ud835\udc1c\ud835\udc22\ud835\udc1e\ud835\udc27\ud835\udc1c\ud835\udc32, the problem isn't your model. It's most probably a \ud835\udc26\ud835\udc22\ud835\udc2c\ud835\udc2e\ud835\udc2c\ud835\udc1e \ud835\udc28\ud835\udc1f \ud835\udc2d\ud835\udc21\ud835\udc1e \ud835\udc21\ud835\udc1a\ud835\udc2b\ud835\udc1d\ud835\udc30\ud835\udc1a\ud835\udc2b\ud835\udc1e. \ud83d\udee0\ufe0f Questions that seemed simple but had no clear answers: Why is \ud835\udc0c\ud835\udc28\ud835\udc04 \ud835\udc2d\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc2c\ud835\udc25\ud835\udc28\ud835\udc30\ud835\udc1e\ud835\udc2b \ud835\udc2d\ud835\udc21\ud835\udc1a\ud835\udc27 \ud835\udc1d\ud835\udc1e\ud835\udc27\ud835\udc2c\ud835\udc1e \ud835\udc26\ud835\udc28\ud835\udc1d\ud835\udc1e\ud835\udc25\ud835\udc2c? Which \ud835\udc0d\ud835\udc02\ud835\udc02\ud835\udc0b \ud835\udc1f\ud835\udc25\ud835\udc1a\ud835\udc20\ud835\udc2c should we actually set? How often should we checkpoint without killing throughput? That's why we built \ud835\udc13\ud835\udc21\ud835\udc1e \ud835\udc12\ud835\udc26\ud835\udc28\ud835\udc25 \ud835\udc13\ud835\udc2b\ud835\udc1a\ud835\udc22\ud835\udc27\ud835\udc22\ud835\udc27\ud835\udc20 \ud835\udc0f\ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1b\ud835\udc28\ud835\udc28\ud835\udc24 \ud83d\udcd6: a complete guide covering everything from model architecture and data curation to the SmolLM3 training marathon, post-training techniques, and crucially, the \ud835\udc22\ud835\udc27\ud835\udc1f\ud835\udc2b\ud835\udc1a\ud835\udc2c\ud835\udc2d\ud835\udc2b\ud835\udc2e\ud835\udc1c\ud835\udc2d\ud835\udc2e\ud835\udc2b\ud835\udc1e \ud835\udc25\ud835\udc1a\ud835\udc32\ud835\udc1e\ud835\udc2b that most teams get wrong. We validated real vs...", "url": "https://huggingface.co/posts/nouamanetazi/972464132222376", "date_published": "2025-10-31T09:26:01.892429"}, {"id": "https://huggingface.co/posts/prithivMLmods/821680680057976", "image": "", "title": "Implemented DeepSeek-OCR to support the latest transformers on the", "content_text": "Implemented DeepSeek-OCR to support the latest transformers on the strangervisionhf page. The page includes the model weights and corrected configuration, which fix the issues and allow transformers inference to run smoothly.\ud83e\udd17\ud83d\udd25 > Model: strangervisionhf/deepseek-ocr-latest-transformers > Demo Space: prithivMLmods/DeepSeek-OCR-experimental \u2705Supports the latest transformers \u2705You can also opt out of the attention implementation if needed. \u2705Supports torch version 2.6.0 or higher \u2705torch version cuda: 12.4 If you are interested in experimenting with new things and streamlining compatibility, the strangervisionhf organization is open for you, and you can join the community. > Multimodal Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 , https://huggingface.co/collections/strangervisionhf/october-2025-models > Thank you, @ merve , for assigning the blazing-fast Zero GPU support! > Notebook : https://github.com/PRITHIVSAKTHIUR/Multimodal-Outpost-...", "url": "https://huggingface.co/posts/prithivMLmods/821680680057976", "date_published": "2025-10-31T09:26:01.892892"}, {"id": "https://huggingface.co/posts/prithivMLmods/213895647112348", "image": "", "title": "Introducing Gliese-OCR-7B-Post2.0-final, a document content-structure retrieval VLM designed for content extraction (OCR), summarization, and document visual question answering. This is the fourth and final model in the Camel Doc OCR VLM series, following Gliese-OCR-7B-Post1.0. The model delivers superior accuracy across a wide range of document types, including scanned PDFs, handwritten pages, structured forms, and analytical reports.\ud83d\ude80\ud83e\udd17", "content_text": "Introducing Gliese-OCR-7B-Post2.0-final, a document content-structure retrieval VLM designed for content extraction (OCR), summarization, and document visual question answering. This is the fourth and final model in the Camel Doc OCR VLM series, following Gliese-OCR-7B-Post1.0. The model delivers superior accuracy across a wide range of document types, including scanned PDFs, handwritten pages, structured forms, and analytical reports.\ud83d\ude80\ud83e\udd17 > Gliese-OCR-7B-Post2.0-final : prithivMLmods/Gliese-OCR-7B-Post2.0-final > Gliese-OCR-7B-Post1.0 (previous) : prithivMLmods/Gliese-OCR-7B-Post1.0 > Gliese OCR Post-x.0 (collection) : https://huggingface.co/collections/prithivMLmods/gliese-ocr-post-x0 > Multimodal Implementations (collection) : https://huggingface.co/collections/prithivMLmods/multimodal-implementations > Qwen VL Captions (other-collection) : https://huggingface.co/collections/prithivMLmods/qwen-vl-captions > Run Demo Here : prithivMLmods/Gliese-OCR-7B-Post2.0-final > GitHub (4bit) :...", "url": "https://huggingface.co/posts/prithivMLmods/213895647112348", "date_published": "2025-10-31T09:26:01.893354"}, {"id": "https://huggingface.co/posts/branikita/467826227099244", "image": "", "title": "With", "content_text": "With Robonine team we recently verified the rotational speed of the Feetech STS3250 servo motor (12 V, 50 kg\u00b7cm torque, magnetic encoder) to compare measured performance with the official specification. According to the datasheet: - Rated speed: 0.133 s per 60\u00b0 Calculation: - 0.133 s \u00d7 6 = 0.798 s per full rotation - 1 / 0.798 = 1.253 revolutions per second - 1.253 \u00d7 60 = 75.2 RPM This confirms the official specification of approximately 75 RPM at 12 V under no load. Our measurement: - Encoder output: 5,300 values per second - Encoder resolution: 4,096 counts per revolution Calculation: - Revolutions per second = 5,300 \u00f7 4,096 = 1.294 rev/s - RPM = 1.294 \u00d7 60 = 77.6 RPM Result: The measured value differs by only about 2\u20133% from the datasheet specification, confirming that the STS3250 performs very close to its rated no-load speed. This close agreement validates both the servo\u2019s performance and our measurement approach. Video: https://youtube.com/shorts/_O_mVZvYQlQ?feature=share...", "url": "https://huggingface.co/posts/branikita/467826227099244", "date_published": "2025-10-31T09:26:01.893834"}, {"id": "https://huggingface.co/posts/pagezyhf/128586778684407", "image": "", "title": "\ud83d\ude80 Big news for AI builders!", "content_text": "\ud83d\ude80 Big news for AI builders! We\u2019re thrilled to announce that the Qwen3-VL family of vision-language models is now available on Azure AI Foundry, thanks to our collaboration with Microsoft. We bring open-source innovation to enterprise-grade AI infrastructure, making it easier than ever for enterprise to deploy and scale the latest and greatest from models from hugging Face securely within Azure. \ud83d\udd0d Highlights: - Deploy Qwen3-VL instantly via managed endpoints - Built-in governance, telemetry, and lifecycle management - True multimodal reasoning \u2014 vision, language, and code understanding - State-of-the-art performance, outperforming closed-source models like Gemini 2.5 Pro and GPT-5 - Available in both *Instruct* and *Thinking* modes, across 24 model sizes \ud83d\udc49 Get started today: search for Qwen3-VL in the Hugging Face Collection on Azure AI Foundry. See translation", "url": "https://huggingface.co/posts/pagezyhf/128586778684407", "date_published": "2025-10-31T09:26:01.894230"}]}