{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/tomaarsen/527498981313495", "image": "", "title": "\ud83e\udd17 Sentence Transformers is joining Hugging Face! \ud83e\udd17 This formalizes the existing maintenance structure, as I've personally led the project for the past two years on behalf of Hugging Face! Details:", "content_text": "\ud83e\udd17 Sentence Transformers is joining Hugging Face! \ud83e\udd17 This formalizes the existing maintenance structure, as I've personally led the project for the past two years on behalf of Hugging Face! Details: Today, the Ubiquitous Knowledge Processing (UKP) Lab is transferring the project to Hugging Face. Sentence Transformers will remain a community-driven, open-source project, with the same open-source license (Apache 2.0) as before. Contributions from researchers, developers, and enthusiasts are welcome and encouraged. The project will continue to prioritize transparency, collaboration, and broad accessibility. Read our full announcement for more details and quotes from UKP and Hugging Face leadership: https://huggingface.co/blog/sentence-transformers-joins-hf We see an increasing wish from companies to move from large LLM APIs to local models for better control and privacy, reflected in the library's growth: in just the last 30 days, Sentence Transformer models have been downloaded >270...", "url": "https://huggingface.co/posts/tomaarsen/527498981313495", "date_published": "2025-10-23T17:20:39.370211"}, {"id": "https://huggingface.co/posts/mitkox/331863384538017", "image": "", "title": "I see all Chinese labs are turning TL;DR into TL;DRGB", "content_text": "I see all Chinese labs are turning TL;DR into TL;DRGB Problem: 1M text tokens == 1 M opportunities for your GPU to file worker-comp Solution: don\u2019t feed the model War & Peace\u2014feed it the movie poster. This is Glyph, Zai\u2019s new visual-text compression voodoo: \u2022 10 k words \u2192 3 PNGs \u2248 3 k visual tokens \u2022 Compression ratio: 4.3\u00d7 \u2022 Throughput: 40-60 tok/s i.e. your context window now finishes before my coffee does So I did the only reasonable thing: asked GLM-4.6 to port Glyph for Qwen3-VL-8B-Thinking. Translation: I made one model compress a novel into a comic strip, then made another model read the comic strip and still ace QA. It\u2019s basically passing notes in class, except the note is a 1920\u00d71080 meme and the teacher is a transformer. We've gone from \"Attention is All You Need\" to \"Attention is Too Expensive, Just Use Your Eyes.\" Remember kids: in 2025 literacy is optional, but JPEG is forever. See translation", "url": "https://huggingface.co/posts/mitkox/331863384538017", "date_published": "2025-10-23T17:20:39.370676"}, {"id": "https://huggingface.co/posts/paulml/601253280118572", "image": "", "title": "Qwen3-VL-4B is incredibly easy to fine-tune!", "content_text": "Qwen3-VL-4B is incredibly easy to fine-tune! We've trained the first DSE model based on this model, and it's already performing at the same level as Jina v4! While Jina Embeddings v4 is built on Qwen2.5-VL-3B (which has a non-commercial license), our model is based on Qwen3-VL-4B and released under Apache 2.0\u2014making it fully commercially permissive. Check out our DSE model here: racineai/QwenAmann-4B-dse See translation", "url": "https://huggingface.co/posts/paulml/601253280118572", "date_published": "2025-10-23T17:20:39.370964"}, {"id": "https://huggingface.co/posts/branikita/764373120316800", "image": "", "title": "With", "content_text": "With Robonine team we released an open-source 3D-printed parallel gripper designed for robotics applications, compatible with popular budget servos like Feetech STS3215 and Waveshare ST3215. This precision gripper offers parallel jaw movement, real-time monitoring, and positioning accuracy of \u00b10.1\u00b0, making it ideal for both robotics enthusiasts and professionals. Complete build cost: Just $69.45\u2013$74.45, with all components available for purchase on Amazon. Direct links are provided in the Bill of Materials on GitHub. Check out the project on github: https://github.com/roboninecom/3D-Printed-Parallel-Gripper-for-Robotics-Arms We encourage you to Watch, Fork, and Star the repository to support our open-source initiative and stay updated on future developments. Your feedback is also welcome! See translation", "url": "https://huggingface.co/posts/branikita/764373120316800", "date_published": "2025-10-23T17:20:39.371341"}, {"id": "https://huggingface.co/posts/andito/792285802244623", "image": "", "title": "Finally, our new paper is out! \"\ud835\uddd9\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\udde9\ud835\uddf6\ud835\ude00\ud835\uddf6\ud835\uddfc\ud835\uddfb: \ud835\udde2\ud835\uddfd\ud835\uddf2\ud835\uddfb \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee \ud835\udddc\ud835\ude00 \ud835\uddd4\ud835\uddf9\ud835\uddf9 \ud835\uddec\ud835\uddfc\ud835\ude02 \ud835\udde1\ud835\uddf2\ud835\uddf2\ud835\uddf1\"! \ud83e\udd73", "content_text": "Finally, our new paper is out! \"\ud835\uddd9\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\udde9\ud835\uddf6\ud835\ude00\ud835\uddf6\ud835\uddfc\ud835\uddfb: \ud835\udde2\ud835\uddfd\ud835\uddf2\ud835\uddfb \ud835\uddd7\ud835\uddee\ud835\ude01\ud835\uddee \ud835\udddc\ud835\ude00 \ud835\uddd4\ud835\uddf9\ud835\uddf9 \ud835\uddec\ud835\uddfc\ud835\ude02 \ud835\udde1\ud835\uddf2\ud835\uddf2\ud835\uddf1\"! \ud83e\udd73 FineVision: Open Data Is All You Need (2510.17269) If you've ever trained a VLM, you know this problem: nobody shares their data mixtures. It's a black box, making replicating SOTA work impossible. We wanted to change that. FineVision unifies 200 sources into 24 million samples. With 17.3 million images and 9.5 billion answer tokens, it's the largest open resource of its kind. In the paper, we share how we built it: \ud83d\udd0d finding and cleaning data at scale \ud83e\uddf9 removing excessive duplicates across sources \ud83e\udd17 decontaminating against 66 public benchmarks My favorite part is Figure 6 (in the video!). It's our visual diversity analysis. It shows that FineVision isn't just bigger; it's more balanced and conceptually richer than other open datasets. NVIDIA's Eagle 2 paper highlighted just how critical this visual diversity is, and our results confirm it: models trained on FineVision consistently outperform those trained on...", "url": "https://huggingface.co/posts/andito/792285802244623", "date_published": "2025-10-23T17:20:39.371874"}, {"id": "https://huggingface.co/posts/AdinaY/459235610615805", "image": "", "title": "HunyuanWorld Mirror\ud83d\udd25a versatile feed forward model for universal 3D world reconstruction by Tencent", "content_text": "HunyuanWorld Mirror\ud83d\udd25a versatile feed forward model for universal 3D world reconstruction by Tencent tencent/HunyuanWorld-Mirror \u2728 Any prior in \u2192 3D world out \u2728 Mix camera, intrinsics, depth as priors \u2728 Predict point clouds, normals, Gaussians & more in one pass \u2728 Unified architecture for all 3D task See translation", "url": "https://huggingface.co/posts/AdinaY/459235610615805", "date_published": "2025-10-23T17:20:39.372130"}, {"id": "https://huggingface.co/posts/sequelbox/934890543066201", "image": "", "title": "NEW RELEASE: Esper 3.1 for gpt-oss-20b!", "content_text": "NEW RELEASE: Esper 3.1 for gpt-oss-20b! - Esper is our full-stack, full-cycle coding, DevOps, and architecture specialist! - Our newest, best DeepSeek technical datasets emphasize more challenging queries and tough real-world coding tasks across a variety of programming languages and development paradigms: - Titanium 3 for coding and reasoning in DevOps and architecture: sequelbox/Titanium3-DeepSeek-V3.1-Terminus - Tachibana 3 for high-difficulty code production in a variety of topics and programming languages: - sequelbox/Tachibana3-Part1-DeepSeek-V3.1-Terminus - sequelbox/Tachibana3-Part2-DeepSeek-V3.2 - Mitakihara for MLOps, AI building, use, expertise, and research: sequelbox/Mitakihara-DeepSeek-R1-0528 GET IT NOW, FOR EVERYONE: ValiantLabs/gpt-oss-20b-Esper3.1 We'll have more releases of Esper coming up, plus more experimental open-source releases :) find open source datasets and experimental models at @ sequelbox Help us keep working for open source AI with a donation:...", "url": "https://huggingface.co/posts/sequelbox/934890543066201", "date_published": "2025-10-23T17:20:39.372512"}, {"id": "https://huggingface.co/posts/piercus/315056772255613", "image": "", "title": "\ud83d\udea7 Reproducing LBM-Eraser\u2026 in progress! [1]", "content_text": "\ud83d\udea7 Reproducing LBM-Eraser\u2026 in progress! [1] When repurposing a T2I model into a pure I2I model, there\u2019s always that orphaned text path \u2014 what do we do with it? \ud83e\udd14 You can reuse it as learnable embeddings in multi-task setups [2], freeze an empty text prompt, distillate or prune the corresponding part. In LBM, they take a clever route \u2014 zeroing [3] and reshaping [4] the text-related cross-attentions into self-attentions. This gives you fresh weights for I2I computation, nicely integrated into your SD architecture. \ud83d\udcce References [1] Our LBM Fork: https://github.com/finegrain-ai/LBM [2] OmniPaint: OmniPaint: Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting (2503.08677) [3] LBM Zeroing: https://github.com/gojasper/LBM/blob/cafebc46a9ac16dcc61691d289cc4676b5c75380/examples/training/train_lbm_surface.py#L147-L148 [4] LBM Reshaping: https://github.com/gojasper/LBM/blob/cafebc46a9ac16dcc61691d289cc4676b5c75380/examples/training/train_lbm_surface.py#L100 See...", "url": "https://huggingface.co/posts/piercus/315056772255613", "date_published": "2025-10-23T17:20:39.372923"}, {"id": "https://huggingface.co/posts/prithivMLmods/161223202037322", "image": "", "title": "Let\u2019s have the comparison again with Multimodal OCR3:", "content_text": "Let\u2019s have the comparison again with Multimodal OCR3: nanonets/Nanonets-OCR2-3B vs allenai/olmOCR-2-7B-1025 vs rednote-hilab/dots.ocr vs datalab-to/chandra Try it here @ prithivMLmods/Multimodal-OCR3 Collection: prithivMLmods/multimodal-implementations-67c9982ea04b39f0608badb0 See translation", "url": "https://huggingface.co/posts/prithivMLmods/161223202037322", "date_published": "2025-10-23T17:20:39.373168"}, {"id": "https://huggingface.co/posts/merve/349000009530858", "image": "", "title": "deepseek-ai/DeepSeek-OCR", "content_text": "deepseek-ai/DeepSeek-OCR is out! \ud83d\udd25 my take \u2935\ufe0f > pretty insane it can parse and re-render charts in HTML > it uses CLIP and SAM features concatenated, so better grounding > very efficient per vision tokens/performance ratio > covers 100 languages See translation", "url": "https://huggingface.co/posts/merve/349000009530858", "date_published": "2025-10-23T17:20:39.373448"}]}