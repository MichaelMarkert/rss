{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/ginipick/923354082387927", "image": "", "title": "\ud83d\ude80 Introducing MOUSE: Space Research Thinking on HuggingFace Spaces", "content_text": "\ud83d\ude80 Introducing MOUSE: Space Research Thinking on HuggingFace Spaces \ud83d\ude80 How to Get Started ginipick/spaces-research-think Welcome to **MOUSE: Space Research Thinking** \u2013 an innovative HuggingFace Spaces project designed to transform how you analyze and interact with Python code. Whether you're a developer, researcher, or simply passionate about coding, this tool provides state-of-the-art analysis, summarization, and usage guidance, all powered by advanced AI. --- ## \ud83c\udf1f Key Features - **Real-Time Code Analysis** Instantly dissect your Python code to reveal its structure, functionality, and potential applications. Our tool delivers: - **Background & Necessity**: Understand the context behind the code. - **Functional Utility & Value**: Highlight core functionalities and benefits. - **Distinctive Features**: Discover what sets the project apart. - **Target Audience & Applications**: Identify who can benefit and how. - **Expected Impact**: Envision the improvements and innovations the code...", "url": "https://huggingface.co/posts/ginipick/923354082387927", "date_published": "2025-02-27T13:28:23.853273"}, {"id": "https://huggingface.co/posts/burtenshaw/352638065928004", "image": "", "title": "Now the Hugging Face agent course is getting real! With frameworks like smolagents, LlamaIndex, and LangChain.", "content_text": "Now the Hugging Face agent course is getting real! With frameworks like smolagents, LlamaIndex, and LangChain. \ud83d\udd17 Follow the org for updates https://huggingface.co/agents-course This week we are releasing the first framework unit in the course and it\u2019s on smolagents. This is what the unit covers: - why should you use smolagents vs another library? - how to build agents that use code - build multiagents systems - use vision language models for browser use The team has been working flat out on this for a few weeks. Led by @ sergiopaniego and supported by smolagents author @ m-ric . See translation", "url": "https://huggingface.co/posts/burtenshaw/352638065928004", "date_published": "2025-02-27T13:28:23.853655"}, {"id": "https://huggingface.co/posts/prithivMLmods/305640045790864", "image": "", "title": "Dropping some of the custom fine-tunes based on SigLIP2,", "content_text": "Dropping some of the custom fine-tunes based on SigLIP2, with a single-label classification problem type! \ud83c\udf00\ud83e\udde4 - AI vs Deepfake vs Real : prithivMLmods/AI-vs-Deepfake-vs-Real-Siglip2 - Deepfake Detect : prithivMLmods/Deepfake-Detect-Siglip2 - Fire Detection : prithivMLmods/Fire-Detection-Siglip2 - Deepfake Quality Assess : prithivMLmods/Deepfake-Quality-Assess-Siglip2 - Guard Against Unsafe Content : prithivMLmods/Guard-Against-Unsafe-Content-Siglip2 \ud83c\udf20Collection : prithivMLmods/siglip2-custom-67bcdb2de8fe96b99fb4e19e See translation", "url": "https://huggingface.co/posts/prithivMLmods/305640045790864", "date_published": "2025-02-27T13:28:23.854067"}, {"id": "https://huggingface.co/posts/freddyaboulton/628292960317038", "image": "", "title": "Getting WebRTC and Websockets right in python is very tricky. If you've tried to wrap an LLM in a real-time audio layer then you know what I'm talking about.", "content_text": "Getting WebRTC and Websockets right in python is very tricky. If you've tried to wrap an LLM in a real-time audio layer then you know what I'm talking about. That's where FastRTC comes in! It makes WebRTC and Websocket streams super easy with minimal code and overhead. Check out our org: hf.co/fastrtc See translation", "url": "https://huggingface.co/posts/freddyaboulton/628292960317038", "date_published": "2025-02-27T13:28:23.854330"}, {"id": "https://huggingface.co/posts/AdinaY/200256238569001", "image": "", "title": "Wan2.1 \ud83d\udd25\ud83d\udcf9 new OPEN video model by Alibaba Wan team!", "content_text": "Wan2.1 \ud83d\udd25\ud83d\udcf9 new OPEN video model by Alibaba Wan team! Model: Wan-AI/Wan2.1-T2V-14B Demo: Wan-AI/Wan2.1 \u2728Apache 2.0 \u27288.19GB VRAM, runs on most GPUs \u2728Multi-Tasking: T2V, I2V, Video Editing, T2I, V2A \u2728Text Generation: Supports Chinese & English \u2728Powerful Video VAE: Encode/decode 1080P w/ temporal precision See translation", "url": "https://huggingface.co/posts/AdinaY/200256238569001", "date_published": "2025-02-27T13:28:23.854610"}, {"id": "https://huggingface.co/posts/alvarobartt/393660009896131", "image": "", "title": "\ud83d\udd25 Agents can do anything!", "content_text": "\ud83d\udd25 Agents can do anything! @ microsoft Research just announced the release of Magma 8B! Magma is a new Visual Language Model (VLM) with 8B parameters for multi-modal agents designed to handle complex interactions across virtual and real environments; and it's MIT licensed! Magma comes with exciting new features such as: - Introduces the Set-of-Mark and Trace-of-Mark techniques for fine-tuning - Leverages a large amount of unlabeled video data to learn the spatial-temporal grounding and planning - A strong generalization and ability to be fine-tuned for other agentic tasks - SOTA in different multi-modal benchmarks spanning across UI navigation, robotics manipulation, image / video understanding and spatial understanding and reasoning - Generates goal-driven visual plans and actions for agentic use cases Model: microsoft/Magma-8B Technical Report: Magma: A Foundation Model for Multimodal AI Agents (2502.13130) See translation", "url": "https://huggingface.co/posts/alvarobartt/393660009896131", "date_published": "2025-02-27T13:28:23.855048"}, {"id": "https://huggingface.co/posts/onekq/116274779602090", "image": "", "title": "Necessity is mother of invention. To understand \u26a1FlashMLA\u26a1 by", "content_text": "Necessity is mother of invention. To understand \u26a1FlashMLA\u26a1 by \ud83d\udc0bDeepSeek \ud83d\udc0b, the first question to ask is why. The keyword here is H800, a lower-end product tailored for export control. The purpose here is to squeeze out as much performance as possible. But here is the most important takeaway: this invention benefits EVERYONE. See translation", "url": "https://huggingface.co/posts/onekq/116274779602090", "date_published": "2025-02-27T13:28:23.855328"}, {"id": "https://huggingface.co/posts/Locutusque/390800785751051", "image": "", "title": "\ud83c\udf89 Exciting news, everyone! I've just released **Thespis-Llama-3.1-8B**, a new language model designed for enhanced roleplaying! \u2728\ufe0f", "content_text": "\ud83c\udf89 Exciting news, everyone! I've just released **Thespis-Llama-3.1-8B**, a new language model designed for enhanced roleplaying! \u2728\ufe0f It's built on Llama-3.1 and fine-tuned with a focus on Theory of Mind reasoning to create more believable and engaging characters. It even learned a few tricks on its own, like adding in-character thought processes! \ud83e\udde0 Check it out here: Locutusque/Thespis-Llama-3.1-8B Give it a try and let me know what you think! I'm especially interested in feedback on how well the characters stay in role and if the responses feel natural. Looking forward to seeing what amazing stories you create! \u270d\ufe0f See translation", "url": "https://huggingface.co/posts/Locutusque/390800785751051", "date_published": "2025-02-27T13:28:23.855686"}, {"id": "https://huggingface.co/posts/fdaudens/982146976081521", "image": "", "title": "\ud83d\ude80 Just launched: A toolkit of 20 powerful AI tools that journalists can use right now - transcribe, analyze, create. 100% free & open-source.", "content_text": "\ud83d\ude80 Just launched: A toolkit of 20 powerful AI tools that journalists can use right now - transcribe, analyze, create. 100% free & open-source. Been testing all these tools myself and created a searchable collection of the most practical ones - from audio transcription to image generation to document analysis. No coding needed, no expensive subscriptions. Some highlights I've tested personally: - Private, on-device transcription with speaker ID in 100+ languages using Whisper - Website scraping that just works - paste a URL, get structured data - Local image editing with tools like Finegrain (impressive results) - Document chat using Qwen 2.5 72B (handles technical papers well) Sharing this early because the best tools come from the community. Drop your favorite tools in the comments or join the discussion on what to add next! \ud83d\udc49 JournalistsonHF/ai-toolkit See translation", "url": "https://huggingface.co/posts/fdaudens/982146976081521", "date_published": "2025-02-27T13:28:23.856176"}, {"id": "https://huggingface.co/posts/stefan-it/765581033311913", "image": "", "title": "She arrived \ud83d\ude0d", "content_text": "She arrived \ud83d\ude0d [Expect more models soon...] See translation", "url": "https://huggingface.co/posts/stefan-it/765581033311913", "date_published": "2025-02-27T13:28:23.856391"}]}