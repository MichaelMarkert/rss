{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/sergiopaniego/319778709690075", "image": "", "title": "gpt-oss was possible thanks to new engineering efforts in \ud83e\udd17 transformers. We just dropped a blog covering them:", "content_text": "gpt-oss was possible thanks to new engineering efforts in \ud83e\udd17 transformers. We just dropped a blog covering them: - Kernels from the Hub - MXFP4 Quantization - Tensor & Expert Parallelism - Dynamic Sliding Window & Cache - Continuous Batching & Paged Attention Grab a coffee & dive in! \u2615\ufe0f https://huggingface.co/blog/faster-transformers See translation", "url": "https://huggingface.co/posts/sergiopaniego/319778709690075", "date_published": "2025-09-12T17:18:23.779335"}, {"id": "https://huggingface.co/posts/Abhaykoul/356298508659280", "image": "", "title": "\ud83d\ude80 Ever dreamed of training your own Large Language Model from scratch? What if I told you it doesn't require a supercomputer or PhD in ML? \ud83e\udd2f", "content_text": "\ud83d\ude80 Ever dreamed of training your own Large Language Model from scratch? What if I told you it doesn't require a supercomputer or PhD in ML? \ud83e\udd2f Introducing LLM Trainer - the educational framework that makes LLM training accessible to EVERYONE! Whether you're on a CPU-only laptop or scaling to distributed GPUs, we've got you covered. \ud83d\udcbb\u27a1\ufe0f\ud83d\udda5\ufe0f Why LLM Trainer? Because existing tools are either too simplistic (hiding the magic) or too complex (requiring expert knowledge). We bridge the gap with: \ud83c\udf93 Educational transparency - every component built from scratch with clear code \ud83d\udcbb CPU-first approach - start training immediately, no GPU needed \ud83d\udd27 Full customization - modify anything you want \ud83d\udcc8 Seamless scaling - from laptop to cluster without code changes \ud83e\udd1d HuggingFace integration - works with existing models & tokenizers Key highlights: \u2705 Built-in tokenizers (BPE, WordPiece, HF wrappers) \u2705 Complete Transformer implementation from scratch \u2705 Optimized for CPU training \u2705 Advanced features: mixed...", "url": "https://huggingface.co/posts/Abhaykoul/356298508659280", "date_published": "2025-09-12T17:18:23.779942"}, {"id": "https://huggingface.co/posts/MonsterMMORPG/330916531424120", "image": "", "title": "Hunyuan Image 2.1 by Tencent Full Tutorial and 1-Click to Install Ultra Advanced App to Use Locally :", "content_text": "Hunyuan Image 2.1 by Tencent Full Tutorial and 1-Click to Install Ultra Advanced App to Use Locally : https://youtu.be/dNeA5mJ36hA Tutorial video : https://youtu.be/dNeA5mJ36hA Check the below screenshots Hunyuan Image 2.1 just published by Tencent and I have been working on developing the very best app to let you use HunyuanImage-2.1 with easiest and most accurate way. In this tutorial video, I will show you how to literally 1-click to install this model and our app on Windows (locally), Massed Compute (cloud) and RunPod (cloud). The images are all raw 2560x1440 pixels with 8-steps Refiner of Hunyuan Image 2.1 model This model native resolution is 2048x2048 pixels See translation", "url": "https://huggingface.co/posts/MonsterMMORPG/330916531424120", "date_published": "2025-09-12T17:18:23.780263"}, {"id": "https://huggingface.co/posts/drvsbrkcn/494150147788454", "image": "", "title": "Hello everyone,", "content_text": "Hello everyone, It is my first time using Hugging Face. It is hella nice. Take care. See translation", "url": "https://huggingface.co/posts/drvsbrkcn/494150147788454", "date_published": "2025-09-12T17:18:23.780463"}, {"id": "https://huggingface.co/posts/salma-remyx/853424776483426", "image": "", "title": "Most apps don't have great full-text search over their assets.", "content_text": "Most apps don't have great full-text search over their assets. We've developed an agent to automate the environment building and testing of experimental codebases sourced from arXiv. We push these containerized reproductions daily to Docker Hub: https://hub.docker.com/u/remyxai However, searching for them can be challenging unless you know the specific arXiv ID associated with each paper. We are currently working on implementing a search feature in Remyx, which will make these assets easily discoverable and ready for testing \ud83d\udd0d Stay tuned! Discover your next best idea to experiment with here: https://engine.remyx.ai See translation", "url": "https://huggingface.co/posts/salma-remyx/853424776483426", "date_published": "2025-09-12T17:18:23.780788"}, {"id": "https://huggingface.co/posts/tomaarsen/906557413568289", "image": "", "title": "ModernBERT goes MULTILINGUAL! One of the most requested models I've seen, The Johns Hopkins University's CLSP has trained state-of-the-art massively multilingual encoders using the ModernBERT architecture: mmBERT.", "content_text": "ModernBERT goes MULTILINGUAL! One of the most requested models I've seen, The Johns Hopkins University's CLSP has trained state-of-the-art massively multilingual encoders using the ModernBERT architecture: mmBERT. Model details: - 2 model sizes: - jhu-clsp/mmBERT-small - jhu-clsp/mmBERT-base - Uses the ModernBERT architecture, but with the Gemma2 multilingual tokenizer (so: flash attention, alternating global/local attention, unpadding/sequence packing, etc.) - Maximum sequence length of 8192 tokens, on the high end for encoders - Trained on 1833 languages using DCLM, FineWeb2, and many more sources - 3 training phases: 2.3T tokens pretraining on 60 languages, 600B tokens mid-training on 110 languages, and 100B tokens decay training on all 1833 languages. - Both models are MIT Licensed, and the full datasets and intermediary checkpoints are also publicly released Evaluation details: - Very competitive with ModernBERT at equivalent sizes on English (GLUE, MTEB v2 English after...", "url": "https://huggingface.co/posts/tomaarsen/906557413568289", "date_published": "2025-09-12T17:18:23.781286"}, {"id": "https://huggingface.co/posts/sanaka87/107769277937246", "image": "", "title": "Excited to share our Unified Multimodal Models new work Reconstruction Alignment (RecA)! \ud83d\ude80 Just 6 \u00d7 80GB A100s \u00d7 4.5 hours to boost BAGEL performance across all tasks! Outperforms FLUX-Kontext in image editing capabilities!", "content_text": "Excited to share our Unified Multimodal Models new work Reconstruction Alignment (RecA)! \ud83d\ude80 Just 6 \u00d7 80GB A100s \u00d7 4.5 hours to boost BAGEL performance across all tasks! Outperforms FLUX-Kontext in image editing capabilities! \ud83d\udcc4 Paper: https://alphaxiv.org/abs/2509.07295 \ud83d\udcbb Code: https://github.com/HorizonWind2004/reconstruction-alignment \ud83e\udd17 HF Models: sanaka87/reca-68ad2176380355a3dcedc068 \u270d\ufe0f DEMO: sanaka87/BAGEL-RecA \ud83c\udf10 Project Page: https://reconstruction-alignment.github.io \ud83d\udd25 X: https://x.com/XDWang101/status/1965908302581420204 \ud83d\udcf0 Zhihu: https://zhuanlan.zhihu.com/p/1947584568187159814 \ud83e\udd17 HF Daily Paper: Reconstruction Alignment Improves Unified Multimodal Models (2509.07295) \u26a1 <10k images & 27 GPU hours (no-arch-changes) \u2192 SOTA, surpassing much larger open-source & private models: \ud83d\udcca GenEval: 0.73 \u2192 0.90 | \ud83d\udcca DPGBench: 80.93 \u2192 88.15 \ud83d\uddbc\ufe0f ImgEdit: 3.38 \u2192 3.75 | \ud83d\udd8c\ufe0f GEdit: 6.94 \u2192 7.25 \u2705 RecA trains UMMs to reconstruct images from their own visual understanding encoder embeddings \u2192 big gains...", "url": "https://huggingface.co/posts/sanaka87/107769277937246", "date_published": "2025-09-12T17:18:23.781725"}, {"id": "https://huggingface.co/posts/prithivMLmods/195813965636174", "image": "", "title": "Build something cool with Nano Banana aka Gemini 2.5 Flash Image AIO [All-in-One]. Draw and transform on canvas, edit images, and generate images\u2014all in one place!\ud83c\udf4c", "content_text": "Build something cool with Nano Banana aka Gemini 2.5 Flash Image AIO [All-in-One]. Draw and transform on canvas, edit images, and generate images\u2014all in one place!\ud83c\udf4c \u2726\ufe0e Constructed with the Gemini API (GCP). Try it here: https://nano-banana-aio-886892687963.us-west1.run.app See translation", "url": "https://huggingface.co/posts/prithivMLmods/195813965636174", "date_published": "2025-09-12T17:18:23.781993"}, {"id": "https://huggingface.co/posts/prithivMLmods/478074363544608", "image": "", "title": "The POINTS-Reader, a vision-language model for end-to-end document conversion, is a powerful, distillation-free Vision-Language Model that sets new SoTA benchmarks. The demo is now available on HF (Extraction, Preview, Documentation). The input consists of a fixed prompt and a document image, while the output contains only a string (the text extracted from the document image). \ud83d\udd25\ud83e\udd17", "content_text": "The POINTS-Reader, a vision-language model for end-to-end document conversion, is a powerful, distillation-free Vision-Language Model that sets new SoTA benchmarks. The demo is now available on HF (Extraction, Preview, Documentation). The input consists of a fixed prompt and a document image, while the output contains only a string (the text extracted from the document image). \ud83d\udd25\ud83e\udd17 \u2726 Space/App: prithivMLmods/POINTS-Reader-OCR \u2726 Model: tencent/POINTS-Reader \u2726 Paper: https://arxiv.org/pdf/2509.01215 \ud83e\udd17 The app is done and ready to go brrrr with zero GPU. Thankyou @ merve . . . To know more about it, visit the app page or the respective model page!! See translation", "url": "https://huggingface.co/posts/prithivMLmods/478074363544608", "date_published": "2025-09-12T17:18:23.782339"}, {"id": "https://huggingface.co/posts/burtenshaw/833511511767176", "image": "", "title": "Smol course has a distinctive approach to teaching post-training, so I'm posting about how it\u2019s different to other post-training courses, including the llm course that\u2019s already available.", "content_text": "Smol course has a distinctive approach to teaching post-training, so I'm posting about how it\u2019s different to other post-training courses, including the llm course that\u2019s already available. In short, the smol course is just more direct that any of the other course, and intended for semi-pro post trainers. - It\u2019s a minimal set of instructions on the core parts. - It\u2019s intended to bootstrap real projects you're working on. - The material handsover to existing documentation for details - Likewise, it handsover to the LLM course for basics. - Assessment is based on a leaderboard, without reading all the material. To start the smol course, follow here: smol-course See translation", "url": "https://huggingface.co/posts/burtenshaw/833511511767176", "date_published": "2025-09-12T17:18:23.782679"}]}