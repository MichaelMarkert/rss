{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/m-ric/175050207181959", "image": "", "title": "STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! \ud83d\udea8", "content_text": "STOP EVERYTHING NOW - we might finally have a radical architecture improvement over Transformers!!! \ud83d\udea8 A lone scientist just proposed Tiny Recursive Model (TRM), and it is literally the most impressive model that I've seen this year. \u27a1\ufe0f Tiny Recursive Model is 7M parameters \u27a1\ufe0f On ARC-AGI, it beats flagship models like Gemini-2.5-pro Consider how wild this is: Gemini-2.5-pro must be over 10,000x bigger and had 1,000 as many authors \ud83d\ude02 (Alexia is alone on the paper) What's this sorcery? In short: it's a very tiny Transformers, but it loops over itself at two different frequencies, updating two latent variables: one for the proposed answer and one for the reasoning. @ AlexiaJM started from the paper Hierarchical Reasoning Model, published a few months ago, that already showed breakthrough improvement on AGI for its small size (27M) Hierarchical Reasoning Model had introduced one main feature: \ud83d\udd0e Deep supervision In their model, one part (here one layer) would run at high frequency, and...", "url": "https://huggingface.co/posts/m-ric/175050207181959", "date_published": "2025-10-10T17:19:49.537669"}, {"id": "https://huggingface.co/posts/hba123/315319549896319", "image": "", "title": "\ud83e\udd16 What if building your own robot arm costs less than \u00a3220?", "content_text": "\ud83e\udd16 What if building your own robot arm costs less than \u00a3220? For years, robotics has been locked behind high prices and complex systems. So we decided to change that. Today, we\u2019re open-sourcing Ark-Bot \u2014 a fully 3D-printed, 6-DOF robot arm that works seamlessly with our Python robotics library, Ark. And yes\u2026 It\u2019s only \u00a3215.86 to build. \ud83e\udde0ArkBot Specs \ud83e\udde0 1\ufe0f\u20e3 Reach: 1 meter 2\ufe0f\u20e3 Weight: 2.6 kg 3\ufe0f\u20e3 Payload: 1.8 kg \ud83d\udcaa 4\ufe0f\u20e3 DOF: 6 5\ufe0f\u20e3 Input Voltage: DC 12V \ud83e\udd1fFully 3D-printable & open-source \ud83e\udd1fIntegrated with Ark \u2014 no ROS required \ud83d\udcf9 We\u2019ve also released a video showing the full assembly process \u2014 because robotics should be something everyone can learn, build, and improve on. \ud83d\udc69\u200d\ud83c\udf93 With Ark-Bot, anyone \u2014 from students to AI researchers \u2014 can experiment with embodied AI, robot learning, and control algorithms on real hardware, affordably. If you could control a 1-meter robot arm from your laptop for under \u00a3220\u2026 \ud83d\udc49 What would you build first? \ud83d\udd17https://github.com/Robotics-Ark/ark_bot \ud83c\udfa5...", "url": "https://huggingface.co/posts/hba123/315319549896319", "date_published": "2025-10-10T17:19:49.538157"}, {"id": "https://huggingface.co/posts/AdinaY/255762505303069", "image": "", "title": "At the close of the National Holiday\ud83c\udde8\ud83c\uddf3, Antgroup drops a new SoTA model.", "content_text": "At the close of the National Holiday\ud83c\udde8\ud83c\uddf3, Antgroup drops a new SoTA model. Ling-1T \ud83d\udd25 the trillion-parameter flagship of the Ling 2.0 series. inclusionAI/Ling-1T \u27281T total / 50B active params per token \u272820T+ reasoning-dense tokens (Evo-CoT) \u2728128K context via YaRN \u2728FP8 training: 15%+ faster, same precision as BF16 \u2728Hybrid Syntax-Function-Aesthetics reward for front-end & visual generation See translation", "url": "https://huggingface.co/posts/AdinaY/255762505303069", "date_published": "2025-10-10T17:19:49.538449"}, {"id": "https://huggingface.co/posts/mlabonne/852622328581891", "image": "", "title": "LiquidAI/LFM2-8B-A1B", "content_text": "LiquidAI/LFM2-8B-A1B just dropped! 8.3B params with only 1.5B active/token \ud83d\ude80 > Quality \u2248 3\u20134B dense, yet faster than Qwen3-1.7B > MoE designed to run on phones/laptops (llama.cpp / vLLM) > Pre-trained on 12T tokens \u2192 strong math/code/IF See translation", "url": "https://huggingface.co/posts/mlabonne/852622328581891", "date_published": "2025-10-10T17:19:49.538694"}, {"id": "https://huggingface.co/posts/piercus/787328298619334", "image": "", "title": "We've just forked LBM to reproduce the LBM eraser results", "content_text": "We've just forked LBM to reproduce the LBM eraser results Our fork : https://github.com/finegrain-ai/LBM LBM paper: LBM: Latent Bridge Matching for Fast Image-to-Image Translation (2503.07535) LBM relighting demo : jasperai/LBM_relighting See translation", "url": "https://huggingface.co/posts/piercus/787328298619334", "date_published": "2025-10-10T17:19:49.538920"}, {"id": "https://huggingface.co/posts/Severian/804699132791352", "image": "", "title": "MLX port of BDH (Baby Dragon Hatchling) is up!", "content_text": "MLX port of BDH (Baby Dragon Hatchling) is up! I\u2019ve ported the BDH ( https://github.com/pathwaycom/bdh ) model to MLX for Apple Silicon. It\u2019s a faithful conversion of the PyTorch version: same math, same architecture (byte-level vocab, shared weights across layers, ReLU sparsity, RoPE attention with Q=K), with MLX-friendly APIs and a detailed README explaining the few API-level differences and why results are equivalent. Code, docs, and training script are ready to use. You may need to adjust the training script a bit to fit your own custom dataset. Only tested on M4 so far, but should work perfect for any M1/M2/M3 users out there. I\u2019m currently training this MLX build on my Internal Knowledge Map (IKM) dataset Severian/Internal-Knowledge-Map Training\u2019s underway; expect a day or so before I publish weights. When it\u2019s done, I\u2019ll upload the checkpoint to Hugging Face for anyone to test. Repo: https://github.com/severian42/BDH-MLX HF model (coming soon): Severian/BDH-MLX If you try it...", "url": "https://huggingface.co/posts/Severian/804699132791352", "date_published": "2025-10-10T17:19:49.539376"}, {"id": "https://huggingface.co/posts/jwgu/145606397801534", "image": "", "title": "\ud83c\udf89 NEW RELEASES: Cosmos Predict 2.5 and Transfer 2.5", "content_text": "\ud83c\udf89 NEW RELEASES: Cosmos Predict 2.5 and Transfer 2.5 Cosmos Predict 2.5: - Combines Text2World, Image2World, and Video2World - Multimodal, future-state video prediction Cosmos Transfer 2.5: - High-fidelity multicontrol world simulations - Inputs: RGB, depth, segmentation\u2014blended seamlessly These updates boost development of autonomous vehicles, robotics, and video analytics. Don\u2019t miss Jensen Huang\u2019s keynote at NVIDIA GTC Washington, D.C. on 10/28 to hear the latest in physical AI. \ud83d\udcfa Watch live: https://nvda.ws/4pUjF4x \ud83d\udd17 Try Predict 2.5: https://nvda.ws/4otReZZ \ud83d\udd17 Try Transfer 2.5: https://nvda.ws/46GEx7T See translation", "url": "https://huggingface.co/posts/jwgu/145606397801534", "date_published": "2025-10-10T17:19:49.539715"}, {"id": "https://huggingface.co/posts/AdamF92/854077189361039", "image": "", "title": "Hi, I just published research paper that's introducing my Reactive Transformer (RxT) architecture. I would be grateful if you could check it and upvote on HuggingFace Daily Papers -", "content_text": "Hi, I just published research paper that's introducing my Reactive Transformer (RxT) architecture. I would be grateful if you could check it and upvote on HuggingFace Daily Papers - Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models (2510.03561) Architecture is based on stateful real-time processing with innovational asynchronous memory update. Instead of reprocessing all the conversation history for each message, it's processing only single query with all the context moved to dedicated memory layers. Memory is updated after generating the answer, so it's not influencing latency - in tests, time to first token was almost the same as generating a single token. It has also better quality/accuracy in multi-turn dialogue than the same size stateless decoder-only model. Initial experiments were small scale (12M to 160M params models trained on simple synthetic datasets), but just now I'm starting training of bigger 270M params model on...", "url": "https://huggingface.co/posts/AdamF92/854077189361039", "date_published": "2025-10-10T17:19:49.540095"}, {"id": "https://huggingface.co/posts/Ethank01/441451500060564", "image": "", "title": "No invitation code needed \u2014 create AI videos with one click!", "content_text": "No invitation code needed \u2014 create AI videos with one click! Experience Sora 2, Veo 3, and Wan 2.2 all in one place on iMini. \ud83d\udc49 Try it here: https://imini.com/ See translation", "url": "https://huggingface.co/posts/Ethank01/441451500060564", "date_published": "2025-10-10T17:19:49.540327"}, {"id": "https://huggingface.co/posts/giadap/452837154929545", "image": "", "title": "\ud83c\udf0e AI ethics and sustainability are two sides of the same coin.", "content_text": "\ud83c\udf0e AI ethics and sustainability are two sides of the same coin. In our new blog post with Dr. Sasha Luccioni, we argue that separating them (as is too often the case) means missing the bigger picture of how AI systems impact both people and the planet. Ethical and sustainable AI development can\u2019t be pursued in isolation. The same choices that affect who benefits or is harmed by AI systems also determine how much energy and resources they consume. We explore how two key concepts, evaluation and transparency, can serve as bridges between these domains: \ud83d\udcca Evaluation, by moving beyond accuracy or performance metrics to include environmental and social costs, as we\u2019ve done with tools like the AI Energy Score. \ud83d\udd0d Transparency, by enabling reproducibility, accountability, and environmental reporting through open tools like the Environmental Transparency Space. AI systems mirror our priorities. If we separate ethics from sustainability, we risk building technologies that are efficient but...", "url": "https://huggingface.co/posts/giadap/452837154929545", "date_published": "2025-10-10T17:19:49.540776"}]}