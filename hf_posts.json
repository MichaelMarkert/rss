{"version": "https://jsonfeed.org/version/1", "title": "Hugging Face Posts", "home_page_url": "https://huggingface.co/", "feed_url": "https://raw.githubusercontent.com/MichaelMarkert/rss/refs/heads/main/hf_posts.json", "items": [{"id": "https://huggingface.co/posts/danielhanchen/212249714773740", "image": "", "title": "Qwen3-Next can now be Run locally! (30GB RAM)", "content_text": "Qwen3-Next can now be Run locally! (30GB RAM) Instruct GGUF: unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF The models come in Thinking and Instruct versions and utilize a new architecture, allowing it to have ~10x faster inference than Qwen32B. \ud83d\udc9c Step-by-step Guide: https://docs.unsloth.ai/models/qwen3-next Thinking GGUF: unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF See translation", "url": "https://huggingface.co/posts/danielhanchen/212249714773740", "date_published": "2025-11-30T05:24:23.963529"}, {"id": "https://huggingface.co/posts/prithivMLmods/107899220924462", "image": "", "title": "Introducing the  Super-OCRs Demo, a comparison of state-of-the-art multimodal OCR VLMs, including HunyuanOCR, DeepSeekOCR, Dots, and Nanonets in one space for performing OCR, rendering LaTeX and Markdown, and visual grounding (layout). Find the related Spaces and models below.\ud83e\udd17\ud83d\udd25", "content_text": "Introducing the Super-OCRs Demo, a comparison of state-of-the-art multimodal OCR VLMs, including HunyuanOCR, DeepSeekOCR, Dots, and Nanonets in one space for performing OCR, rendering LaTeX and Markdown, and visual grounding (layout). Find the related Spaces and models below.\ud83e\udd17\ud83d\udd25 \u2728Super-OCRs[Demo]: prithivMLmods/Super-OCRs-Demo \u2728Collection: https://huggingface.co/collections/prithivMLmods/multimodal-implementations \u2728GitHub: https://github.com/PRITHIVSAKTHIUR/Super-OCRs-Demo \u2b50 Models Used: \u2726 HunyuanOCR: tencent/HunyuanOCR \u2726 DeepSeek-OCR: (-) deepseek-ai/DeepSeek-OCR (+) prithivMLmods/DeepSeek-OCR-Latest-BF16.I64 \u2726 Dots.OCR: (-) rednote-hilab/dots.ocr (+) prithivMLmods/Dots.OCR-Latest-BF16 \u2726 Nanonets-OCR2-3B: nanonets/Nanonets-OCR2-3B \u2b50 Some Other Relevant Apps: \u2726 Qwen3-VL-HF-Demo: prithivMLmods/Qwen3-VL-HF-Demo \u2726 Qwen3-VL-Outpost: prithivMLmods/Qwen3-VL-Outpost \u2726 Multimodal-OCR: prithivMLmods/Multimodal-OCR \u2726 Multimodal-OCR2: prithivMLmods/Multimodal-OCR2 \u2726 Multimodal-OCR3:...", "url": "https://huggingface.co/posts/prithivMLmods/107899220924462", "date_published": "2025-11-30T05:24:23.964035"}, {"id": "https://huggingface.co/posts/sergiopaniego/367599205240435", "image": "", "title": "nanochat is now in transformers!", "content_text": "nanochat is now in transformers! The LLM by @ karpathy is officially in the library, and we wrote a blog covering: how did we port the model, differences from the original, and how to run or train it. go read it \ud83e\udd13 nanochat-students/transformers See translation", "url": "https://huggingface.co/posts/sergiopaniego/367599205240435", "date_published": "2025-11-30T05:24:23.964288"}, {"id": "https://huggingface.co/posts/Holy-fox/916850799845292", "image": "", "title": "4\u6708\uff1f\u3054\u308d\u306b\u53c2\u52a0\u3057\u305fCerebras\u306e\u30cf\u30c3\u30ab\u30bd\u30f3\u304b\u3089\u4f55\u6545\u304bHuggingface\u306epro\u30d7\u30e9\u30f3\u304c\u7d9a\u3044\u3066\u308b\u3093\u3067\u3059\u3088\u306d...", "content_text": "4\u6708\uff1f\u3054\u308d\u306b\u53c2\u52a0\u3057\u305fCerebras\u306e\u30cf\u30c3\u30ab\u30bd\u30f3\u304b\u3089\u4f55\u6545\u304bHuggingface\u306epro\u30d7\u30e9\u30f3\u304c\u7d9a\u3044\u3066\u308b\u3093\u3067\u3059\u3088\u306d... \u591a\u5206\u30cf\u30c3\u30ab\u30bd\u30f3\u671f\u9593\u3060\u3051\u306e\u306f\u305a\u306a\u3093\u3060\u3051\u3069\u3001\u5916\u308c\u306a\u3044\u306e\u3088\u306d\u3002 \u307e\u3042\u3001\u30af\u30ec\u30ab\u3068\u304b\u306f\u767b\u9332\u3057\u3066\u306a\u3044\u304b\u3089\u5927\u4e08\u592b\u3060\u3068\u306f\u601d\u3046\u3051\u3069 See translation", "url": "https://huggingface.co/posts/Holy-fox/916850799845292", "date_published": "2025-11-30T05:24:23.964508"}, {"id": "https://huggingface.co/posts/abidlabs/941146046599374", "image": "", "title": "Why I think local, open-source models will eventually win.", "content_text": "Why I think local, open-source models will eventually win. The most useful AI applications are moving toward multi-turn agentic behavior: systems that take hundreds or even thousands of iterative steps to complete a task, e.g. Claude Code, computer-control agents that click, type, and test repeatedly. In these cases, the power of the model is not how smart it is per token, but in how quickly it can interact with its environment and tools across many steps. In that regime, model quality becomes secondary to latency. An open-source model that can call tools quickly, check that the right thing was clicked, or verify that a code change actually passes tests can easily outperform a slightly \u201csmarter\u201d closed model that has to make remote API calls for every move. Eventually, the balance tips: it becomes impractical for an agent to rely on remote inference for every micro-action. Just as no one would tolerate a keyboard that required a network request per keystroke, users won\u2019t accept...", "url": "https://huggingface.co/posts/abidlabs/941146046599374", "date_published": "2025-11-30T05:24:23.964997"}, {"id": "https://huggingface.co/posts/IniNLP247/427756844233002", "image": "", "title": "Made a multimodal mental health system using Llama-3.2-3B, Kokoro TTS, and DeepFace FER! Currently exploring the use of APIs for better quality given my lack of resources for a more thorough Llama 3 fine-tune \ud83e\udd19\ud83c\udffb", "content_text": "Made a multimodal mental health system using Llama-3.2-3B, Kokoro TTS, and DeepFace FER! Currently exploring the use of APIs for better quality given my lack of resources for a more thorough Llama 3 fine-tune \ud83e\udd19\ud83c\udffb IniNLP247/Kenko See translation", "url": "https://huggingface.co/posts/IniNLP247/427756844233002", "date_published": "2025-11-30T05:24:23.965243"}, {"id": "https://huggingface.co/posts/ronantakizawa/608886392582341", "image": "", "title": "Reached 2500+ total downloads across my models and datasets! \ud83c\udf89", "content_text": "Reached 2500+ total downloads across my models and datasets! \ud83c\udf89 Follow me for more @ ronantakizawa See translation", "url": "https://huggingface.co/posts/ronantakizawa/608886392582341", "date_published": "2025-11-30T05:24:23.965443"}, {"id": "https://huggingface.co/posts/onekq/703993717643284", "image": "", "title": "Ilya's interview has been widely cited. I won't address meta points but share 2 cents on two mundane issues.", "content_text": "Ilya's interview has been widely cited. I won't address meta points but share 2 cents on two mundane issues. I will start with the leaderboard phenomena. This is a feature, not bug. Model training is a project under founder mode. But still like all projects, it needs north stars. And you guess right, (famous) leaderboards are the north stars. For those startups which found PMFs, many maintain their own proprietary leaderboards/benchmarks condensed from user traffic. The path is blocked on both directions: startups won's share their moats, model makers won't prioritize either. So instead of complaining, we should celebrate that our prompts work (most of the time) See translation", "url": "https://huggingface.co/posts/onekq/703993717643284", "date_published": "2025-11-30T05:24:23.965758"}, {"id": "https://huggingface.co/posts/kanaria007/247583181528224", "image": "", "title": "\u2705 New Article: *Structured Intelligence Computers \u2014 A Stack That Fixes Its Own Bottlenecks*", "content_text": "\u2705 New Article: *Structured Intelligence Computers \u2014 A Stack That Fixes Its Own Bottlenecks* Title: \ud83e\udde0\u27a1\ufe0f\ud83d\udda5\ufe0f Structured Intelligence Computers: A Stack That Fixes Its Own Bottlenecks \ud83d\udd17 https://huggingface.co/blog/kanaria007/sic-fixes-its-own-bottlenecks --- Summary: Traditional supercomputers scale FLOPS, bandwidth, and memory. Structured Intelligence Computers (SICs) scale something else entirely: *meaning throughput*. This article gives a stack-level tour of SIC \u2014 from SI-Core and SI-NOS to SIL, GSPUs, Semantic Compression, and SCP \u2014 and shows how *each layer is designed to absorb a specific bottleneck* in the layer beneath it. > Supercomputers calculate faster. > *SICs think with structure.* --- Why It Matters: * Moves from \u201cbits over wires\u201d to *semantic graph packets with causal & ethical context* * Treats *rollback, ethics, and determinism* as kernel concerns, not app glue * Bridges *specs + PoCs* (SI-Core, SI-NOS, SIL, GSPU, SCP) into one coherent paradigm * Gives AI/infra teams a...", "url": "https://huggingface.co/posts/kanaria007/247583181528224", "date_published": "2025-11-30T05:24:23.966365"}, {"id": "https://huggingface.co/posts/InezCornell/623914128392808", "image": "", "title": "Free Radar Dataset to for System Integrators and AI/ML Innovators", "content_text": "Free Radar Dataset to for System Integrators and AI/ML Innovators Plextek, a leading UK-based electronics design consultancy, is supporting AI/ML and System Integration companies who want to experiment with radar but lack access to quality datasets. This free radar dataset service is designed to help businesses explore and expand their offerings by incorporating radar as a data type into their solutions. Small scale start-ups - to medium-sized companies eager to explore radar-enabled capabilities often find that they are held back by the initial hardware purchase cost, steep learning curve and the limited availability of realistic, accessible radar data. Plextek\u2019s initiative directly addresses this gap by providing: Curated, real-world radar data for non-commercial experimentation Expert guidance to help teams interpret and work with radar data A potential path to future collaboration on radar-enabled projects \u201cWhether you're training algorithms, validating models, or responding to...", "url": "https://huggingface.co/posts/InezCornell/623914128392808", "date_published": "2025-11-30T05:24:23.966924"}]}